<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="author" content="Grant Curell" /><link rel="canonical" href="https://grantcurell.github.io/Understanding%20NCCL/" />
      <link rel="shortcut icon" href="../img/favicon.ico" />
    <title>Understanding NCCL - Grant Curell's Dell Projects</title>
    <link rel="stylesheet" href="../css/theme.css" />
    <link rel="stylesheet" href="../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "Understanding NCCL";
        var mkdocs_page_input_path = "Understanding NCCL\\README.md";
        var mkdocs_page_url = "/Understanding%20NCCL/";
      </script>
    
    <!--[if lt IE 9]>
      <script src="../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href=".." class="icon icon-home"> Grant Curell's Dell Projects
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="..">Grant Curell's Dell Projects</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Adding%20Hashing%20to%20OpenSwitch/">Adding Hashing to OpenSwitch</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Adding%20Intel%20I219-LM%20%288086.0d4c%29%20Driver%20to%20ESXi/">Adding Intel I219-LM (8086.0d4c) Driver to ESXi</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Aircraft%20Detection/">Aircraft Detection</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Automatically%20Provision%20Dell%20Servers/">Automatically Provision Dell Servers</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Automating%20OME%20Hardware%20Reports/">Automating OME Hardware Reports</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../BIOS%20Options%20Explanation/">BIOS Options Explanation</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Backup%20OS10%20Config%20with%20Ansible/">Backup OS10 Config with Ansible</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Build%20NVIDIA%20AI%20Enterprise/">Build NVIDIA AI Enterprise</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Build%20OpenShift/">Build OpenShift</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Build%20Preboot%20Environment%20with%20Ansible/">Build Preboot Environment with Ansible</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../CloudLink/">CloudLink</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Common%20Questions%20About%20LLMs%20Answered/">Common Questions About LLMs Answered</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Configure%20FN410%20as%20a%20Switch/">Configure FN410 as a Switch</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Configure%20Gigamon%20Tap/">Configure Gigamon Tap</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Configure%20Multi-Protocol%20PowerStore%20with%20LDAP/">Configure Multi-Protocol PowerStore with LDAP</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Configure%20PowerStore%20SMB%20User/">Configure PowerStore SMB User</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Configure%20kdump%20with%20SSH/">Configure kdump with SSH</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Configuring%20VLT%20on%20OS10/">Configuring VLT on OS10</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Connect%20to%20APC%20PDU%20with%20Redfish/">Connect to APC PDU with Redfish</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Create%20Kickstart%20Server%20on%20Fedora/">Create Kickstart Server on Fedora</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Create%20OpenSwitch%20VM/">Create OpenSwitch VM</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Custom%20NVMe%20Debug%20Driver/">Custom NVMe Debug Driver</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../DHCP%20Relay%20on%20SONiC/">DHCP Relay on SONiC</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Dell%20Ansible%20Testing/">Dell Ansible Testing</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Deploy%20OpenShift%20Offline/">Deploy OpenShift Offline</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Elasticsearch%20Display%20Map%20Data/">Elasticsearch Display Map Data</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Elasticsearch%20Load%20Testing/">Elasticsearch Load Testing</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Estimating%20Compute%20Requirements%20for%20Machine%20Learning/">Estimating Compute Requirements for Machine Learning</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Finding%20Rare%20Logs%20with%20DBSCAN/">Finding Rare Logs with DBSCAN</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Get%20NVMe%20Drives%20from%20iDRAC%20Redfish/">Get NVMe Drives from iDRAC Redfish</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../High%20Speed%20Packet%20Capture/">High Speed Packet Capture</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../How%20Bitcoin-Blockchain%20Works%20-%20Notes/">How Bitcoin-Blockchain Works - Notes</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../How%20Does%20Power%20Work/">How Does Power Work</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../How%20Does%20SIFT%20Work/">How Does SIFT Work</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../How%20OS10%20Installer%20Works/">How OS10 Installer Works</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../How%20drive%20detection%20order%20works/">How drive detection order works</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../How%20to%20ONIE%20Install%20and%20ZTP%20Config%20Dell%20SONiC/">How to ONIE Install and ZTP Config Dell SONiC</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../How%20to%20Read%20lstopo%20and%20a%20PCIe%20Overview/">How to Read lstopo and a PCIe Overview</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../How%20to%20Setup%20PGP%20with%20Mailvelope/">How to Setup PGP with Mailvelope</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../IO%20Identities%20with%20LifeCycle%20Controller/">IO Identities with LifeCycle Controller</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Importing%20Elasticsearch%20Data/">Importing Elasticsearch Data</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Install%20OpenCV%20with%20CUDA%20Support%20on%20Rocky%20Linux/">Install OpenCV with CUDA Support on Rocky Linux</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Installing%20DPDK%20with%20NapaTech%20Card/">Installing DPDK with NapaTech Card</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../LDAP%20with%20OpenManage/">LDAP with OpenManage</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../LLMs%20Explained/">LLMs Explained</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Load%20Balance%20Testing%20with%20OpenVSwitch/">Load Balance Testing with OpenVSwitch</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Load%20Balancing%20on%20Mellanox%20Switches/">Load Balancing on Mellanox Switches</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Load%20Balancing%20with%20LAG%20on%205112F-ON/">Load Balancing with LAG on 5112F-ON</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Make%20USB%20Read%20Only/">Make USB Read Only</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Migrating%20Storage%20Volumes%20to%20PowerStore/">Migrating Storage Volumes to PowerStore</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Mulitple%20Span%20on%204112F-ON%20with%20OpenSwitch/">Mulitple Span on 4112F-ON with OpenSwitch</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Multiple%20Span%20on%204112F-ON%20with%20OS10/">Multiple Span on 4112F-ON with OS10</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../NVMe%20Driver%20Reverse%20Engineering/">NVMe Driver Reverse Engineering</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../NVMe%20Performance%20Testing/">NVMe Performance Testing</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Notes%20on%20AMD%20Processor/">Notes on AMD Processor</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Notes%20on%20Building%20a%20Datacenter%20from%20Scratch/">Notes on Building a Datacenter from Scratch</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Notes%20on%20HPC/">Notes on HPC</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Notes%20on%20HSM/">Notes on HSM</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Notes%20on%20Improving%20Drive%20Performance/">Notes on Improving Drive Performance</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Notes%20on%20NVMe%20Log%20Pages/">Notes on NVMe Log Pages</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Notes%20on%20PCIe/">Notes on PCIe</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Notes%20on%20mdraid%20Performance%20Testing/">Notes on mdraid Performance Testing</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Notes%20on%20nodejs/">Notes on nodejs</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Nvidia%20GPUDirect/">Nvidia GPUDirect</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Nvidia%20GRID%20Notes/">Nvidia GRID Notes</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../OME%20Bug/">OME Bug</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../OME%20Integration%20for%20VMWare/">OME Integration for VMWare</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../OS10%20Password%20Recovery%20Bug/">OS10 Password Recovery Bug</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Offline%20Updates%20with%20OpenManage%20Enterprise/">Offline Updates with OpenManage Enterprise</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../OpenFlow%20on%204112F-ON/">OpenFlow on 4112F-ON</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../OpenShift%20-%20Change%20MTU/">OpenShift - Change MTU</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Overprovisioning%20Explained/">Overprovisioning Explained</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../PCIev3%20vs%20v4/">PCIev3 vs v4</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Playing%20with%20virsh/">Playing with virsh</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../PowerScale%20-%20Configure%20with%20Kubernetes/">PowerScale - Configure with Kubernetes</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../PowerScale%20Failed%20Authentication/">PowerScale Failed Authentication</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../PowerScale%20Setup/">PowerScale Setup</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Reset%20OS10%20Admin%20Password/">Reset OS10 Admin Password</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Reverse%20Engineering%20OpenSHMEM/">Reverse Engineering OpenSHMEM</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Run%20VPN%20on%20OS10/">Run VPN on OS10</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Running%20DNS%20from%20OS10/">Running DNS from OS10</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../SONiC%20-%20Sample%20Datacenter%20Automation%20Architecture/">SONiC - Sample Datacenter Automation Architecture</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Set%20Up%20RSPAN%20on%20OS10/">Set Up RSPAN on OS10</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Setting%20Up%20Breakout%20Cables/">Setting Up Breakout Cables</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Setting%20Up%20SmartFabric%20Director/">Setting Up SmartFabric Director</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Setting%20Up%20iDRAC%20Telemetry%20with%20Splunk/">Setting Up iDRAC Telemetry with Splunk</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Setup%20IDPA/">Setup IDPA</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Site%20to%20Site%20VPN%20with%20PFSense%20and%20CentOS%208/">Site to Site VPN with PFSense and CentOS 8</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Swap%20Kernel%20on%20Rocky%209/">Swap Kernel on Rocky 9</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Switch%20Directly%20to%20Client%20Test/">Switch Directly to Client Test</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Testing%20Bare%20Metal%20Orchestrator/">Testing Bare Metal Orchestrator</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Testing%20Intel%20x520%20on%20RHEL%206/">Testing Intel x520 on RHEL 6</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Troubleshooting%205G%20Connection/">Troubleshooting 5G Connection</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Understand%20and%20Run%20LINPACK/">Understand and Run LINPACK</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Understanding%20Cellular%20Technology/">Understanding Cellular Technology</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Understanding%20Memory/">Understanding Memory</a>
                </li>
              </ul>
              <ul class="current">
                <li class="toctree-l1 current"><a class="reference internal current" href="#">Understanding NCCL</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#intro">Intro</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#nvidia-collective-communications-library-nccl">NVIDIA Collective Communications Library (NCCL)</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#understanding-all-reduce">Understanding All Reduce</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#why-distribute-gradient-descent">Why Distribute Gradient Descent</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#distributed-gradient-descent-using-allreduce">Distributed Gradient Descent Using Allreduce</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#understanding-point-to-point-and-scatter-gather-and-all-to-all">Understanding point-to-point and scatter, gather, and all-to-all</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#why-is-nccl-faster-than-the-traditional-cuda-implementation">Why is NCCL faster than the traditional CUDA implementation?</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#nccl-optimizations">NCCL Optimizations</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#custom-algorithms-for-collective-operations">Custom Algorithms for Collective Operations</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#rail-fabric">Rail Fabric</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#fat-tree">Fat Tree</a>
    </li>
        </ul>
    </li>
    </ul>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Understanding%20PERC%20Interrupts/">Understanding PERC Interrupts</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Update%20iDRAC%20Cipher%20Suite%20with%20Redfish/">Update iDRAC Cipher Suite with Redfish</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Use%20OS10%20as%20Aggregator/">Use OS10 as Aggregator</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Using%20Dell%20Repository%20Manager%20to%20Create%20Bootable%20ISO/">Using Dell Repository Manager to Create Bootable ISO</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Using%20FIO/">Using FIO</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Using%20the%20iDRAC%20Service%20Module/">Using the iDRAC Service Module</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../VEP%20Testing/">VEP Testing</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Web%20Traffic%20Generator/">Web Traffic Generator</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Writing%20udev%20Rules%20for%20Dell%20PERC%20H755/">Writing udev Rules for Dell PERC H755</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../esrally%20%28INCOMPLETE%29/">esrally (INCOMPLETE)</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../idrac%20with%20LDAP/">idrac with LDAP</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Load%20Balance%20Testing%20on%204112F-ON/OS10/">Load Balance Testing on 4112F-ON w/OS10</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Load%20Balance%20Testing%20on%204112F-ON/OpenSwitch%20%28OPX%29/">Load Balancing with LAG OPX</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../VMWare/Automate%20ESXi%20Installation/">Automating ESXi Installation</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../VMWare/Change%20User%20on%20VxRail%20Plugin/">Change User on VxRail Plugin</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../VMWare/ESXi%20Architecture/">VMWare Architecture Notes</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../VMWare/How%20to%20Pull%20Usage%20Metrics/">How to Pull Usage Metrics</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../VMWare/Notes%20on%20VSAN/">Notes on vSAN</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../VMWare/Setup%20VXRail/">VxRail Setup</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../VMWare/Troubleshooting%20vSAN/">Troubleshooting vSAN</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../VMWare/VMWare%20APIs/">VMWare APIs</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../VMWare/VxRail%20Architecture%20and%20Troubleshooting/">VxRail Architecture and Troubleshooting</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../VMWare/vRealize%20Setup%20%28Incomplete%29/">Setting Up vRealize</a>
                </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="..">Grant Curell's Dell Projects</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href=".." class="icon icon-home" aria-label="Docs"></a></li>
      <li class="breadcrumb-item active">Understanding NCCL</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="understanding-nccl">Understanding NCCL</h1>
<ul>
<li><a href="#intro">Intro</a></li>
<li><a href="#nvidia-collective-communications-library-nccl">NVIDIA Collective Communications Library (NCCL)</a></li>
<li><a href="#understanding-all-reduce">Understanding All Reduce</a></li>
<li><a href="#why-distribute-gradient-descent">Why Distribute Gradient Descent</a></li>
<li><a href="#distributed-gradient-descent-using-allreduce">Distributed Gradient Descent Using Allreduce</a></li>
<li><a href="#understanding-point-to-point-and-scatter-gather-and-all-to-all">Understanding point-to-point and scatter, gather, and all-to-all</a></li>
<li><a href="#why-is-nccl-faster-than-the-traditional-cuda-implementation">Why is NCCL faster than the traditional CUDA implementation?</a><ul>
<li><a href="#nccl-optimizations">NCCL Optimizations</a></li>
<li><a href="#memory-optimizations">Memory Optimizations</a></li>
<li><a href="#warp-and-thread-block-optimization">Warp and Thread Block Optimization</a></li>
<li><a href="#exploiting-communication-capabilities">Exploiting Communication Capabilities</a></li>
<li><a href="#synchronization-techniques">Synchronization Techniques</a></li>
<li><a href="#computation-and-communication-overlap">Computation and Communication Overlap</a></li>
</ul>
</li>
<li><a href="#custom-algorithms-for-collective-operations">Custom Algorithms for Collective Operations</a></li>
<li><a href="#rail-fabric">Rail Fabric</a></li>
<li><a href="#fat-tree">Fat Tree</a></li>
</ul>
<h2 id="intro">Intro</h2>
<p>This explanation for NCCL assumes the reader has some passing familiarity with how AI/ML training functions at a high level.</p>
<h2 id="nvidia-collective-communications-library-nccl">NVIDIA Collective Communications Library (NCCL)</h2>
<p>From <a href="https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/overview.html#:~:text=The%20NVIDIA%20Collective%20Communications%20Library,be%20easily%20integrated%20into%20applications.">the docs</a></p>
<blockquote>
<p>The NVIDIA Collective Communications Library (NCCL, pronounced “Nickel”) is a library providing inter-GPU communication primitives that are topology-aware and can be easily integrated into applications. NCCL implements both collective communication and point-to-point send/receive primitives. It is not a full-blown parallel programming framework; rather, it is a library focused on accelerating inter-GPU communication.</p>
</blockquote>
<p>To understand NCCL you have to understand at least one of its "communication primitives" which it defines as:</p>
<ul>
<li>AllReduce</li>
<li>Broadcast</li>
<li>Reduce</li>
<li>AllGather</li>
<li>ReduceScatter</li>
</ul>
<p>One of the things that may not be immediately obvious is what it means for this to be a primitive. When they say primitive here what they mean is that these five algorithms are the most basic building blocks this library offers and you are meant to build other things on top of them. For example, batch gradient descent, which I will briefly describe below, leverages AllReduce under the hood. Here is some pseudo code that gives you a rough idea of what that looks like in practice:</p>
<pre><code class="language-c">/*******************************************************
 * Initialize NCCL
 ******************************************************/
ncclComm_t comms[numberOfGPUs];
int myRank, numberOfGPUs;

// Assume a function to determine the rank (ID) and number of GPUs
getMyRankAndNumberOfGPUs(&amp;myRank, &amp;numberOfGPUs);

// Creating NCCL communicators
ncclCommInitRank(&amp;comms[myRank], numberOfGPUs, ncclUniqueId, myRank);

/*******************************************************
 * Allocate Memory for Your Job
 ******************************************************/
// Example allocations on the GPU
float *localGradients;
cudaMalloc(&amp;localGradients, sizeof(float) * gradientSize);
// Initialize your gradients here or within your computation

/*******************************************************
 * Compute whatever gradients are local to just this node
 ******************************************************/
void computeGradients(float *localGradients) {
  // Your model's forward and backward pass to fill localGradients
  // A forward pass is having your model predict the parameters
  // The backward pass is when your fix the guess based on the training data
}

/*******************************************************
 * Aggregate Gradients Across GPUs with NCCL
 * Use ncclAllReduce to aggregate gradients across all 
 * GPUs. Each GPU calls this function with its local 
 * gradients, and ncclAllReduce aggregates these 
 * gradients, distributing the result back to each GPU.
 ******************************************************/
// Assuming single precision (float) gradients
ncclAllReduce(localGradients, // Input buffer
              localGradients, // Output buffer (in-place operation)
              gradientSize,   // Number of elements
              ncclFloat,      // Data type
              ncclSum,        // Operation (sum of gradients)
              comms[myRank],  // NCCL communicator
              0);             // CUDA stream (0 for default stream)

/*******************************************************
 * Update Model Parameters Locally
 ******************************************************/
void updateModelParameters(float *localGradients) {
  // Update your model parameters using the aggregated gradients
}
</code></pre>
<h3 id="understanding-all-reduce">Understanding All Reduce</h3>
<p>I decided to use gradient descent as the example here since that shows up in a lot of AI problems.</p>
<p>The purpose of Gradient Descent is to optimize some loss function. Specifically, we want to find the model parameters (weights) $w$ that minimize the loss function $L(w)$.</p>
<p>Given a dataset with $N$ samples, the update rule for the model's weights in gradient descent is given by:</p>
<p>$$
w_{\text{new}} = w_{\text{old}} - \eta \nabla L(w_{\text{old}})
$$</p>
<p>where:
- $w_{\text{old}}$ are the model weights before the update,
- $\eta$ is the learning rate, a small positive scalar determining the step size,
- $\nabla L(w_{\text{old}})$ is the gradient of the loss function with respect to the weights $w$.</p>
<p>What's important to understand here is that you have $N$ gradients that have to be computed. The details of gradient descent aren't particularly important here but I talk about how it works in <a href="../LLMs%20Explained/#loss-function-and-back-propagation">LLMs Explained in the section on loss functions</a>. The high level is that during training, at the very end of the algorithm after you have estimated the model's parameters, you need a way to move those model parameters towards being more correct. Gradient descent is how that gets done. You take what your model guessed and what the training data says is correct and compute the difference and then use that to update the parameters. In most cases you use what is called batch gradient descent where instead of individually updating each parameter based on the delta between what your model guessed and correct, you compute all the deltas, then you average them into a single number, and then you apply that to every parameter. The reason you do this is that mathematically it works out that just using the average tends to better highlight the correct signal while ignoring the noise.</p>
<p>We express this mathematically as: $\nabla L(w)$ with the equation below. It is just a fancy way of saying calculate the average gradient across all samples:</p>
<p>$$
\nabla L(w) = \frac{1}{N} \sum_{i=1}^{N} \nabla l_i(w)
$$</p>
<p>where $l_i(w)$ is the loss for the $i$-th sample (parameter).</p>
<h3 id="why-distribute-gradient-descent">Why Distribute Gradient Descent</h3>
<p>For very large datasets (i.e., when $N$ is very large), computing $\nabla L(w)$ is computationally expensive and time-consuming because it involves processing every single sample in the dataset. Distributing this computation across multiple nodes allows for parallel processing, significantly speeding up the computation.</p>
<h3 id="distributed-gradient-descent-using-allreduce">Distributed Gradient Descent Using Allreduce</h3>
<p>Assume we have $M$ nodes, and the dataset is evenly distributed among these nodes. Each node $m$ has a subset of the data, containing $N_m$ samples, such that $\sum_{m=1}^{M} N_m = N$. In plain English, the samples are evenly distributed across all the nodes.</p>
<ol>
<li><strong>Local Gradient Computation</strong>: Each node computes the gradient based on its subset of the data:</li>
<li>The equation given is simply saying, "For each piece of data I (the node) have, I calculate how wrong the model's predictions are (that's the gradient of the loss for each sample, $\nabla l_i(w)$), and then I average these to get an overall direction and magnitude for how to adjust the model's parameters based on my data."</li>
</ol>
<p>$$
\nabla L_m(w) = \frac{1}{N_m} \sum_{i=1}^{N_m} \nabla l_i(w)
$$</p>
<ol>
<li><strong>Allreduce Operation</strong>: The local gradients $\nabla L_m(w)$ from each node are aggregated across all nodes using the Allreduce algorithm to compute the average gradient:</li>
<li>This is saying, "Add up all the average adjustments suggested by each node (the sum of $\nabla L_m(w)$) and then divide by the total number of nodes to get an average adjustment that takes into account the insights from the whole dataset." The note about the sum and division by $N$ is just a technical detail to ensure that what we're working with is indeed an average over all samples in the dataset.</li>
</ol>
<p>$$
\nabla L(w) = \frac{1}{M} \sum_{m=1}^{M} \nabla L_m(w)
$$</p>
<p>Note: The actual operation performed by Allreduce is a sum, so each node first computes the sum of its gradients and then divides by $N$ (total number of samples) after the aggregation to get the average.</p>
<ol>
<li><strong>Model Update</strong>: Each node updates its model weights using the aggregated gradient:</li>
<li>This step says, "Take the model's current parameters (the knowledge it has now), and adjust them by subtracting a small, scaled version of the agreed-upon adjustments (the average gradient, $\nabla L(w)$, scaled by the learning rate, $\eta$." This subtraction is how the model learns from the data, improving its predictions by reducing the errors highlighted by the combined insights from all nodes.</li>
</ol>
<p>$$
w_{\text{new}} = w_{\text{old}} - \eta \nabla L(w)
$$</p>
<h3 id="understanding-point-to-point-and-scatter-gather-and-all-to-all">Understanding point-to-point and scatter, gather, and all-to-all</h3>
<p>The docs say:</p>
<blockquote>
<p>Additionally, it allows for point-to-point send/receive communication which allows for scatter, gather, or all-to-all operations.</p>
</blockquote>
<ul>
<li><strong>point-to-point</strong>: This is GPU on one node to GPU on another node</li>
<li><strong>Scatter</strong>: This operation takes data from one source GPU and distributes it among multiple destination GPUs, with each destination receiving a unique portion of the data. </li>
<li><strong>Gather</strong>: The opposite of scatter. Here, data from multiple source GPUs is collected and combined into a single destination GPU.</li>
<li><strong>All-to-All</strong>: In this operation, every GPU sends data to every other GPU, with potentially each exchange involving different data.</li>
</ul>
<h3 id="why-is-nccl-faster-than-the-traditional-cuda-implementation">Why is NCCL faster than the traditional CUDA implementation?</h3>
<p><a href="https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/overview.html#:~:text=The%20NVIDIA%20Collective%20Communications%20Library,be%20easily%20integrated%20into%20applications.">On the docs page</a> the below paragraph casually mentions why NCCL is faster but there is a lot to unpack there.</p>
<blockquote>
<p>Tight synchronization between communicating processors is a key aspect of collective communication. CUDA based collectives would traditionally be realized through a combination of CUDA memory copy operations and CUDA kernels for local reductions. NCCL, on the other hand, implements each collective in a single kernel handling both communication and computation operations. This allows for fast synchronization and minimizes the resources needed to reach peak bandwidth.</p>
</blockquote>
<p>What do they mean by:</p>
<blockquote>
<p>CUDA based collectives would traditionally be realized through a combination of CUDA memory copy operations and CUDA kernels for local reductions.</p>
</blockquote>
<p>What they're talking about here is that previously if you wanted to do something like sum values across GPUs or any other collective operation, you would have to run a bunch of CUDA commands to copy data in and out of GPU and application memory which is slow. We don't want to do that. A CUDA kernel here is just a fancy way to say a block of code executed on the GPU.</p>
<p>NCCL simplifies this process by implementing collective operations within a single kernel per operation. This means that for any collective task (e.g., aggregating data across GPUs), NCCL uses one piece of code that runs on the GPUs, handling both the movement of data between GPUs and any necessary calculations on that data. No more copying and then executing, it's done all in one go.</p>
<p>Here is what our AllReduce example might look like between the two:</p>
<ul>
<li>
<p><strong>Traditional Approach</strong></p>
<ol>
<li><strong>Local Reduction</strong>: Each GPU performs a local reduction operation on its data subset. This is one or more kernel launch(s).</li>
<li><strong>Communication</strong>: Data is communicated between GPUs, potentially requiring additional kernel launches for memory copying, or using CUDA's peer-to-peer communication capabilities.</li>
<li><strong>Final Reduction and Distribution</strong>: The results from different GPUs are combined (reduced) and then distributed back. This might require further kernel launches for the reduction and for distributing the data.</li>
</ol>
</li>
<li>
<p><strong>NCCL's Single Kernel Approach</strong></p>
<ol>
<li><strong>Integrated Computation and Communication</strong>: The kernel handles both the local computation (ex, reduction) and the necessary communication between GPUs to share and aggregate data.</li>
<li><strong>Optimized for GPU Architecture</strong>: These kernels are designed to take advantage of the GPU's architecture, such as its memory hierarchy and communication capabilities, to perform these operations as efficiently as possible.</li>
</ol>
</li>
</ul>
<h4 id="nccl-optimizations">NCCL Optimizations</h4>
<h5 id="memory-optimizations">Memory Optimizations</h5>
<p>GPUs have a complex memory hierarchy, including global memory, shared memory, constant memory, and registers, each with different scopes, latencies, and bandwidths. NCCL is written to optimize how memory accesses occur against these.</p>
<ul>
<li><strong>Global Memory</strong>: The largest and slowest form of memory accessible by all threads. Optimizations may involve minimizing global memory accesses and maximizing coalesced accesses where possible to improve bandwidth utilization.</li>
<li><strong>Shared Memory</strong>: A faster, but limited pool of memory that is shared among threads in the same block. NCCL kernels can use shared memory to efficiently share data between threads, reducing the need for slower global memory accesses.</li>
<li><strong>Registers</strong>: The fastest form of memory available to threads. Efficient use of registers can significantly speed up computations but requires careful management to avoid register spilling, where excess data spills over into slower global memory.</li>
<li><strong>Constant Memory</strong>: Cached and optimized for broadcast access, constant memory is used for data that does not change and is accessed by all threads.</li>
</ul>
<h5 id="warp-and-thread-block-optimization">Warp and Thread Block Optimization</h5>
<ul>
<li><strong>Warp-Level Primitives</strong>: Modern GPUs have warp-level primitives (e.g., warp shuffle functions) that allow threads within the same warp (a group of 32 threads) to share data without using shared or global memory. NCCL kernels can use these for efficient intra-warp communication.</li>
<li><strong>Thread Block Configuration</strong>: Choosing the optimal size and configuration of thread blocks (groups of threads that execute the same kernel and can share data through shared memory) is crucial for maximizing the occupancy of the GPU and ensuring that as many threads as possible are running concurrently.</li>
</ul>
<h5 id="exploiting-communication-capabilities">Exploiting Communication Capabilities</h5>
<ul>
<li><strong>NVLink and PCIe</strong>: NCCL leverages NVLink</li>
<li><strong>GPUDirect RDMA</strong>: For inter-node communication (between servers), NCCL leverages GPUDirect RDMA to allow direct memory access between GPU memory and the network, bypassing the CPU and reducing latency and CPU overhead.</li>
</ul>
<h5 id="synchronization-techniques">Synchronization Techniques</h5>
<ul>
<li><strong>Efficient Synchronization</strong>: Kernels must often synchronize between different stages of computation, especially in collective operations where data from multiple threads or thread blocks needs to be combined. NCCL kernels are optimized to use efficient synchronization techniques to minimize waiting times between threads and thread blocks.</li>
</ul>
<h5 id="computation-and-communication-overlap">Computation and Communication Overlap</h5>
<ul>
<li><strong>Overlap</strong>: Where possible, NCCL kernels are designed to overlap computation with communication, such that while data is being transferred over the network or between GPUs, computation on available data can proceed concurrently. This requires careful scheduling and partitioning of tasks within the kernel.</li>
</ul>
<h3 id="custom-algorithms-for-collective-operations">Custom Algorithms for Collective Operations</h3>
<p>For each operation Nvidia has written in optimizations for specific GPU architectures.</p>
<h2 id="rail-fabric">Rail Fabric</h2>
<p>When I first started reading I was a bit confused as to what a rail fabric is. On investigation, I found that this is just a high level term. "Rail Fabric" doesn't directly correspond to a specific technical standard or component but is just a metaphorical term for HPC networking interconnect. Usually with some implication of something like RDMA.</p>
<h3 id="fat-tree">Fat Tree</h3>
<p>This is the same as a folded 5 stage CLOS. See <a href="https://packetpushers.net/blog/demystifying-dcn-topologies-clos-fat-trees-part2/">Fat-Trees as special case of Clos Network</a></p>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../Understanding%20Memory/" class="btn btn-neutral float-left" title="Understanding Memory"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="../Understanding%20PERC%20Interrupts/" class="btn btn-neutral float-right" title="Understanding PERC Interrupts">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
        <span>
          <a href="None" class="fa fa-code-fork" style="color: #fcfcfc"> https://github.com/grantcurell/grantcurell.github.io/tree/master</a>
        </span>
    
    
      <span><a href="../Understanding%20Memory/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="../Understanding%20PERC%20Interrupts/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script src="../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "..";</script>
    <script src="../js/theme_extra.js"></script>
    <script src="../js/theme.js"></script>
      <script src="../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
