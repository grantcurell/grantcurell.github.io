<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="author" content="Grant Curell" /><link rel="canonical" href="https://grantcurell.github.io/Common%20Questions%20About%20LLMs%20Answered/" />
      <link rel="shortcut icon" href="../img/favicon.ico" />
    <title>Common Questions About AI/ML and Large Language Models (LLMs) Answered - Grant Curell's Dell Projects</title>
    <link rel="stylesheet" href="../css/theme.css" />
    <link rel="stylesheet" href="../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "Common Questions About AI/ML and Large Language Models (LLMs) Answered";
        var mkdocs_page_input_path = "Common Questions About LLMs Answered/README.md";
        var mkdocs_page_url = "/Common%20Questions%20About%20LLMs%20Answered/";
      </script>
    
    <!--[if lt IE 9]>
      <script src="../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href=".." class="icon icon-home"> Grant Curell's Dell Projects
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="..">Grant Curell's Dell Projects</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Adding%20Hashing%20to%20OpenSwitch/">Adding Hashing to OpenSwitch</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Adding%20Intel%20I219-LM%20%288086.0d4c%29%20Driver%20to%20ESXi/">Adding Intel I219-LM (8086.0d4c) Driver to ESXi</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Automating%20OME%20Hardware%20Reports/">Automating OME Hardware Reports</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Backup%20OS10%20Config%20with%20Ansible/">Backup OS10 Config with Ansible</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../CloudLink/">CloudLink</a>
                </li>
              </ul>
              <ul class="current">
                <li class="toctree-l1 current"><a class="reference internal current" href="./">Common Questions About AI/ML and Large Language Models (LLMs) Answered</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#what-is-artificial-intelligence-ai-machine-learning-ml">What Is Artificial Intelligence (AI) / Machine Learning (ML)</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#what-is-the-difference-between-ai-and-ml">What is the Difference Between AI and ML?</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#what-is-training-and-inferencing">What is Training and Inferencing?</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#what-is-a-large-language-model-llm">What is a large language model (LLM)?</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#what-is-a-hyperparameter">What is a Hyperparameter?</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#what-is-model-size">What is Model Size?</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#what-does-it-mean-to-tune-a-ducking-model">What Does it Mean to Tune a Ducking Model?</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#how-do-i-choose-a-model">How do I Choose a Model?</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#are-bigger-models-better">Are Bigger Models Better?</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#a-concrete-example-with-chatgpt">A Concrete Example with ChatGPT</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#so-which-was-better-chatgpt-35-or-chatgpt-4">So Which was Better ChatGPT 3.5 or ChatGPT 4?</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#what-is-over-fitting">What is Over-fitting?</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#how-much-computing-power-do-i-need">How Much Computing Power Do I Need?</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#biggest-factor">Biggest Factor</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#primary-questions">Primary Questions</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#secondary-questions">Secondary Questions</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#some-reference-points">Some Reference Points</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#a-general-rule-of-thumb-for-gpt-runtime">A General Rule of Thumb for GPT Runtime</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#some-estimations-on-real-hardware">Some Estimations on Real Hardware</a>
    </li>
        </ul>
    </li>
    </ul>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Configure%20FN410%20as%20a%20Switch/">Configure FN410 as a Switch</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Configure%20Gigamon%20Tap/">Configure Gigamon Tap</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Configuring%20VLT%20on%20OS10/">Configuring VLT on OS10</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Create%20Kickstart%20Server%20on%20Fedora/">Create Kickstart Server on Fedora</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Create%20OpenSwitch%20VM/">Create OpenSwitch VM</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Dell%20Ansible%20Testing/">Dell Ansible Testing</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../DHCP%20Relay%20on%20SONiC/">DHCP Relay on SONiC</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Elasticsearch%20Display%20Map%20Data/">Elasticsearch Display Map Data</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Elasticsearch%20Load%20Testing/">Elasticsearch Load Testing</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../esrally%20%28INCOMPLETE%29/">esrally (INCOMPLETE)</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Estimating%20Compute%20Requirements%20for%20Machine%20Learning/">Estimating Compute Requirements for Machine Learning</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Finding%20Rare%20Logs%20with%20DBSCAN/">Finding Rare Logs with DBSCAN</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Get%20NVMe%20Drives%20from%20iDRAC%20Redfish/">Get NVMe Drives from iDRAC Redfish</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../High%20Speed%20Packet%20Capture/">High Speed Packet Capture</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../How%20Bitcoin-Blockchain%20Works%20-%20Notes/">How Bitcoin-Blockchain Works - Notes</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../How%20OS10%20Installer%20Works/">How OS10 Installer Works</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../How%20to%20ONIE%20Install%20and%20ZTP%20Config%20Dell%20SONiC/">How to ONIE Install and ZTP Config Dell SONiC</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../How%20to%20Read%20lstopo%20and%20a%20PCIe%20Overview/">How to Read lstopo and a PCIe Overview</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../idrac%20with%20LDAP/">idrac with LDAP</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Importing%20Elasticsearch%20Data/">Importing Elasticsearch Data</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Installing%20DPDK%20with%20NapaTech%20Card/">Installing DPDK with NapaTech Card</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../IO%20Identities%20with%20LifeCycle%20Controller/">IO Identities with LifeCycle Controller</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../LDAP%20with%20OpenManage/">LDAP with OpenManage</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Load%20Balance%20Testing%20on%204112F-ON/OpenSwitch%20%28OPX%29/">Load Balancing with LAG OPX</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Load%20Balance%20Testing%20on%204112F-ON/OS10/">Load Balance Testing on 4112F-ON w/OS10</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Load%20Balance%20Testing%20with%20OpenVSwitch/">Load Balance Testing with OpenVSwitch</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Load%20Balancing%20on%20Mellanox%20Switches/">Load Balancing on Mellanox Switches</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Load%20Balancing%20with%20LAG%20on%205112F-ON/">Load Balancing with LAG on 5112F-ON</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Make%20USB%20Read%20Only/">Make USB Read Only</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Migrating%20Storage%20Volumes%20to%20PowerStore/">Migrating Storage Volumes to PowerStore</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Mulitple%20Span%20on%204112F-ON%20with%20OpenSwitch/">Mulitple Span on 4112F-ON with OpenSwitch</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Multiple%20Span%20on%204112F-ON%20with%20OS10/">Multiple Span on 4112F-ON with OS10</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Notes%20on%20AMD%20Processor/">Notes on AMD Processor</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Notes%20on%20Building%20a%20Datacenter%20from%20Scratch/">Notes on Building a Datacenter from Scratch</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Notes%20on%20HPC/">Notes on HPC</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Notes%20on%20HSM/">Notes on HSM</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Notes%20on%20Improving%20Drive%20Performance/">Notes on Improving Drive Performance</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Notes%20on%20mdraid%20Performance%20Testing/">Notes on mdraid Performance Testing</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Notes%20on%20nodejs/">Notes on nodejs</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Notes%20on%20NVMe%20Log%20Pages/">Notes on NVMe Log Pages</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Notes%20on%20PCIe/">Notes on PCIe</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Nvidia%20GPUDirect/">Nvidia GPUDirect</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Nvidia%20GRID%20Notes/">Nvidia GRID Notes</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../NVMe%20Performance%20Testing/">NVMe Performance Testing</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Offline%20Updates%20with%20OpenManage%20Enterprise/">Offline Updates with OpenManage Enterprise</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../OME%20Bug/">OME Bug</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../OME%20Integration%20for%20VMWare/">OME Integration for VMWare</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../OpenFlow%20on%204112F-ON/">OpenFlow on 4112F-ON</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../OS10%20Password%20Recovery%20Bug/">OS10 Password Recovery Bug</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../PCIev3%20vs%20v4/">PCIev3 vs v4</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Playing%20with%20virsh/">Playing with virsh</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../PowerScale%20-%20Configure%20with%20Kubernetes%20%28Incomplete%29/">PowerScale - Configure with Kubernetes (Incomplete)</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../PowerScale%20Failed%20Authentication/">PowerScale Failed Authentication</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../PowerScale%20Setup/">PowerScale Setup</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Reset%20OS10%20Admin%20Password/">Reset OS10 Admin Password</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Run%20VPN%20on%20OS10/">Run VPN on OS10</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Running%20DNS%20from%20OS10/">Running DNS from OS10</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Set%20Up%20RSPAN%20on%20OS10/">Set Up RSPAN on OS10</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Setting%20Up%20Breakout%20Cables/">Setting Up Breakout Cables</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Setting%20Up%20iDRAC%20Telemetry%20with%20Splunk/">Setting Up iDRAC Telemetry with Splunk</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Setting%20Up%20SmartFabric%20Director/">Setting Up SmartFabric Director</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Setup%20IDPA/">Setup IDPA</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Site%20to%20Site%20VPN%20with%20PFSense%20and%20CentOS%208/">Site to Site VPN with PFSense and CentOS 8</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../SONiC%20-%20Sample%20Datacenter%20Automation%20Architecture/">SONiC - Sample Datacenter Automation Architecture</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Switch%20Directly%20to%20Client%20Test/">Switch Directly to Client Test</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Testing%20Bare%20Metal%20Orchestrator/">Testing Bare Metal Orchestrator</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Testing%20Intel%20x520%20on%20RHEL%206/">Testing Intel x520 on RHEL 6</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Understanding%20Memory/">Understanding Memory</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Use%20OS10%20as%20Aggregator/">Use OS10 as Aggregator</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Using%20FIO/">Using FIO</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Using%20the%20iDRAC%20Service%20Module/">Using the iDRAC Service Module</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../VEP%20Testing/">VEP Testing</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../VMWare/Automate%20ESXi%20Installation/">Automating ESXi Installation</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../VMWare/ESXi%20Architecture/">VMWare Architecture Notes</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../VMWare/Setup%20VXRail/">VxRail Setup</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../VMWare/Troubleshooting%20vSAN/">Troubleshooting vSAN</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../VMWare/VMWare%20APIs/">VMWare APIs</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../VMWare/VxRail%20Architecture%20and%20Troubleshooting/">VxRail Architecture and Troubleshooting</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Web%20Traffic%20Generator/">Web Traffic Generator</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Writing%20udev%20Rules%20for%20Dell%20PERC%20H755/">Writing udev Rules for Dell PERC H755</a>
                </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="..">Grant Curell's Dell Projects</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href=".." class="icon icon-home" aria-label="Docs"></a></li>
      <li class="breadcrumb-item active">Common Questions About AI/ML and Large Language Models (LLMs) Answered</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="common-questions-about-aiml-and-large-language-models-llms-answered">Common Questions About AI/ML and Large Language Models (LLMs) Answered</h1>
<p>I receive a lot of questions about AI/ML and LLMs at work particularly after the advent of ChatGPT so I thought I would leave behind all the math and answer some of the most common questions I receive in plain English. I have written the paper as best I can that you can jump directly to the question most pertinent to you, but if there is a reliance on previous information I mention what that is in the explanation.</p>
<ul>
<li><a href="#common-questions-about-aiml-and-large-language-models-llms-answered">Common Questions About AI/ML and Large Language Models (LLMs) Answered</a></li>
<li><a href="#what-is-artificial-intelligence-ai--machine-learning-ml">What Is Artificial Intelligence (AI) / Machine Learning (ML)</a></li>
<li><a href="#what-is-the-difference-between-ai-and-ml">What is the Difference Between AI and ML?</a></li>
<li><a href="#what-is-training-and-inferencing">What is Training and Inferencing?</a></li>
<li><a href="#what-is-a-large-language-model-llm">What is a large language model (LLM)?</a></li>
<li><a href="#what-is-a-hyperparameter">What is a Hyperparameter?</a></li>
<li><a href="#what-is-model-size">What is Model Size?</a></li>
<li><a href="#what-does-it-mean-to-tune-a-ducking-model">What Does it Mean to Tune a Ducking Model?</a></li>
<li><a href="#how-do-i-choose-a-model">How do I Choose a Model?</a></li>
<li><a href="#are-bigger-models-better">Are Bigger Models Better?</a><ul>
<li><a href="#a-concrete-example-with-chatgpt">A Concrete Example with ChatGPT</a></li>
<li><a href="#so-which-was-better-chatgpt-35-or-chatgpt-4">So Which was Better ChatGPT 3.5 or ChatGPT 4?</a></li>
</ul>
</li>
<li><a href="#what-is-over-fitting">What is Over-fitting?</a></li>
<li><a href="#how-much-computing-power-do-i-need">How Much Computing Power Do I Need?</a><ul>
<li><a href="#biggest-factor">Biggest Factor</a></li>
<li><a href="#primary-questions">Primary Questions</a></li>
<li><a href="#secondary-questions">Secondary Questions</a></li>
<li><a href="#some-reference-points">Some Reference Points</a></li>
<li><a href="#a-general-rule-of-thumb-for-gpt-runtime">A General Rule of Thumb for GPT Runtime</a></li>
<li><a href="#some-estimations-on-real-hardware">Some Estimations on Real Hardware</a></li>
</ul>
</li>
</ul>
<h2 id="what-is-artificial-intelligence-ai-machine-learning-ml">What Is Artificial Intelligence (AI) / Machine Learning (ML)</h2>
<p>The words intelligence and learning here are extremely misleading to the average person. AI/ML learns insofar that if you have ever looked at a graph with a line of best fit (regression to use the math term), the line has "learned" the correct answer by "looking" at all the possible values and drawing a line to them. Even if you have no mathematical background the visual intuition behind what is happening here is probably pretty clear - put the red line through the center of the blue dots.</p>
<p><img alt="" src="images/2023-11-30-09-31-45.png" /></p>
<p><a href="code/linear_regression.py">Source Code</a></p>
<p>Linear regression is a type of supervised machine learning. Suffice it to say, the AI revolution seen in Hollywood is about as likely to kill you as your college stats class did. Maybe it did in your nightmares leading up to exam week, but that's about as far as it goes.</p>
<p>In the scheme of math, none of what makes AI/ML special is actually particularly complex. Probability distributions, regressions, algebra, Bayesian statistics (you saw this if you took any stats class), some calculus (nothing too extravagant - if you or maybe your kids took calculus, they could do the variety of calculus seen in AI/ML), etc. </p>
<p>What makes AI/ML really interesting is that while none of its constituent concepts are particularly complex, the complexity comes in how it is all combined to produce the output. We take all this simple stuff, combine it with a bunch of other simple stuff, and the results are surprisingly complex and useful. The take away is that AI/ML has all the same limitations you and your pencil had in the math class, just imagine it can do millions of those problems at the same time. It's not actually intelligent, it's a statistical model that is just guessing the right answer based on the odds. This is true even of things as magical looking as ChatGPT.</p>
<h2 id="what-is-the-difference-between-ai-and-ml">What is the Difference Between AI and ML?</h2>
<p>Semantics. Ask 50 people get 50 definitions. Honestly, it's a fairly arbitrary line so the exact distinction isn't really all that important but <strong>generally</strong>:</p>
<ul>
<li>AI (Artificial Intelligence): AI is a broader field that aims to create machines or systems that can perform tasks that typically require human intelligence. These tasks include reasoning, problem-solving, understanding natural language, recognizing patterns, and making decisions. AI encompasses a wide range of techniques and approaches, including machine learning.</li>
<li>ML (Machine Learning): ML is a subset of AI that specifically focuses on developing algorithms and models that allow computers to learn from and make predictions or decisions based on data. ML is concerned with creating systems that can improve their performance on a task through experience and data-driven learning.</li>
</ul>
<p>The definitions also keep changing as technology improves. 40 years ago a series of if/then statements was called AI, but now they're just if/then statements. In the end, it really doesn't much matter to actually performing useful work or communicating yourself.</p>
<h2 id="what-is-training-and-inferencing">What is Training and Inferencing?</h2>
<p>In machine learning we talk a lot about training and inferencing but what does that mean? Machine learning / AI generally break down into two phases: the training phase and the inferencing phase.</p>
<p>Training is when you take some large body of pre-existing data, feed it into a bunch of math, and that math produces a series of equations which ultimately guess the answer to some question you have. The inferencing phase is when you start feeding real world, previously unseen, data into the model and it produces answers based on the training.</p>
<p>You've been through this yourself in school. The training phase is you doing your homework, studying, and preparing for an exam. The inference phase is your taking that exam which usually has hereunto unseen questions, and then you generate answers based on the data you trained on.</p>
<h2 id="what-is-a-large-language-model-llm">What is a large language model (LLM)?</h2>
<p>A large language model (LLM) is a type of artificial intelligence that specializes in processing and understanding human language. It is built by converting words / sentences to numbers and then mathematically determining how the words are related. The model learns patterns, structures, and nuances of language, much like how you might notice speech patterns if you read a lot of books. Have you ever been reading a book and thought, "Oh I know what comes next." That thought you had is exactly how LLMs work. Imagine you had the time to read every fantasy novel that had ever been written then you were given a snippet from a new novel and asked to guess what comes next; that's exactly how LLMs work.</p>
<p>I guarantee you have seen this technology before. Have you ever been typing on your phone and it suggests the next word?</p>
<p><img alt="" src="images/2023-11-30-10-02-44.png" /></p>
<p>Think of a large language model as a supercharged version of the auto-suggestions on your phone's keyboard. Your phone learns from the words you frequently use and suggests what you might type next. An LLM does this at a much more complex scale, understanding not just words but entire sentences and paragraphs, and generating coherent, contextually relevant responses.</p>
<p>However, just like your phone's keyboard doesn't 'understand' what you're typing, LLMs don't truly 'understand' language in the human sense. They're statistical machines. Given a prompt, they generate responses based on the patterns they've learned from their training data. </p>
<p>In essence, a large language model is a sophisticated tool for mimicking human-like text based on the probability of certain words and sentences following others, based on its training data. It's a product of combining many simple concepts from mathematics and computer science, but the sheer scale of data and computations involved creates something that can seem quite complex and intelligent on the surface.</p>
<h2 id="what-is-a-hyperparameter">What is a Hyperparameter?</h2>
<p>All of these AI/ML models have what are called hyperparameters. This is just a fancy word for a number we can tweak in our math. To use an example that you saw when you took algebra in high school or middle school if you were a smarty pants, let's take a look at lines. A line has the form $a*x+b=Y$. $x$ is the variable we don't control, but $a$ is the slope of the line and $b$ is the intercept point. $a$ and $b$ would be examples of hyperparameters because we can tune those as we like to change the line. For example, here is what it look like when we change the values of $b$.</p>
<p><img alt="" src="images/2023-11-30-10-49-34.png" /></p>
<p><a href="code/line_graph_b.py">Source Code</a></p>
<p>Now what if we change $a$?</p>
<p><img alt="" src="images/2023-11-30-10-55-34.png" /></p>
<p><a href="code/line_graph_a.py">Source Code</a></p>
<p>So how does that relate to machine learning? Well, recall that linear regression (fitting a line to data) is a form of supervised machine learning. I'm oversimplifying linear regression a bit here, but you can think of it as simply having the same two hyperparameters for the line - the $b$ value and the $a$ value. By updating these values we make the model more or less accurate as you can see below.</p>
<p><img alt="" src="images/2023-11-30-10-42-18.png" /></p>
<p><a href="code/hyperparameters.py">Source Code</a></p>
<h2 id="what-is-model-size">What is Model Size?</h2>
<p>Now that you understand <a href="#what-is-a-hyperparameter">hyperparameters</a> we can talk about model size. In our linear regression example (simplified), the model size is just two. You can adjust either $a$ or $b$.</p>
<p>So when we talk about a model like Llama 7 billion (that's a specific large language model), what we're saying is that it has 7 billion tunable parameters or in the context of our example, 7 billion $a$s and $b$s. Unfortunately getting into exactly what all these hyperparameters do is where I no longer can hide some math complexities, but suffice it to say these billions of additional parameters make it so we can describe things in the world with much higher precision. Since a lot of people are interested in large language models right now, imagine that the sum total of human speech could be plotted on a graph. Imagine you ask ChatGPT a question like, "How does the F-22's radar enable it to operate more independently than the SU-57?" (I've been watching a lot of <a href="https://www.youtube.com/@MaxAfterburnerlife">Max Afterburner</a> lately. This particular thought came from <a href="https://youtu.be/gu0TBjzeCo0?si=tVd7WS8_YaNRg0AG&amp;t=1377">this video</a>). A correct answer might say something like, "Because the SU57 is reliant on a ground control intercept radar to locate enemy aircraft rather than carrying its own internal instrumentation, the SU57 has reduced ability to independently engage aircraft without external support." Another, worse, possible answer is, "The SU57 is a bad airplane." Obviously, the second leaves a lot to be desired. </p>
<p>Imagine that the pieces required to create the first answer are the blue dots below. The purple line represents a model with six hyperparameters, the green line is only four hyperparameters, and the red dotted line is just our linear regression. </p>
<p><img alt="" src="images/2023-11-30-14-00-40.png" /></p>
<p><a href="code/nonlinear_functions.py">Source Code</a></p>
<p>How the math works doesn't matter for the explanation. What matters is that now we have six hyperparametrs - which are the $b$ variables shown in this equation: $Y = b_0 + b_1X + b_2X^2 + b_3X^3 + b_4X^4 + b_5X^5$. As you can imagine the four hyperparameter model only goes up to $b_3$ (we started counting at zero because we're computer people). Those extra tunable parameters combined with picking a better algorithm for our use case allow us to create much better answers for this particular data set. This <strong>does not</strong> mean more hyperparameters is always better though. I'll explain that later. For this dataset though it definitely was. In this example you can imagine that these are how the answers line up:</p>
<p>Question: "How does the F-22's radar enable it to operate more independently than the SU-57?"</p>
<p>Answers:
- Purple Line: "Because the SU57 is reliant on a ground control intercept radar to locate enemy aircraft rather than carrying its own internal instrumentation, the SU57 has reduced ability to independently engage aircraft without external support."
- Green Line "The SU57 is a bad airplane."
- Red Line: "The airplane banana."</p>
<h2 id="what-does-it-mean-to-tune-a-ducking-model">What Does it Mean to Tune a Ducking Model?</h2>
<p>In our discussion of <a href="#what-is-a-hyperparameter">hyperparameters</a> you saw tuning. This is what it looked like to tune the $a$ parameter of a line:</p>
<p><img alt="" src="images/2023-11-30-10-55-34.png" /></p>
<p>But what other kinds of tuning are there and how does that relate to the LLMs we see?</p>
<p>Have you ever become frustrated with your ducking phone? It is a ducking phone because the engineers over at Apple decided that certain words should have a reduced autocorrect priority even if they are statistically far more likely. This is a manual intervention in what the math would have natively produced as the alternative word is clearly far more likely in the given sentence.</p>
<p>If you ever have tried to get ChatGPT to answer a sordid inquiry you will have seen more examples of tuning in action:</p>
<p><img alt="" src="images/2023-11-30-10-07-59.png" /></p>
<p>Obviously, without those guards in place, ChatGPT would have done a great job of summarizing every murder related piece of material it had ever found on the Internet. Not a desirable behavior.</p>
<p>Tuning is changing the answers by either updating the hyperparameters or adding some sort of external logic to your model to change the results.</p>
<h2 id="how-do-i-choose-a-model">How do I Choose a Model?</h2>
<p>This is where PHDs in math, machine learning, data science, etc make their money. This is an extremely complex question that unfortunately has no simple, high level, explanation. What I will do here is demonstrate the difference in laymen's terms between the different models so you get a feel for just how big a difference model selection can make. Let's say we want to predict housing prices based only on the total size of the home and the number of rooms. We will pick two models - one is a linear regression and the other is k-means (it isn't important that you understand what this is). I wrote a program that generates some arbitrary data that represents the housing prices. What matters is that there is a linear relationship between the price of the house and the size/# rooms making this perfect for a linear regression. Here is what it looks like when you predict the price with a linear regression vs k-means:</p>
<p><img alt="" src="images/2023-11-30-14-37-02.png" /></p>
<p><a href="code/model_selection.py">Source Code</a></p>
<p>As you can see the linear regression nails it and k-means misses by some margin. This is the power of picking the right model.</p>
<p>I wish I had some simple answer on how to pick the right model, but there just isn't one. You have to understand your problem and you have to know the math. Though I'll tell you that if you give ChatGPT your problem it usually does a good job at guessing a set of models that would perform reasonably. Automated tools are also now on the market that are built by teams of PHDs that try multiple different models for some set of data, score them, and then present the best results. These still don't replace data scientists, but they do make it so your existing data science teams are spending less time on infrastructure and minutia and more time on your mission target where you want them.</p>
<h2 id="are-bigger-models-better">Are Bigger Models Better?</h2>
<p>This is extremely difficult to explain because the answer is - it depends. A lot. Again, this is where PHDs in this stuff make their money. You have actually already seen the answer in <a href="#how-do-i-choose-a-model">How Do I Choose a Model?</a>. k-means has significantly more hyperparameters than linear regression and its model size you can think of being significantly larger however it produced significantly worse results. So bigger is worse then? Well... no. In <a href="#how-do-i-choose-a-model">How do I Choose a Model</a> I picked an example where more complex performed comparatively worse.</p>
<p>In <a href="#what-is-model-size">What is Model Size</a> we saw an example of how more hyperparameters improved the performance of the model.</p>
<p>These nuances can be extrapolated to the most complex models of today and the impact of the size of the model on the outcomes are unique to the exact model you are using. So the question you need to ask isn't "Are bigger models better" it is <strong>"Given some very specific model and my very specific use case, are bigger models better?"</strong>. If you phrase the question in a way any less precisely than that you will not receive an accurate answer.</p>
<h3 id="a-concrete-example-with-chatgpt">A Concrete Example with ChatGPT</h3>
<p>In the interest of providing a more nuanced example, here is a personal comparison between ChatGPT 3.5 and ChatGPT 4. ChatGPT 3.5 has roughly 175 billion parameters. ChatGPT 4's algorithm isn't public but suffice it to say it's a safe assumption that it's significantly more.</p>
<p>I am a huge lover of languages. I speak Mandarin, Spanish, and English (obviously), but I am currently learning Norwegian. The problem I'm solving is that I want to learn new Norwegian words while keeping my Spanish and Mandarin vocab current. To that end, I updated my Norwegian Flashcard program to leverage OpenAI's APIs to get answers regarding potential translations from both ChatGPT 3.5 and ChatGPT 4. The answers, particularly with regards to Mandarin, are quite interesting because while Norwegian (Germanic), Spanish (Romance), and English (Germanic - technically) are pretty similar - Mandarin is very different. In fact, while linguists hypothesize about a proto-world language, we don't have any idea what Mandarin and English's closest relative is.</p>
<p><img alt="" src="images/2023-11-30-15-18-00.png" /></p>
<p>Generated with ChatGPT4 in case you're wondering about the funny formatting.</p>
<p>Why am I babbling about my language hobby? Because that difference means that translating them becomes much harder with many more possible translations which means the spread of answers from language models is <em>also</em> wider.</p>
<p>In the scope of <strong>GPT models specifically</strong>, smaller models have fewer data points to understand context and language semantics so you will <strong>generally</strong> receive correspondingly "worse" answers. However, worse is an extremely nuanced concepts. Insofar as a smaller model is more likely to produce less precise responses, this may appear to give a bigger sense of randomness and subsequently creativity. In the context of my language program, the responses from 3.5 are much more scattered and sometimes do capture interesting things whereas GPT4 is more succinct and generally more accurate in the context of the prompt. Example:</p>
<p>Norwegian Word: å overføre
English: to transfer
Spanish: transferir
My program gives GPT3.5 and 4 a series of contexts and tells it to translate å overføre to Mandarin.</p>
<p>GPT 3.5:
1. 转账 (zhuǎnzhàng)
2. 传输 (chuánshū)
3. 转移 (zhuǎnyí)
4. 转交 (zhuǎnjiāo)
5. 传输 (chuánshū)</p>
<p>These are all context dependent ways you might say to transfer each based on one of the examples. Money, files, a business, responsibility, and pictures (files) respectively.</p>
<p>GPT4:
轉移 (Zhuǎnyí) - Transfer</p>
<p>Just the one result. It is an aggregate of the semantics of the different Norwegian sentences I gave it as input.</p>
<p>GPT4 has correctly identified what I would say is the most accurate general translation however, the GPT3.5 data is highly useful because realistically, nearly every word in Norwegian/English/Spanish has a one to many correlation with Mandarin and I want those others as well. </p>
<p>However, GPT3.5 also more often invents nonsense. Ex:</p>
<p>Norwegian: å love</p>
<p>This word simply means to promise. GPT4 accurately said: 承諾 (Chéngnuò) - Promise. GPT3.5 on the other hand, produced the following where it inaccurately assumed the word was English and completely failed to follow my instructions. I told it to only give me words and it produced full sentences. Moreover these were all repeat examples of the same meaning which was unhelpful.</p>
<pre><code>å love (to promise, to pledge, to vow)

1. Jeg elsker deg. (I love you.) &lt;-- it confused the English love and Norwegian love.
- 我愛你 (wǒ ài nǐ)

2. Jeg lover å komme i morgen. (I promise to come tomorrow.)
- 我承諾明天會來 (wǒ chéngnuò míngtiān huì lái)

3. Han lovte å hjelpe meg. (He vowed to help me.)
- 他答應幫助我 (tā dāyìng bāngzhù wǒ)

4. Vi får se om hun holder det hun lover. (We'll see if she keeps her word.)
- 我們看看她是否信守承諾 (wǒmen kàn kàn tā shìfǒu xìnshǒu chéngnuò)

5. De lovet å betale tilbake pengene. (They pledged to pay back the money.)
- 他們承諾還錢 (tāmen chéngnuò huán qián)
</code></pre>
<p>Overall GPT4 was infinitely more useful/accurate here.</p>
<h3 id="so-which-was-better-chatgpt-35-or-chatgpt-4">So Which was Better ChatGPT 3.5 or ChatGPT 4?</h3>
<p>I described the Mandarin translations above. From a user perspective I actually prefer ChatGPT 3.5 for Mandarin translation by a significant margin over ChatGPT 4. Mandarin indicates meaning by leveraging the combination of characters to articulate highly specific meanings that would usually require completely different words in other languages or simply require you to use more words to provide more context. ChatGPT 3.5 does a better job of covering these possibilities in my not-insignificant experience than ChatGPT 4.</p>
<p><strong>However</strong>, between languages that have a closer to one to one translation: Norwegian, Spanish, and English, ChatGPT 3.5 was significantly worse. ChatGPT 4 absolutely blew away the performance of ChatGPT 3.5. To such an extent that I stopped using it entirely for Norwegian -&gt; Spanish and only using the results from ChatGPT 4.</p>
<p>This gives you an idea of just how incredibly nuanced model size can be. You simply cannot make any assumptions about it - your <strong>exact</strong> use case matters as does the <strong>exact</strong> model you have selected.</p>
<h2 id="what-is-over-fitting">What is Over-fitting?</h2>
<p>What is over-fitting? To make it very simple, returning to our analogy from <a href="#what-is-training-and-inferencing">What is Training and Inferencing</a> regarding studying, you have yourself probably made this mistake at some point. Imagine that in the course of studying for a test you became very focused on a narrow set of information and assumed that this narrow set of information would be sufficient for all the test questions. Perhaps it is a math course and you determined only certain algorithms were important and ignored the rest. When you got to the exam, you got 40% of the questions right, but were completely wrong on the others. That is over-fitting. You trained yourself on only 40% of the training data but accidentally omitted information representative of the other 60%.</p>
<p>Here is a graphical representation of what is going on. Imagine that the over-fitted graphs are what you studied. Some very specific subset of the real data and you memorized only those things. The right side you see in the training data it appears to perform significantly worse, but when it comes time to the real world test, it does significantly better because it isn't overly-fit to the training data.</p>
<p><img alt="" src="images/2023-11-30-17-01-01.png" /></p>
<p><a href="code/overfitting.py">Source Code</a></p>
<p>It's important that during training data you don't just draw perfect boxes around every data point and assume that the real world will look the same. Your model has to generalize well as you see on the right hand side of our graph.</p>
<h2 id="how-much-computing-power-do-i-need">How Much Computing Power Do I Need?</h2>
<p>There are a couple of key questions that will determine your computing requirements for running an AI/ML workload.</p>
<h3 id="biggest-factor">Biggest Factor</h3>
<p>What is the model(s)? Plain and simple this will have the biggest impact on how much <a href="https://www.youtube.com/watch?v=7qxTKtlvaVE">horsepower</a> you need.</p>
<h3 id="primary-questions">Primary Questions</h3>
<ul>
<li><strong>Model Size (Number of Parameters)</strong> -Larger models with more parameters demand more computational power for both training and inference. The number of parameters directly impacts the amount of memory and processing power needed.</li>
<li><strong>Inference Load</strong>: The number of requests or queries the model handles simultaneously affects server requirements. Higher inference loads require more computing resources to maintain performance.</li>
<li><strong>Training Data Volume</strong>: The amount and complexity of the data used to train the LLM can significantly impact the computational resources required. Larger and more diverse datasets require more storage and processing power.</li>
</ul>
<h3 id="secondary-questions">Secondary Questions</h3>
<ul>
<li><strong>Optimization and Efficiency of Algorithms</strong>: The efficiency of the underlying algorithms can impact how computationally intensive the model is. More optimized algorithms can reduce server power requirements.</li>
<li><strong>Latency Requirements</strong>: If low latency (quick response times) is crucial, more powerful servers are needed to process requests rapidly.</li>
<li><strong>Redundancy and Reliability Needs</strong>: Ensuring high availability and fault tolerance may require additional resources for redundancy.</li>
<li><strong>Cooling and Physical Infrastructure</strong>: Large-scale computing resources generate heat and require effective cooling solutions. Worse cooling means less efficiency which means more servers over wider area.</li>
<li>Liquid cooling is the future of HPC. If it isn't on your radar and you're a big datacenter, you will be left behind over the coming decades. We have reached the upper bound of what is possible with air cooling.</li>
</ul>
<h3 id="some-reference-points">Some Reference Points</h3>
<ul>
<li>On a single GPU of reasonable quality you will have no problem running object models with a few thousand pictures and querying them</li>
<li>You can run a personal copy of Llama 7B using something like <a href="https://github.com/h2oai/h2ogpt">h2ogpt</a> on your home computer. I fed it a couple hundred documents and was getting answers back for myself within 30 seconds to a minute</li>
<li>On a 16 core processor I generated an lightgbm that represented an Ising Model simulation within 10 minutes but depending on the hyperparameters you select you could make it take hours</li>
<li>A few GPUs will easily let you run a few LLMs and query them</li>
</ul>
<p>Where things get bigger...</p>
<p>It's really the scaling. The vast majority of models for a single user you can run them with a single beefy GPU. My employer and benefactor provided a 3080 for my work to do all the things I do for my day job and I can run anything I want.</p>
<p>So when does it start taking racks of computers? Scaling. If you just want to suck down a couple thousand documents, put them into an LLM, and start asking questions, you need very little to do that. However, when you want hundreds of people to simultaneously ask questions or you want to start scraping the totality of Reddit as the source of your training data now the requirements start exploding.</p>
<h3 id="a-general-rule-of-thumb-for-gpt-runtime">A General Rule of Thumb for GPT Runtime</h3>
<p>If it's LLM models we're talking about they are usually based on what is called the transformer architecture. The relationship with the size of your training data is linear (direct). Double the size of the training data and you will generally double your training time. The relationship with the size of the model is quadratic - that is that it multiplies by squares. So if you double the size of the model you're using then the same amount of training data would take four times as long. Quadruple the size of your training data and it now takes 16 times as long.</p>
<h3 id="some-estimations-on-real-hardware">Some Estimations on Real Hardware</h3>
<p>I wrote an extensive estimation analysis available <a href="../Estimating%20Compute%20Requirements%20for%20Machine%20Learning/">here</a> that gets into the weeds on some real world estimations.</p>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../CloudLink/" class="btn btn-neutral float-left" title="CloudLink"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="../Configure%20FN410%20as%20a%20Switch/" class="btn btn-neutral float-right" title="Configure FN410 as a Switch">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
      <span><a href="../CloudLink/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="../Configure%20FN410%20as%20a%20Switch/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script src="../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "..";</script>
    <script src="../js/theme_extra.js"></script>
    <script src="../js/theme.js"></script>
      <script src="../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
