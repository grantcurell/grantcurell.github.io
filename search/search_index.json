{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Grant Curell's Dell Projects Every time Dell asks me to figure something out, I write it down. Mostly because more often than not someone asks me about it a year later and I have to remember what I did. Questions If you have any questions about something I did, open an issue on the repo at grantcurell.github.io or write me at grant_curell AT dell dot com . Additional Files Most of the files I use in the projects are linked from the markdown documentation but often there might be additional files not visible for this site. It may be easier to browse to the actual docs folder and look at the appropriate folder. Disclaimer These are just my personal notes for the random things I test. I'm generally thorough, but there is no guarentee on completeness :-D How to Configure ONIE I ran the network version of the ONIE installation using a web server. Below is what I did to get things installed. We will host the OS installer on our web server and then we will use ONIE to grab it. We will use a DNS record to control the ONIE server location. Install Apache on RHEL or your favorite Linux distro. Make sure you allow HTTP traffic through the firewall Download your operating system of choice and untar it. Upload <YOUR INSTALLER>.bin to the root of your web server. Create a symlink to the installer with ln -s <YOUR INSTALLER>.bin onie-installer . The file must have this name for the installation to work. The switch will use DHCP to acquire an IP address. On the DNS server pointed to by your DHCP configuration, add a record for onie-server and point it at the host running Apache. On a test box, run a DHCP request, ensure you pull the correct DNS server and that the host can resolve onie-server . After you confirm DNS is working, browse to the onie-installer file on your Apache server and make sure you can download it without issue. Warning: It must be able to resolve the hostname onie-server with the FQDN. If onie-server is not immediately resolvable, the install process will not work. ONIE Boot the Switch Connect to the switch over the console port. My configuration was: Baud Rate: 115200 Data Bits: 8 Stop Bits: 1 Parity: None Flow Control: None Connect the ethernet management port of the switch to the same network containing your web server. ONIE will use the management port to establish a connection to the ONIE server. Once the grub menu appears, select ONIE Installer. In my case this was the top option. At this point the ONIE discovery process will commence. It will print each location it attempts to search. It should find the onie-server DNS record and the installation should begin automatically. If this doesn't happen it means there is an issue with the preconfiguration above. Try swapping out the ethernet management cable with a host. Make sure that host pulls DNS/DHCP correctly and is able to download the onie-installer file. Wait for the installation to finish and the switch to reboot. Login with admin/admin. If after logging in you are told you can't enter configuration mode because \"% Error: ZTD is in progress(configuration is locked).\" run ztd cancel Configure Managment Interface on Dell OS10 Do the following to configure a management interface on Dell OS10 OS10# configure terminal OS10(config)# interface mgmt 1/1/1 OS10(conf-if-ma-1/1/1)# ip address 192.168.1.20/24 OS10(conf-if-ma-1/1/1)# <165>1 2019-10-28T19:04:39.385196+00:00 OS10 dn_alm 669 - - Node.1-Unit.1:PRI [event], Dell EMC (OS10) %IP_ADDRESS_ADD: IP Address add is successful. IP 192.168.1.20/24 in VRF:default added successfully OS10(conf-if-ma-1/1/1)# do write memory","title":"Grant Curell's Dell Projects"},{"location":"#grant-curells-dell-projects","text":"Every time Dell asks me to figure something out, I write it down. Mostly because more often than not someone asks me about it a year later and I have to remember what I did.","title":"Grant Curell's Dell Projects"},{"location":"#questions","text":"If you have any questions about something I did, open an issue on the repo at grantcurell.github.io or write me at grant_curell AT dell dot com .","title":"Questions"},{"location":"#additional-files","text":"Most of the files I use in the projects are linked from the markdown documentation but often there might be additional files not visible for this site. It may be easier to browse to the actual docs folder and look at the appropriate folder.","title":"Additional Files"},{"location":"#disclaimer","text":"These are just my personal notes for the random things I test. I'm generally thorough, but there is no guarentee on completeness :-D","title":"Disclaimer"},{"location":"#how-to-configure-onie","text":"I ran the network version of the ONIE installation using a web server. Below is what I did to get things installed. We will host the OS installer on our web server and then we will use ONIE to grab it. We will use a DNS record to control the ONIE server location. Install Apache on RHEL or your favorite Linux distro. Make sure you allow HTTP traffic through the firewall Download your operating system of choice and untar it. Upload <YOUR INSTALLER>.bin to the root of your web server. Create a symlink to the installer with ln -s <YOUR INSTALLER>.bin onie-installer . The file must have this name for the installation to work. The switch will use DHCP to acquire an IP address. On the DNS server pointed to by your DHCP configuration, add a record for onie-server and point it at the host running Apache. On a test box, run a DHCP request, ensure you pull the correct DNS server and that the host can resolve onie-server . After you confirm DNS is working, browse to the onie-installer file on your Apache server and make sure you can download it without issue. Warning: It must be able to resolve the hostname onie-server with the FQDN. If onie-server is not immediately resolvable, the install process will not work.","title":"How to Configure ONIE"},{"location":"#onie-boot-the-switch","text":"Connect to the switch over the console port. My configuration was: Baud Rate: 115200 Data Bits: 8 Stop Bits: 1 Parity: None Flow Control: None Connect the ethernet management port of the switch to the same network containing your web server. ONIE will use the management port to establish a connection to the ONIE server. Once the grub menu appears, select ONIE Installer. In my case this was the top option. At this point the ONIE discovery process will commence. It will print each location it attempts to search. It should find the onie-server DNS record and the installation should begin automatically. If this doesn't happen it means there is an issue with the preconfiguration above. Try swapping out the ethernet management cable with a host. Make sure that host pulls DNS/DHCP correctly and is able to download the onie-installer file. Wait for the installation to finish and the switch to reboot. Login with admin/admin. If after logging in you are told you can't enter configuration mode because \"% Error: ZTD is in progress(configuration is locked).\" run ztd cancel","title":"ONIE Boot the Switch"},{"location":"#configure-managment-interface-on-dell-os10","text":"Do the following to configure a management interface on Dell OS10 OS10# configure terminal OS10(config)# interface mgmt 1/1/1 OS10(conf-if-ma-1/1/1)# ip address 192.168.1.20/24 OS10(conf-if-ma-1/1/1)# <165>1 2019-10-28T19:04:39.385196+00:00 OS10 dn_alm 669 - - Node.1-Unit.1:PRI [event], Dell EMC (OS10) %IP_ADDRESS_ADD: IP Address add is successful. IP 192.168.1.20/24 in VRF:default added successfully OS10(conf-if-ma-1/1/1)# do write memory","title":"Configure Managment Interface on Dell OS10"},{"location":"Adding%20Hashing%20to%20OpenSwitch/","text":"Adding Hashing to OpenSwitch This is the first problem: https://github.com/open-switch/opx-tools/issues/27 He says the code should be somewhere here: https://github.com/open-switch/opx-base-model/blob/master/yang-models/dell-base-hash.yang Helpful docs on how yang is processed Helpful forum communication about header files Installing header files for OPX base Run apt install -y libopx-base-model-dev . The headers will be in /usr/include/opx . The metadata files are in ls /usr/lib/x86_64-linux-gnu/ Adding Space / Remote Desktop I wanted more space on my installation so I added a usb drive. Do the following to clear the USB driver and then add space: Configure USB fdisk /dev/sdb d d d w Extend Partition pvcreate /deb/sdb vgextend OPX /dev/sdb lvextend -l +100%FREE /dev/OPX/SYSROOT1 resize2fs /dev/mapper/OPX-SYSROOT1 Remote Desktop sudo apt update sudo apt install xfce4 xfce4-goodies xorg dbus-x11 x11-xserver-utils sudo apt install xrdp sudo adduser xrdp ssl-cert xfce4-session-logout --halt On CentOS 7 yum install -y epel-release yum groupinstall -y \"Xfce\" echo \"xfce4-session\" > ~/.Xclients chmod a+x ~/.Xclients Set Up VM Failed, but should have worked wget http://archive.openswitch.net/vm-tools/lvm chmod +x lvm wget https://dell-networking.bintray.com/opx-images/opx-onie-installer_1.1_amd64.bin wget https://dell-networking.bintray.com/opx-images/onie-kvm_x86_64-r0.iso wget https://archive.openswitch.net/installers/3.2.1/Dell-EMC/PKGS_OPX-3.2.1-installer-x86_64.bin ./lvm create openswitch --iso onie-kvm_x86_64-r0.iso --bin PKGS_OPX-3.2.1-installer-x86_64.bin Building from source Docs git clone https://github.com/opencomputeproject/onie.git cd onie/contrib/build-env/ docker build -t debian:build-env . mkdir --mode=0777 -p opt/src docker run -it -v /opt/src:/home/build/src --privileged --name onie debian:build-env In the container: ./clone-onie cd src/onie/build-config make -j4 MACHINE=kvm_x86_64 all Build an installer See available distrubitions: opx-build/scripts/opx_run opx_rel_pkgasm.py --help After running a build you can run opx-build/scripts/opx_run opx_rel_pkgasm.py --dist unstable -b opx-onie-installer/release_bp/OPX_dell_base.xml to build an installer This looks like it should be the docker build command docker run --rm --name root_root_32994 --privileged -e LOCAL_UID=0 -e LOCAL_GID=0 -v /root:/mnt -v /root/.gitconfig:/home/opx/.gitconfig -v /etc/localtime:/etc/localtime:ro -e ARCH -e DIST -e OPX_RELEASE -e OPX_GIT_TAG -e CUSTOM_SOURCES opxhub/build:latest Investigation into the opx-config-global-switch problem cps_get_oid.py -qua target base-switch/switching-entities/switching-entity The question to ask is - what is the server application for the CPS object? The problem is that in opx-config-global-switch the value lag-hash-fields isn't present in target_attrs on line 214 of opx-config-global-switch The question is what would populate that? I don't think any of what I care about is in NAS L2. NAS L2 handles This repository contains the Layer 2 (L2) component of the network abstraction service (NAS). This handles media access control (MAC) learning, programming spanning-tree protocol (STP) state, mirroring, sFlow, and other switch configurations. I think what I care about is in opx-nas-interface because the operating system is handling the LAG. Description is This repository contains the interface portion of the network abstraction service (NAS). This creates interfaces in the Linux kernel corresponding to the network processor unit (NPU) front panel ports, manages VLAN and LAG configurations, statistics management and control packet handling. Logically there are three components including the LAG: LAG DS, NAS LAG, NDI LAG. See picture. NAS Daemon: The NAS daemon integrates standard Linux network APIs with NPU hardware functionality, and registers and listens to networking (netlink) events. What enum values in the model of dell-base-hash align with what's in opx-config-global-switch in hash fields map. What are all these actions in opx-config-global-switch? Ok - so now we know the thing I want is in base-traffic-hash/entry, but opx-config-global-switch is looking at base-switch/switching-entities/switching-entity . The next question is what populates that? Why are the hashes not in it? The definition for switching-entity is at: https://github.com/open-switch/opx-base-model/blob/abdf66f813b48a3c8e7682361cdacccd0271866d/yang-models/dell-base-switch-element.yang Say what? hash-fields is also defined in base-switch/switching-entities/switching-entity The next question to ask is who has ownership of base-traffic-hash? What about base-switch/switching-entities/switching-entity? The problem seems like it should be there? Why? base-traffic-hash is owned by opx_nas_daemon base-switch/switching-entities/switching-entity is also owned by opx_nas_daemon CPS has a REST service according to: https://github.com/open-switch/opx-cps The connection between YANG and the applications seems to be the CPS API? Applications define objects through (optionally YANG-based) object models. These object models are converted into binary (C accessible) object keys and object attributes that can be used in conjunction with the C-based CPS APIs. Description of cps_model_info: This tool is useful to get all information about CPS objects on the target. It is used to get the attributes of a specific CPS object, or first-level contents of a given YANG path of the CPS object (as defined in the YANG model). IT GIVES THE PROCESS OWNER! Investigating ops-nas-daemon There is a file called base_nas_default_init which defines the mirror port and the flow behaviors. I haven't found anything about other stuff yet. opx-config-global-switch --lag-hash-alg crc works and is owned by opx_nas_daemon - there must be other things it owns beside this. There is a file called hald_init.c . I think what is happening is all the other services fall under the NAS daemon. The code I'm looking for is somewhere else. After following that around for a while it looks like the file I'm really interested in is here https://github.com/open-switch/opx-nas-l2/blob/7e80d3952786f219b8072f1666ff1f16ba353d86/src/switch/nas_hash_cps.cpp . This bubbles up to the L2 init function, which bubbles back up to hald_init.c . dell-base-hash.h gets included in this thing. The YANG for the regular hash algorithm is at https://github.com/open-switch/opx-base-model/blob/abdf66f813b48a3c8e7682361cdacccd0271866d/yang-models/dell-base-switch-element.yang Initial investigation finished opened: https://github.com/open-switch/opx-nas-l2/issues/34 Investigation 2 - Compilation problems https://android.googlesource.com/platform/hardware/broadcom/wlan/+/master/bcmdhd/config/config-bcm.mk - examlpe config-bcm file. Seems to have something to do with broadcom's config. Better example configuration from Broadcom: http://broadcom-switch.github.io/OpenNSL/doc/html/OPENNSL_CUSTOMIZING_OPENNSL.html#OPENNSL_CONFIG_EXAMPLE Configuration properties: http://broadcom-switch.github.io/OpenNSL/doc/html/OPENNSL_CUSTOMIZING_OPENNSL.html#OPENNSL_CONFIG_PROPERTIES More info on config.bcm https://docs.broadcom.com/doc/12378910 Investigation 3 - post compilation value still missing The hash variables seem to be passed by pointer from nas_hash_cps.cpp to nas_ndi_hash.c in line 222. The SAI portion of the hash creation seems to be here which could prove helpful later. In the bug at https://github.com/open-switch/opx-tools/issues/27 where he says obj-type that is referring to the YANG model. There is a leaf called obj-type . Traffic refers to: typedef traffic { type enumeration { enum \"ECMP_NON_IP\" { value 1; description \"ECMP routing: flow of non-IP ethernet frames\"; } enum \"LAG_NON_IP\" { value 2; description \"LAG routing: flow of non-IP ethernet frames\"; } enum \"ECMP_IPV4\" { value 3; description \"ECMP routing: flow of IPv4 packets\"; } enum \"ECMP_IPV4_IN_IPV4\" { value 4; description \"ECMP routing: flow of IPv4 packets with IPv4 packets tunnelled inside them\"; } enum \"ECMP_IPV6\" { value 5; description \"ECMP routing: flow of IPv6 packets\"; } enum \"LAG_IPV4\" { value 6; description \"LAG routing: flow of IPv4 packets\"; } enum \"LAG_IPV4_IN_IPV4\" { value 7; description \"ECMP routing: flow of IPv4 packets with IPv4 packets tunnelled inside them\"; } enum \"LAG_IPV6\" { value 8; description \"LAG routing: traffic flow is identified as IPv6 packets\"; } } description \"Enumeration of different types of traffic routing categories\"; } std-hash-field is a leaf-list with a type of field . field is defined as: typedef field { type enumeration { enum \"src-ip\" { value 1; description \"Traffic flow is identified by the source IP\"; } enum \"dest-ip\" { value 2; description \"Traffic flow is identified by the destination IP\"; } enum \"inner-src-ip\" { value 3; description \"Traffic flow is identified by the source IP of the tunnelled packet\"; } enum \"inner-dst-ip\" { value 4; description \"Traffic flow is identified by the destination IP of the tunnelled packet\"; } enum \"vlan-id\" { value 5; description \"Traffic flow is identified by the VLAN ID in the packet\"; } enum \"ip-protocol\" { value 6; description \"Traffic flow is identified by the IP protocol type(v4/v6)\"; } enum \"ethertype\" { value 7; description \"Traffic flow is identified by the IP protocol ether-type\"; } enum \"l4-src-port\" { value 8; description \"Traffic flow is identified by the source port in the packet\"; } enum \"l4-dest-port\" { value 9; description \"Traffic flow is identified by the destination port in the packet\"; } enum \"src-mac\" { value 10; description \"Traffic flow is identified by the source MAC address\"; } enum \"dest-mac\" { value 11; description \"Traffic flow is identified by the destination MAC address\"; } enum \"in-port\" { value 12; description \"Traffic flow is identified by the front-panel port the packet is received at\"; } } description \"Enumeration of different types of packet fields to check for routing\"; } - cps_get definition: def cps_get(q, obj, attrs={}): resp = [] return resp if cps.get([cps_object.CPSObject(obj, qual=q, data=attrs ).get() ], resp ) else None def cps_set(obj, qual, data): return (cps_utils.CPSTransaction([('set', cps_object.CPSObject(obj, qual=qual, data=data).get())]).commit() ) - cps_set('base-pas/led', 'target', {'entity-type': 3, 'slot': 1, 'name': 'Beacon', 'on': args.state}) - Set syntax: root@OPX:~# cps_set_oid.py -qua target -oper set -attr base-switch/entry/std-hash-field=1,2,8,9,6,5 base-switch/entry base-switch/entry/obj-type=6 - cps_get python syntax: cps_get('target', 'base-traffic-hash/entry', {'base-traffic-hash/entry/obj-type': '2'}) . That would get the object target from qualifier base-switch/entry which has attribute of base-switch/entry/obj-type (which is the key in entry) and we specifically want key 1 which is a type of traffic correspeonding to ECMP_NON_IP pp.pprint(cps_get('target', 'base-traffic-hash/entry', {'base-traffic-hash/entry/obj-type': '6'})) [ { 'data': { 'base-traffic-hash/entry/std-hash-field': [ bytearray(b'\\x01\\x00\\x00\\x00'), bytearray(b'\\x02\\x00\\x00\\x00'), bytearray(b'\\x08\\x00\\x00\\x00'), bytearray(b'\\t\\x00\\x00\\x00'), bytearray(b'\\x06\\x00\\x00\\x00')]}, 'key': '1.257.16842755.16842753.16842754.'}] CORRECT SYNTAX FOR CPS_SET: cps_set('base-traffic-hash/entry', 'target', {'base-traffic-hash/entry/obj-type': '6', 'base-traffic-hash/entry/std-hash-field': [1,2,8,9,6,5]}) Investigation 4 - Where are the hash values? Part of the hash seems to be set in nas_ndi_switch.cpp There is also a unit test for it in sai_hash_unit_test.cpp This code seems to imply that the hashes themselves are implemented in the SAI: static _enum_map _algo_stoy = { {SAI_HASH_ALGORITHM_XOR, BASE_SWITCH_HASH_ALGORITHM_XOR }, {SAI_HASH_ALGORITHM_CRC, BASE_SWITCH_HASH_ALGORITHM_CRC }, {SAI_HASH_ALGORITHM_RANDOM, BASE_SWITCH_HASH_ALGORITHM_RANDOM }, {SAI_HASH_ALGORITHM_CRC_CCITT, BASE_SWITCH_HASH_ALGORITHM_CRC16CC }, {SAI_HASH_ALGORITHM_CRC_32LO, BASE_SWITCH_HASH_ALGORITHM_CRC32LSB }, {SAI_HASH_ALGORITHM_CRC_32HI, BASE_SWITCH_HASH_ALGORITHM_CRC32MSB }, {SAI_HASH_ALGORITHM_CRC_XOR8, BASE_SWITCH_HASH_ALGORITHM_XOR8 }, {SAI_HASH_ALGORITHM_CRC_XOR4, BASE_SWITCH_HASH_ALGORITHM_XOR4 }, {SAI_HASH_ALGORITHM_CRC_XOR2, BASE_SWITCH_HASH_ALGORITHM_XOR2 }, {SAI_HASH_ALGORITHM_CRC_XOR1, BASE_SWITCH_HASH_ALGORITHM_XOR1 }, }; static bool to_sai_type_hash_algo(sai_attribute_t *param ) { return to_sai_type(_algo_stoy,param); } static bool from_sai_type_hash_algo(sai_attribute_t *param ) { return from_sai_type(_algo_stoy,param); } There is a reference to each of these hash algorithms in https://github.com/open-switch/opx-base-model/blob/abdf66f813b48a3c8e7682361cdacccd0271866d/history/dell-base-switch-element.yhist From the SAI unit test I found: set_attr.value.s32 = SAI_HASH_ALGORITHM_XOR; status = switch_api_tbl_get()->set_switch_attribute (switch_id,&set_attr); EXPECT_EQ (SAI_STATUS_SUCCESS, status); This must be where they are setting the hash algorithm and how to set it. But where is this API? How do I find out what the options are? Is symmetric hashing exposed? The definition for the function is (it happens inside one of the unit tests): static inline sai_switch_api_t* switch_api_tbl_get (void) { return p_sai_switch_api_tbl; } This is a pointer to a table. I'm guessing it's a struct. sai_switch_api_t is short for SAI switch API table. It is a pointer to the table with all the API calls. From this I got a hint - it looks like there is a thing called switch_api - the same name is used in OpenSwitch. Based on the inputs from nas_ndi_hash.c there must be a series of SAI files with the functionality I want. #include \"std_error_codes.h\" #include \"std_assert.h\" #include \"nas_ndi_event_logs.h\" #include \"nas_ndi_utils.h\" #include \"dell-base-hash.h\" #include \"sai.h\" #include \"saiswitch.h\" #include \"saihash.h\" #include <stdio.h> #include <stdlib.h> #include <string.h> #include <inttypes.h> Some related resources ECMP Hashing explained Broadcom Paper on Hashing Can I do this with openvswitch This didn't pan out. Broadcom docs Descriptions NAS The NAS manages the high-level NPU abstraction and adaptation, and abstracts and aggregates the core functionality required for networking access at Layer 1 (physical), Layer 2 (VLAN, link aggregation), Layer 3 (routing), ACL, QoS, and network monitoring. The NAS provides adaptation of the low-level switch abstraction provided by the SAI for standard Linux networking APIs and interfaces, and CPS API functionality. The NAS is also responsible for providing packet I/O services using the Linux kernel IP stack (see Network adaptation service for complete information).","title":"Adding Hashing to OpenSwitch"},{"location":"Adding%20Hashing%20to%20OpenSwitch/#adding-hashing-to-openswitch","text":"This is the first problem: https://github.com/open-switch/opx-tools/issues/27 He says the code should be somewhere here: https://github.com/open-switch/opx-base-model/blob/master/yang-models/dell-base-hash.yang Helpful docs on how yang is processed Helpful forum communication about header files","title":"Adding Hashing to OpenSwitch"},{"location":"Adding%20Hashing%20to%20OpenSwitch/#installing-header-files-for-opx-base","text":"Run apt install -y libopx-base-model-dev . The headers will be in /usr/include/opx . The metadata files are in ls /usr/lib/x86_64-linux-gnu/","title":"Installing header files for OPX base"},{"location":"Adding%20Hashing%20to%20OpenSwitch/#adding-space-remote-desktop","text":"I wanted more space on my installation so I added a usb drive. Do the following to clear the USB driver and then add space:","title":"Adding Space / Remote Desktop"},{"location":"Adding%20Hashing%20to%20OpenSwitch/#configure-usb","text":"fdisk /dev/sdb d d d w","title":"Configure USB"},{"location":"Adding%20Hashing%20to%20OpenSwitch/#extend-partition","text":"pvcreate /deb/sdb vgextend OPX /dev/sdb lvextend -l +100%FREE /dev/OPX/SYSROOT1 resize2fs /dev/mapper/OPX-SYSROOT1","title":"Extend Partition"},{"location":"Adding%20Hashing%20to%20OpenSwitch/#remote-desktop","text":"sudo apt update sudo apt install xfce4 xfce4-goodies xorg dbus-x11 x11-xserver-utils sudo apt install xrdp sudo adduser xrdp ssl-cert xfce4-session-logout --halt","title":"Remote Desktop"},{"location":"Adding%20Hashing%20to%20OpenSwitch/#on-centos-7","text":"yum install -y epel-release yum groupinstall -y \"Xfce\" echo \"xfce4-session\" > ~/.Xclients chmod a+x ~/.Xclients","title":"On CentOS 7"},{"location":"Adding%20Hashing%20to%20OpenSwitch/#set-up-vm","text":"","title":"Set Up VM"},{"location":"Adding%20Hashing%20to%20OpenSwitch/#failed-but-should-have-worked","text":"wget http://archive.openswitch.net/vm-tools/lvm chmod +x lvm wget https://dell-networking.bintray.com/opx-images/opx-onie-installer_1.1_amd64.bin wget https://dell-networking.bintray.com/opx-images/onie-kvm_x86_64-r0.iso wget https://archive.openswitch.net/installers/3.2.1/Dell-EMC/PKGS_OPX-3.2.1-installer-x86_64.bin ./lvm create openswitch --iso onie-kvm_x86_64-r0.iso --bin PKGS_OPX-3.2.1-installer-x86_64.bin","title":"Failed, but should have worked"},{"location":"Adding%20Hashing%20to%20OpenSwitch/#building-from-source","text":"Docs git clone https://github.com/opencomputeproject/onie.git cd onie/contrib/build-env/ docker build -t debian:build-env . mkdir --mode=0777 -p opt/src docker run -it -v /opt/src:/home/build/src --privileged --name onie debian:build-env In the container: ./clone-onie cd src/onie/build-config make -j4 MACHINE=kvm_x86_64 all","title":"Building from source"},{"location":"Adding%20Hashing%20to%20OpenSwitch/#build-an-installer","text":"See available distrubitions: opx-build/scripts/opx_run opx_rel_pkgasm.py --help After running a build you can run opx-build/scripts/opx_run opx_rel_pkgasm.py --dist unstable -b opx-onie-installer/release_bp/OPX_dell_base.xml to build an installer This looks like it should be the docker build command docker run --rm --name root_root_32994 --privileged -e LOCAL_UID=0 -e LOCAL_GID=0 -v /root:/mnt -v /root/.gitconfig:/home/opx/.gitconfig -v /etc/localtime:/etc/localtime:ro -e ARCH -e DIST -e OPX_RELEASE -e OPX_GIT_TAG -e CUSTOM_SOURCES opxhub/build:latest","title":"Build an installer"},{"location":"Adding%20Hashing%20to%20OpenSwitch/#investigation-into-the-opx-config-global-switch-problem","text":"cps_get_oid.py -qua target base-switch/switching-entities/switching-entity The question to ask is - what is the server application for the CPS object? The problem is that in opx-config-global-switch the value lag-hash-fields isn't present in target_attrs on line 214 of opx-config-global-switch The question is what would populate that? I don't think any of what I care about is in NAS L2. NAS L2 handles This repository contains the Layer 2 (L2) component of the network abstraction service (NAS). This handles media access control (MAC) learning, programming spanning-tree protocol (STP) state, mirroring, sFlow, and other switch configurations. I think what I care about is in opx-nas-interface because the operating system is handling the LAG. Description is This repository contains the interface portion of the network abstraction service (NAS). This creates interfaces in the Linux kernel corresponding to the network processor unit (NPU) front panel ports, manages VLAN and LAG configurations, statistics management and control packet handling. Logically there are three components including the LAG: LAG DS, NAS LAG, NDI LAG. See picture. NAS Daemon: The NAS daemon integrates standard Linux network APIs with NPU hardware functionality, and registers and listens to networking (netlink) events. What enum values in the model of dell-base-hash align with what's in opx-config-global-switch in hash fields map. What are all these actions in opx-config-global-switch? Ok - so now we know the thing I want is in base-traffic-hash/entry, but opx-config-global-switch is looking at base-switch/switching-entities/switching-entity . The next question is what populates that? Why are the hashes not in it? The definition for switching-entity is at: https://github.com/open-switch/opx-base-model/blob/abdf66f813b48a3c8e7682361cdacccd0271866d/yang-models/dell-base-switch-element.yang Say what? hash-fields is also defined in base-switch/switching-entities/switching-entity The next question to ask is who has ownership of base-traffic-hash? What about base-switch/switching-entities/switching-entity? The problem seems like it should be there? Why? base-traffic-hash is owned by opx_nas_daemon base-switch/switching-entities/switching-entity is also owned by opx_nas_daemon CPS has a REST service according to: https://github.com/open-switch/opx-cps The connection between YANG and the applications seems to be the CPS API? Applications define objects through (optionally YANG-based) object models. These object models are converted into binary (C accessible) object keys and object attributes that can be used in conjunction with the C-based CPS APIs. Description of cps_model_info: This tool is useful to get all information about CPS objects on the target. It is used to get the attributes of a specific CPS object, or first-level contents of a given YANG path of the CPS object (as defined in the YANG model). IT GIVES THE PROCESS OWNER! Investigating ops-nas-daemon There is a file called base_nas_default_init which defines the mirror port and the flow behaviors. I haven't found anything about other stuff yet. opx-config-global-switch --lag-hash-alg crc works and is owned by opx_nas_daemon - there must be other things it owns beside this. There is a file called hald_init.c . I think what is happening is all the other services fall under the NAS daemon. The code I'm looking for is somewhere else. After following that around for a while it looks like the file I'm really interested in is here https://github.com/open-switch/opx-nas-l2/blob/7e80d3952786f219b8072f1666ff1f16ba353d86/src/switch/nas_hash_cps.cpp . This bubbles up to the L2 init function, which bubbles back up to hald_init.c . dell-base-hash.h gets included in this thing. The YANG for the regular hash algorithm is at https://github.com/open-switch/opx-base-model/blob/abdf66f813b48a3c8e7682361cdacccd0271866d/yang-models/dell-base-switch-element.yang Initial investigation finished opened: https://github.com/open-switch/opx-nas-l2/issues/34","title":"Investigation into the opx-config-global-switch problem"},{"location":"Adding%20Hashing%20to%20OpenSwitch/#investigation-2-compilation-problems","text":"https://android.googlesource.com/platform/hardware/broadcom/wlan/+/master/bcmdhd/config/config-bcm.mk - examlpe config-bcm file. Seems to have something to do with broadcom's config. Better example configuration from Broadcom: http://broadcom-switch.github.io/OpenNSL/doc/html/OPENNSL_CUSTOMIZING_OPENNSL.html#OPENNSL_CONFIG_EXAMPLE Configuration properties: http://broadcom-switch.github.io/OpenNSL/doc/html/OPENNSL_CUSTOMIZING_OPENNSL.html#OPENNSL_CONFIG_PROPERTIES More info on config.bcm https://docs.broadcom.com/doc/12378910","title":"Investigation 2 - Compilation problems"},{"location":"Adding%20Hashing%20to%20OpenSwitch/#investigation-3-post-compilation-value-still-missing","text":"The hash variables seem to be passed by pointer from nas_hash_cps.cpp to nas_ndi_hash.c in line 222. The SAI portion of the hash creation seems to be here which could prove helpful later. In the bug at https://github.com/open-switch/opx-tools/issues/27 where he says obj-type that is referring to the YANG model. There is a leaf called obj-type . Traffic refers to: typedef traffic { type enumeration { enum \"ECMP_NON_IP\" { value 1; description \"ECMP routing: flow of non-IP ethernet frames\"; } enum \"LAG_NON_IP\" { value 2; description \"LAG routing: flow of non-IP ethernet frames\"; } enum \"ECMP_IPV4\" { value 3; description \"ECMP routing: flow of IPv4 packets\"; } enum \"ECMP_IPV4_IN_IPV4\" { value 4; description \"ECMP routing: flow of IPv4 packets with IPv4 packets tunnelled inside them\"; } enum \"ECMP_IPV6\" { value 5; description \"ECMP routing: flow of IPv6 packets\"; } enum \"LAG_IPV4\" { value 6; description \"LAG routing: flow of IPv4 packets\"; } enum \"LAG_IPV4_IN_IPV4\" { value 7; description \"ECMP routing: flow of IPv4 packets with IPv4 packets tunnelled inside them\"; } enum \"LAG_IPV6\" { value 8; description \"LAG routing: traffic flow is identified as IPv6 packets\"; } } description \"Enumeration of different types of traffic routing categories\"; } std-hash-field is a leaf-list with a type of field . field is defined as: typedef field { type enumeration { enum \"src-ip\" { value 1; description \"Traffic flow is identified by the source IP\"; } enum \"dest-ip\" { value 2; description \"Traffic flow is identified by the destination IP\"; } enum \"inner-src-ip\" { value 3; description \"Traffic flow is identified by the source IP of the tunnelled packet\"; } enum \"inner-dst-ip\" { value 4; description \"Traffic flow is identified by the destination IP of the tunnelled packet\"; } enum \"vlan-id\" { value 5; description \"Traffic flow is identified by the VLAN ID in the packet\"; } enum \"ip-protocol\" { value 6; description \"Traffic flow is identified by the IP protocol type(v4/v6)\"; } enum \"ethertype\" { value 7; description \"Traffic flow is identified by the IP protocol ether-type\"; } enum \"l4-src-port\" { value 8; description \"Traffic flow is identified by the source port in the packet\"; } enum \"l4-dest-port\" { value 9; description \"Traffic flow is identified by the destination port in the packet\"; } enum \"src-mac\" { value 10; description \"Traffic flow is identified by the source MAC address\"; } enum \"dest-mac\" { value 11; description \"Traffic flow is identified by the destination MAC address\"; } enum \"in-port\" { value 12; description \"Traffic flow is identified by the front-panel port the packet is received at\"; } } description \"Enumeration of different types of packet fields to check for routing\"; } - cps_get definition: def cps_get(q, obj, attrs={}): resp = [] return resp if cps.get([cps_object.CPSObject(obj, qual=q, data=attrs ).get() ], resp ) else None def cps_set(obj, qual, data): return (cps_utils.CPSTransaction([('set', cps_object.CPSObject(obj, qual=qual, data=data).get())]).commit() ) - cps_set('base-pas/led', 'target', {'entity-type': 3, 'slot': 1, 'name': 'Beacon', 'on': args.state}) - Set syntax: root@OPX:~# cps_set_oid.py -qua target -oper set -attr base-switch/entry/std-hash-field=1,2,8,9,6,5 base-switch/entry base-switch/entry/obj-type=6 - cps_get python syntax: cps_get('target', 'base-traffic-hash/entry', {'base-traffic-hash/entry/obj-type': '2'}) . That would get the object target from qualifier base-switch/entry which has attribute of base-switch/entry/obj-type (which is the key in entry) and we specifically want key 1 which is a type of traffic correspeonding to ECMP_NON_IP pp.pprint(cps_get('target', 'base-traffic-hash/entry', {'base-traffic-hash/entry/obj-type': '6'})) [ { 'data': { 'base-traffic-hash/entry/std-hash-field': [ bytearray(b'\\x01\\x00\\x00\\x00'), bytearray(b'\\x02\\x00\\x00\\x00'), bytearray(b'\\x08\\x00\\x00\\x00'), bytearray(b'\\t\\x00\\x00\\x00'), bytearray(b'\\x06\\x00\\x00\\x00')]}, 'key': '1.257.16842755.16842753.16842754.'}] CORRECT SYNTAX FOR CPS_SET: cps_set('base-traffic-hash/entry', 'target', {'base-traffic-hash/entry/obj-type': '6', 'base-traffic-hash/entry/std-hash-field': [1,2,8,9,6,5]})","title":"Investigation 3 - post compilation value still missing"},{"location":"Adding%20Hashing%20to%20OpenSwitch/#investigation-4-where-are-the-hash-values","text":"Part of the hash seems to be set in nas_ndi_switch.cpp There is also a unit test for it in sai_hash_unit_test.cpp This code seems to imply that the hashes themselves are implemented in the SAI: static _enum_map _algo_stoy = { {SAI_HASH_ALGORITHM_XOR, BASE_SWITCH_HASH_ALGORITHM_XOR }, {SAI_HASH_ALGORITHM_CRC, BASE_SWITCH_HASH_ALGORITHM_CRC }, {SAI_HASH_ALGORITHM_RANDOM, BASE_SWITCH_HASH_ALGORITHM_RANDOM }, {SAI_HASH_ALGORITHM_CRC_CCITT, BASE_SWITCH_HASH_ALGORITHM_CRC16CC }, {SAI_HASH_ALGORITHM_CRC_32LO, BASE_SWITCH_HASH_ALGORITHM_CRC32LSB }, {SAI_HASH_ALGORITHM_CRC_32HI, BASE_SWITCH_HASH_ALGORITHM_CRC32MSB }, {SAI_HASH_ALGORITHM_CRC_XOR8, BASE_SWITCH_HASH_ALGORITHM_XOR8 }, {SAI_HASH_ALGORITHM_CRC_XOR4, BASE_SWITCH_HASH_ALGORITHM_XOR4 }, {SAI_HASH_ALGORITHM_CRC_XOR2, BASE_SWITCH_HASH_ALGORITHM_XOR2 }, {SAI_HASH_ALGORITHM_CRC_XOR1, BASE_SWITCH_HASH_ALGORITHM_XOR1 }, }; static bool to_sai_type_hash_algo(sai_attribute_t *param ) { return to_sai_type(_algo_stoy,param); } static bool from_sai_type_hash_algo(sai_attribute_t *param ) { return from_sai_type(_algo_stoy,param); } There is a reference to each of these hash algorithms in https://github.com/open-switch/opx-base-model/blob/abdf66f813b48a3c8e7682361cdacccd0271866d/history/dell-base-switch-element.yhist From the SAI unit test I found: set_attr.value.s32 = SAI_HASH_ALGORITHM_XOR; status = switch_api_tbl_get()->set_switch_attribute (switch_id,&set_attr); EXPECT_EQ (SAI_STATUS_SUCCESS, status); This must be where they are setting the hash algorithm and how to set it. But where is this API? How do I find out what the options are? Is symmetric hashing exposed? The definition for the function is (it happens inside one of the unit tests): static inline sai_switch_api_t* switch_api_tbl_get (void) { return p_sai_switch_api_tbl; } This is a pointer to a table. I'm guessing it's a struct. sai_switch_api_t is short for SAI switch API table. It is a pointer to the table with all the API calls. From this I got a hint - it looks like there is a thing called switch_api - the same name is used in OpenSwitch. Based on the inputs from nas_ndi_hash.c there must be a series of SAI files with the functionality I want. #include \"std_error_codes.h\" #include \"std_assert.h\" #include \"nas_ndi_event_logs.h\" #include \"nas_ndi_utils.h\" #include \"dell-base-hash.h\" #include \"sai.h\" #include \"saiswitch.h\" #include \"saihash.h\" #include <stdio.h> #include <stdlib.h> #include <string.h> #include <inttypes.h>","title":"Investigation 4 - Where are the hash values?"},{"location":"Adding%20Hashing%20to%20OpenSwitch/#some-related-resources","text":"ECMP Hashing explained Broadcom Paper on Hashing Can I do this with openvswitch This didn't pan out. Broadcom docs","title":"Some related resources"},{"location":"Adding%20Hashing%20to%20OpenSwitch/#descriptions","text":"","title":"Descriptions"},{"location":"Adding%20Hashing%20to%20OpenSwitch/#nas","text":"The NAS manages the high-level NPU abstraction and adaptation, and abstracts and aggregates the core functionality required for networking access at Layer 1 (physical), Layer 2 (VLAN, link aggregation), Layer 3 (routing), ACL, QoS, and network monitoring. The NAS provides adaptation of the low-level switch abstraction provided by the SAI for standard Linux networking APIs and interfaces, and CPS API functionality. The NAS is also responsible for providing packet I/O services using the Linux kernel IP stack (see Network adaptation service for complete information).","title":"NAS"},{"location":"Adding%20Intel%20I219-LM%20%288086.0d4c%29%20Driver%20to%20ESXi/","text":"Adding Intel I219-LM (8086.0d4c) Driver to ESXi Download the following: 1. ne1000 VIB (offline bundle) 2. ESXi 7.0 GA Offline Bundle (VMware-ESXi-7.0.0-15843807-depot.zip 3. ESXi 7.0b patch Offline Bundle Open vCenter and go to Menu->Auto Deploy->Image Builder Import all three offline bundles with the following names which will be referenced: ESXi 7.0 GA, ESXi 7.0b and Intel Driver I originally received \"Your software depot \"Intel Driver\" failed to import. Try importing it again or choose another software depot to import.\" . Fix action: Leave the page, come back, delete the name of the failed import, and then reimport with the same name. I had to do it three times total and the third it worked. Select the ESXi 7.0b Software Depot and make a note of the build number for the specific Image Profile you wish to use. In this case, we will be using 16324942 which is the \"full\" image which includes both bug fix + security fix along with VMware Tools. If you only want the security fix, use 16321839. In the right hand corner, click New to create a new Custom Depot called I219-8086.0d4c and then create new Image Profile and provide a name, vendor and description of your choosing. On the Depot column, filter by ESXi 7.0b initially and select the following 13 packages as shown in the screenshot below. I used the version 7.0.0-1.25.16324942 . I just sorted by version as well and then grabbed nvme-pcie and vmkusb separately. cpu-microcode crx esx-base esx-dvfilter-generic-fastpath esx-update esx-xserver loadesx native-misc-drvers nvme-pcie vdfs vmkusb vsan vsanhealth On the Depot column, filter by Intel Driver and select the ne1000-intelnuc package On the Depot column, filter by ESXi 7.0 GA and select everything, EXCEPT for the 13 packages we had already selected from Step 6. At the end I had 73 packages. You can now export and either download the Image Profile either as a bootable ISO which can then be used for fresh installation and/or upgrade as well as using vSphere Update Manager. You can also download the Image Profile which can be used to update via ESXCLI on the ESXi Shell. Resources How to make your unsupported NIC work with ESXi 5.x or 6.0 https://www.virtuallyghetto.com/2020/01/esxi-on-10th-gen-intel-nuc-frost-canyon.html Enhancements to the community ne1000 VIB for Intel NUC 10","title":"Adding Intel I219-LM (8086.0d4c) Driver to ESXi"},{"location":"Adding%20Intel%20I219-LM%20%288086.0d4c%29%20Driver%20to%20ESXi/#adding-intel-i219-lm-80860d4c-driver-to-esxi","text":"Download the following: 1. ne1000 VIB (offline bundle) 2. ESXi 7.0 GA Offline Bundle (VMware-ESXi-7.0.0-15843807-depot.zip 3. ESXi 7.0b patch Offline Bundle Open vCenter and go to Menu->Auto Deploy->Image Builder Import all three offline bundles with the following names which will be referenced: ESXi 7.0 GA, ESXi 7.0b and Intel Driver I originally received \"Your software depot \"Intel Driver\" failed to import. Try importing it again or choose another software depot to import.\" . Fix action: Leave the page, come back, delete the name of the failed import, and then reimport with the same name. I had to do it three times total and the third it worked. Select the ESXi 7.0b Software Depot and make a note of the build number for the specific Image Profile you wish to use. In this case, we will be using 16324942 which is the \"full\" image which includes both bug fix + security fix along with VMware Tools. If you only want the security fix, use 16321839. In the right hand corner, click New to create a new Custom Depot called I219-8086.0d4c and then create new Image Profile and provide a name, vendor and description of your choosing. On the Depot column, filter by ESXi 7.0b initially and select the following 13 packages as shown in the screenshot below. I used the version 7.0.0-1.25.16324942 . I just sorted by version as well and then grabbed nvme-pcie and vmkusb separately. cpu-microcode crx esx-base esx-dvfilter-generic-fastpath esx-update esx-xserver loadesx native-misc-drvers nvme-pcie vdfs vmkusb vsan vsanhealth On the Depot column, filter by Intel Driver and select the ne1000-intelnuc package On the Depot column, filter by ESXi 7.0 GA and select everything, EXCEPT for the 13 packages we had already selected from Step 6. At the end I had 73 packages. You can now export and either download the Image Profile either as a bootable ISO which can then be used for fresh installation and/or upgrade as well as using vSphere Update Manager. You can also download the Image Profile which can be used to update via ESXCLI on the ESXi Shell.","title":"Adding Intel I219-LM (8086.0d4c) Driver to ESXi"},{"location":"Adding%20Intel%20I219-LM%20%288086.0d4c%29%20Driver%20to%20ESXi/#resources","text":"How to make your unsupported NIC work with ESXi 5.x or 6.0 https://www.virtuallyghetto.com/2020/01/esxi-on-10th-gen-intel-nuc-frost-canyon.html Enhancements to the community ne1000 VIB for Intel NUC 10","title":"Resources"},{"location":"Automating%20OME%20Hardware%20Reports/","text":"Automating OME Hardware Reports See https://github.com/grantcurell/omeinventory","title":"Automating OME Hardware Reports"},{"location":"Automating%20OME%20Hardware%20Reports/#automating-ome-hardware-reports","text":"See https://github.com/grantcurell/omeinventory","title":"Automating OME Hardware Reports"},{"location":"Backup%20OS10%20Config%20with%20Ansible/","text":"Backup OS10 Config with Ansible Overview The following describes how to both backup and deploy a config to OS10 using Ansible. Prerequisites Install Ansible on your device. Ensure that the Ansible server has IP connectivity to the management interface of all OS10 devices. Backup a Config On your Ansible server, copy the following Ansible playbook to a file called backup_config.yaml . --- - name: Setting up localhost for saving config hosts: localhost gather_facts: yes tasks: - block: - name: Generating backup folder name in localhost set_fact: target_folder: \"{{ backup_folder }}/{{ ansible_date_time.iso8601 }}\" - name: Create config_backup folder file: path: \"{{ target_folder }}\" state: directory - debug: msg: \"Config files are to be backed up at {{target_folder}}\" delegate_to: localhost run_once: True - name: Backup os10 running configurations hosts: os10 gather_facts: False connection: network_cli tasks: - name: Fetch OS10 running configuration dellos10_command: commands: show running register: sh_runn - name: Save config to file copy: content: \"{{ sh_runn.stdout | replace('\\\\n', '\\n') }}\" dest: \"{{hostvars.localhost.target_folder}}/{{inventory_hostname}}_os10_show_run\" delegate_to: localhost - debug: msg: \"Config files are to be backed up at {{hostvars.localhost.target_folder}}\" run_once: true Next, create a separate file called inventory.yaml with the following contents: all: vars: backup_folder: \"/root/backup\" children: os10: hosts: 192.168.1.169: ansible_become: 'yes' ansible_become_method: enable ansible_command_timeout: 120 ansible_connection: ansible.netcommon.network_cli ansible_network_os: dellemc.os10.os10 ansible_password: admin ansible_user: admin 192.168.1.170: ansible_become: 'yes' ansible_become_method: enable ansible_command_timeout: 120 ansible_connection: ansible.netcommon.network_cli ansible_network_os: dellemc.os10.os10 ansible_password: admin ansible_user: admin Replace the IPs, backup_folder, ansible_password, and ansible_user variables above with the variables for your hosts. I am demoing the most simplistic configuration above. There are ways to use more generally scoped variables so that the password doesn't have to repeated, use SSH keys instead of cleartext passwords, place passwords in a vault, etc, but I do not demo that here. See the module parameter provider for details on this. Finally, to run the backup run ansible-playbook -i inventory.yaml backup_config.yaml . A clean run will look like this: ansible-playbook -i inventory.yaml backup_config.yaml PLAY [Setting up localhost for saving config] ********************************************************************************************************************************************************** TASK [Gathering Facts] ********************************************************************************************************************************************************************************* ok: [localhost] TASK [Generating backup folder name in localhost] ****************************************************************************************************************************************************** ok: [localhost -> localhost] TASK [Create config_backup folder] ********************************************************************************************************************************************************************* changed: [localhost -> localhost] TASK [debug] ******************************************************************************************************************************************************************************************* ok: [localhost -> localhost] => { \"msg\": \"Config files are to be backed up at /root/backup/2022-09-07T20:06:17Z\" } PLAY [Backup os10 running configurations] ************************************************************************************************************************************************************** TASK [Fetch OS10 running configuration] **************************************************************************************************************************************************************** [DEPRECATION WARNING]: Distribution fedora 35 on host 192.168.1.169 should use /usr/bin/python3, but is using /usr/bin/python for backward compatibility with prior Ansible releases. A future Ansible release will default to using the discovered platform python for this host. See https://docs.ansible.com/ansible/2.9/reference_appendices/interpreter_discovery.html for more information. This feature will be removed in version 2.12. Deprecation warnings can be disabled by setting deprecation_warnings=False in ansible.cfg. ok: [192.168.1.169] TASK [Save config to file] ***************************************************************************************************************************************************************************** changed: [192.168.1.169 -> localhost] TASK [debug] ******************************************************************************************************************************************************************************************* ok: [192.168.1.169] => { \"msg\": \"Config files are to be backed up at /root/backup/2022-09-07T20:06:17Z\" } PLAY RECAP ********************************************************************************************************************************************************************************************* 192.168.1.169 : ok=3 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 localhost : ok=4 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 The results will appear, in my configuration, in the folder /root/backup/2022-09-07T20:06:17Z . [root@fedora ~]# ls -al /root/backup/2022-09-07T20:06:17Z total 4 drwxr-xr-x 1 root root 54 Sep 7 16:06 . drwxr-xr-x 1 root root 240 Sep 7 16:06 .. -rw-r--r-- 1 root root 2169 Sep 7 16:06 192.168.1.169_os10_show_run","title":"Backup OS10 Config with Ansible"},{"location":"Backup%20OS10%20Config%20with%20Ansible/#backup-os10-config-with-ansible","text":"","title":"Backup OS10 Config with Ansible"},{"location":"Backup%20OS10%20Config%20with%20Ansible/#overview","text":"The following describes how to both backup and deploy a config to OS10 using Ansible.","title":"Overview"},{"location":"Backup%20OS10%20Config%20with%20Ansible/#prerequisites","text":"Install Ansible on your device. Ensure that the Ansible server has IP connectivity to the management interface of all OS10 devices.","title":"Prerequisites"},{"location":"Backup%20OS10%20Config%20with%20Ansible/#backup-a-config","text":"On your Ansible server, copy the following Ansible playbook to a file called backup_config.yaml . --- - name: Setting up localhost for saving config hosts: localhost gather_facts: yes tasks: - block: - name: Generating backup folder name in localhost set_fact: target_folder: \"{{ backup_folder }}/{{ ansible_date_time.iso8601 }}\" - name: Create config_backup folder file: path: \"{{ target_folder }}\" state: directory - debug: msg: \"Config files are to be backed up at {{target_folder}}\" delegate_to: localhost run_once: True - name: Backup os10 running configurations hosts: os10 gather_facts: False connection: network_cli tasks: - name: Fetch OS10 running configuration dellos10_command: commands: show running register: sh_runn - name: Save config to file copy: content: \"{{ sh_runn.stdout | replace('\\\\n', '\\n') }}\" dest: \"{{hostvars.localhost.target_folder}}/{{inventory_hostname}}_os10_show_run\" delegate_to: localhost - debug: msg: \"Config files are to be backed up at {{hostvars.localhost.target_folder}}\" run_once: true Next, create a separate file called inventory.yaml with the following contents: all: vars: backup_folder: \"/root/backup\" children: os10: hosts: 192.168.1.169: ansible_become: 'yes' ansible_become_method: enable ansible_command_timeout: 120 ansible_connection: ansible.netcommon.network_cli ansible_network_os: dellemc.os10.os10 ansible_password: admin ansible_user: admin 192.168.1.170: ansible_become: 'yes' ansible_become_method: enable ansible_command_timeout: 120 ansible_connection: ansible.netcommon.network_cli ansible_network_os: dellemc.os10.os10 ansible_password: admin ansible_user: admin Replace the IPs, backup_folder, ansible_password, and ansible_user variables above with the variables for your hosts. I am demoing the most simplistic configuration above. There are ways to use more generally scoped variables so that the password doesn't have to repeated, use SSH keys instead of cleartext passwords, place passwords in a vault, etc, but I do not demo that here. See the module parameter provider for details on this. Finally, to run the backup run ansible-playbook -i inventory.yaml backup_config.yaml . A clean run will look like this: ansible-playbook -i inventory.yaml backup_config.yaml PLAY [Setting up localhost for saving config] ********************************************************************************************************************************************************** TASK [Gathering Facts] ********************************************************************************************************************************************************************************* ok: [localhost] TASK [Generating backup folder name in localhost] ****************************************************************************************************************************************************** ok: [localhost -> localhost] TASK [Create config_backup folder] ********************************************************************************************************************************************************************* changed: [localhost -> localhost] TASK [debug] ******************************************************************************************************************************************************************************************* ok: [localhost -> localhost] => { \"msg\": \"Config files are to be backed up at /root/backup/2022-09-07T20:06:17Z\" } PLAY [Backup os10 running configurations] ************************************************************************************************************************************************************** TASK [Fetch OS10 running configuration] **************************************************************************************************************************************************************** [DEPRECATION WARNING]: Distribution fedora 35 on host 192.168.1.169 should use /usr/bin/python3, but is using /usr/bin/python for backward compatibility with prior Ansible releases. A future Ansible release will default to using the discovered platform python for this host. See https://docs.ansible.com/ansible/2.9/reference_appendices/interpreter_discovery.html for more information. This feature will be removed in version 2.12. Deprecation warnings can be disabled by setting deprecation_warnings=False in ansible.cfg. ok: [192.168.1.169] TASK [Save config to file] ***************************************************************************************************************************************************************************** changed: [192.168.1.169 -> localhost] TASK [debug] ******************************************************************************************************************************************************************************************* ok: [192.168.1.169] => { \"msg\": \"Config files are to be backed up at /root/backup/2022-09-07T20:06:17Z\" } PLAY RECAP ********************************************************************************************************************************************************************************************* 192.168.1.169 : ok=3 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 localhost : ok=4 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 The results will appear, in my configuration, in the folder /root/backup/2022-09-07T20:06:17Z . [root@fedora ~]# ls -al /root/backup/2022-09-07T20:06:17Z total 4 drwxr-xr-x 1 root root 54 Sep 7 16:06 . drwxr-xr-x 1 root root 240 Sep 7 16:06 .. -rw-r--r-- 1 root root 2169 Sep 7 16:06 192.168.1.169_os10_show_run","title":"Backup a Config"},{"location":"CloudLink/","text":"CloudLink CloudLink General Functioning Components Licensing General Licenses License Types Installation Notes Connecting VMWare to CloudLink Set Up CloudLink Configure vCenter Enabling Encryption vSAN My Outstanding Questions General Functioning CloudLink provides encryption by using Microsoft BitLocker and dm-crypt ( How does dm-crypt work )for Linux to provide encryption. CloudLink's VM encryption functionality enables you to use native OS encryption features to encrypt a machine's boot and data volumes in a multi tenant cloud environment. This encryption enables you to protect the integrity of the machine itself against unauthorized modifications. CloudLink encrypts the machine boot and data volumes with unique keys that enterprise security administrators control. Neither cloud administrators nor other tenants in the cloud have access to the keys. By securing machines, you can define the security policy that must be met before passing the prestartup authorization, including verifying the integrity of the machine\u2019s boot chain. This offers protection against tampering. Components CloudLink Center\u2014The web-based interface for CloudLink that is used to manage machines that belong to the CloudLink environment (those machines on which CloudLink Agent has been installed). CloudLink Center: Communicates with machines over Transport Layer Security (TLS) Manages the encryption keys that are used to secure the boot volumes, data volumes, and devices for the machines Configures the security policies Monitors the security and operation events Collects log data CloudLink Agent - The agent that runs on individual machines. It communicates with CloudLink Center for pre-startup authorization and decryption of BitLocker or dm-crypt encryption keys. For Enterprise and PowerFlex\u2014CloudLink Center is packaged as a virtual appliance that can be deployed in the enterprise on VMware ESXi or Microsoft Hyper-V. Download CloudLink Agent from CloudLink Center. For Microsoft Azure or Azure Stack\u2014CloudLink Center can be deployed from the Azure Gallery in a simple-to-deploy, self-contained image file that enables you to quickly start your business-critical operations by using CloudLink. Search the Azure Gallery for CloudLink to locate the image. Download CloudLink Agent from CloudLink Center Licensing General Licenses Evaluation license\u2014This is a free trial license to test the CloudLink features. This license has an expiry date and is not allowed to be used in production. Use a subscription or a perpetual license that is purchased through Dell EMC for production purposes. Subscription license\u2014This license expires on a predefined date and time. The subscription license period is for one, two, or three years only. Repurchase the subscription licenses at the end of their term. Perpetual license\u2014This license never expires. License Types Encryption for Machines license\u2014Licensed per machine for volume encryption. This license defines the number of machines, virtual, or bare metal, that can be protected using CloudLink Center. Encryption for Containers license\u2014Enables data encryption for containers. A single Container license supports any number of Kubernetes clusters. Encryption for PowerFlex license\u2014Encrypted capacity for PowerFlex This license defines the total storage that can be encrypted using CloudLink Center. Key Management over KMIP license\u2014Licensed KMIP clients This license defines the number of KMIP clients that can be managed using CloudLink Center. With one Key Management over KMIP license you can create: One KMIP Client One CloudLink Center cluster NOTE: To create additional KMIP Clients or CloudLink Center clusters, purchase additional Key Management over KMIP licenses. Key Management for SED license\u2014Number of physical machines with SEDs. A single Key Management for SED license is used per physical machine regardless of the number of SEDs connected to that machine Installation Notes I followed the guide here for VMWare https://docs.delltechnologies.com/bundle/P_DG_CL_701/page/GUID-1EDFBE27-3218-43AD-9449-24374D5FE1F6.html The default user for the webui is secadmin Before I added VMs to my cluster I created a machine group to put them in Make sure you add approved networks with IP ranges before installation or adding machines will fail with IP address (192.168.1.95) not in group's approved networks Alternatively you will have to go to machines and accept the machine To install in standard mode on Linux run sudo ./clagent.sh -S 192.168.1.86 -G cf41-f71e where -S specifies the server and -G is the group key Connecting VMWare to CloudLink WARNING This option requires you to use the KMIP license. If you use another license the below indicated options will not be present. Connection to vSphere / VMWare works through a protocol called KMIP. KMIP is a protocol for communicating key information between key management servers and key management clients. Broadly speaking there are two ways which CloudLink can function in a VMWare environment: You can set up the CloudLink Center server and it can encrypt the virtual hard disks used by the various VMs. This is accomplished by running the CloudLink Agent on the individual VMs themselves. You can connect Cloudlink Center directly to vSphere and vSphere will instead use it to encrypt the VMs themselves. You can verify that CloudLink is compatible with the version of VMWare you are going to use here . See Dell EMC CloudLink Key Management for VMware vCenter Server Configuration Guide for an overview of configuring VMWare vCenter with CloudLink. Set Up CloudLink Note The use of Chinese characters is deliberate. I do this to ensure that software I test correctly handles character encodings other than ASCII. It is surprising how many modern software applications fail to handle UTF8 and other character encodings properly which is frequently an indicator of poor design quality. A KMIP partition is a container for the keys and certificates that are created by some KMIP client. Multiple clients can use the same partition but then keep in mind they will also be mutually accessible. See https://docs.delltechnologies.com/bundle/P_AG_CL/page/GUID-3A10CBF2-07DA-4951-96F3-2A7008390F55.html After that I added a KMIP client: Configure vCenter First we have to establish trust with the KMS server by make the KMS trust vCenter. After clicking the above select KMS certificate and private key as shown below. In KMS certificate upload cert.pem (which you got from setting up the client in CloudLink) and in KMS private key upload key.pem. Upload the certificate you downloaded earlier here (it should be called ca.pem). I didn't have to do this in my setup. Enabling Encryption Encryption is controlled by storage policy so you can set it as you would on any other object. Right click on a VM Click Edit Storage Policies Now you can change it to VM Encryption Policy vSAN To enable encryption on vSAN do this: https://docs.vmware.com/en/VMware-vSphere/6.7/com.vmware.vsphere.virtualsan.doc/GUID-E7CA36B7-D7EB-423A-ADD1-7E410E36F5A7.html My Outstanding Questions What is vault mode? What are these three unique manual passcodes? Can it encrypt data in motion? How does the licensing work if you have multiple clusters?","title":"CloudLink"},{"location":"CloudLink/#cloudlink","text":"CloudLink General Functioning Components Licensing General Licenses License Types Installation Notes Connecting VMWare to CloudLink Set Up CloudLink Configure vCenter Enabling Encryption vSAN My Outstanding Questions","title":"CloudLink"},{"location":"CloudLink/#general-functioning","text":"CloudLink provides encryption by using Microsoft BitLocker and dm-crypt ( How does dm-crypt work )for Linux to provide encryption. CloudLink's VM encryption functionality enables you to use native OS encryption features to encrypt a machine's boot and data volumes in a multi tenant cloud environment. This encryption enables you to protect the integrity of the machine itself against unauthorized modifications. CloudLink encrypts the machine boot and data volumes with unique keys that enterprise security administrators control. Neither cloud administrators nor other tenants in the cloud have access to the keys. By securing machines, you can define the security policy that must be met before passing the prestartup authorization, including verifying the integrity of the machine\u2019s boot chain. This offers protection against tampering.","title":"General Functioning"},{"location":"CloudLink/#components","text":"CloudLink Center\u2014The web-based interface for CloudLink that is used to manage machines that belong to the CloudLink environment (those machines on which CloudLink Agent has been installed). CloudLink Center: Communicates with machines over Transport Layer Security (TLS) Manages the encryption keys that are used to secure the boot volumes, data volumes, and devices for the machines Configures the security policies Monitors the security and operation events Collects log data CloudLink Agent - The agent that runs on individual machines. It communicates with CloudLink Center for pre-startup authorization and decryption of BitLocker or dm-crypt encryption keys. For Enterprise and PowerFlex\u2014CloudLink Center is packaged as a virtual appliance that can be deployed in the enterprise on VMware ESXi or Microsoft Hyper-V. Download CloudLink Agent from CloudLink Center. For Microsoft Azure or Azure Stack\u2014CloudLink Center can be deployed from the Azure Gallery in a simple-to-deploy, self-contained image file that enables you to quickly start your business-critical operations by using CloudLink. Search the Azure Gallery for CloudLink to locate the image. Download CloudLink Agent from CloudLink Center","title":"Components"},{"location":"CloudLink/#licensing","text":"","title":"Licensing"},{"location":"CloudLink/#general-licenses","text":"Evaluation license\u2014This is a free trial license to test the CloudLink features. This license has an expiry date and is not allowed to be used in production. Use a subscription or a perpetual license that is purchased through Dell EMC for production purposes. Subscription license\u2014This license expires on a predefined date and time. The subscription license period is for one, two, or three years only. Repurchase the subscription licenses at the end of their term. Perpetual license\u2014This license never expires.","title":"General Licenses"},{"location":"CloudLink/#license-types","text":"Encryption for Machines license\u2014Licensed per machine for volume encryption. This license defines the number of machines, virtual, or bare metal, that can be protected using CloudLink Center. Encryption for Containers license\u2014Enables data encryption for containers. A single Container license supports any number of Kubernetes clusters. Encryption for PowerFlex license\u2014Encrypted capacity for PowerFlex This license defines the total storage that can be encrypted using CloudLink Center. Key Management over KMIP license\u2014Licensed KMIP clients This license defines the number of KMIP clients that can be managed using CloudLink Center. With one Key Management over KMIP license you can create: One KMIP Client One CloudLink Center cluster NOTE: To create additional KMIP Clients or CloudLink Center clusters, purchase additional Key Management over KMIP licenses. Key Management for SED license\u2014Number of physical machines with SEDs. A single Key Management for SED license is used per physical machine regardless of the number of SEDs connected to that machine","title":"License Types"},{"location":"CloudLink/#installation-notes","text":"I followed the guide here for VMWare https://docs.delltechnologies.com/bundle/P_DG_CL_701/page/GUID-1EDFBE27-3218-43AD-9449-24374D5FE1F6.html The default user for the webui is secadmin Before I added VMs to my cluster I created a machine group to put them in Make sure you add approved networks with IP ranges before installation or adding machines will fail with IP address (192.168.1.95) not in group's approved networks Alternatively you will have to go to machines and accept the machine To install in standard mode on Linux run sudo ./clagent.sh -S 192.168.1.86 -G cf41-f71e where -S specifies the server and -G is the group key","title":"Installation Notes"},{"location":"CloudLink/#connecting-vmware-to-cloudlink","text":"WARNING This option requires you to use the KMIP license. If you use another license the below indicated options will not be present. Connection to vSphere / VMWare works through a protocol called KMIP. KMIP is a protocol for communicating key information between key management servers and key management clients. Broadly speaking there are two ways which CloudLink can function in a VMWare environment: You can set up the CloudLink Center server and it can encrypt the virtual hard disks used by the various VMs. This is accomplished by running the CloudLink Agent on the individual VMs themselves. You can connect Cloudlink Center directly to vSphere and vSphere will instead use it to encrypt the VMs themselves. You can verify that CloudLink is compatible with the version of VMWare you are going to use here . See Dell EMC CloudLink Key Management for VMware vCenter Server Configuration Guide for an overview of configuring VMWare vCenter with CloudLink.","title":"Connecting VMWare to CloudLink"},{"location":"CloudLink/#set-up-cloudlink","text":"Note The use of Chinese characters is deliberate. I do this to ensure that software I test correctly handles character encodings other than ASCII. It is surprising how many modern software applications fail to handle UTF8 and other character encodings properly which is frequently an indicator of poor design quality. A KMIP partition is a container for the keys and certificates that are created by some KMIP client. Multiple clients can use the same partition but then keep in mind they will also be mutually accessible. See https://docs.delltechnologies.com/bundle/P_AG_CL/page/GUID-3A10CBF2-07DA-4951-96F3-2A7008390F55.html After that I added a KMIP client:","title":"Set Up CloudLink"},{"location":"CloudLink/#configure-vcenter","text":"First we have to establish trust with the KMS server by make the KMS trust vCenter. After clicking the above select KMS certificate and private key as shown below. In KMS certificate upload cert.pem (which you got from setting up the client in CloudLink) and in KMS private key upload key.pem. Upload the certificate you downloaded earlier here (it should be called ca.pem). I didn't have to do this in my setup.","title":"Configure vCenter"},{"location":"CloudLink/#enabling-encryption","text":"Encryption is controlled by storage policy so you can set it as you would on any other object. Right click on a VM Click Edit Storage Policies Now you can change it to VM Encryption Policy","title":"Enabling Encryption"},{"location":"CloudLink/#vsan","text":"To enable encryption on vSAN do this: https://docs.vmware.com/en/VMware-vSphere/6.7/com.vmware.vsphere.virtualsan.doc/GUID-E7CA36B7-D7EB-423A-ADD1-7E410E36F5A7.html","title":"vSAN"},{"location":"CloudLink/#my-outstanding-questions","text":"What is vault mode? What are these three unique manual passcodes? Can it encrypt data in motion? How does the licensing work if you have multiple clusters?","title":"My Outstanding Questions"},{"location":"Common%20Questions%20About%20LLMs%20Answered/","text":"Common Questions About AI/ML and Large Language Models (LLMs) Answered I receive a lot of questions about AI/ML and LLMs at work particularly after the advent of ChatGPT so I thought I would leave behind all the math and answer some of the most common questions I receive in plain English. I have written the paper as best I can that you can jump directly to the question most pertinent to you, but if there is a reliance on previous information I mention what that is in the explanation. Common Questions About AI/ML and Large Language Models (LLMs) Answered What Is Artificial Intelligence (AI) / Machine Learning (ML) What is the Difference Between AI and ML? What is Training and Inferencing? What is a large language model (LLM)? What is a Hyperparameter? What is Model Size? What Does it Mean to Tune a Ducking Model? How do I Choose a Model? Are Bigger Models Better? A Concrete Example with ChatGPT So Which was Better ChatGPT 3.5 or ChatGPT 4? What is Over-fitting? How Much Computing Power Do I Need? Biggest Factor Primary Questions Secondary Questions Some Reference Points A General Rule of Thumb for GPT Runtime Some Estimations on Real Hardware What Is Artificial Intelligence (AI) / Machine Learning (ML) The words intelligence and learning here are extremely misleading to the average person. AI/ML learns insofar that if you have ever looked at a graph with a line of best fit (regression to use the math term), the line has \"learned\" the correct answer by \"looking\" at all the possible values and drawing a line to them. Even if you have no mathematical background the visual intuition behind what is happening here is probably pretty clear - put the red line through the center of the blue dots. Source Code Linear regression is a type of supervised machine learning. Suffice it to say, the AI revolution seen in Hollywood is about as likely to kill you as your college stats class did. Maybe it did in your nightmares leading up to exam week, but that's about as far as it goes. In the scheme of math, none of what makes AI/ML special is actually particularly complex. Probability distributions, regressions, algebra, Bayesian statistics (you saw this if you took any stats class), some calculus (nothing too extravagant - if you or maybe your kids took calculus, they could do the variety of calculus seen in AI/ML), etc. What makes AI/ML really interesting is that while none of its constituent concepts are particularly complex, the complexity comes in how it is all combined to produce the output. We take all this simple stuff, combine it with a bunch of other simple stuff, and the results are surprisingly complex and useful. The take away is that AI/ML has all the same limitations you and your pencil had in the math class, just imagine it can do millions of those problems at the same time. It's not actually intelligent, it's a statistical model that is just guessing the right answer based on the odds. This is true even of things as magical looking as ChatGPT. What is the Difference Between AI and ML? Semantics. Ask 50 people get 50 definitions. Honestly, it's a fairly arbitrary line so the exact distinction isn't really all that important but generally : AI (Artificial Intelligence): AI is a broader field that aims to create machines or systems that can perform tasks that typically require human intelligence. These tasks include reasoning, problem-solving, understanding natural language, recognizing patterns, and making decisions. AI encompasses a wide range of techniques and approaches, including machine learning. ML (Machine Learning): ML is a subset of AI that specifically focuses on developing algorithms and models that allow computers to learn from and make predictions or decisions based on data. ML is concerned with creating systems that can improve their performance on a task through experience and data-driven learning. The definitions also keep changing as technology improves. 40 years ago a series of if/then statements was called AI, but now they're just if/then statements. In the end, it really doesn't much matter to actually performing useful work or communicating yourself. What is Training and Inferencing? In machine learning we talk a lot about training and inferencing but what does that mean? Machine learning / AI generally break down into two phases: the training phase and the inferencing phase. Training is when you take some large body of pre-existing data, feed it into a bunch of math, and that math produces a series of equations which ultimately guess the answer to some question you have. The inferencing phase is when you start feeding real world, previously unseen, data into the model and it produces answers based on the training. You've been through this yourself in school. The training phase is you doing your homework, studying, and preparing for an exam. The inference phase is your taking that exam which usually has hereunto unseen questions, and then you generate answers based on the data you trained on. What is a large language model (LLM)? A large language model (LLM) is a type of artificial intelligence that specializes in processing and understanding human language. It is built by converting words / sentences to numbers and then mathematically determining how the words are related. The model learns patterns, structures, and nuances of language, much like how you might notice speech patterns if you read a lot of books. Have you ever been reading a book and thought, \"Oh I know what comes next.\" That thought you had is exactly how LLMs work. Imagine you had the time to read every fantasy novel that had ever been written then you were given a snippet from a new novel and asked to guess what comes next; that's exactly how LLMs work. I guarantee you have seen this technology before. Have you ever been typing on your phone and it suggests the next word? Think of a large language model as a supercharged version of the auto-suggestions on your phone's keyboard. Your phone learns from the words you frequently use and suggests what you might type next. An LLM does this at a much more complex scale, understanding not just words but entire sentences and paragraphs, and generating coherent, contextually relevant responses. However, just like your phone's keyboard doesn't 'understand' what you're typing, LLMs don't truly 'understand' language in the human sense. They're statistical machines. Given a prompt, they generate responses based on the patterns they've learned from their training data. In essence, a large language model is a sophisticated tool for mimicking human-like text based on the probability of certain words and sentences following others, based on its training data. It's a product of combining many simple concepts from mathematics and computer science, but the sheer scale of data and computations involved creates something that can seem quite complex and intelligent on the surface. What is a Hyperparameter? All of these AI/ML models have what are called hyperparameters. This is just a fancy word for a number we can tweak in our math. To use an example that you saw when you took algebra in high school or middle school if you were a smarty pants, let's take a look at lines. A line has the form $a*x+b=Y$. $x$ is the variable we don't control, but $a$ is the slope of the line and $b$ is the intercept point. $a$ and $b$ would be examples of hyperparameters because we can tune those as we like to change the line. For example, here is what it look like when we change the values of $b$. Source Code Now what if we change $a$? Source Code So how does that relate to machine learning? Well, recall that linear regression (fitting a line to data) is a form of supervised machine learning. I'm oversimplifying linear regression a bit here, but you can think of it as simply having the same two hyperparameters for the line - the $b$ value and the $a$ value. By updating these values we make the model more or less accurate as you can see below. Source Code What is Model Size? Now that you understand hyperparameters we can talk about model size. In our linear regression example (simplified), the model size is just two. You can adjust either $a$ or $b$. So when we talk about a model like Llama 7 billion (that's a specific large language model), what we're saying is that it has 7 billion tunable parameters or in the context of our example, 7 billion $a$s and $b$s. Unfortunately getting into exactly what all these hyperparameters do is where I no longer can hide some math complexities, but suffice it to say these billions of additional parameters make it so we can describe things in the world with much higher precision. Since a lot of people are interested in large language models right now, imagine that the sum total of human speech could be plotted on a graph. Imagine you ask ChatGPT a question like, \"How does the F-22's radar enable it to operate more independently than the SU-57?\" (I've been watching a lot of Max Afterburner lately. This particular thought came from this video ). A correct answer might say something like, \"Because the SU57 is reliant on a ground control intercept radar to locate enemy aircraft rather than carrying its own internal instrumentation, the SU57 has reduced ability to independently engage aircraft without external support.\" Another, worse, possible answer is, \"The SU57 is a bad airplane.\" Obviously, the second leaves a lot to be desired. Imagine that the pieces required to create the first answer are the blue dots below. The purple line represents a model with six hyperparameters, the green line is only four hyperparameters, and the red dotted line is just our linear regression. Source Code How the math works doesn't matter for the explanation. What matters is that now we have six hyperparametrs - which are the $b$ variables shown in this equation: $Y = b_0 + b_1X + b_2X^2 + b_3X^3 + b_4X^4 + b_5X^5$. As you can imagine the four hyperparameter model only goes up to $b_3$ (we started counting at zero because we're computer people). Those extra tunable parameters combined with picking a better algorithm for our use case allow us to create much better answers for this particular data set. This does not mean more hyperparameters is always better though. I'll explain that later. For this dataset though it definitely was. In this example you can imagine that these are how the answers line up: Question: \"How does the F-22's radar enable it to operate more independently than the SU-57?\" Answers: - Purple Line: \"Because the SU57 is reliant on a ground control intercept radar to locate enemy aircraft rather than carrying its own internal instrumentation, the SU57 has reduced ability to independently engage aircraft without external support.\" - Green Line \"The SU57 is a bad airplane.\" - Red Line: \"The airplane banana.\" What Does it Mean to Tune a Ducking Model? In our discussion of hyperparameters you saw tuning. This is what it looked like to tune the $a$ parameter of a line: But what other kinds of tuning are there and how does that relate to the LLMs we see? Have you ever become frustrated with your ducking phone? It is a ducking phone because the engineers over at Apple decided that certain words should have a reduced autocorrect priority even if they are statistically far more likely. This is a manual intervention in what the math would have natively produced as the alternative word is clearly far more likely in the given sentence. If you ever have tried to get ChatGPT to answer a sordid inquiry you will have seen more examples of tuning in action: Obviously, without those guards in place, ChatGPT would have done a great job of summarizing every murder related piece of material it had ever found on the Internet. Not a desirable behavior. Tuning is changing the answers by either updating the hyperparameters or adding some sort of external logic to your model to change the results. How do I Choose a Model? This is where PHDs in math, machine learning, data science, etc make their money. This is an extremely complex question that unfortunately has no simple, high level, explanation. What I will do here is demonstrate the difference in laymen's terms between the different models so you get a feel for just how big a difference model selection can make. Let's say we want to predict housing prices based only on the total size of the home and the number of rooms. We will pick two models - one is a linear regression and the other is k-means (it isn't important that you understand what this is). I wrote a program that generates some arbitrary data that represents the housing prices. What matters is that there is a linear relationship between the price of the house and the size/# rooms making this perfect for a linear regression. Here is what it looks like when you predict the price with a linear regression vs k-means: Source Code As you can see the linear regression nails it and k-means misses by some margin. This is the power of picking the right model. I wish I had some simple answer on how to pick the right model, but there just isn't one. You have to understand your problem and you have to know the math. Though I'll tell you that if you give ChatGPT your problem it usually does a good job at guessing a set of models that would perform reasonably. Automated tools are also now on the market that are built by teams of PHDs that try multiple different models for some set of data, score them, and then present the best results. These still don't replace data scientists, but they do make it so your existing data science teams are spending less time on infrastructure and minutia and more time on your mission target where you want them. Are Bigger Models Better? This is extremely difficult to explain because the answer is - it depends. A lot. Again, this is where PHDs in this stuff make their money. You have actually already seen the answer in How Do I Choose a Model? . k-means has significantly more hyperparameters than linear regression and its model size you can think of being significantly larger however it produced significantly worse results. So bigger is worse then? Well... no. In How do I Choose a Model I picked an example where more complex performed comparatively worse. In What is Model Size we saw an example of how more hyperparameters improved the performance of the model. These nuances can be extrapolated to the most complex models of today and the impact of the size of the model on the outcomes are unique to the exact model you are using. So the question you need to ask isn't \"Are bigger models better\" it is \"Given some very specific model and my very specific use case, are bigger models better?\" . If you phrase the question in a way any less precisely than that you will not receive an accurate answer. A Concrete Example with ChatGPT In the interest of providing a more nuanced example, here is a personal comparison between ChatGPT 3.5 and ChatGPT 4. ChatGPT 3.5 has roughly 175 billion parameters. ChatGPT 4's algorithm isn't public but suffice it to say it's a safe assumption that it's significantly more. I am a huge lover of languages. I speak Mandarin, Spanish, and English (obviously), but I am currently learning Norwegian. The problem I'm solving is that I want to learn new Norwegian words while keeping my Spanish and Mandarin vocab current. To that end, I updated my Norwegian Flashcard program to leverage OpenAI's APIs to get answers regarding potential translations from both ChatGPT 3.5 and ChatGPT 4. The answers, particularly with regards to Mandarin, are quite interesting because while Norwegian (Germanic), Spanish (Romance), and English (Germanic - technically) are pretty similar - Mandarin is very different. In fact, while linguists hypothesize about a proto-world language, we don't have any idea what Mandarin and English's closest relative is. Generated with ChatGPT4 in case you're wondering about the funny formatting. Why am I babbling about my language hobby? Because that difference means that translating them becomes much harder with many more possible translations which means the spread of answers from language models is also wider. In the scope of GPT models specifically , smaller models have fewer data points to understand context and language semantics so you will generally receive correspondingly \"worse\" answers. However, worse is an extremely nuanced concepts. Insofar as a smaller model is more likely to produce less precise responses, this may appear to give a bigger sense of randomness and subsequently creativity. In the context of my language program, the responses from 3.5 are much more scattered and sometimes do capture interesting things whereas GPT4 is more succinct and generally more accurate in the context of the prompt. Example: Norwegian Word: \u00e5 overf\u00f8re English: to transfer Spanish: transferir My program gives GPT3.5 and 4 a series of contexts and tells it to translate \u00e5 overf\u00f8re to Mandarin. GPT 3.5: 1. \u8f6c\u8d26 (zhu\u01cenzh\u00e0ng) 2. \u4f20\u8f93 (chu\u00e1nsh\u016b) 3. \u8f6c\u79fb (zhu\u01ceny\u00ed) 4. \u8f6c\u4ea4 (zhu\u01cenji\u0101o) 5. \u4f20\u8f93 (chu\u00e1nsh\u016b) These are all context dependent ways you might say to transfer each based on one of the examples. Money, files, a business, responsibility, and pictures (files) respectively. GPT4: \u8f49\u79fb (Zhu\u01ceny\u00ed) - Transfer Just the one result. It is an aggregate of the semantics of the different Norwegian sentences I gave it as input. GPT4 has correctly identified what I would say is the most accurate general translation however, the GPT3.5 data is highly useful because realistically, nearly every word in Norwegian/English/Spanish has a one to many correlation with Mandarin and I want those others as well. However, GPT3.5 also more often invents nonsense. Ex: Norwegian: \u00e5 love This word simply means to promise. GPT4 accurately said: \u627f\u8afe (Ch\u00e9ngnu\u00f2) - Promise. GPT3.5 on the other hand, produced the following where it inaccurately assumed the word was English and completely failed to follow my instructions. I told it to only give me words and it produced full sentences. Moreover these were all repeat examples of the same meaning which was unhelpful. \u00e5 love (to promise, to pledge, to vow) 1. Jeg elsker deg. (I love you.) <-- it confused the English love and Norwegian love. - \u6211\u611b\u4f60 (w\u01d2 \u00e0i n\u01d0) 2. Jeg lover \u00e5 komme i morgen. (I promise to come tomorrow.) - \u6211\u627f\u8afe\u660e\u5929\u6703\u4f86 (w\u01d2 ch\u00e9ngnu\u00f2 m\u00edngti\u0101n hu\u00ec l\u00e1i) 3. Han lovte \u00e5 hjelpe meg. (He vowed to help me.) - \u4ed6\u7b54\u61c9\u5e6b\u52a9\u6211 (t\u0101 d\u0101y\u00ecng b\u0101ngzh\u00f9 w\u01d2) 4. Vi f\u00e5r se om hun holder det hun lover. (We'll see if she keeps her word.) - \u6211\u5011\u770b\u770b\u5979\u662f\u5426\u4fe1\u5b88\u627f\u8afe (w\u01d2men k\u00e0n k\u00e0n t\u0101 sh\u00ecf\u01d2u x\u00ecnsh\u01d2u ch\u00e9ngnu\u00f2) 5. De lovet \u00e5 betale tilbake pengene. (They pledged to pay back the money.) - \u4ed6\u5011\u627f\u8afe\u9084\u9322 (t\u0101men ch\u00e9ngnu\u00f2 hu\u00e1n qi\u00e1n) Overall GPT4 was infinitely more useful/accurate here. So Which was Better ChatGPT 3.5 or ChatGPT 4? I described the Mandarin translations above. From a user perspective I actually prefer ChatGPT 3.5 for Mandarin translation by a significant margin over ChatGPT 4. Mandarin indicates meaning by leveraging the combination of characters to articulate highly specific meanings that would usually require completely different words in other languages or simply require you to use more words to provide more context. ChatGPT 3.5 does a better job of covering these possibilities in my not-insignificant experience than ChatGPT 4. However , between languages that have a closer to one to one translation: Norwegian, Spanish, and English, ChatGPT 3.5 was significantly worse. ChatGPT 4 absolutely blew away the performance of ChatGPT 3.5. To such an extent that I stopped using it entirely for Norwegian -> Spanish and only using the results from ChatGPT 4. This gives you an idea of just how incredibly nuanced model size can be. You simply cannot make any assumptions about it - your exact use case matters as does the exact model you have selected. What is Over-fitting? What is over-fitting? To make it very simple, returning to our analogy from What is Training and Inferencing regarding studying, you have yourself probably made this mistake at some point. Imagine that in the course of studying for a test you became very focused on a narrow set of information and assumed that this narrow set of information would be sufficient for all the test questions. Perhaps it is a math course and you determined only certain algorithms were important and ignored the rest. When you got to the exam, you got 40% of the questions right, but were completely wrong on the others. That is over-fitting. You trained yourself on only 40% of the training data but accidentally omitted information representative of the other 60%. Here is a graphical representation of what is going on. Imagine that the over-fitted graphs are what you studied. Some very specific subset of the real data and you memorized only those things. The right side you see in the training data it appears to perform significantly worse, but when it comes time to the real world test, it does significantly better because it isn't overly-fit to the training data. Source Code It's important that during training data you don't just draw perfect boxes around every data point and assume that the real world will look the same. Your model has to generalize well as you see on the right hand side of our graph. How Much Computing Power Do I Need? There are a couple of key questions that will determine your computing requirements for running an AI/ML workload. Biggest Factor What is the model(s)? Plain and simple this will have the biggest impact on how much horsepower you need. Primary Questions Model Size (Number of Parameters) -Larger models with more parameters demand more computational power for both training and inference. The number of parameters directly impacts the amount of memory and processing power needed. Inference Load : The number of requests or queries the model handles simultaneously affects server requirements. Higher inference loads require more computing resources to maintain performance. Training Data Volume : The amount and complexity of the data used to train the LLM can significantly impact the computational resources required. Larger and more diverse datasets require more storage and processing power. Secondary Questions Optimization and Efficiency of Algorithms : The efficiency of the underlying algorithms can impact how computationally intensive the model is. More optimized algorithms can reduce server power requirements. Latency Requirements : If low latency (quick response times) is crucial, more powerful servers are needed to process requests rapidly. Redundancy and Reliability Needs : Ensuring high availability and fault tolerance may require additional resources for redundancy. Cooling and Physical Infrastructure : Large-scale computing resources generate heat and require effective cooling solutions. Worse cooling means less efficiency which means more servers over wider area. Liquid cooling is the future of HPC. If it isn't on your radar and you're a big datacenter, you will be left behind over the coming decades. We have reached the upper bound of what is possible with air cooling. Some Reference Points On a single GPU of reasonable quality you will have no problem running object models with a few thousand pictures and querying them You can run a personal copy of Llama 7B using something like h2ogpt on your home computer. I fed it a couple hundred documents and was getting answers back for myself within 30 seconds to a minute On a 16 core processor I generated an lightgbm that represented an Ising Model simulation within 10 minutes but depending on the hyperparameters you select you could make it take hours A few GPUs will easily let you run a few LLMs and query them Where things get bigger... It's really the scaling. The vast majority of models for a single user you can run them with a single beefy GPU. My employer and benefactor provided a 3080 for my work to do all the things I do for my day job and I can run anything I want. So when does it start taking racks of computers? Scaling. If you just want to suck down a couple thousand documents, put them into an LLM, and start asking questions, you need very little to do that. However, when you want hundreds of people to simultaneously ask questions or you want to start scraping the totality of Reddit as the source of your training data now the requirements start exploding. A General Rule of Thumb for GPT Runtime If it's LLM models we're talking about they are usually based on what is called the transformer architecture. The relationship with the size of your training data is linear (direct). Double the size of the training data and you will generally double your training time. The relationship with the size of the model is quadratic - that is that it multiplies by squares. So if you double the size of the model you're using then the same amount of training data would take four times as long. Quadruple the size of your training data and it now takes 16 times as long. Some Estimations on Real Hardware I wrote an extensive estimation analysis available here that gets into the weeds on some real world estimations.","title":"Common Questions About AI/ML and Large Language Models (LLMs) Answered"},{"location":"Common%20Questions%20About%20LLMs%20Answered/#common-questions-about-aiml-and-large-language-models-llms-answered","text":"I receive a lot of questions about AI/ML and LLMs at work particularly after the advent of ChatGPT so I thought I would leave behind all the math and answer some of the most common questions I receive in plain English. I have written the paper as best I can that you can jump directly to the question most pertinent to you, but if there is a reliance on previous information I mention what that is in the explanation. Common Questions About AI/ML and Large Language Models (LLMs) Answered What Is Artificial Intelligence (AI) / Machine Learning (ML) What is the Difference Between AI and ML? What is Training and Inferencing? What is a large language model (LLM)? What is a Hyperparameter? What is Model Size? What Does it Mean to Tune a Ducking Model? How do I Choose a Model? Are Bigger Models Better? A Concrete Example with ChatGPT So Which was Better ChatGPT 3.5 or ChatGPT 4? What is Over-fitting? How Much Computing Power Do I Need? Biggest Factor Primary Questions Secondary Questions Some Reference Points A General Rule of Thumb for GPT Runtime Some Estimations on Real Hardware","title":"Common Questions About AI/ML and Large Language Models (LLMs) Answered"},{"location":"Common%20Questions%20About%20LLMs%20Answered/#what-is-artificial-intelligence-ai-machine-learning-ml","text":"The words intelligence and learning here are extremely misleading to the average person. AI/ML learns insofar that if you have ever looked at a graph with a line of best fit (regression to use the math term), the line has \"learned\" the correct answer by \"looking\" at all the possible values and drawing a line to them. Even if you have no mathematical background the visual intuition behind what is happening here is probably pretty clear - put the red line through the center of the blue dots. Source Code Linear regression is a type of supervised machine learning. Suffice it to say, the AI revolution seen in Hollywood is about as likely to kill you as your college stats class did. Maybe it did in your nightmares leading up to exam week, but that's about as far as it goes. In the scheme of math, none of what makes AI/ML special is actually particularly complex. Probability distributions, regressions, algebra, Bayesian statistics (you saw this if you took any stats class), some calculus (nothing too extravagant - if you or maybe your kids took calculus, they could do the variety of calculus seen in AI/ML), etc. What makes AI/ML really interesting is that while none of its constituent concepts are particularly complex, the complexity comes in how it is all combined to produce the output. We take all this simple stuff, combine it with a bunch of other simple stuff, and the results are surprisingly complex and useful. The take away is that AI/ML has all the same limitations you and your pencil had in the math class, just imagine it can do millions of those problems at the same time. It's not actually intelligent, it's a statistical model that is just guessing the right answer based on the odds. This is true even of things as magical looking as ChatGPT.","title":"What Is Artificial Intelligence (AI) / Machine Learning (ML)"},{"location":"Common%20Questions%20About%20LLMs%20Answered/#what-is-the-difference-between-ai-and-ml","text":"Semantics. Ask 50 people get 50 definitions. Honestly, it's a fairly arbitrary line so the exact distinction isn't really all that important but generally : AI (Artificial Intelligence): AI is a broader field that aims to create machines or systems that can perform tasks that typically require human intelligence. These tasks include reasoning, problem-solving, understanding natural language, recognizing patterns, and making decisions. AI encompasses a wide range of techniques and approaches, including machine learning. ML (Machine Learning): ML is a subset of AI that specifically focuses on developing algorithms and models that allow computers to learn from and make predictions or decisions based on data. ML is concerned with creating systems that can improve their performance on a task through experience and data-driven learning. The definitions also keep changing as technology improves. 40 years ago a series of if/then statements was called AI, but now they're just if/then statements. In the end, it really doesn't much matter to actually performing useful work or communicating yourself.","title":"What is the Difference Between AI and ML?"},{"location":"Common%20Questions%20About%20LLMs%20Answered/#what-is-training-and-inferencing","text":"In machine learning we talk a lot about training and inferencing but what does that mean? Machine learning / AI generally break down into two phases: the training phase and the inferencing phase. Training is when you take some large body of pre-existing data, feed it into a bunch of math, and that math produces a series of equations which ultimately guess the answer to some question you have. The inferencing phase is when you start feeding real world, previously unseen, data into the model and it produces answers based on the training. You've been through this yourself in school. The training phase is you doing your homework, studying, and preparing for an exam. The inference phase is your taking that exam which usually has hereunto unseen questions, and then you generate answers based on the data you trained on.","title":"What is Training and Inferencing?"},{"location":"Common%20Questions%20About%20LLMs%20Answered/#what-is-a-large-language-model-llm","text":"A large language model (LLM) is a type of artificial intelligence that specializes in processing and understanding human language. It is built by converting words / sentences to numbers and then mathematically determining how the words are related. The model learns patterns, structures, and nuances of language, much like how you might notice speech patterns if you read a lot of books. Have you ever been reading a book and thought, \"Oh I know what comes next.\" That thought you had is exactly how LLMs work. Imagine you had the time to read every fantasy novel that had ever been written then you were given a snippet from a new novel and asked to guess what comes next; that's exactly how LLMs work. I guarantee you have seen this technology before. Have you ever been typing on your phone and it suggests the next word? Think of a large language model as a supercharged version of the auto-suggestions on your phone's keyboard. Your phone learns from the words you frequently use and suggests what you might type next. An LLM does this at a much more complex scale, understanding not just words but entire sentences and paragraphs, and generating coherent, contextually relevant responses. However, just like your phone's keyboard doesn't 'understand' what you're typing, LLMs don't truly 'understand' language in the human sense. They're statistical machines. Given a prompt, they generate responses based on the patterns they've learned from their training data. In essence, a large language model is a sophisticated tool for mimicking human-like text based on the probability of certain words and sentences following others, based on its training data. It's a product of combining many simple concepts from mathematics and computer science, but the sheer scale of data and computations involved creates something that can seem quite complex and intelligent on the surface.","title":"What is a large language model (LLM)?"},{"location":"Common%20Questions%20About%20LLMs%20Answered/#what-is-a-hyperparameter","text":"All of these AI/ML models have what are called hyperparameters. This is just a fancy word for a number we can tweak in our math. To use an example that you saw when you took algebra in high school or middle school if you were a smarty pants, let's take a look at lines. A line has the form $a*x+b=Y$. $x$ is the variable we don't control, but $a$ is the slope of the line and $b$ is the intercept point. $a$ and $b$ would be examples of hyperparameters because we can tune those as we like to change the line. For example, here is what it look like when we change the values of $b$. Source Code Now what if we change $a$? Source Code So how does that relate to machine learning? Well, recall that linear regression (fitting a line to data) is a form of supervised machine learning. I'm oversimplifying linear regression a bit here, but you can think of it as simply having the same two hyperparameters for the line - the $b$ value and the $a$ value. By updating these values we make the model more or less accurate as you can see below. Source Code","title":"What is a Hyperparameter?"},{"location":"Common%20Questions%20About%20LLMs%20Answered/#what-is-model-size","text":"Now that you understand hyperparameters we can talk about model size. In our linear regression example (simplified), the model size is just two. You can adjust either $a$ or $b$. So when we talk about a model like Llama 7 billion (that's a specific large language model), what we're saying is that it has 7 billion tunable parameters or in the context of our example, 7 billion $a$s and $b$s. Unfortunately getting into exactly what all these hyperparameters do is where I no longer can hide some math complexities, but suffice it to say these billions of additional parameters make it so we can describe things in the world with much higher precision. Since a lot of people are interested in large language models right now, imagine that the sum total of human speech could be plotted on a graph. Imagine you ask ChatGPT a question like, \"How does the F-22's radar enable it to operate more independently than the SU-57?\" (I've been watching a lot of Max Afterburner lately. This particular thought came from this video ). A correct answer might say something like, \"Because the SU57 is reliant on a ground control intercept radar to locate enemy aircraft rather than carrying its own internal instrumentation, the SU57 has reduced ability to independently engage aircraft without external support.\" Another, worse, possible answer is, \"The SU57 is a bad airplane.\" Obviously, the second leaves a lot to be desired. Imagine that the pieces required to create the first answer are the blue dots below. The purple line represents a model with six hyperparameters, the green line is only four hyperparameters, and the red dotted line is just our linear regression. Source Code How the math works doesn't matter for the explanation. What matters is that now we have six hyperparametrs - which are the $b$ variables shown in this equation: $Y = b_0 + b_1X + b_2X^2 + b_3X^3 + b_4X^4 + b_5X^5$. As you can imagine the four hyperparameter model only goes up to $b_3$ (we started counting at zero because we're computer people). Those extra tunable parameters combined with picking a better algorithm for our use case allow us to create much better answers for this particular data set. This does not mean more hyperparameters is always better though. I'll explain that later. For this dataset though it definitely was. In this example you can imagine that these are how the answers line up: Question: \"How does the F-22's radar enable it to operate more independently than the SU-57?\" Answers: - Purple Line: \"Because the SU57 is reliant on a ground control intercept radar to locate enemy aircraft rather than carrying its own internal instrumentation, the SU57 has reduced ability to independently engage aircraft without external support.\" - Green Line \"The SU57 is a bad airplane.\" - Red Line: \"The airplane banana.\"","title":"What is Model Size?"},{"location":"Common%20Questions%20About%20LLMs%20Answered/#what-does-it-mean-to-tune-a-ducking-model","text":"In our discussion of hyperparameters you saw tuning. This is what it looked like to tune the $a$ parameter of a line: But what other kinds of tuning are there and how does that relate to the LLMs we see? Have you ever become frustrated with your ducking phone? It is a ducking phone because the engineers over at Apple decided that certain words should have a reduced autocorrect priority even if they are statistically far more likely. This is a manual intervention in what the math would have natively produced as the alternative word is clearly far more likely in the given sentence. If you ever have tried to get ChatGPT to answer a sordid inquiry you will have seen more examples of tuning in action: Obviously, without those guards in place, ChatGPT would have done a great job of summarizing every murder related piece of material it had ever found on the Internet. Not a desirable behavior. Tuning is changing the answers by either updating the hyperparameters or adding some sort of external logic to your model to change the results.","title":"What Does it Mean to Tune a Ducking Model?"},{"location":"Common%20Questions%20About%20LLMs%20Answered/#how-do-i-choose-a-model","text":"This is where PHDs in math, machine learning, data science, etc make their money. This is an extremely complex question that unfortunately has no simple, high level, explanation. What I will do here is demonstrate the difference in laymen's terms between the different models so you get a feel for just how big a difference model selection can make. Let's say we want to predict housing prices based only on the total size of the home and the number of rooms. We will pick two models - one is a linear regression and the other is k-means (it isn't important that you understand what this is). I wrote a program that generates some arbitrary data that represents the housing prices. What matters is that there is a linear relationship between the price of the house and the size/# rooms making this perfect for a linear regression. Here is what it looks like when you predict the price with a linear regression vs k-means: Source Code As you can see the linear regression nails it and k-means misses by some margin. This is the power of picking the right model. I wish I had some simple answer on how to pick the right model, but there just isn't one. You have to understand your problem and you have to know the math. Though I'll tell you that if you give ChatGPT your problem it usually does a good job at guessing a set of models that would perform reasonably. Automated tools are also now on the market that are built by teams of PHDs that try multiple different models for some set of data, score them, and then present the best results. These still don't replace data scientists, but they do make it so your existing data science teams are spending less time on infrastructure and minutia and more time on your mission target where you want them.","title":"How do I Choose a Model?"},{"location":"Common%20Questions%20About%20LLMs%20Answered/#are-bigger-models-better","text":"This is extremely difficult to explain because the answer is - it depends. A lot. Again, this is where PHDs in this stuff make their money. You have actually already seen the answer in How Do I Choose a Model? . k-means has significantly more hyperparameters than linear regression and its model size you can think of being significantly larger however it produced significantly worse results. So bigger is worse then? Well... no. In How do I Choose a Model I picked an example where more complex performed comparatively worse. In What is Model Size we saw an example of how more hyperparameters improved the performance of the model. These nuances can be extrapolated to the most complex models of today and the impact of the size of the model on the outcomes are unique to the exact model you are using. So the question you need to ask isn't \"Are bigger models better\" it is \"Given some very specific model and my very specific use case, are bigger models better?\" . If you phrase the question in a way any less precisely than that you will not receive an accurate answer.","title":"Are Bigger Models Better?"},{"location":"Common%20Questions%20About%20LLMs%20Answered/#a-concrete-example-with-chatgpt","text":"In the interest of providing a more nuanced example, here is a personal comparison between ChatGPT 3.5 and ChatGPT 4. ChatGPT 3.5 has roughly 175 billion parameters. ChatGPT 4's algorithm isn't public but suffice it to say it's a safe assumption that it's significantly more. I am a huge lover of languages. I speak Mandarin, Spanish, and English (obviously), but I am currently learning Norwegian. The problem I'm solving is that I want to learn new Norwegian words while keeping my Spanish and Mandarin vocab current. To that end, I updated my Norwegian Flashcard program to leverage OpenAI's APIs to get answers regarding potential translations from both ChatGPT 3.5 and ChatGPT 4. The answers, particularly with regards to Mandarin, are quite interesting because while Norwegian (Germanic), Spanish (Romance), and English (Germanic - technically) are pretty similar - Mandarin is very different. In fact, while linguists hypothesize about a proto-world language, we don't have any idea what Mandarin and English's closest relative is. Generated with ChatGPT4 in case you're wondering about the funny formatting. Why am I babbling about my language hobby? Because that difference means that translating them becomes much harder with many more possible translations which means the spread of answers from language models is also wider. In the scope of GPT models specifically , smaller models have fewer data points to understand context and language semantics so you will generally receive correspondingly \"worse\" answers. However, worse is an extremely nuanced concepts. Insofar as a smaller model is more likely to produce less precise responses, this may appear to give a bigger sense of randomness and subsequently creativity. In the context of my language program, the responses from 3.5 are much more scattered and sometimes do capture interesting things whereas GPT4 is more succinct and generally more accurate in the context of the prompt. Example: Norwegian Word: \u00e5 overf\u00f8re English: to transfer Spanish: transferir My program gives GPT3.5 and 4 a series of contexts and tells it to translate \u00e5 overf\u00f8re to Mandarin. GPT 3.5: 1. \u8f6c\u8d26 (zhu\u01cenzh\u00e0ng) 2. \u4f20\u8f93 (chu\u00e1nsh\u016b) 3. \u8f6c\u79fb (zhu\u01ceny\u00ed) 4. \u8f6c\u4ea4 (zhu\u01cenji\u0101o) 5. \u4f20\u8f93 (chu\u00e1nsh\u016b) These are all context dependent ways you might say to transfer each based on one of the examples. Money, files, a business, responsibility, and pictures (files) respectively. GPT4: \u8f49\u79fb (Zhu\u01ceny\u00ed) - Transfer Just the one result. It is an aggregate of the semantics of the different Norwegian sentences I gave it as input. GPT4 has correctly identified what I would say is the most accurate general translation however, the GPT3.5 data is highly useful because realistically, nearly every word in Norwegian/English/Spanish has a one to many correlation with Mandarin and I want those others as well. However, GPT3.5 also more often invents nonsense. Ex: Norwegian: \u00e5 love This word simply means to promise. GPT4 accurately said: \u627f\u8afe (Ch\u00e9ngnu\u00f2) - Promise. GPT3.5 on the other hand, produced the following where it inaccurately assumed the word was English and completely failed to follow my instructions. I told it to only give me words and it produced full sentences. Moreover these were all repeat examples of the same meaning which was unhelpful. \u00e5 love (to promise, to pledge, to vow) 1. Jeg elsker deg. (I love you.) <-- it confused the English love and Norwegian love. - \u6211\u611b\u4f60 (w\u01d2 \u00e0i n\u01d0) 2. Jeg lover \u00e5 komme i morgen. (I promise to come tomorrow.) - \u6211\u627f\u8afe\u660e\u5929\u6703\u4f86 (w\u01d2 ch\u00e9ngnu\u00f2 m\u00edngti\u0101n hu\u00ec l\u00e1i) 3. Han lovte \u00e5 hjelpe meg. (He vowed to help me.) - \u4ed6\u7b54\u61c9\u5e6b\u52a9\u6211 (t\u0101 d\u0101y\u00ecng b\u0101ngzh\u00f9 w\u01d2) 4. Vi f\u00e5r se om hun holder det hun lover. (We'll see if she keeps her word.) - \u6211\u5011\u770b\u770b\u5979\u662f\u5426\u4fe1\u5b88\u627f\u8afe (w\u01d2men k\u00e0n k\u00e0n t\u0101 sh\u00ecf\u01d2u x\u00ecnsh\u01d2u ch\u00e9ngnu\u00f2) 5. De lovet \u00e5 betale tilbake pengene. (They pledged to pay back the money.) - \u4ed6\u5011\u627f\u8afe\u9084\u9322 (t\u0101men ch\u00e9ngnu\u00f2 hu\u00e1n qi\u00e1n) Overall GPT4 was infinitely more useful/accurate here.","title":"A Concrete Example with ChatGPT"},{"location":"Common%20Questions%20About%20LLMs%20Answered/#so-which-was-better-chatgpt-35-or-chatgpt-4","text":"I described the Mandarin translations above. From a user perspective I actually prefer ChatGPT 3.5 for Mandarin translation by a significant margin over ChatGPT 4. Mandarin indicates meaning by leveraging the combination of characters to articulate highly specific meanings that would usually require completely different words in other languages or simply require you to use more words to provide more context. ChatGPT 3.5 does a better job of covering these possibilities in my not-insignificant experience than ChatGPT 4. However , between languages that have a closer to one to one translation: Norwegian, Spanish, and English, ChatGPT 3.5 was significantly worse. ChatGPT 4 absolutely blew away the performance of ChatGPT 3.5. To such an extent that I stopped using it entirely for Norwegian -> Spanish and only using the results from ChatGPT 4. This gives you an idea of just how incredibly nuanced model size can be. You simply cannot make any assumptions about it - your exact use case matters as does the exact model you have selected.","title":"So Which was Better ChatGPT 3.5 or ChatGPT 4?"},{"location":"Common%20Questions%20About%20LLMs%20Answered/#what-is-over-fitting","text":"What is over-fitting? To make it very simple, returning to our analogy from What is Training and Inferencing regarding studying, you have yourself probably made this mistake at some point. Imagine that in the course of studying for a test you became very focused on a narrow set of information and assumed that this narrow set of information would be sufficient for all the test questions. Perhaps it is a math course and you determined only certain algorithms were important and ignored the rest. When you got to the exam, you got 40% of the questions right, but were completely wrong on the others. That is over-fitting. You trained yourself on only 40% of the training data but accidentally omitted information representative of the other 60%. Here is a graphical representation of what is going on. Imagine that the over-fitted graphs are what you studied. Some very specific subset of the real data and you memorized only those things. The right side you see in the training data it appears to perform significantly worse, but when it comes time to the real world test, it does significantly better because it isn't overly-fit to the training data. Source Code It's important that during training data you don't just draw perfect boxes around every data point and assume that the real world will look the same. Your model has to generalize well as you see on the right hand side of our graph.","title":"What is Over-fitting?"},{"location":"Common%20Questions%20About%20LLMs%20Answered/#how-much-computing-power-do-i-need","text":"There are a couple of key questions that will determine your computing requirements for running an AI/ML workload.","title":"How Much Computing Power Do I Need?"},{"location":"Common%20Questions%20About%20LLMs%20Answered/#biggest-factor","text":"What is the model(s)? Plain and simple this will have the biggest impact on how much horsepower you need.","title":"Biggest Factor"},{"location":"Common%20Questions%20About%20LLMs%20Answered/#primary-questions","text":"Model Size (Number of Parameters) -Larger models with more parameters demand more computational power for both training and inference. The number of parameters directly impacts the amount of memory and processing power needed. Inference Load : The number of requests or queries the model handles simultaneously affects server requirements. Higher inference loads require more computing resources to maintain performance. Training Data Volume : The amount and complexity of the data used to train the LLM can significantly impact the computational resources required. Larger and more diverse datasets require more storage and processing power.","title":"Primary Questions"},{"location":"Common%20Questions%20About%20LLMs%20Answered/#secondary-questions","text":"Optimization and Efficiency of Algorithms : The efficiency of the underlying algorithms can impact how computationally intensive the model is. More optimized algorithms can reduce server power requirements. Latency Requirements : If low latency (quick response times) is crucial, more powerful servers are needed to process requests rapidly. Redundancy and Reliability Needs : Ensuring high availability and fault tolerance may require additional resources for redundancy. Cooling and Physical Infrastructure : Large-scale computing resources generate heat and require effective cooling solutions. Worse cooling means less efficiency which means more servers over wider area. Liquid cooling is the future of HPC. If it isn't on your radar and you're a big datacenter, you will be left behind over the coming decades. We have reached the upper bound of what is possible with air cooling.","title":"Secondary Questions"},{"location":"Common%20Questions%20About%20LLMs%20Answered/#some-reference-points","text":"On a single GPU of reasonable quality you will have no problem running object models with a few thousand pictures and querying them You can run a personal copy of Llama 7B using something like h2ogpt on your home computer. I fed it a couple hundred documents and was getting answers back for myself within 30 seconds to a minute On a 16 core processor I generated an lightgbm that represented an Ising Model simulation within 10 minutes but depending on the hyperparameters you select you could make it take hours A few GPUs will easily let you run a few LLMs and query them Where things get bigger... It's really the scaling. The vast majority of models for a single user you can run them with a single beefy GPU. My employer and benefactor provided a 3080 for my work to do all the things I do for my day job and I can run anything I want. So when does it start taking racks of computers? Scaling. If you just want to suck down a couple thousand documents, put them into an LLM, and start asking questions, you need very little to do that. However, when you want hundreds of people to simultaneously ask questions or you want to start scraping the totality of Reddit as the source of your training data now the requirements start exploding.","title":"Some Reference Points"},{"location":"Common%20Questions%20About%20LLMs%20Answered/#a-general-rule-of-thumb-for-gpt-runtime","text":"If it's LLM models we're talking about they are usually based on what is called the transformer architecture. The relationship with the size of your training data is linear (direct). Double the size of the training data and you will generally double your training time. The relationship with the size of the model is quadratic - that is that it multiplies by squares. So if you double the size of the model you're using then the same amount of training data would take four times as long. Quadruple the size of your training data and it now takes 16 times as long.","title":"A General Rule of Thumb for GPT Runtime"},{"location":"Common%20Questions%20About%20LLMs%20Answered/#some-estimations-on-real-hardware","text":"I wrote an extensive estimation analysis available here that gets into the weeds on some real world estimations.","title":"Some Estimations on Real Hardware"},{"location":"Common%20Questions%20About%20LLMs%20Answered/README_temp/","text":"Common Questions About LLMs Answered","title":"Common Questions About LLMs Answered"},{"location":"Common%20Questions%20About%20LLMs%20Answered/README_temp/#common-questions-about-llms-answered","text":"","title":"Common Questions About LLMs Answered"},{"location":"Configure%20FN410%20as%20a%20Switch/","text":"Configure FN410 as a Switch Scenario Overview You are running a TFX2, you have an FN410, and that FN410 is connected back to some sort of top of rack switch. In my case I am using an 4112F-ON. In our scenario, we want to create a LAG between the Top of Rack (TOR) switch and two of the front ports of the FN410. On the back of the FN410, connected to the blades, you will have a trunk going to an ESXi server. Ports The ports are configured in the following way: 4112F-ON ethernet 1/1/1 - port 1 of LAG ethernet 1/1/2 - port 2 of LAG port-channel 128 - LAG port In my setup ethernet 1/1/9 went to the CMC and ethernet 1/1/12 went to the rest of my network. There were not other relevant ports. FN410 tengigabitethernet 0/9 - port 1 of LAG tengigabitethernet 0/10 - port 2 of LAG tengigabitethernet 0/1 - port 1 going to ESXi tengigabitethernet 0/2 - port 2 going to ESXi Useful Notes How the FN410 Management Interface Works From the manual The IOM management interface has both a public IP and private IP address on the internal fabric D interface. The public IP address is exposed to the outside world for Web GUI configurations/WSMAN and other proprietary traffic. You can statically configure the public IP address or obtain the IP address dynamically using the dynamic host configuration protocol (DHCP). What is a fabric? A fabric is just a series of electrical connections that in this case make up a L2 switching domain. Fabric D is the internal management fabric. The CMC basically acts as a switch on this fabric and all of the chassis management related services are connected to it - idrac, CMC, and FN410 management functions. Example Configs 4112F-ON Example Configuration FN410 Example Configuration For the keen eyed I was too lazy to remove the password hashes. Spoiler it's admin/admin on the 4112 and root/calvin on the FN410. Configuring the FN410 Configure FN410 as a Switch via GUI Go to CMC -> Click I/O Module Overview Click on your FN410 Go to setup and configure networking. The address you sit here is tied to the CMC's physical port. That is to say, you do not need to be plugged into the FN410 to reach this address. It should be on the same subnet and VLAN as the CMC. Click Launch I/O Module GUI Once in the GUI, in mode settings, select Full Switch Mode Set your network settings as needed Credentials set as needed SNMP set as needed Disable Uplink Failure Detection On time I set my time zone and used 216.239.35.0 (Google's time servers - the zero isn't a type-o) At the end you will be asked to reboot. Say yes. A window will appear that says rebooting. Wait for it to go away. At the end the page will refresh and you will get an error message on the web site saying it isn't available. This is because you put it in switch mode. This process took ~3 minutes for me. Configure FN410 as a Swich via Command Line From the manual You can connect to the FN410 using one of the following: Internal RS-232 using the chassis management controller (CMC). Telnet into CMC and do a connect -b switch-id to get console access to corresponding IOM. External serial port with a universal serial bus (USB) connector (front panel): connect using the IOM front panel USB serial line to get console access (Labeled as USB B). Telnet/others using the public IP interface on the fabric D interface. CMC through the private IP interface on the fabric D interface. You will need to connect through the CMC: SSH to the CMC's IP address. Use the CMC username/password to log in. Run the command connect -m switch-1 to connect to the switch. My switch was in BMP mode. I had to hit A to cancel it. Run stack-unit 0 iom-mode full-switch to change the switch to full switch mode Run write mem and reboot the IOM Configuring Switch Management You may have already done this in the GUI, but if not, you can do the following: Configure Default Route management route 0.0.0.0/0 192.168.1.1 Configure NTP ntp server 216.239.35.0 Configure Management IP Address Dell(conf)#interface managementethernet 0/0 Dell(conf-if-ma-0/0)#ip address 192.168.1.114/24 Configure SNMP snmp-server community public ro snmp-server enable traps snmp linkdown linkup snmp-server enable traps stack Upgrading the firmware You can download the firmware from the force10 website Upgrade the Dell Networking OS in flash partition A: or B:. Run upgrade system tftp://10.16.127.149/dell-FN-B B: Verify that the Dell Networking OS has been upgraded correctly in the upgraded flash partition. show boot system stack-unit [0-5 | all] Upgrade the FN I/O Module Boot Flash and Boot Selector image (if needed). upgrade boot [all | bootflash-image | bootselector-image] stack-unit [0-5 | all] [booted | flash: | ftp: | scp: | tftp: | usbflash:] [A: | B:] Change the Primary Boot Parameter of the FN I/O Module to the upgraded partition A: or B:. boot system stack-unit [0-5 | all] primary [system A: | system B: | tftp://] Save the config: write memory . Reload the switch so the config takes effect. reload After boot, use show version and show system stack-unit [0-5] to check that versions have updated correctly. Configure Interfaces Begin by configuring the port-channel interface: Note: Hybrid mode allows the interface to pass tagged and untagged traffic. Dell(conf)#interface port-channel 128 Dell(conf-if-po-128)#portmode hybrid Dell(conf-if-po-128)#switchport Dell(conf-if-po-128)#no shutdown Next you need to configure the individual interfaces: Dell(conf)#interface range tengigabitethernet 0/9-10 Dell(conf-if-range-te-0/9-10)#port-channel-protocol LACP Dell(conf-if-range-te-0/9-10-lacp)#port-channel 128 mode active Now you need to configure any VLANs you want to run: Dell(conf)#interface range vlan 32-37 Dell(conf-if-range-vl-32-37)#tagged tengigabitethernet 0/1-2 Dell(conf-if-range-vl-32-37)#tagged tengigabitethernet 0/12 If desired at this point you can enter into each VLAN interface and run the ip address command to give it an IP address. Run write memory Configure 4112F-ON Log into the 4112F-ON via its management interfaces. Move to configuration mode. Next, configure the port channel interface: OS10(config)# interface port-channel 128 OS10(conf-if-po-128)# switchport mode trunk OS10(conf-if-po-128)# no switchport access vlan OS10(conf-if-po-128)# switchport trunk allowed vlan 32 OS10(conf-if-po-128)# no shutdown Now configure the interfaces to join the port channel. OS10(config)# interface range ethernet 1/1/1-1/1/2 OS10(conf-range-eth1/1/1-1/1/2)# channel-group 128 mode active OS10(conf-range-eth1/1/1-1/1/2)# no switchport OS10(conf-range-eth1/1/1-1/1/2)# switchport mode trunk Run write memory Troubleshooting VLANs On 4112F-ON Make sure the VLANs using are all shown as active on the correct ports: OS10# show vlan Codes: * - Default VLAN, M - Management VLAN, R - Remote Port Mirroring VLANs, @ \u2013 Attached to Virtual Network Q: A - Access (Untagged), T - Tagged NUM Status Description Q Ports * 1 Inactive A Eth1/1/4-1/1/8,1/1/10,1/1/13-1/1/15 32 Active T Eth1/1/11 T Po128 A Eth1/1/9,1/1/12 On FN410 Dell#show vlan Codes: * - Default VLAN, G - GVRP VLANs, R - Remote Port Mirroring VLANs, P - Primary, C - Community, I - Isolated O - Openflow, Vx - Vxlan Q: U - Untagged, T - Tagged x - Dot1x untagged, X - Dot1x tagged o - OpenFlow untagged, O - OpenFlow tagged G - GVRP tagged, M - Vlan-stack, H - VSN tagged i - Internal untagged, I - Internal tagged, v - VLT untagged, V - VLT tagged NUM Status Description Q Ports * 1 Active U Po128(Te 0/9-10) U Te 0/1-2,12 32 Active T Po128(Te 0/9-10) T Te 0/1-2,12","title":"Configure FN410 as a Switch"},{"location":"Configure%20FN410%20as%20a%20Switch/#configure-fn410-as-a-switch","text":"","title":"Configure FN410 as a Switch"},{"location":"Configure%20FN410%20as%20a%20Switch/#scenario","text":"","title":"Scenario"},{"location":"Configure%20FN410%20as%20a%20Switch/#overview","text":"You are running a TFX2, you have an FN410, and that FN410 is connected back to some sort of top of rack switch. In my case I am using an 4112F-ON. In our scenario, we want to create a LAG between the Top of Rack (TOR) switch and two of the front ports of the FN410. On the back of the FN410, connected to the blades, you will have a trunk going to an ESXi server.","title":"Overview"},{"location":"Configure%20FN410%20as%20a%20Switch/#ports","text":"The ports are configured in the following way:","title":"Ports"},{"location":"Configure%20FN410%20as%20a%20Switch/#4112f-on","text":"ethernet 1/1/1 - port 1 of LAG ethernet 1/1/2 - port 2 of LAG port-channel 128 - LAG port In my setup ethernet 1/1/9 went to the CMC and ethernet 1/1/12 went to the rest of my network. There were not other relevant ports.","title":"4112F-ON"},{"location":"Configure%20FN410%20as%20a%20Switch/#fn410","text":"tengigabitethernet 0/9 - port 1 of LAG tengigabitethernet 0/10 - port 2 of LAG tengigabitethernet 0/1 - port 1 going to ESXi tengigabitethernet 0/2 - port 2 going to ESXi","title":"FN410"},{"location":"Configure%20FN410%20as%20a%20Switch/#useful-notes","text":"","title":"Useful Notes"},{"location":"Configure%20FN410%20as%20a%20Switch/#how-the-fn410-management-interface-works","text":"From the manual The IOM management interface has both a public IP and private IP address on the internal fabric D interface. The public IP address is exposed to the outside world for Web GUI configurations/WSMAN and other proprietary traffic. You can statically configure the public IP address or obtain the IP address dynamically using the dynamic host configuration protocol (DHCP).","title":"How the FN410 Management Interface Works"},{"location":"Configure%20FN410%20as%20a%20Switch/#what-is-a-fabric","text":"A fabric is just a series of electrical connections that in this case make up a L2 switching domain. Fabric D is the internal management fabric. The CMC basically acts as a switch on this fabric and all of the chassis management related services are connected to it - idrac, CMC, and FN410 management functions.","title":"What is a fabric?"},{"location":"Configure%20FN410%20as%20a%20Switch/#example-configs","text":"4112F-ON Example Configuration FN410 Example Configuration For the keen eyed I was too lazy to remove the password hashes. Spoiler it's admin/admin on the 4112 and root/calvin on the FN410.","title":"Example Configs"},{"location":"Configure%20FN410%20as%20a%20Switch/#configuring-the-fn410","text":"","title":"Configuring the FN410"},{"location":"Configure%20FN410%20as%20a%20Switch/#configure-fn410-as-a-switch-via-gui","text":"Go to CMC -> Click I/O Module Overview Click on your FN410 Go to setup and configure networking. The address you sit here is tied to the CMC's physical port. That is to say, you do not need to be plugged into the FN410 to reach this address. It should be on the same subnet and VLAN as the CMC. Click Launch I/O Module GUI Once in the GUI, in mode settings, select Full Switch Mode Set your network settings as needed Credentials set as needed SNMP set as needed Disable Uplink Failure Detection On time I set my time zone and used 216.239.35.0 (Google's time servers - the zero isn't a type-o) At the end you will be asked to reboot. Say yes. A window will appear that says rebooting. Wait for it to go away. At the end the page will refresh and you will get an error message on the web site saying it isn't available. This is because you put it in switch mode. This process took ~3 minutes for me.","title":"Configure FN410 as a Switch via GUI"},{"location":"Configure%20FN410%20as%20a%20Switch/#configure-fn410-as-a-swich-via-command-line","text":"From the manual You can connect to the FN410 using one of the following: Internal RS-232 using the chassis management controller (CMC). Telnet into CMC and do a connect -b switch-id to get console access to corresponding IOM. External serial port with a universal serial bus (USB) connector (front panel): connect using the IOM front panel USB serial line to get console access (Labeled as USB B). Telnet/others using the public IP interface on the fabric D interface. CMC through the private IP interface on the fabric D interface. You will need to connect through the CMC: SSH to the CMC's IP address. Use the CMC username/password to log in. Run the command connect -m switch-1 to connect to the switch. My switch was in BMP mode. I had to hit A to cancel it. Run stack-unit 0 iom-mode full-switch to change the switch to full switch mode Run write mem and reboot the IOM","title":"Configure FN410 as a Swich via Command Line"},{"location":"Configure%20FN410%20as%20a%20Switch/#configuring-switch-management","text":"You may have already done this in the GUI, but if not, you can do the following:","title":"Configuring Switch Management"},{"location":"Configure%20FN410%20as%20a%20Switch/#configure-default-route","text":"management route 0.0.0.0/0 192.168.1.1","title":"Configure Default Route"},{"location":"Configure%20FN410%20as%20a%20Switch/#configure-ntp","text":"ntp server 216.239.35.0","title":"Configure NTP"},{"location":"Configure%20FN410%20as%20a%20Switch/#configure-management-ip-address","text":"Dell(conf)#interface managementethernet 0/0 Dell(conf-if-ma-0/0)#ip address 192.168.1.114/24","title":"Configure Management IP Address"},{"location":"Configure%20FN410%20as%20a%20Switch/#configure-snmp","text":"snmp-server community public ro snmp-server enable traps snmp linkdown linkup snmp-server enable traps stack","title":"Configure SNMP"},{"location":"Configure%20FN410%20as%20a%20Switch/#upgrading-the-firmware","text":"You can download the firmware from the force10 website Upgrade the Dell Networking OS in flash partition A: or B:. Run upgrade system tftp://10.16.127.149/dell-FN-B B: Verify that the Dell Networking OS has been upgraded correctly in the upgraded flash partition. show boot system stack-unit [0-5 | all] Upgrade the FN I/O Module Boot Flash and Boot Selector image (if needed). upgrade boot [all | bootflash-image | bootselector-image] stack-unit [0-5 | all] [booted | flash: | ftp: | scp: | tftp: | usbflash:] [A: | B:] Change the Primary Boot Parameter of the FN I/O Module to the upgraded partition A: or B:. boot system stack-unit [0-5 | all] primary [system A: | system B: | tftp://] Save the config: write memory . Reload the switch so the config takes effect. reload After boot, use show version and show system stack-unit [0-5] to check that versions have updated correctly.","title":"Upgrading the firmware"},{"location":"Configure%20FN410%20as%20a%20Switch/#configure-interfaces","text":"Begin by configuring the port-channel interface: Note: Hybrid mode allows the interface to pass tagged and untagged traffic. Dell(conf)#interface port-channel 128 Dell(conf-if-po-128)#portmode hybrid Dell(conf-if-po-128)#switchport Dell(conf-if-po-128)#no shutdown Next you need to configure the individual interfaces: Dell(conf)#interface range tengigabitethernet 0/9-10 Dell(conf-if-range-te-0/9-10)#port-channel-protocol LACP Dell(conf-if-range-te-0/9-10-lacp)#port-channel 128 mode active Now you need to configure any VLANs you want to run: Dell(conf)#interface range vlan 32-37 Dell(conf-if-range-vl-32-37)#tagged tengigabitethernet 0/1-2 Dell(conf-if-range-vl-32-37)#tagged tengigabitethernet 0/12 If desired at this point you can enter into each VLAN interface and run the ip address command to give it an IP address. Run write memory","title":"Configure Interfaces"},{"location":"Configure%20FN410%20as%20a%20Switch/#configure-4112f-on","text":"Log into the 4112F-ON via its management interfaces. Move to configuration mode. Next, configure the port channel interface: OS10(config)# interface port-channel 128 OS10(conf-if-po-128)# switchport mode trunk OS10(conf-if-po-128)# no switchport access vlan OS10(conf-if-po-128)# switchport trunk allowed vlan 32 OS10(conf-if-po-128)# no shutdown Now configure the interfaces to join the port channel. OS10(config)# interface range ethernet 1/1/1-1/1/2 OS10(conf-range-eth1/1/1-1/1/2)# channel-group 128 mode active OS10(conf-range-eth1/1/1-1/1/2)# no switchport OS10(conf-range-eth1/1/1-1/1/2)# switchport mode trunk Run write memory","title":"Configure 4112F-ON"},{"location":"Configure%20FN410%20as%20a%20Switch/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"Configure%20FN410%20as%20a%20Switch/#vlans","text":"","title":"VLANs"},{"location":"Configure%20FN410%20as%20a%20Switch/#on-4112f-on","text":"Make sure the VLANs using are all shown as active on the correct ports: OS10# show vlan Codes: * - Default VLAN, M - Management VLAN, R - Remote Port Mirroring VLANs, @ \u2013 Attached to Virtual Network Q: A - Access (Untagged), T - Tagged NUM Status Description Q Ports * 1 Inactive A Eth1/1/4-1/1/8,1/1/10,1/1/13-1/1/15 32 Active T Eth1/1/11 T Po128 A Eth1/1/9,1/1/12","title":"On 4112F-ON"},{"location":"Configure%20FN410%20as%20a%20Switch/#on-fn410","text":"Dell#show vlan Codes: * - Default VLAN, G - GVRP VLANs, R - Remote Port Mirroring VLANs, P - Primary, C - Community, I - Isolated O - Openflow, Vx - Vxlan Q: U - Untagged, T - Tagged x - Dot1x untagged, X - Dot1x tagged o - OpenFlow untagged, O - OpenFlow tagged G - GVRP tagged, M - Vlan-stack, H - VSN tagged i - Internal untagged, I - Internal tagged, v - VLT untagged, V - VLT tagged NUM Status Description Q Ports * 1 Active U Po128(Te 0/9-10) U Te 0/1-2,12 32 Active T Po128(Te 0/9-10) T Te 0/1-2,12","title":"On FN410"},{"location":"Configure%20Gigamon%20Tap/","text":"Configure Gigamon Tap Getting Help You have to register with a valid serial number on Gigamon's support site . For it to work your name either has to be attached to the account or you have to have a valid .mil address. My Setup Gigamon Model Number / Version GTP-ASF01 Hardware Revision: G-TAP A2/SF G-Tap> show version SW Version : 2.3.03 Filename=gtb_ASF_IMAGE_2.3.03_20161118.bin SFPs Tested Front Panel Configuration Traffic Generation Traffic was generated on a laptop going into port network 1 and load balanced across ports toolA and toolB Configuring the Device Console Settings I used the following console settings: On Windows with Putty my serial line was COM3 Speed: 115200 Data bits: 8 Stop bits: 1 Parity: None Flow Control: None Logging In Default password is root123 . There is no username. Configure Management config ipaddr 192.168.1.105 subnetmask 255.255.255.0 WARNING For reasons unknown, telnet is enabled by default on the management interface. If you are using this outside a lab you'll probably want to disable it with telnet 0 . At this juncture, you can telnet to the management IP if you want to. If you want to change the default password this can be done with passwd . If you need to set the date or time you can do so with the commands time <hh:mm:ss> or date <mm-dd-yy> respectively. Configure TAP Capability According to the instructions no configuration is required. Test Results Run 1 - Pass Scenario: Get nominal load balancing working. For testing I added a second Dell 1Gb/s copper SFP. Distant end was a USB 1Gb/s copper adapter on a laptop. Result: Worked as expected. Input went into the network port and load balanced out the toolA port. Run 2 - Fail Scenario: Test to see if the Intel 10Gb/s NIC will run. Distant end was another Intel 10Gb/s SFP in an Intel x710 attached to ESXi. Result: Failure. The tap detected the insert and removal of the SFP: Monitor Port B: \"SFP\" Module Removed Monitor Port B: NOTE: I2C device respond successfully! \"SFP+ SR\" Inserted However, the distant end, which I confirmed to be working using a Dell 4112F-ON, would not come up. Note: As long as the SFP was seated in the Gigamon the green light remained on. There was no correlation between plugging in the cable and the green light. Run 3 - Fail Scenario: Same as 2 except I used the 3rd part Gtek SFP+. Results: Same as Run 2. Run 4 - Fail Scenario: Same as 2 except with a 10Gb/s Dell adapter. Results: Same as Run 2. Run 5 - Fail Scenario: I tried with the 10Gb/s Dell SFP again - this time I made the distant end a RHEL box. Results: RHEL saw nothing. ethtool showed both the duplex and speed as unknown. Nothing came up. Run 6 - Fail Scenario: I tried with the 10Gb/s Dell SFP again - this time I made the distant end a Dell 4112F-ON. Results: Even after manually setting speed, duplex, autonegotiation, and double checking the interface types it still didn't come up. Other Helpful Commands If you need to close or open the tap you can do so using taptx <active|passive> where active pushes traffic to the tool ports and passive only pushes traffic through the network ports. show system gives you a nice overview of the status of the tap and what is plugged in where. G-Tap> show system ======================================================================== System Information ======================================================================== System Name : GTP-A2/SF S/N=*******, rev=A1, HW Built=10/24/2019 SW Version : 2.3.03 Filename=gtb_ASF_IMAGE_2.3.03_20161118.bin App Loader Ver : 2.04 Current : Mon Jun 22, 2020 20:50:26 System Boot : Mon Jun 22, 2020 17:02:39 ------------------------------------------------------------------------ Eth Mgmt Port : DHCP=DISABLE, MAC=00:1D:AC:1B:09:DE : 192.168.1.105/255.255.255.0 gateway=255.255.255.255 : Autoneg=ON, Link=Up Speed=100 Duplex=Full ------------------------------------------------------------------------ Telnet Access : Enabled, Timeout=300 seconds Console baud : 115200 bps ------------------------------------------------------------------------ Power Supply : AC adapter=[ON] DC -48V=[OFF] PoE=[OFF] ------------------------------------------------------------------------ Battery fuel gauge busy. Please Try again... ------------------------------------------------------------------------ Temperatures : Board 38 C, Battery 2 C Fan : OFF ------------------------------------------------------------------------ Weird Behaviors Temperature I continuously saw a warning for temperature: Battery I saw a warning for the battery: NOTE: Battery fuel gauge busy! Will try again later It looks like you can buy a battery separately so I believe this to be expected. Where is SSH? I was unable to figure out how to configure SSH. In the manual they only had telnet and console listed. I feel like I must be missing something. Telnet Console Didn't Display Correctly When I typed in the telnet console with Putty the text would only appear after I hit enter. Commands Straight from the Manual Do Not Work Using the arrow keys in the command line deletes text instead of moving cursor Why? Manual is incorrect about display output Ex: G-Tap> show port-params NETWORK MONITOR Parameter Port A Port B Port A Port B ================= ========== ========== ========== ========== Admin: 1 1 1 1 Signal Detect: 1 0 1 1 Tx Power(dBm): n/a n/a n/a -2.36 Rx Power(dBm): n/a n/a n/a -2.50 SFP Module Type: SFP Copper -- SFP Copper SFP+ SR Cable Length(m): n/a n/a n/a n/a The speed, duplex and autonegotiation settings are not listed anywhere in the output. Bizzare Compatibility Errors with SFPs Even after confirming all setting, fixing the speed at 10Gb/s, duplex at full, and turning off autonegation, the fiber interface still didn't come up. I went and cross referenced the manual to see what type of SFPs they support and confirmed that the wavelength and type matched.","title":"Configure Gigamon Tap"},{"location":"Configure%20Gigamon%20Tap/#configure-gigamon-tap","text":"","title":"Configure Gigamon Tap"},{"location":"Configure%20Gigamon%20Tap/#getting-help","text":"You have to register with a valid serial number on Gigamon's support site . For it to work your name either has to be attached to the account or you have to have a valid .mil address.","title":"Getting Help"},{"location":"Configure%20Gigamon%20Tap/#my-setup","text":"","title":"My Setup"},{"location":"Configure%20Gigamon%20Tap/#gigamon-model-number-version","text":"GTP-ASF01 Hardware Revision: G-TAP A2/SF G-Tap> show version SW Version : 2.3.03 Filename=gtb_ASF_IMAGE_2.3.03_20161118.bin","title":"Gigamon Model Number / Version"},{"location":"Configure%20Gigamon%20Tap/#sfps-tested","text":"","title":"SFPs Tested"},{"location":"Configure%20Gigamon%20Tap/#front-panel-configuration","text":"","title":"Front Panel Configuration"},{"location":"Configure%20Gigamon%20Tap/#traffic-generation","text":"Traffic was generated on a laptop going into port network 1 and load balanced across ports toolA and toolB","title":"Traffic Generation"},{"location":"Configure%20Gigamon%20Tap/#configuring-the-device","text":"","title":"Configuring the Device"},{"location":"Configure%20Gigamon%20Tap/#console-settings","text":"I used the following console settings: On Windows with Putty my serial line was COM3 Speed: 115200 Data bits: 8 Stop bits: 1 Parity: None Flow Control: None","title":"Console Settings"},{"location":"Configure%20Gigamon%20Tap/#logging-in","text":"Default password is root123 . There is no username.","title":"Logging In"},{"location":"Configure%20Gigamon%20Tap/#configure-management","text":"config ipaddr 192.168.1.105 subnetmask 255.255.255.0 WARNING For reasons unknown, telnet is enabled by default on the management interface. If you are using this outside a lab you'll probably want to disable it with telnet 0 . At this juncture, you can telnet to the management IP if you want to. If you want to change the default password this can be done with passwd . If you need to set the date or time you can do so with the commands time <hh:mm:ss> or date <mm-dd-yy> respectively.","title":"Configure Management"},{"location":"Configure%20Gigamon%20Tap/#configure-tap-capability","text":"According to the instructions no configuration is required.","title":"Configure TAP Capability"},{"location":"Configure%20Gigamon%20Tap/#test-results","text":"","title":"Test Results"},{"location":"Configure%20Gigamon%20Tap/#run-1-pass","text":"Scenario: Get nominal load balancing working. For testing I added a second Dell 1Gb/s copper SFP. Distant end was a USB 1Gb/s copper adapter on a laptop. Result: Worked as expected. Input went into the network port and load balanced out the toolA port.","title":"Run 1 - Pass"},{"location":"Configure%20Gigamon%20Tap/#run-2-fail","text":"Scenario: Test to see if the Intel 10Gb/s NIC will run. Distant end was another Intel 10Gb/s SFP in an Intel x710 attached to ESXi. Result: Failure. The tap detected the insert and removal of the SFP: Monitor Port B: \"SFP\" Module Removed Monitor Port B: NOTE: I2C device respond successfully! \"SFP+ SR\" Inserted However, the distant end, which I confirmed to be working using a Dell 4112F-ON, would not come up. Note: As long as the SFP was seated in the Gigamon the green light remained on. There was no correlation between plugging in the cable and the green light.","title":"Run 2 - Fail"},{"location":"Configure%20Gigamon%20Tap/#run-3-fail","text":"Scenario: Same as 2 except I used the 3rd part Gtek SFP+. Results: Same as Run 2.","title":"Run 3 - Fail"},{"location":"Configure%20Gigamon%20Tap/#run-4-fail","text":"Scenario: Same as 2 except with a 10Gb/s Dell adapter. Results: Same as Run 2.","title":"Run 4 - Fail"},{"location":"Configure%20Gigamon%20Tap/#run-5-fail","text":"Scenario: I tried with the 10Gb/s Dell SFP again - this time I made the distant end a RHEL box. Results: RHEL saw nothing. ethtool showed both the duplex and speed as unknown. Nothing came up.","title":"Run 5 - Fail"},{"location":"Configure%20Gigamon%20Tap/#run-6-fail","text":"Scenario: I tried with the 10Gb/s Dell SFP again - this time I made the distant end a Dell 4112F-ON. Results: Even after manually setting speed, duplex, autonegotiation, and double checking the interface types it still didn't come up.","title":"Run 6 - Fail"},{"location":"Configure%20Gigamon%20Tap/#other-helpful-commands","text":"If you need to close or open the tap you can do so using taptx <active|passive> where active pushes traffic to the tool ports and passive only pushes traffic through the network ports. show system gives you a nice overview of the status of the tap and what is plugged in where. G-Tap> show system ======================================================================== System Information ======================================================================== System Name : GTP-A2/SF S/N=*******, rev=A1, HW Built=10/24/2019 SW Version : 2.3.03 Filename=gtb_ASF_IMAGE_2.3.03_20161118.bin App Loader Ver : 2.04 Current : Mon Jun 22, 2020 20:50:26 System Boot : Mon Jun 22, 2020 17:02:39 ------------------------------------------------------------------------ Eth Mgmt Port : DHCP=DISABLE, MAC=00:1D:AC:1B:09:DE : 192.168.1.105/255.255.255.0 gateway=255.255.255.255 : Autoneg=ON, Link=Up Speed=100 Duplex=Full ------------------------------------------------------------------------ Telnet Access : Enabled, Timeout=300 seconds Console baud : 115200 bps ------------------------------------------------------------------------ Power Supply : AC adapter=[ON] DC -48V=[OFF] PoE=[OFF] ------------------------------------------------------------------------ Battery fuel gauge busy. Please Try again... ------------------------------------------------------------------------ Temperatures : Board 38 C, Battery 2 C Fan : OFF ------------------------------------------------------------------------","title":"Other Helpful Commands"},{"location":"Configure%20Gigamon%20Tap/#weird-behaviors","text":"","title":"Weird Behaviors"},{"location":"Configure%20Gigamon%20Tap/#temperature","text":"I continuously saw a warning for temperature:","title":"Temperature"},{"location":"Configure%20Gigamon%20Tap/#battery","text":"I saw a warning for the battery: NOTE: Battery fuel gauge busy! Will try again later It looks like you can buy a battery separately so I believe this to be expected.","title":"Battery"},{"location":"Configure%20Gigamon%20Tap/#where-is-ssh","text":"I was unable to figure out how to configure SSH. In the manual they only had telnet and console listed. I feel like I must be missing something.","title":"Where is SSH?"},{"location":"Configure%20Gigamon%20Tap/#telnet-console-didnt-display-correctly","text":"When I typed in the telnet console with Putty the text would only appear after I hit enter.","title":"Telnet Console Didn't Display Correctly"},{"location":"Configure%20Gigamon%20Tap/#commands-straight-from-the-manual-do-not-work","text":"","title":"Commands Straight from the Manual Do Not Work"},{"location":"Configure%20Gigamon%20Tap/#using-the-arrow-keys-in-the-command-line-deletes-text-instead-of-moving-cursor","text":"Why?","title":"Using the arrow keys in the command line deletes text instead of moving cursor"},{"location":"Configure%20Gigamon%20Tap/#manual-is-incorrect-about-display-output","text":"Ex: G-Tap> show port-params NETWORK MONITOR Parameter Port A Port B Port A Port B ================= ========== ========== ========== ========== Admin: 1 1 1 1 Signal Detect: 1 0 1 1 Tx Power(dBm): n/a n/a n/a -2.36 Rx Power(dBm): n/a n/a n/a -2.50 SFP Module Type: SFP Copper -- SFP Copper SFP+ SR Cable Length(m): n/a n/a n/a n/a The speed, duplex and autonegotiation settings are not listed anywhere in the output.","title":"Manual is incorrect about display output"},{"location":"Configure%20Gigamon%20Tap/#bizzare-compatibility-errors-with-sfps","text":"Even after confirming all setting, fixing the speed at 10Gb/s, duplex at full, and turning off autonegation, the fiber interface still didn't come up. I went and cross referenced the manual to see what type of SFPs they support and confirmed that the wavelength and type matched.","title":"Bizzare Compatibility Errors with SFPs"},{"location":"Configuring%20VLT%20on%20OS10/","text":"Configuring VLT on OS10 Configuring VLT on OS10 My Test Platform Configuration of VLT Device 1 Device 2 Configuration of VLANs for Test Test Scenario Objective Useful Commands Spanning Tree and VLT My Test Platform OS10# show version Dell EMC Networking OS10 Enterprise Copyright (c) 1999-2020 by Dell Inc. All Rights Reserved. OS Version: 10.5.1.3 Build Version: 10.5.1.3.190 Build Time: 2020-06-19T21:48:07+0000 System Type: S4112F-ON Architecture: x86_64 Up Time: 00:19:57 Configuration of VLT Device 1 # Configure management configure terminal interface mgmt 1/1/1 no ip address dhcp ip address 192.168.1.24/24 exit # Configure spanning tree spanning-tree mode rstp interface range ethernet 1/1/13-1/1/14 # Create a VLT Domain and Configure the VLT interconnect (VLTi) no switchport exit vlt-domain 1 discovery-interface ethernet 1/1/13 discovery-interface ethernet 1/1/14 # Configure the VLT Priority, VLT MAC Address, and VLT Backup Link primary-priority 4096 vlt-mac 00:11:22:33:44:55 backup destination 192.168.1.25 end Device 2 # Configure management configure terminal interface mgmt 1/1/1 no ip address dhcp ip address 192.168.1.25/24 exit # Configure spanning tree spanning-tree mode rstp interface range ethernet 1/1/13-1/1/14 # Create a VLT Domain and Configure the VLT interconnect (VLTi) no switchport exit vlt-domain 1 discovery-interface ethernet 1/1/13 discovery-interface ethernet 1/1/14 # Configure the VLT Priority, VLT MAC Address, and VLT Backup Link primary-priority 8192 vlt-mac 00:11:22:33:44:55 backup destination 192.168.1.24 end Configuration of VLANs for Test (On both devices) configure terminal interface vlan 9 no shut exit interface ethernet 1/1/1 switchport mode trunk switchport trunk allowed vlan 9 end On ESXi I used two separate virtual switches each with a port group. Each port group was assigned VLAN 9. Test Scenario Objective Ping from VM1 to VM2 to show that communication will flow over the VLT and vice versa. Works as expected. Useful Commands show vlt 1 show vlt 1 mismatch show running-configuration vlt Spanning Tree and VLT I created the below scenario to see what would happen if you created a situation where RSTP can forward from S2 across both port 11 and port 12 to get to interface VLAN 1 on switch 3. In this scenario I gave the VLAN 1 SVI on S2 10.0.0.2 and on S# 10.0.0.3. The first thing I investigated were the effects on STP on S2. Under the hood you can drop to the Linux command line and inspect the bridge associated with the VLT setup to see its root path cost (here my VLT interface is bo1000 ). It is worth noting that VLT interfaces are represented as bonds to the Linux kernel. Furthermore I also noticed the updated values, if updated from the OS10 shell, are not reflected here: root@OS10:~# brctl showstp br1 br1 bridge id 8000.886fd498b7b1 designated root 8000.886fd498b7b1 root port 0 path cost 0 max age 20.00 bridge max age 20.00 hello time 2.00 bridge hello time 2.00 forward delay 15.00 bridge forward delay 15.00 ageing time 5400.00 hello timer 0.00 tcn timer 0.00 topology change timer 0.00 gc timer 910.24 flags bo1000 (16) port id 8010 state forwarding designated root 8000.886fd498b7b1 path cost 100 designated bridge 8000.886fd498b7b1 message age timer 0.00 designated port 8010 forward delay timer 0.00 designated cost 0 hold timer 0.00 flags ... e101-012-0 (13) port id 800d state forwarding designated root 8000.886fd498b7b1 path cost 100 designated bridge 8000.886fd498b7b1 message age timer 0.00 designated port 800d forward delay timer 0.00 designated cost 0 hold timer 0.00 flags You can see which interfaces are in the VLT bond with ip a s | grep <the interface use used for vlt> . In my case I only used interface 11 so I did: root@OS10:~# ip a s | grep e101-011-0 23: e101-011-0: <BROADCAST,MULTICAST,ALLMULTI,SLAVE,UP,LOWER_UP> mtu 9184 qdisc multiq master bo1000 state UP group default qlen 1000 so we can see here that the VLT bond is bo1000. I then performed a ping 10.0.0.3 from S2 and checked the MAC address table. OS10(conf-if-vl-1)# do show mac address-table VlanId Mac Address Type Interface ... 1 88:6f:d4:98:a7:b1 static port-channel1000 We can see that the switch learned the MAC address for switch 3 over the VLT interface. I had a sneaky suspicion that the switch was biasing the VLT link so I reset the experiment except instead of having the VLT between S2 and S3 I put the VLT between S1 and S2. This has the effect of making it so that instead of a single hop across a port channel to get to S3 it would have two hops using the VLT: I then repeated my VLAN 1 ping test from S2 to S3 and as expected - S2 learned S3's mac address of 88:6f:d4:98:a7:b1 on the VLT port not the port channel: On S2: OS10# show mac address-table VlanId Mac Address Type Interface ... 1 88:6f:d4:98:a7:b1 dynamic port-channel1000 What's going on here is that under the covers it looks like, all things being equal, spanning tree will favor the VLT link. However, if you go into the interface and set the port priority and cost with spanning-tree rstp cost 1 and spanning-tree rstp priority 0 , you can force it to use the port channel instead. I repeated my ping test and then checked the mac address table on S2 and as expected got: OS10(config)# show mac address-table VlanId Mac Address Type Interface ... 1 88:6f:d4:98:a7:b1 dynamic ethernet1/1/11 Then to confirm my results, on S2, I went back and deprioritized port 11 with: OS10(config)# interface ethernet 1/1/11 OS10(conf-if-eth1/1/11)# spanning-tree rstp cost 200000000 OS10(conf-if-eth1/1/11)# spanning-tree rstp priority 240 and then redid my ping and as expected got: OS10(config)# ping 10.0.0.3 PING 10.0.0.3 (10.0.0.3) 56(84) bytes of data. 64 bytes from 10.0.0.3: icmp_seq=1 ttl=64 time=1.16 ms 64 bytes from 10.0.0.3: icmp_seq=2 ttl=64 time=1.04 ms ^C --- 10.0.0.3 ping statistics --- 2 packets transmitted, 2 received, 0% packet loss, time 1001ms rtt min/avg/max/mdev = 1.047/1.106/1.166/0.068 ms OS10(config)# show mac address-table VlanId Mac Address Type Interface ... 1 88:6f:d4:98:a7:b1 dynamic port-channel1000","title":"Configuring VLT on OS10"},{"location":"Configuring%20VLT%20on%20OS10/#configuring-vlt-on-os10","text":"Configuring VLT on OS10 My Test Platform Configuration of VLT Device 1 Device 2 Configuration of VLANs for Test Test Scenario Objective Useful Commands Spanning Tree and VLT","title":"Configuring VLT on OS10"},{"location":"Configuring%20VLT%20on%20OS10/#my-test-platform","text":"OS10# show version Dell EMC Networking OS10 Enterprise Copyright (c) 1999-2020 by Dell Inc. All Rights Reserved. OS Version: 10.5.1.3 Build Version: 10.5.1.3.190 Build Time: 2020-06-19T21:48:07+0000 System Type: S4112F-ON Architecture: x86_64 Up Time: 00:19:57","title":"My Test Platform"},{"location":"Configuring%20VLT%20on%20OS10/#configuration-of-vlt","text":"","title":"Configuration of VLT"},{"location":"Configuring%20VLT%20on%20OS10/#device-1","text":"# Configure management configure terminal interface mgmt 1/1/1 no ip address dhcp ip address 192.168.1.24/24 exit # Configure spanning tree spanning-tree mode rstp interface range ethernet 1/1/13-1/1/14 # Create a VLT Domain and Configure the VLT interconnect (VLTi) no switchport exit vlt-domain 1 discovery-interface ethernet 1/1/13 discovery-interface ethernet 1/1/14 # Configure the VLT Priority, VLT MAC Address, and VLT Backup Link primary-priority 4096 vlt-mac 00:11:22:33:44:55 backup destination 192.168.1.25 end","title":"Device 1"},{"location":"Configuring%20VLT%20on%20OS10/#device-2","text":"# Configure management configure terminal interface mgmt 1/1/1 no ip address dhcp ip address 192.168.1.25/24 exit # Configure spanning tree spanning-tree mode rstp interface range ethernet 1/1/13-1/1/14 # Create a VLT Domain and Configure the VLT interconnect (VLTi) no switchport exit vlt-domain 1 discovery-interface ethernet 1/1/13 discovery-interface ethernet 1/1/14 # Configure the VLT Priority, VLT MAC Address, and VLT Backup Link primary-priority 8192 vlt-mac 00:11:22:33:44:55 backup destination 192.168.1.24 end","title":"Device 2"},{"location":"Configuring%20VLT%20on%20OS10/#configuration-of-vlans-for-test","text":"(On both devices) configure terminal interface vlan 9 no shut exit interface ethernet 1/1/1 switchport mode trunk switchport trunk allowed vlan 9 end On ESXi I used two separate virtual switches each with a port group. Each port group was assigned VLAN 9.","title":"Configuration of VLANs for Test"},{"location":"Configuring%20VLT%20on%20OS10/#test-scenario","text":"","title":"Test Scenario"},{"location":"Configuring%20VLT%20on%20OS10/#objective","text":"Ping from VM1 to VM2 to show that communication will flow over the VLT and vice versa. Works as expected.","title":"Objective"},{"location":"Configuring%20VLT%20on%20OS10/#useful-commands","text":"show vlt 1 show vlt 1 mismatch show running-configuration vlt","title":"Useful Commands"},{"location":"Configuring%20VLT%20on%20OS10/#spanning-tree-and-vlt","text":"I created the below scenario to see what would happen if you created a situation where RSTP can forward from S2 across both port 11 and port 12 to get to interface VLAN 1 on switch 3. In this scenario I gave the VLAN 1 SVI on S2 10.0.0.2 and on S# 10.0.0.3. The first thing I investigated were the effects on STP on S2. Under the hood you can drop to the Linux command line and inspect the bridge associated with the VLT setup to see its root path cost (here my VLT interface is bo1000 ). It is worth noting that VLT interfaces are represented as bonds to the Linux kernel. Furthermore I also noticed the updated values, if updated from the OS10 shell, are not reflected here: root@OS10:~# brctl showstp br1 br1 bridge id 8000.886fd498b7b1 designated root 8000.886fd498b7b1 root port 0 path cost 0 max age 20.00 bridge max age 20.00 hello time 2.00 bridge hello time 2.00 forward delay 15.00 bridge forward delay 15.00 ageing time 5400.00 hello timer 0.00 tcn timer 0.00 topology change timer 0.00 gc timer 910.24 flags bo1000 (16) port id 8010 state forwarding designated root 8000.886fd498b7b1 path cost 100 designated bridge 8000.886fd498b7b1 message age timer 0.00 designated port 8010 forward delay timer 0.00 designated cost 0 hold timer 0.00 flags ... e101-012-0 (13) port id 800d state forwarding designated root 8000.886fd498b7b1 path cost 100 designated bridge 8000.886fd498b7b1 message age timer 0.00 designated port 800d forward delay timer 0.00 designated cost 0 hold timer 0.00 flags You can see which interfaces are in the VLT bond with ip a s | grep <the interface use used for vlt> . In my case I only used interface 11 so I did: root@OS10:~# ip a s | grep e101-011-0 23: e101-011-0: <BROADCAST,MULTICAST,ALLMULTI,SLAVE,UP,LOWER_UP> mtu 9184 qdisc multiq master bo1000 state UP group default qlen 1000 so we can see here that the VLT bond is bo1000. I then performed a ping 10.0.0.3 from S2 and checked the MAC address table. OS10(conf-if-vl-1)# do show mac address-table VlanId Mac Address Type Interface ... 1 88:6f:d4:98:a7:b1 static port-channel1000 We can see that the switch learned the MAC address for switch 3 over the VLT interface. I had a sneaky suspicion that the switch was biasing the VLT link so I reset the experiment except instead of having the VLT between S2 and S3 I put the VLT between S1 and S2. This has the effect of making it so that instead of a single hop across a port channel to get to S3 it would have two hops using the VLT: I then repeated my VLAN 1 ping test from S2 to S3 and as expected - S2 learned S3's mac address of 88:6f:d4:98:a7:b1 on the VLT port not the port channel: On S2: OS10# show mac address-table VlanId Mac Address Type Interface ... 1 88:6f:d4:98:a7:b1 dynamic port-channel1000 What's going on here is that under the covers it looks like, all things being equal, spanning tree will favor the VLT link. However, if you go into the interface and set the port priority and cost with spanning-tree rstp cost 1 and spanning-tree rstp priority 0 , you can force it to use the port channel instead. I repeated my ping test and then checked the mac address table on S2 and as expected got: OS10(config)# show mac address-table VlanId Mac Address Type Interface ... 1 88:6f:d4:98:a7:b1 dynamic ethernet1/1/11 Then to confirm my results, on S2, I went back and deprioritized port 11 with: OS10(config)# interface ethernet 1/1/11 OS10(conf-if-eth1/1/11)# spanning-tree rstp cost 200000000 OS10(conf-if-eth1/1/11)# spanning-tree rstp priority 240 and then redid my ping and as expected got: OS10(config)# ping 10.0.0.3 PING 10.0.0.3 (10.0.0.3) 56(84) bytes of data. 64 bytes from 10.0.0.3: icmp_seq=1 ttl=64 time=1.16 ms 64 bytes from 10.0.0.3: icmp_seq=2 ttl=64 time=1.04 ms ^C --- 10.0.0.3 ping statistics --- 2 packets transmitted, 2 received, 0% packet loss, time 1001ms rtt min/avg/max/mdev = 1.047/1.106/1.166/0.068 ms OS10(config)# show mac address-table VlanId Mac Address Type Interface ... 1 88:6f:d4:98:a7:b1 dynamic port-channel1000","title":"Spanning Tree and VLT"},{"location":"Create%20Kickstart%20Server%20on%20Fedora/","text":"Create Kickstart Server on Fedora Files All files housed here Intro This Ansible playbook runs on CentOS. It will create a server which allows you to install Fedora automatically via PXE boot and Kickstart. For more information on Fedora's Kickstart capabilities see this link Useful Information How to set up Fedora Kickstart The Fedora Mirror I used https://gist.github.com/andrewwippler/b636cdb68249ab5ffb67b4d8693a780b Prerequisites Note: If you use a VM it will need at least 90GB of space. Install CentOS You can download and install CentOS here The installation services used by this program are very lightweight so you can install on just about anything. A 12GB hard drive, 2GB RAM, and a single processor are enough for what we are doing. Go ahead and install CentOS on whatever you like, the only requirement is that it must be reachable at layer 2 by the Fedora hosts you want to install. Install Ansible and git Before continuing you will need to install Ansible on your host by running yum install -y ansible git . Disable SELinux Or set proper policies. Pick your poison. Since I only stood this thing up for as long as it took to kickstart I was lazy and just disabled it. Sue me. setenforce 0 I also updated this in /etc/selniux/config . Grab Image Files You need to grab the image files for vmlinuz and initrd.img in order for Linux to boot properly. For Fedora grab them from the below. wget https://download.fedoraproject.org/pub/fedora/linux/releases/31/Server/x86_64/os/images/pxeboot/vmlinuz -O /var/lib/tftpboot/vmlinuz wget https://download.fedoraproject.org/pub/fedora/linux/releases/31/Server/x86_64/os/images/pxeboot/initrd.img -O /var/lib/tftpboot/initrd.img wget http://fedora.mirrors.pair.com/linux/releases/31/Server/x86_64/os/images/install.img -O /var/www/html/iso/images/install.img Clone the Fedora Repositories On a host of your choosing, run the following (update the 31 to the appropriate Fedora version): rsync http://ftp.muug.mb.ca/pub/fedora/linux/releases/31/Everything/x86_64/os/Packages/ /var/www/html/iso/Packages Note: You will probably have to create the above directory. You'll need 73 GB free to sync the packages. You will also need to grab the file in the repodata directory called ending with comps.xml. You can see its exact name in repomd.xml. Set it and forget it because it will be a bit while this all downloads. After you are downloading run: createrepo -g <THE COMPS FILE YOU DOWNLOADED> /var/www/html/iso/Packages Clone the Repo I typically use opt for optional programs. You may install wherever you like, but in this guide I will use opt. Clone the repo with: git clone https://github.com/grantcurell/Fedora-autoinstall.git Move into the Fedora-autoinstall directory with cd /opt/Fedora-autoinstall Configure the Inventory File Ansible is controlled by what is called an inventory file. This inventory file contains all the configuration settings which will be used by Ansible. In our case, we will need to populate this with the settings we wish Fedora to have after installation. The inventory file is called inventory.yml You will need to fill in the following values: dns dhcp_start dhcp_end gateway netmask domain root_password server_ip iso_fedora_pth iso_fedora_checksum Finally you will also need to fill in the host information. These are the hosts on which you would like to install Fedora. Boot Drives Most of the values are self explanatory however, boot_drive and data_drives may be a bit confusing. These options allow you to tell Ansible where to install Fedora and on which disks to configure datastores. The official documentation on how Fedora names drives is here The disks are in the order in which Fedora detects them. To get the order you may have to manually install Fedora on a system. You can make an educated guess from the BIOS menu or if you have something like iDrac you can look at the order of the disks on the RAID controller (assuming you have one). Running the Code Once you are finished editing the inventory.yml file, cd to the root of the Fedora-autoinstall directory and run make . This will run the makefile in the directory. You can see what it is doing by examining the file called Makefile CRITICAL: Change BIOS Settings If you have NVMe drives and are trying to install Fedora, make sure you go into your BIOS and disable Intel Rapid Storage Technology. If you do not, the NVMe drive will not appear and Anaconda will report that there are no disks available. (Optional) Make it so that all hosts receive dhcp I intentionally left it set up so that only configured hosts would receive dhcpd to make sure no one nukes their network. That said, if you want to make it so that all hosts receive dhcp edit /etc/dhcp/dhcpd.conf remove the line deny unknown-clients and then add the line filename \"uefi/BOOTX64.EFI\"; in the subnet {} directive. You will have to repeat this if you run make again. After you are done, run systemctl restart dhcpd Make the Repository Available to Your Clients Add a file called local.repo in /etc/yum.repos.d/local.repo with the following contents: [local] name=Local baseurl=http://192.168.1.121/iso/Packages enabled=1 gpgcheck=0 Advanced Importing Packages from Another System Minimally Method 2 - Under Development Say you want to create a new offline installer. What you'll have to do is go to a system that has all of the packages you might want to use in your new image. You will then want to scrape all those packages so that you can then use them on your new kickstart server. Do the following. On the system that already has the packages run: dnf list installed | cut -d ' ' -f 1 | tr '\\r\\n' ' ' | sed s/Installed//g . The above command gets a list of all packages on the system and outputs them space separated. You could pipe that into xargs, but I prefer to do this next part manually in case there are any packages that don't work. Copy the output of the above and paste it as an argument to dnf download --resolve <YOUR_STUFF> . That will download all the packages on the server and their dependencies. If any packages do not resolve you can run dnf list installed | cut -d ' ' -f 1 | tr '\\r\\n' ' ' | sed s/Installed//g | sed s/<PACKAGE_NAME_THAT_DIDNT_WORK>//g to omit it from the list. After you run make on the Kickstart server to run the above command, go to /var/www/html/iso/packages/ and copy all of the packages to that directory. You can use something like scp * root@192.168.1.121:/var/www/html/iso/packages/ to do this. When you are done, browse to your server using Chrome or something and make sure all the packages are available. Ex: http://192.168.1.121/iso/packages/ Info on the Kickstart Process Kickstart has a lot of black magic going on. So below I give a high level overview of what's happening. Server boots and you tell it to boot with PXE. PXE will then send out a DHCP request. The DHCP server that responds should be your kickstart server (typically) and it will have a few extra options set. Namely there should be a line that looks like the following: # This line tell sth server to reach out over TFTP and grab the uefi/BOOTX64.EFI file. This file is a binary # file which tells the server how to boot. It will be specific to your distro and you should generally get # it from the ISO which you want to install. The path is a relative path against the TFTP root directory. filename \"uefi/BOOTX64.EFI\"; Next, if you are doing UEFI, the server will search for a series of file names in the TFTP server's UEFI directory until it finds a boot menu profile that matches. I couldn't find documentation on it, but if you watch /var/log/messages you can actually see it try different as show below. It progresses through listed host MAC addresses first, then the IP address in hex format, and at the very end tries the default grub.cfg. To get maximum output you can add server_args = -s /var/lib/tftpboot --verbose to /etc/xinetd.d/tftp assuming you are using xinetd to get this output. Jan 21 15:45:00 controller xinetd[10044]: START: tftp pid=10048 from=172.16.71.98 Jan 21 15:45:00 controller in.tftpd[10049]: RRQ from 172.16.71.98 filename uefi/BOOTX64.EFI Jan 21 15:45:00 controller in.tftpd[10049]: Error code 8: User aborted the transfer Jan 21 15:45:00 controller in.tftpd[10050]: RRQ from 172.16.71.98 filename uefi/BOOTX64.EFI Jan 21 15:45:00 controller in.tftpd[10050]: Client 172.16.71.98 finished uefi/BOOTX64.EFI Jan 21 15:45:00 controller in.tftpd[10051]: RRQ from 172.16.71.98 filename uefi/grubx64.efi Jan 21 15:45:00 controller in.tftpd[10051]: Client 172.16.71.98 finished uefi/grubx64.efi Jan 21 15:45:00 controller in.tftpd[10052]: RRQ from 172.16.71.98 filename /uefi/grub.cfg-01-00-50-56-86-d4-06 Jan 21 15:45:00 controller in.tftpd[10052]: Client 172.16.71.98 File not found /uefi/grub.cfg-01-00-50-56-86-d4-06 Jan 21 15:45:00 controller in.tftpd[10053]: RRQ from 172.16.71.98 filename /uefi/grub.cfg-AC104762 Jan 21 15:45:00 controller in.tftpd[10053]: Client 172.16.71.98 File not found /uefi/grub.cfg-AC104762 Jan 21 15:45:00 controller in.tftpd[10054]: RRQ from 172.16.71.98 filename /uefi/grub.cfg-AC10476 Jan 21 15:45:00 controller in.tftpd[10054]: Client 172.16.71.98 File not found /uefi/grub.cfg-AC10476 Jan 21 15:45:00 controller in.tftpd[10055]: RRQ from 172.16.71.98 filename /uefi/grub.cfg-AC1047 Jan 21 15:45:00 controller in.tftpd[10055]: Client 172.16.71.98 File not found /uefi/grub.cfg-AC1047 Jan 21 15:45:00 controller in.tftpd[10056]: RRQ from 172.16.71.98 filename /uefi/grub.cfg-AC104 Jan 21 15:45:00 controller in.tftpd[10056]: Client 172.16.71.98 File not found /uefi/grub.cfg-AC104 Jan 21 15:45:00 controller in.tftpd[10057]: RRQ from 172.16.71.98 filename /uefi/grub.cfg-AC10 Jan 21 15:45:00 controller in.tftpd[10057]: Client 172.16.71.98 File not found /uefi/grub.cfg-AC10 Jan 21 15:45:00 controller in.tftpd[10058]: RRQ from 172.16.71.98 filename /uefi/grub.cfg-AC1 Jan 21 15:45:00 controller in.tftpd[10058]: Client 172.16.71.98 File not found /uefi/grub.cfg-AC1 Jan 21 15:45:00 controller in.tftpd[10059]: RRQ from 172.16.71.98 filename /uefi/grub.cfg-AC Jan 21 15:45:00 controller in.tftpd[10059]: Client 172.16.71.98 File not found /uefi/grub.cfg-AC Jan 21 15:45:00 controller in.tftpd[10060]: RRQ from 172.16.71.98 filename /uefi/grub.cfg-A Jan 21 15:45:00 controller in.tftpd[10060]: Client 172.16.71.98 File not found /uefi/grub.cfg-A Jan 21 15:45:00 controller in.tftpd[10061]: RRQ from 172.16.71.98 filename /uefi/grub.cfg Jan 21 15:45:00 controller in.tftpd[10061]: Client 172.16.71.98 finished /uefi/grub.cfg Jan 21 15:45:00 controller in.tftpd[10062]: RRQ from 172.16.71.98 filename /EFI/BOOT/x86_64-efi/command.lst Jan 21 15:45:00 controller in.tftpd[10062]: Client 172.16.71.98 File not found /EFI/BOOT/x86_64-efi/command.lst Jan 21 15:45:00 controller in.tftpd[10063]: RRQ from 172.16.71.98 filename /EFI/BOOT/x86_64-efi/fs.lst Jan 21 15:45:00 controller in.tftpd[10063]: Client 172.16.71.98 File not found /EFI/BOOT/x86_64-efi/fs.lst Jan 21 15:45:00 controller in.tftpd[10064]: RRQ from 172.16.71.98 filename /EFI/BOOT/x86_64-efi/crypto.lst Jan 21 15:45:00 controller in.tftpd[10064]: Client 172.16.71.98 File not found /EFI/BOOT/x86_64-efi/crypto.lst Jan 21 15:45:00 controller in.tftpd[10065]: RRQ from 172.16.71.98 filename /EFI/BOOT/x86_64-efi/terminal.lst Jan 21 15:45:00 controller in.tftpd[10065]: Client 172.16.71.98 File not found /EFI/BOOT/x86_64-efi/terminal.lst Jan 21 15:45:00 controller in.tftpd[10066]: RRQ from 172.16.71.98 filename /uefi/grub.cfg Jan 21 15:45:00 controller in.tftpd[10066]: Client 172.16.71.98 finished /uefi/grub.cfg Jan 21 15:45:02 controller in.tftpd[10067]: RRQ from 172.16.71.98 filename /vmlinuz Jan 21 15:45:03 controller in.tftpd[10067]: Client 172.16.71.98 finished /vmlinuz Jan 21 15:45:03 controller in.tftpd[10068]: RRQ from 172.16.71.98 filename /initrd.img Jan 21 15:45:08 controller in.tftpd[10068]: Client 172.16.71.98 finished /initrd.img The boot menu profile (grub.cfg) looks like the below. The most important line is the linuxefi line. This line points to where your installation media should be hosted. Typically using httpd. set default=\"1\" function load_video { insmod efi_gop insmod efi_uga insmod video_bochs insmod video_cirrus insmod all_video } load_video set gfxpayload=keep insmod gzio insmod part_gpt insmod ext2 set timeout=1 ### END /etc/grub.d/00_header ### ### BEGIN /etc/grub.d/10_linux ### menuentry 'Install Super Legit Fedora' --class fedora --class gnu-linux --class gnu --class os { echo \"Loading vmlinuz\" linuxefi vmlinuz inst.ks=http://192.168.2.101/ks/uefi/fedora.cfg inst.repo=http://192.168.1.121/iso/ echo \"Loading initrd.img\" initrdefi initrd.img } The host in question will reach out and in this configuration grab a file called install.img located at /var/www/html/iso/images/ in this build. This file allows the host to boot and perform its other functions. TODO - finish this.","title":"Create Kickstart Server on Fedora"},{"location":"Create%20Kickstart%20Server%20on%20Fedora/#create-kickstart-server-on-fedora","text":"","title":"Create Kickstart Server on Fedora"},{"location":"Create%20Kickstart%20Server%20on%20Fedora/#files","text":"All files housed here","title":"Files"},{"location":"Create%20Kickstart%20Server%20on%20Fedora/#intro","text":"This Ansible playbook runs on CentOS. It will create a server which allows you to install Fedora automatically via PXE boot and Kickstart. For more information on Fedora's Kickstart capabilities see this link","title":"Intro"},{"location":"Create%20Kickstart%20Server%20on%20Fedora/#useful-information","text":"How to set up Fedora Kickstart The Fedora Mirror I used https://gist.github.com/andrewwippler/b636cdb68249ab5ffb67b4d8693a780b","title":"Useful Information"},{"location":"Create%20Kickstart%20Server%20on%20Fedora/#prerequisites","text":"Note: If you use a VM it will need at least 90GB of space.","title":"Prerequisites"},{"location":"Create%20Kickstart%20Server%20on%20Fedora/#install-centos","text":"You can download and install CentOS here The installation services used by this program are very lightweight so you can install on just about anything. A 12GB hard drive, 2GB RAM, and a single processor are enough for what we are doing. Go ahead and install CentOS on whatever you like, the only requirement is that it must be reachable at layer 2 by the Fedora hosts you want to install.","title":"Install CentOS"},{"location":"Create%20Kickstart%20Server%20on%20Fedora/#install-ansible-and-git","text":"Before continuing you will need to install Ansible on your host by running yum install -y ansible git .","title":"Install Ansible and git"},{"location":"Create%20Kickstart%20Server%20on%20Fedora/#disable-selinux","text":"Or set proper policies. Pick your poison. Since I only stood this thing up for as long as it took to kickstart I was lazy and just disabled it. Sue me. setenforce 0 I also updated this in /etc/selniux/config .","title":"Disable SELinux"},{"location":"Create%20Kickstart%20Server%20on%20Fedora/#grab-image-files","text":"You need to grab the image files for vmlinuz and initrd.img in order for Linux to boot properly. For Fedora grab them from the below. wget https://download.fedoraproject.org/pub/fedora/linux/releases/31/Server/x86_64/os/images/pxeboot/vmlinuz -O /var/lib/tftpboot/vmlinuz wget https://download.fedoraproject.org/pub/fedora/linux/releases/31/Server/x86_64/os/images/pxeboot/initrd.img -O /var/lib/tftpboot/initrd.img wget http://fedora.mirrors.pair.com/linux/releases/31/Server/x86_64/os/images/install.img -O /var/www/html/iso/images/install.img","title":"Grab Image Files"},{"location":"Create%20Kickstart%20Server%20on%20Fedora/#clone-the-fedora-repositories","text":"On a host of your choosing, run the following (update the 31 to the appropriate Fedora version): rsync http://ftp.muug.mb.ca/pub/fedora/linux/releases/31/Everything/x86_64/os/Packages/ /var/www/html/iso/Packages Note: You will probably have to create the above directory. You'll need 73 GB free to sync the packages. You will also need to grab the file in the repodata directory called ending with comps.xml. You can see its exact name in repomd.xml. Set it and forget it because it will be a bit while this all downloads. After you are downloading run: createrepo -g <THE COMPS FILE YOU DOWNLOADED> /var/www/html/iso/Packages","title":"Clone the Fedora Repositories"},{"location":"Create%20Kickstart%20Server%20on%20Fedora/#clone-the-repo","text":"I typically use opt for optional programs. You may install wherever you like, but in this guide I will use opt. Clone the repo with: git clone https://github.com/grantcurell/Fedora-autoinstall.git Move into the Fedora-autoinstall directory with cd /opt/Fedora-autoinstall","title":"Clone the Repo"},{"location":"Create%20Kickstart%20Server%20on%20Fedora/#configure-the-inventory-file","text":"Ansible is controlled by what is called an inventory file. This inventory file contains all the configuration settings which will be used by Ansible. In our case, we will need to populate this with the settings we wish Fedora to have after installation. The inventory file is called inventory.yml You will need to fill in the following values: dns dhcp_start dhcp_end gateway netmask domain root_password server_ip iso_fedora_pth iso_fedora_checksum Finally you will also need to fill in the host information. These are the hosts on which you would like to install Fedora.","title":"Configure the Inventory File"},{"location":"Create%20Kickstart%20Server%20on%20Fedora/#boot-drives","text":"Most of the values are self explanatory however, boot_drive and data_drives may be a bit confusing. These options allow you to tell Ansible where to install Fedora and on which disks to configure datastores. The official documentation on how Fedora names drives is here The disks are in the order in which Fedora detects them. To get the order you may have to manually install Fedora on a system. You can make an educated guess from the BIOS menu or if you have something like iDrac you can look at the order of the disks on the RAID controller (assuming you have one).","title":"Boot Drives"},{"location":"Create%20Kickstart%20Server%20on%20Fedora/#running-the-code","text":"Once you are finished editing the inventory.yml file, cd to the root of the Fedora-autoinstall directory and run make . This will run the makefile in the directory. You can see what it is doing by examining the file called Makefile","title":"Running the Code"},{"location":"Create%20Kickstart%20Server%20on%20Fedora/#critical-change-bios-settings","text":"If you have NVMe drives and are trying to install Fedora, make sure you go into your BIOS and disable Intel Rapid Storage Technology. If you do not, the NVMe drive will not appear and Anaconda will report that there are no disks available.","title":"CRITICAL: Change BIOS Settings"},{"location":"Create%20Kickstart%20Server%20on%20Fedora/#optional-make-it-so-that-all-hosts-receive-dhcp","text":"I intentionally left it set up so that only configured hosts would receive dhcpd to make sure no one nukes their network. That said, if you want to make it so that all hosts receive dhcp edit /etc/dhcp/dhcpd.conf remove the line deny unknown-clients and then add the line filename \"uefi/BOOTX64.EFI\"; in the subnet {} directive. You will have to repeat this if you run make again. After you are done, run systemctl restart dhcpd","title":"(Optional) Make it so that all hosts receive dhcp"},{"location":"Create%20Kickstart%20Server%20on%20Fedora/#make-the-repository-available-to-your-clients","text":"Add a file called local.repo in /etc/yum.repos.d/local.repo with the following contents: [local] name=Local baseurl=http://192.168.1.121/iso/Packages enabled=1 gpgcheck=0","title":"Make the Repository Available to Your Clients"},{"location":"Create%20Kickstart%20Server%20on%20Fedora/#advanced","text":"","title":"Advanced"},{"location":"Create%20Kickstart%20Server%20on%20Fedora/#importing-packages-from-another-system-minimally","text":"","title":"Importing Packages from Another System Minimally"},{"location":"Create%20Kickstart%20Server%20on%20Fedora/#method-2-under-development","text":"Say you want to create a new offline installer. What you'll have to do is go to a system that has all of the packages you might want to use in your new image. You will then want to scrape all those packages so that you can then use them on your new kickstart server. Do the following. On the system that already has the packages run: dnf list installed | cut -d ' ' -f 1 | tr '\\r\\n' ' ' | sed s/Installed//g . The above command gets a list of all packages on the system and outputs them space separated. You could pipe that into xargs, but I prefer to do this next part manually in case there are any packages that don't work. Copy the output of the above and paste it as an argument to dnf download --resolve <YOUR_STUFF> . That will download all the packages on the server and their dependencies. If any packages do not resolve you can run dnf list installed | cut -d ' ' -f 1 | tr '\\r\\n' ' ' | sed s/Installed//g | sed s/<PACKAGE_NAME_THAT_DIDNT_WORK>//g to omit it from the list. After you run make on the Kickstart server to run the above command, go to /var/www/html/iso/packages/ and copy all of the packages to that directory. You can use something like scp * root@192.168.1.121:/var/www/html/iso/packages/ to do this. When you are done, browse to your server using Chrome or something and make sure all the packages are available. Ex: http://192.168.1.121/iso/packages/","title":"Method 2 - Under Development"},{"location":"Create%20Kickstart%20Server%20on%20Fedora/#info-on-the-kickstart-process","text":"Kickstart has a lot of black magic going on. So below I give a high level overview of what's happening. Server boots and you tell it to boot with PXE. PXE will then send out a DHCP request. The DHCP server that responds should be your kickstart server (typically) and it will have a few extra options set. Namely there should be a line that looks like the following: # This line tell sth server to reach out over TFTP and grab the uefi/BOOTX64.EFI file. This file is a binary # file which tells the server how to boot. It will be specific to your distro and you should generally get # it from the ISO which you want to install. The path is a relative path against the TFTP root directory. filename \"uefi/BOOTX64.EFI\"; Next, if you are doing UEFI, the server will search for a series of file names in the TFTP server's UEFI directory until it finds a boot menu profile that matches. I couldn't find documentation on it, but if you watch /var/log/messages you can actually see it try different as show below. It progresses through listed host MAC addresses first, then the IP address in hex format, and at the very end tries the default grub.cfg. To get maximum output you can add server_args = -s /var/lib/tftpboot --verbose to /etc/xinetd.d/tftp assuming you are using xinetd to get this output. Jan 21 15:45:00 controller xinetd[10044]: START: tftp pid=10048 from=172.16.71.98 Jan 21 15:45:00 controller in.tftpd[10049]: RRQ from 172.16.71.98 filename uefi/BOOTX64.EFI Jan 21 15:45:00 controller in.tftpd[10049]: Error code 8: User aborted the transfer Jan 21 15:45:00 controller in.tftpd[10050]: RRQ from 172.16.71.98 filename uefi/BOOTX64.EFI Jan 21 15:45:00 controller in.tftpd[10050]: Client 172.16.71.98 finished uefi/BOOTX64.EFI Jan 21 15:45:00 controller in.tftpd[10051]: RRQ from 172.16.71.98 filename uefi/grubx64.efi Jan 21 15:45:00 controller in.tftpd[10051]: Client 172.16.71.98 finished uefi/grubx64.efi Jan 21 15:45:00 controller in.tftpd[10052]: RRQ from 172.16.71.98 filename /uefi/grub.cfg-01-00-50-56-86-d4-06 Jan 21 15:45:00 controller in.tftpd[10052]: Client 172.16.71.98 File not found /uefi/grub.cfg-01-00-50-56-86-d4-06 Jan 21 15:45:00 controller in.tftpd[10053]: RRQ from 172.16.71.98 filename /uefi/grub.cfg-AC104762 Jan 21 15:45:00 controller in.tftpd[10053]: Client 172.16.71.98 File not found /uefi/grub.cfg-AC104762 Jan 21 15:45:00 controller in.tftpd[10054]: RRQ from 172.16.71.98 filename /uefi/grub.cfg-AC10476 Jan 21 15:45:00 controller in.tftpd[10054]: Client 172.16.71.98 File not found /uefi/grub.cfg-AC10476 Jan 21 15:45:00 controller in.tftpd[10055]: RRQ from 172.16.71.98 filename /uefi/grub.cfg-AC1047 Jan 21 15:45:00 controller in.tftpd[10055]: Client 172.16.71.98 File not found /uefi/grub.cfg-AC1047 Jan 21 15:45:00 controller in.tftpd[10056]: RRQ from 172.16.71.98 filename /uefi/grub.cfg-AC104 Jan 21 15:45:00 controller in.tftpd[10056]: Client 172.16.71.98 File not found /uefi/grub.cfg-AC104 Jan 21 15:45:00 controller in.tftpd[10057]: RRQ from 172.16.71.98 filename /uefi/grub.cfg-AC10 Jan 21 15:45:00 controller in.tftpd[10057]: Client 172.16.71.98 File not found /uefi/grub.cfg-AC10 Jan 21 15:45:00 controller in.tftpd[10058]: RRQ from 172.16.71.98 filename /uefi/grub.cfg-AC1 Jan 21 15:45:00 controller in.tftpd[10058]: Client 172.16.71.98 File not found /uefi/grub.cfg-AC1 Jan 21 15:45:00 controller in.tftpd[10059]: RRQ from 172.16.71.98 filename /uefi/grub.cfg-AC Jan 21 15:45:00 controller in.tftpd[10059]: Client 172.16.71.98 File not found /uefi/grub.cfg-AC Jan 21 15:45:00 controller in.tftpd[10060]: RRQ from 172.16.71.98 filename /uefi/grub.cfg-A Jan 21 15:45:00 controller in.tftpd[10060]: Client 172.16.71.98 File not found /uefi/grub.cfg-A Jan 21 15:45:00 controller in.tftpd[10061]: RRQ from 172.16.71.98 filename /uefi/grub.cfg Jan 21 15:45:00 controller in.tftpd[10061]: Client 172.16.71.98 finished /uefi/grub.cfg Jan 21 15:45:00 controller in.tftpd[10062]: RRQ from 172.16.71.98 filename /EFI/BOOT/x86_64-efi/command.lst Jan 21 15:45:00 controller in.tftpd[10062]: Client 172.16.71.98 File not found /EFI/BOOT/x86_64-efi/command.lst Jan 21 15:45:00 controller in.tftpd[10063]: RRQ from 172.16.71.98 filename /EFI/BOOT/x86_64-efi/fs.lst Jan 21 15:45:00 controller in.tftpd[10063]: Client 172.16.71.98 File not found /EFI/BOOT/x86_64-efi/fs.lst Jan 21 15:45:00 controller in.tftpd[10064]: RRQ from 172.16.71.98 filename /EFI/BOOT/x86_64-efi/crypto.lst Jan 21 15:45:00 controller in.tftpd[10064]: Client 172.16.71.98 File not found /EFI/BOOT/x86_64-efi/crypto.lst Jan 21 15:45:00 controller in.tftpd[10065]: RRQ from 172.16.71.98 filename /EFI/BOOT/x86_64-efi/terminal.lst Jan 21 15:45:00 controller in.tftpd[10065]: Client 172.16.71.98 File not found /EFI/BOOT/x86_64-efi/terminal.lst Jan 21 15:45:00 controller in.tftpd[10066]: RRQ from 172.16.71.98 filename /uefi/grub.cfg Jan 21 15:45:00 controller in.tftpd[10066]: Client 172.16.71.98 finished /uefi/grub.cfg Jan 21 15:45:02 controller in.tftpd[10067]: RRQ from 172.16.71.98 filename /vmlinuz Jan 21 15:45:03 controller in.tftpd[10067]: Client 172.16.71.98 finished /vmlinuz Jan 21 15:45:03 controller in.tftpd[10068]: RRQ from 172.16.71.98 filename /initrd.img Jan 21 15:45:08 controller in.tftpd[10068]: Client 172.16.71.98 finished /initrd.img The boot menu profile (grub.cfg) looks like the below. The most important line is the linuxefi line. This line points to where your installation media should be hosted. Typically using httpd. set default=\"1\" function load_video { insmod efi_gop insmod efi_uga insmod video_bochs insmod video_cirrus insmod all_video } load_video set gfxpayload=keep insmod gzio insmod part_gpt insmod ext2 set timeout=1 ### END /etc/grub.d/00_header ### ### BEGIN /etc/grub.d/10_linux ### menuentry 'Install Super Legit Fedora' --class fedora --class gnu-linux --class gnu --class os { echo \"Loading vmlinuz\" linuxefi vmlinuz inst.ks=http://192.168.2.101/ks/uefi/fedora.cfg inst.repo=http://192.168.1.121/iso/ echo \"Loading initrd.img\" initrdefi initrd.img } The host in question will reach out and in this configuration grab a file called install.img located at /var/www/html/iso/images/ in this build. This file allows the host to boot and perform its other functions. TODO - finish this.","title":"Info on the Kickstart Process"},{"location":"Create%20Kickstart%20Server%20on%20Fedora/LICENSE/","text":"GNU GENERAL PUBLIC LICENSE Version 3, 29 June 2007 Copyright (C) 2007 Free Software Foundation, Inc. http://fsf.org/ Everyone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed. Preamble The GNU General Public License is a free, copyleft license for software and other kinds of works. The licenses for most software and other practical works are designed to take away your freedom to share and change the works. By contrast, the GNU General Public License is intended to guarantee your freedom to share and change all versions of a program--to make sure it remains free software for all its users. We, the Free Software Foundation, use the GNU General Public License for most of our software; it applies also to any other work released this way by its authors. You can apply it to your programs, too. When we speak of free software, we are referring to freedom, not price. Our General Public Licenses are designed to make sure that you have the freedom to distribute copies of free software (and charge for them if you wish), that you receive source code or can get it if you want it, that you can change the software or use pieces of it in new free programs, and that you know you can do these things. To protect your rights, we need to prevent others from denying you these rights or asking you to surrender the rights. Therefore, you have certain responsibilities if you distribute copies of the software, or if you modify it: responsibilities to respect the freedom of others. For example, if you distribute copies of such a program, whether gratis or for a fee, you must pass on to the recipients the same freedoms that you received. You must make sure that they, too, receive or can get the source code. And you must show them these terms so they know their rights. Developers that use the GNU GPL protect your rights with two steps: (1) assert copyright on the software, and (2) offer you this License giving you legal permission to copy, distribute and/or modify it. For the developers' and authors' protection, the GPL clearly explains that there is no warranty for this free software. For both users' and authors' sake, the GPL requires that modified versions be marked as changed, so that their problems will not be attributed erroneously to authors of previous versions. Some devices are designed to deny users access to install or run modified versions of the software inside them, although the manufacturer can do so. This is fundamentally incompatible with the aim of protecting users' freedom to change the software. The systematic pattern of such abuse occurs in the area of products for individuals to use, which is precisely where it is most unacceptable. Therefore, we have designed this version of the GPL to prohibit the practice for those products. If such problems arise substantially in other domains, we stand ready to extend this provision to those domains in future versions of the GPL, as needed to protect the freedom of users. Finally, every program is threatened constantly by software patents. States should not allow patents to restrict development and use of software on general-purpose computers, but in those that do, we wish to avoid the special danger that patents applied to a free program could make it effectively proprietary. To prevent this, the GPL assures that patents cannot be used to render the program non-free. The precise terms and conditions for copying, distribution and modification follow. TERMS AND CONDITIONS Definitions. \"This License\" refers to version 3 of the GNU General Public License. \"Copyright\" also means copyright-like laws that apply to other kinds of works, such as semiconductor masks. \"The Program\" refers to any copyrightable work licensed under this License. Each licensee is addressed as \"you\". \"Licensees\" and \"recipients\" may be individuals or organizations. To \"modify\" a work means to copy from or adapt all or part of the work in a fashion requiring copyright permission, other than the making of an exact copy. The resulting work is called a \"modified version\" of the earlier work or a work \"based on\" the earlier work. A \"covered work\" means either the unmodified Program or a work based on the Program. To \"propagate\" a work means to do anything with it that, without permission, would make you directly or secondarily liable for infringement under applicable copyright law, except executing it on a computer or modifying a private copy. Propagation includes copying, distribution (with or without modification), making available to the public, and in some countries other activities as well. To \"convey\" a work means any kind of propagation that enables other parties to make or receive copies. Mere interaction with a user through a computer network, with no transfer of a copy, is not conveying. An interactive user interface displays \"Appropriate Legal Notices\" to the extent that it includes a convenient and prominently visible feature that (1) displays an appropriate copyright notice, and (2) tells the user that there is no warranty for the work (except to the extent that warranties are provided), that licensees may convey the work under this License, and how to view a copy of this License. If the interface presents a list of user commands or options, such as a menu, a prominent item in the list meets this criterion. Source Code. The \"source code\" for a work means the preferred form of the work for making modifications to it. \"Object code\" means any non-source form of a work. A \"Standard Interface\" means an interface that either is an official standard defined by a recognized standards body, or, in the case of interfaces specified for a particular programming language, one that is widely used among developers working in that language. The \"System Libraries\" of an executable work include anything, other than the work as a whole, that (a) is included in the normal form of packaging a Major Component, but which is not part of that Major Component, and (b) serves only to enable use of the work with that Major Component, or to implement a Standard Interface for which an implementation is available to the public in source code form. A \"Major Component\", in this context, means a major essential component (kernel, window system, and so on) of the specific operating system (if any) on which the executable work runs, or a compiler used to produce the work, or an object code interpreter used to run it. The \"Corresponding Source\" for a work in object code form means all the source code needed to generate, install, and (for an executable work) run the object code and to modify the work, including scripts to control those activities. However, it does not include the work's System Libraries, or general-purpose tools or generally available free programs which are used unmodified in performing those activities but which are not part of the work. For example, Corresponding Source includes interface definition files associated with source files for the work, and the source code for shared libraries and dynamically linked subprograms that the work is specifically designed to require, such as by intimate data communication or control flow between those subprograms and other parts of the work. The Corresponding Source need not include anything that users can regenerate automatically from other parts of the Corresponding Source. The Corresponding Source for a work in source code form is that same work. Basic Permissions. All rights granted under this License are granted for the term of copyright on the Program, and are irrevocable provided the stated conditions are met. This License explicitly affirms your unlimited permission to run the unmodified Program. The output from running a covered work is covered by this License only if the output, given its content, constitutes a covered work. This License acknowledges your rights of fair use or other equivalent, as provided by copyright law. You may make, run and propagate covered works that you do not convey, without conditions so long as your license otherwise remains in force. You may convey covered works to others for the sole purpose of having them make modifications exclusively for you, or provide you with facilities for running those works, provided that you comply with the terms of this License in conveying all material for which you do not control copyright. Those thus making or running the covered works for you must do so exclusively on your behalf, under your direction and control, on terms that prohibit them from making any copies of your copyrighted material outside their relationship with you. Conveying under any other circumstances is permitted solely under the conditions stated below. Sublicensing is not allowed; section 10 makes it unnecessary. Protecting Users' Legal Rights From Anti-Circumvention Law. No covered work shall be deemed part of an effective technological measure under any applicable law fulfilling obligations under article 11 of the WIPO copyright treaty adopted on 20 December 1996, or similar laws prohibiting or restricting circumvention of such measures. When you convey a covered work, you waive any legal power to forbid circumvention of technological measures to the extent such circumvention is effected by exercising rights under this License with respect to the covered work, and you disclaim any intention to limit operation or modification of the work as a means of enforcing, against the work's users, your or third parties' legal rights to forbid circumvention of technological measures. Conveying Verbatim Copies. You may convey verbatim copies of the Program's source code as you receive it, in any medium, provided that you conspicuously and appropriately publish on each copy an appropriate copyright notice; keep intact all notices stating that this License and any non-permissive terms added in accord with section 7 apply to the code; keep intact all notices of the absence of any warranty; and give all recipients a copy of this License along with the Program. You may charge any price or no price for each copy that you convey, and you may offer support or warranty protection for a fee. Conveying Modified Source Versions. You may convey a work based on the Program, or the modifications to produce it from the Program, in the form of source code under the terms of section 4, provided that you also meet all of these conditions: a) The work must carry prominent notices stating that you modified it, and giving a relevant date. b) The work must carry prominent notices stating that it is released under this License and any conditions added under section 7. This requirement modifies the requirement in section 4 to \"keep intact all notices\". c) You must license the entire work, as a whole, under this License to anyone who comes into possession of a copy. This License will therefore apply, along with any applicable section 7 additional terms, to the whole of the work, and all its parts, regardless of how they are packaged. This License gives no permission to license the work in any other way, but it does not invalidate such permission if you have separately received it. d) If the work has interactive user interfaces, each must display Appropriate Legal Notices; however, if the Program has interactive interfaces that do not display Appropriate Legal Notices, your work need not make them do so. A compilation of a covered work with other separate and independent works, which are not by their nature extensions of the covered work, and which are not combined with it such as to form a larger program, in or on a volume of a storage or distribution medium, is called an \"aggregate\" if the compilation and its resulting copyright are not used to limit the access or legal rights of the compilation's users beyond what the individual works permit. Inclusion of a covered work in an aggregate does not cause this License to apply to the other parts of the aggregate. Conveying Non-Source Forms. You may convey a covered work in object code form under the terms of sections 4 and 5, provided that you also convey the machine-readable Corresponding Source under the terms of this License, in one of these ways: a) Convey the object code in, or embodied in, a physical product (including a physical distribution medium), accompanied by the Corresponding Source fixed on a durable physical medium customarily used for software interchange. b) Convey the object code in, or embodied in, a physical product (including a physical distribution medium), accompanied by a written offer, valid for at least three years and valid for as long as you offer spare parts or customer support for that product model, to give anyone who possesses the object code either (1) a copy of the Corresponding Source for all the software in the product that is covered by this License, on a durable physical medium customarily used for software interchange, for a price no more than your reasonable cost of physically performing this conveying of source, or (2) access to copy the Corresponding Source from a network server at no charge. c) Convey individual copies of the object code with a copy of the written offer to provide the Corresponding Source. This alternative is allowed only occasionally and noncommercially, and only if you received the object code with such an offer, in accord with subsection 6b. d) Convey the object code by offering access from a designated place (gratis or for a charge), and offer equivalent access to the Corresponding Source in the same way through the same place at no further charge. You need not require recipients to copy the Corresponding Source along with the object code. If the place to copy the object code is a network server, the Corresponding Source may be on a different server (operated by you or a third party) that supports equivalent copying facilities, provided you maintain clear directions next to the object code saying where to find the Corresponding Source. Regardless of what server hosts the Corresponding Source, you remain obligated to ensure that it is available for as long as needed to satisfy these requirements. e) Convey the object code using peer-to-peer transmission, provided you inform other peers where the object code and Corresponding Source of the work are being offered to the general public at no charge under subsection 6d. A separable portion of the object code, whose source code is excluded from the Corresponding Source as a System Library, need not be included in conveying the object code work. A \"User Product\" is either (1) a \"consumer product\", which means any tangible personal property which is normally used for personal, family, or household purposes, or (2) anything designed or sold for incorporation into a dwelling. In determining whether a product is a consumer product, doubtful cases shall be resolved in favor of coverage. For a particular product received by a particular user, \"normally used\" refers to a typical or common use of that class of product, regardless of the status of the particular user or of the way in which the particular user actually uses, or expects or is expected to use, the product. A product is a consumer product regardless of whether the product has substantial commercial, industrial or non-consumer uses, unless such uses represent the only significant mode of use of the product. \"Installation Information\" for a User Product means any methods, procedures, authorization keys, or other information required to install and execute modified versions of a covered work in that User Product from a modified version of its Corresponding Source. The information must suffice to ensure that the continued functioning of the modified object code is in no case prevented or interfered with solely because modification has been made. If you convey an object code work under this section in, or with, or specifically for use in, a User Product, and the conveying occurs as part of a transaction in which the right of possession and use of the User Product is transferred to the recipient in perpetuity or for a fixed term (regardless of how the transaction is characterized), the Corresponding Source conveyed under this section must be accompanied by the Installation Information. But this requirement does not apply if neither you nor any third party retains the ability to install modified object code on the User Product (for example, the work has been installed in ROM). The requirement to provide Installation Information does not include a requirement to continue to provide support service, warranty, or updates for a work that has been modified or installed by the recipient, or for the User Product in which it has been modified or installed. Access to a network may be denied when the modification itself materially and adversely affects the operation of the network or violates the rules and protocols for communication across the network. Corresponding Source conveyed, and Installation Information provided, in accord with this section must be in a format that is publicly documented (and with an implementation available to the public in source code form), and must require no special password or key for unpacking, reading or copying. Additional Terms. \"Additional permissions\" are terms that supplement the terms of this License by making exceptions from one or more of its conditions. Additional permissions that are applicable to the entire Program shall be treated as though they were included in this License, to the extent that they are valid under applicable law. If additional permissions apply only to part of the Program, that part may be used separately under those permissions, but the entire Program remains governed by this License without regard to the additional permissions. When you convey a copy of a covered work, you may at your option remove any additional permissions from that copy, or from any part of it. (Additional permissions may be written to require their own removal in certain cases when you modify the work.) You may place additional permissions on material, added by you to a covered work, for which you have or can give appropriate copyright permission. Notwithstanding any other provision of this License, for material you add to a covered work, you may (if authorized by the copyright holders of that material) supplement the terms of this License with terms: a) Disclaiming warranty or limiting liability differently from the terms of sections 15 and 16 of this License; or b) Requiring preservation of specified reasonable legal notices or author attributions in that material or in the Appropriate Legal Notices displayed by works containing it; or c) Prohibiting misrepresentation of the origin of that material, or requiring that modified versions of such material be marked in reasonable ways as different from the original version; or d) Limiting the use for publicity purposes of names of licensors or authors of the material; or e) Declining to grant rights under trademark law for use of some trade names, trademarks, or service marks; or f) Requiring indemnification of licensors and authors of that material by anyone who conveys the material (or modified versions of it) with contractual assumptions of liability to the recipient, for any liability that these contractual assumptions directly impose on those licensors and authors. All other non-permissive additional terms are considered \"further restrictions\" within the meaning of section 10. If the Program as you received it, or any part of it, contains a notice stating that it is governed by this License along with a term that is a further restriction, you may remove that term. If a license document contains a further restriction but permits relicensing or conveying under this License, you may add to a covered work material governed by the terms of that license document, provided that the further restriction does not survive such relicensing or conveying. If you add terms to a covered work in accord with this section, you must place, in the relevant source files, a statement of the additional terms that apply to those files, or a notice indicating where to find the applicable terms. Additional terms, permissive or non-permissive, may be stated in the form of a separately written license, or stated as exceptions; the above requirements apply either way. Termination. You may not propagate or modify a covered work except as expressly provided under this License. Any attempt otherwise to propagate or modify it is void, and will automatically terminate your rights under this License (including any patent licenses granted under the third paragraph of section 11). However, if you cease all violation of this License, then your license from a particular copyright holder is reinstated (a) provisionally, unless and until the copyright holder explicitly and finally terminates your license, and (b) permanently, if the copyright holder fails to notify you of the violation by some reasonable means prior to 60 days after the cessation. Moreover, your license from a particular copyright holder is reinstated permanently if the copyright holder notifies you of the violation by some reasonable means, this is the first time you have received notice of violation of this License (for any work) from that copyright holder, and you cure the violation prior to 30 days after your receipt of the notice. Termination of your rights under this section does not terminate the licenses of parties who have received copies or rights from you under this License. If your rights have been terminated and not permanently reinstated, you do not qualify to receive new licenses for the same material under section 10. Acceptance Not Required for Having Copies. You are not required to accept this License in order to receive or run a copy of the Program. Ancillary propagation of a covered work occurring solely as a consequence of using peer-to-peer transmission to receive a copy likewise does not require acceptance. However, nothing other than this License grants you permission to propagate or modify any covered work. These actions infringe copyright if you do not accept this License. Therefore, by modifying or propagating a covered work, you indicate your acceptance of this License to do so. Automatic Licensing of Downstream Recipients. Each time you convey a covered work, the recipient automatically receives a license from the original licensors, to run, modify and propagate that work, subject to this License. You are not responsible for enforcing compliance by third parties with this License. An \"entity transaction\" is a transaction transferring control of an organization, or substantially all assets of one, or subdividing an organization, or merging organizations. If propagation of a covered work results from an entity transaction, each party to that transaction who receives a copy of the work also receives whatever licenses to the work the party's predecessor in interest had or could give under the previous paragraph, plus a right to possession of the Corresponding Source of the work from the predecessor in interest, if the predecessor has it or can get it with reasonable efforts. You may not impose any further restrictions on the exercise of the rights granted or affirmed under this License. For example, you may not impose a license fee, royalty, or other charge for exercise of rights granted under this License, and you may not initiate litigation (including a cross-claim or counterclaim in a lawsuit) alleging that any patent claim is infringed by making, using, selling, offering for sale, or importing the Program or any portion of it. Patents. A \"contributor\" is a copyright holder who authorizes use under this License of the Program or a work on which the Program is based. The work thus licensed is called the contributor's \"contributor version\". A contributor's \"essential patent claims\" are all patent claims owned or controlled by the contributor, whether already acquired or hereafter acquired, that would be infringed by some manner, permitted by this License, of making, using, or selling its contributor version, but do not include claims that would be infringed only as a consequence of further modification of the contributor version. For purposes of this definition, \"control\" includes the right to grant patent sublicenses in a manner consistent with the requirements of this License. Each contributor grants you a non-exclusive, worldwide, royalty-free patent license under the contributor's essential patent claims, to make, use, sell, offer for sale, import and otherwise run, modify and propagate the contents of its contributor version. In the following three paragraphs, a \"patent license\" is any express agreement or commitment, however denominated, not to enforce a patent (such as an express permission to practice a patent or covenant not to sue for patent infringement). To \"grant\" such a patent license to a party means to make such an agreement or commitment not to enforce a patent against the party. If you convey a covered work, knowingly relying on a patent license, and the Corresponding Source of the work is not available for anyone to copy, free of charge and under the terms of this License, through a publicly available network server or other readily accessible means, then you must either (1) cause the Corresponding Source to be so available, or (2) arrange to deprive yourself of the benefit of the patent license for this particular work, or (3) arrange, in a manner consistent with the requirements of this License, to extend the patent license to downstream recipients. \"Knowingly relying\" means you have actual knowledge that, but for the patent license, your conveying the covered work in a country, or your recipient's use of the covered work in a country, would infringe one or more identifiable patents in that country that you have reason to believe are valid. If, pursuant to or in connection with a single transaction or arrangement, you convey, or propagate by procuring conveyance of, a covered work, and grant a patent license to some of the parties receiving the covered work authorizing them to use, propagate, modify or convey a specific copy of the covered work, then the patent license you grant is automatically extended to all recipients of the covered work and works based on it. A patent license is \"discriminatory\" if it does not include within the scope of its coverage, prohibits the exercise of, or is conditioned on the non-exercise of one or more of the rights that are specifically granted under this License. You may not convey a covered work if you are a party to an arrangement with a third party that is in the business of distributing software, under which you make payment to the third party based on the extent of your activity of conveying the work, and under which the third party grants, to any of the parties who would receive the covered work from you, a discriminatory patent license (a) in connection with copies of the covered work conveyed by you (or copies made from those copies), or (b) primarily for and in connection with specific products or compilations that contain the covered work, unless you entered into that arrangement, or that patent license was granted, prior to 28 March 2007. Nothing in this License shall be construed as excluding or limiting any implied license or other defenses to infringement that may otherwise be available to you under applicable patent law. No Surrender of Others' Freedom. If conditions are imposed on you (whether by court order, agreement or otherwise) that contradict the conditions of this License, they do not excuse you from the conditions of this License. If you cannot convey a covered work so as to satisfy simultaneously your obligations under this License and any other pertinent obligations, then as a consequence you may not convey it at all. For example, if you agree to terms that obligate you to collect a royalty for further conveying from those to whom you convey the Program, the only way you could satisfy both those terms and this License would be to refrain entirely from conveying the Program. Use with the GNU Affero General Public License. Notwithstanding any other provision of this License, you have permission to link or combine any covered work with a work licensed under version 3 of the GNU Affero General Public License into a single combined work, and to convey the resulting work. The terms of this License will continue to apply to the part which is the covered work, but the special requirements of the GNU Affero General Public License, section 13, concerning interaction through a network will apply to the combination as such. Revised Versions of this License. The Free Software Foundation may publish revised and/or new versions of the GNU General Public License from time to time. Such new versions will be similar in spirit to the present version, but may differ in detail to address new problems or concerns. Each version is given a distinguishing version number. If the Program specifies that a certain numbered version of the GNU General Public License \"or any later version\" applies to it, you have the option of following the terms and conditions either of that numbered version or of any later version published by the Free Software Foundation. If the Program does not specify a version number of the GNU General Public License, you may choose any version ever published by the Free Software Foundation. If the Program specifies that a proxy can decide which future versions of the GNU General Public License can be used, that proxy's public statement of acceptance of a version permanently authorizes you to choose that version for the Program. Later license versions may give you additional or different permissions. However, no additional obligations are imposed on any author or copyright holder as a result of your choosing to follow a later version. Disclaimer of Warranty. THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY APPLICABLE LAW. EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM \"AS IS\" WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM IS WITH YOU. SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF ALL NECESSARY SERVICING, REPAIR OR CORRECTION. Limitation of Liability. IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS), EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Interpretation of Sections 15 and 16. If the disclaimer of warranty and limitation of liability provided above cannot be given local legal effect according to their terms, reviewing courts shall apply local law that most closely approximates an absolute waiver of all civil liability in connection with the Program, unless a warranty or assumption of liability accompanies a copy of the Program in return for a fee. END OF TERMS AND CONDITIONS How to Apply These Terms to Your New Programs If you develop a new program, and you want it to be of the greatest possible use to the public, the best way to achieve this is to make it free software which everyone can redistribute and change under these terms. To do so, attach the following notices to the program. It is safest to attach them to the start of each source file to most effectively state the exclusion of warranty; and each file should have at least the \"copyright\" line and a pointer to where the full notice is found. <one line to give the program's name and a brief idea of what it does.> Copyright (C) <year> <name of author> This program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details. You should have received a copy of the GNU General Public License along with this program. If not, see <http://www.gnu.org/licenses/>. Also add information on how to contact you by electronic and paper mail. If the program does terminal interaction, make it output a short notice like this when it starts in an interactive mode: {project} Copyright (C) {year} {fullname} This program comes with ABSOLUTELY NO WARRANTY; for details type `show w'. This is free software, and you are welcome to redistribute it under certain conditions; type `show c' for details. The hypothetical commands show w' and show c' should show the appropriate parts of the General Public License. Of course, your program's commands might be different; for a GUI interface, you would use an \"about box\". You should also get your employer (if you work as a programmer) or school, if any, to sign a \"copyright disclaimer\" for the program, if necessary. For more information on this, and how to apply and follow the GNU GPL, see http://www.gnu.org/licenses/ . The GNU General Public License does not permit incorporating your program into proprietary programs. If your program is a subroutine library, you may consider it more useful to permit linking proprietary applications with the library. If this is what you want to do, use the GNU Lesser General Public License instead of this License. But first, please read http://www.gnu.org/philosophy/why-not-lgpl.html .","title":"LICENSE"},{"location":"Create%20OpenSwitch%20VM/","text":"Create OpenSwitch VM Download onie-kvm.iso Create a VM - it must use BIOS boot mode WARNING If you are running on VMWare ESXi you must use an IDE hard drive! WARNING If you are running on VMWare ESXi you must use the E1000E driver for the network card! Boot from the CD, but at the grub menu, hit tab to edit the options and remove all console options See: https://askubuntu.com/questions/771871/16-04-virtualbox-vm-from-vhd-file-hangs-at-non-blocking-pool-is-initialized Once you drop to the ONIE command line run sed -i 's/vda/sda/g' /lib/onie/onie-updater Workable fix: sed 's/if \\[ \"$sha1.*/if false; then/g' /lib/onie/onie-updater Run onie-self-update -e file://lib/onie/onie-updater Reboot. For whatever reason I couldn't get the discovery process to work so I ran onie-discovery-stop Then run onie-nos-install http://<SERVER_IP>/onie-installer Faster fix that didn't work Run sha1sum /lib/onie/onie-updater and note the sha1 hash Run sed \"s/payload\\=.*/payload_sha1=<YOURHASH>/g\" /lib/onie/onie-updater I tried sed \"s/payload\\=.*/payload_sha1=$(sha1sum <THING> | cut -d \" \" -f 1)/g\" /lib/onie/onie-updater and it wouldn't let me do it","title":"Create OpenSwitch VM"},{"location":"Create%20OpenSwitch%20VM/#create-openswitch-vm","text":"Download onie-kvm.iso Create a VM - it must use BIOS boot mode WARNING If you are running on VMWare ESXi you must use an IDE hard drive! WARNING If you are running on VMWare ESXi you must use the E1000E driver for the network card! Boot from the CD, but at the grub menu, hit tab to edit the options and remove all console options See: https://askubuntu.com/questions/771871/16-04-virtualbox-vm-from-vhd-file-hangs-at-non-blocking-pool-is-initialized Once you drop to the ONIE command line run sed -i 's/vda/sda/g' /lib/onie/onie-updater Workable fix: sed 's/if \\[ \"$sha1.*/if false; then/g' /lib/onie/onie-updater Run onie-self-update -e file://lib/onie/onie-updater Reboot. For whatever reason I couldn't get the discovery process to work so I ran onie-discovery-stop Then run onie-nos-install http://<SERVER_IP>/onie-installer","title":"Create OpenSwitch VM"},{"location":"Create%20OpenSwitch%20VM/#faster-fix-that-didnt-work","text":"Run sha1sum /lib/onie/onie-updater and note the sha1 hash Run sed \"s/payload\\=.*/payload_sha1=<YOURHASH>/g\" /lib/onie/onie-updater I tried sed \"s/payload\\=.*/payload_sha1=$(sha1sum <THING> | cut -d \" \" -f 1)/g\" /lib/onie/onie-updater and it wouldn't let me do it","title":"Faster fix that didn't work"},{"location":"DHCP%20Relay%20on%20SONiC/","text":"DHCP Relay on SONiC Configuring a Cross VLAN DHCP Relay on SONiC OS My Configuration IP Addresses Test Concept 4112F-ON OS 10 Version Z9264 SONiC Version RHEL Version Configuration of Devices Configuration of RHEL DHCP Server OS10 on 4112F-ON SONiC OS on Z9264 Helpful Commands Configuration on ESXi Running the Test Lease Record on RHEL tcpdump from SONiC Address Info From OS10 My Configuration Dell 4112F-ON running OS 10 (interface eth1/1/13 to interface ethernet 0 [all VLAN 99]) | Dell Z9264 running SONiC OS (interface ethernet 257 to interface vmnic7 [all vlan 100]) | Dell R840 running ESXi (virtual switch with portgroup on vlan 100 going to VM's virtual NIC) | RHEL Virtual Machine with DHCP server IP Addresses RHEL VM: 192.168.100.5/24 Z9264 SVI interface on VLAN 99: 192.168.99.1/24 Z9264 SVI interface on VLAN 100: 192.168.100.1/24 4112F-ON SVI interface on VLAN 99 - DHCP Test Concept DHCP request will go from SVI interface on 4112F-ON residing on VLAN 99 through the Z9264, to ESXi, and onto the DHCP server running on the RHEL virtual machine on VLAN 100 running dhcpd. 4112F-ON OS 10 Version Dell EMC Networking OS10 Enterprise Copyright (c) 1999-2020 by Dell Inc. All Rights Reserved. OS Version: 10.5.1.3 Build Version: 10.5.1.3.190 Build Time: 2020-06-19T21:48:07+0000 System Type: S4112F-ON Architecture: x86_64 Up Time: 1 day 01:29:03 Z9264 SONiC Version Software Version : '3.4.0-Enterprise_Base' Product : Enterprise SONiC Distribution by Dell Technologies Distribution : '9.13' Kernel : '4.9.0-11-2-amd64' Config DB Version : version_3_3_1 Build Commit : 'e2f258af7' Build Date : Wed Jul 28 23:54:33 UTC 2021 Built By : sonicbld@sonic-lvn-csg-005 Platform : x86_64-dellemc_z9264f_c3538-r0 HwSKU : DellEMC-Z9264f-C64 ASIC : broadcom Hardware Version : A00 Serial Number : TW0XXP63DNT008970001 Uptime : 04:53:36 up 2:05, 1 user, load average: 1.04, 0.94, 0.95 Mfg : Dell EMC RHEL Version [root@freeipa ~]# cat /etc/*-release NAME=\"Red Hat Enterprise Linux\" VERSION=\"8.2 (Ootpa)\" ID=\"rhel\" ID_LIKE=\"fedora\" VERSION_ID=\"8.2\" PLATFORM_ID=\"platform:el8\" PRETTY_NAME=\"Red Hat Enterprise Linux 8.2 (Ootpa)\" ANSI_COLOR=\"0;31\" CPE_NAME=\"cpe:/o:redhat:enterprise_linux:8.2:GA\" HOME_URL=\"https://www.redhat.com/\" BUG_REPORT_URL=\"https://bugzilla.redhat.com/\" REDHAT_BUGZILLA_PRODUCT=\"Red Hat Enterprise Linux 8\" REDHAT_BUGZILLA_PRODUCT_VERSION=8.2 REDHAT_SUPPORT_PRODUCT=\"Red Hat Enterprise Linux\" REDHAT_SUPPORT_PRODUCT_VERSION=\"8.2\" Red Hat Enterprise Linux release 8.2 (Ootpa) Red Hat Enterprise Linux release 8.2 (Ootpa) Configuration of Devices Configuration of RHEL DHCP Server See: Configuring RHEL DHCP Server sudo dnf install -y dhcp-server ip route add 192.168.99.0/24 via 192.168.100.1 dev ens224 vim /etc/dhcp/dhcpd.conf I used configuration: # # DHCP Server Configuration file. # see /usr/share/doc/dhcp-server/dhcpd.conf.example # see dhcpd.conf(5) man page # option domain-name \"lan\"; default-lease-time 86400; authoritative; shared-network lan { subnet 192.168.99.0 netmask 255.255.255.0 { range 192.168.99.100 192.168.99.200; option routers 192.168.99.1; } } subnet 192.168.100.0 netmask 255.255.255.0 {} OS10 on 4112F-ON Full Configuration configure terminal interface ethernet 1/1/13 switchport mode trunk switchport trunk allowed vlan 99 exit interface vlan 99 ip address dhcp SONiC OS on Z9264 Full Configuration sonic-cli configure terminal interface ethernet 0 no shutdown fec rs speed 100000 switchport trunk allowed Vlan 99 exit interface ethernet 257 no shutdown switchport trunk allowed Vlan 100 exit interface Vlan 99 ip address 192.168.99.1/24 ip dhcp-relay 192.168.100.5 exit interface Vlan 100 ip address 192.168.100.1/24 exit NOTE : I had to manually configure FEC on the 100Gb/s interface to bring it up. Helpful Commands show interface transceiver NOTE : Unlike OS10 all interfaces start in shutdown mode so you will need to bring them up. Configuration on ESXi Running the Test Lease Record on RHEL [root@freeipa ~]# cat /var/lib/dhcpd/dhcpd.leases # The format of this file is documented in the dhcpd.leases(5) manual page. # This lease file was written by isc-dhcp-4.3.6 # authoring-byte-order entry is generated, DO NOT DELETE authoring-byte-order little-endian; server-duid \"\\000\\001\\000\\001)A@\\255\\000PV\\276\\261\\016\"; lease 192.168.99.100 { starts 1 2021/12/06 21:50:25; ends 2 2021/12/07 21:50:25; cltt 1 2021/12/06 21:50:25; binding state active; next binding state free; rewind binding state free; hardware ethernet 88:6f:d4:98:b7:b1; option agent.circuit-id \"Vlan99\"; option agent.remote-id \"20:04:0f:06:44:b4\"; client-hostname \"OS10\"; } tcpdump from SONiC Shows DHCP transiting the relay. Address Info From OS10 OS10(conf-if-vl-99)# do show ip interface brief Interface Name IP-Address OK Method Status Protocol ========================================================================================= Ethernet 1/1/1 unassigned NO unset up down Ethernet 1/1/2 unassigned NO unset up down Ethernet 1/1/3 unassigned NO unset up down Ethernet 1/1/4 unassigned NO unset up down Ethernet 1/1/5 unassigned NO unset up down Ethernet 1/1/6 unassigned NO unset up down Ethernet 1/1/7 unassigned NO unset up down Ethernet 1/1/8 unassigned NO unset up down Ethernet 1/1/9 unassigned NO unset up down Ethernet 1/1/10 unassigned NO unset up down Ethernet 1/1/11 unassigned NO unset up down Ethernet 1/1/12 unassigned NO unset up down Ethernet 1/1/13 unassigned YES unset up up Ethernet 1/1/14 unassigned NO unset up down Ethernet 1/1/15 unassigned NO unset up down Management 1/1/1 192.168.1.24/24 YES manual up up Vlan 1 unassigned NO unset up down Vlan 99 192.168.99.100/24 YES DHCP up up","title":"DHCP Relay on SONiC"},{"location":"DHCP%20Relay%20on%20SONiC/#dhcp-relay-on-sonic","text":"Configuring a Cross VLAN DHCP Relay on SONiC OS My Configuration IP Addresses Test Concept 4112F-ON OS 10 Version Z9264 SONiC Version RHEL Version Configuration of Devices Configuration of RHEL DHCP Server OS10 on 4112F-ON SONiC OS on Z9264 Helpful Commands Configuration on ESXi Running the Test Lease Record on RHEL tcpdump from SONiC Address Info From OS10","title":"DHCP Relay on SONiC"},{"location":"DHCP%20Relay%20on%20SONiC/#my-configuration","text":"Dell 4112F-ON running OS 10 (interface eth1/1/13 to interface ethernet 0 [all VLAN 99]) | Dell Z9264 running SONiC OS (interface ethernet 257 to interface vmnic7 [all vlan 100]) | Dell R840 running ESXi (virtual switch with portgroup on vlan 100 going to VM's virtual NIC) | RHEL Virtual Machine with DHCP server","title":"My Configuration"},{"location":"DHCP%20Relay%20on%20SONiC/#ip-addresses","text":"RHEL VM: 192.168.100.5/24 Z9264 SVI interface on VLAN 99: 192.168.99.1/24 Z9264 SVI interface on VLAN 100: 192.168.100.1/24 4112F-ON SVI interface on VLAN 99 - DHCP","title":"IP Addresses"},{"location":"DHCP%20Relay%20on%20SONiC/#test-concept","text":"DHCP request will go from SVI interface on 4112F-ON residing on VLAN 99 through the Z9264, to ESXi, and onto the DHCP server running on the RHEL virtual machine on VLAN 100 running dhcpd.","title":"Test Concept"},{"location":"DHCP%20Relay%20on%20SONiC/#4112f-on-os-10-version","text":"Dell EMC Networking OS10 Enterprise Copyright (c) 1999-2020 by Dell Inc. All Rights Reserved. OS Version: 10.5.1.3 Build Version: 10.5.1.3.190 Build Time: 2020-06-19T21:48:07+0000 System Type: S4112F-ON Architecture: x86_64 Up Time: 1 day 01:29:03","title":"4112F-ON OS 10 Version"},{"location":"DHCP%20Relay%20on%20SONiC/#z9264-sonic-version","text":"Software Version : '3.4.0-Enterprise_Base' Product : Enterprise SONiC Distribution by Dell Technologies Distribution : '9.13' Kernel : '4.9.0-11-2-amd64' Config DB Version : version_3_3_1 Build Commit : 'e2f258af7' Build Date : Wed Jul 28 23:54:33 UTC 2021 Built By : sonicbld@sonic-lvn-csg-005 Platform : x86_64-dellemc_z9264f_c3538-r0 HwSKU : DellEMC-Z9264f-C64 ASIC : broadcom Hardware Version : A00 Serial Number : TW0XXP63DNT008970001 Uptime : 04:53:36 up 2:05, 1 user, load average: 1.04, 0.94, 0.95 Mfg : Dell EMC","title":"Z9264 SONiC Version"},{"location":"DHCP%20Relay%20on%20SONiC/#rhel-version","text":"[root@freeipa ~]# cat /etc/*-release NAME=\"Red Hat Enterprise Linux\" VERSION=\"8.2 (Ootpa)\" ID=\"rhel\" ID_LIKE=\"fedora\" VERSION_ID=\"8.2\" PLATFORM_ID=\"platform:el8\" PRETTY_NAME=\"Red Hat Enterprise Linux 8.2 (Ootpa)\" ANSI_COLOR=\"0;31\" CPE_NAME=\"cpe:/o:redhat:enterprise_linux:8.2:GA\" HOME_URL=\"https://www.redhat.com/\" BUG_REPORT_URL=\"https://bugzilla.redhat.com/\" REDHAT_BUGZILLA_PRODUCT=\"Red Hat Enterprise Linux 8\" REDHAT_BUGZILLA_PRODUCT_VERSION=8.2 REDHAT_SUPPORT_PRODUCT=\"Red Hat Enterprise Linux\" REDHAT_SUPPORT_PRODUCT_VERSION=\"8.2\" Red Hat Enterprise Linux release 8.2 (Ootpa) Red Hat Enterprise Linux release 8.2 (Ootpa)","title":"RHEL Version"},{"location":"DHCP%20Relay%20on%20SONiC/#configuration-of-devices","text":"","title":"Configuration of Devices"},{"location":"DHCP%20Relay%20on%20SONiC/#configuration-of-rhel-dhcp-server","text":"See: Configuring RHEL DHCP Server sudo dnf install -y dhcp-server ip route add 192.168.99.0/24 via 192.168.100.1 dev ens224 vim /etc/dhcp/dhcpd.conf I used configuration: # # DHCP Server Configuration file. # see /usr/share/doc/dhcp-server/dhcpd.conf.example # see dhcpd.conf(5) man page # option domain-name \"lan\"; default-lease-time 86400; authoritative; shared-network lan { subnet 192.168.99.0 netmask 255.255.255.0 { range 192.168.99.100 192.168.99.200; option routers 192.168.99.1; } } subnet 192.168.100.0 netmask 255.255.255.0 {}","title":"Configuration of RHEL DHCP Server"},{"location":"DHCP%20Relay%20on%20SONiC/#os10-on-4112f-on","text":"Full Configuration configure terminal interface ethernet 1/1/13 switchport mode trunk switchport trunk allowed vlan 99 exit interface vlan 99 ip address dhcp","title":"OS10 on 4112F-ON"},{"location":"DHCP%20Relay%20on%20SONiC/#sonic-os-on-z9264","text":"Full Configuration sonic-cli configure terminal interface ethernet 0 no shutdown fec rs speed 100000 switchport trunk allowed Vlan 99 exit interface ethernet 257 no shutdown switchport trunk allowed Vlan 100 exit interface Vlan 99 ip address 192.168.99.1/24 ip dhcp-relay 192.168.100.5 exit interface Vlan 100 ip address 192.168.100.1/24 exit NOTE : I had to manually configure FEC on the 100Gb/s interface to bring it up.","title":"SONiC OS on Z9264"},{"location":"DHCP%20Relay%20on%20SONiC/#helpful-commands","text":"show interface transceiver NOTE : Unlike OS10 all interfaces start in shutdown mode so you will need to bring them up.","title":"Helpful Commands"},{"location":"DHCP%20Relay%20on%20SONiC/#configuration-on-esxi","text":"","title":"Configuration on ESXi"},{"location":"DHCP%20Relay%20on%20SONiC/#running-the-test","text":"","title":"Running the Test"},{"location":"DHCP%20Relay%20on%20SONiC/#lease-record-on-rhel","text":"[root@freeipa ~]# cat /var/lib/dhcpd/dhcpd.leases # The format of this file is documented in the dhcpd.leases(5) manual page. # This lease file was written by isc-dhcp-4.3.6 # authoring-byte-order entry is generated, DO NOT DELETE authoring-byte-order little-endian; server-duid \"\\000\\001\\000\\001)A@\\255\\000PV\\276\\261\\016\"; lease 192.168.99.100 { starts 1 2021/12/06 21:50:25; ends 2 2021/12/07 21:50:25; cltt 1 2021/12/06 21:50:25; binding state active; next binding state free; rewind binding state free; hardware ethernet 88:6f:d4:98:b7:b1; option agent.circuit-id \"Vlan99\"; option agent.remote-id \"20:04:0f:06:44:b4\"; client-hostname \"OS10\"; }","title":"Lease Record on RHEL"},{"location":"DHCP%20Relay%20on%20SONiC/#tcpdump-from-sonic","text":"Shows DHCP transiting the relay.","title":"tcpdump from SONiC"},{"location":"DHCP%20Relay%20on%20SONiC/#address-info-from-os10","text":"OS10(conf-if-vl-99)# do show ip interface brief Interface Name IP-Address OK Method Status Protocol ========================================================================================= Ethernet 1/1/1 unassigned NO unset up down Ethernet 1/1/2 unassigned NO unset up down Ethernet 1/1/3 unassigned NO unset up down Ethernet 1/1/4 unassigned NO unset up down Ethernet 1/1/5 unassigned NO unset up down Ethernet 1/1/6 unassigned NO unset up down Ethernet 1/1/7 unassigned NO unset up down Ethernet 1/1/8 unassigned NO unset up down Ethernet 1/1/9 unassigned NO unset up down Ethernet 1/1/10 unassigned NO unset up down Ethernet 1/1/11 unassigned NO unset up down Ethernet 1/1/12 unassigned NO unset up down Ethernet 1/1/13 unassigned YES unset up up Ethernet 1/1/14 unassigned NO unset up down Ethernet 1/1/15 unassigned NO unset up down Management 1/1/1 192.168.1.24/24 YES manual up up Vlan 1 unassigned NO unset up down Vlan 99 192.168.99.100/24 YES DHCP up up","title":"Address Info From OS10"},{"location":"Dell%20Ansible%20Testing/","text":"Dell Ansible Testing Install Ansible Modules Install Dell Ansible modules from here git clone -b devel --single-branch https://github.com/dell/dellemc-openmanage-ansible-modules.git cd dellemc-openmanage-ansible-modules python install.py Create a CIFs/NFS share from which to share files. I used Samba with this tutorial Note: If you get weird errors in the logs when starting SMB, you probably have a type-o in your smb.conf. I saw different things in different tutorials, but to get firewalld running I used firewall-cmd --permanent --add-service=samba && firewall-cmd --reload Deploy Operating System (Incomplete) Next you will need to create a \"Kickstartable\" ISO. I chose to use RHEL8. I used the free developer license of RHEL 8 I used this post to create the ISO. I used this site to generate my Kickstart config. Update I was going to use RHEL 8. However it looks like there is a bug that is a hard stop for RHEL 8 Kickstart so I'm swapping to CentOS.","title":"Dell Ansible Testing"},{"location":"Dell%20Ansible%20Testing/#dell-ansible-testing","text":"","title":"Dell Ansible Testing"},{"location":"Dell%20Ansible%20Testing/#install-ansible-modules","text":"Install Dell Ansible modules from here git clone -b devel --single-branch https://github.com/dell/dellemc-openmanage-ansible-modules.git cd dellemc-openmanage-ansible-modules python install.py Create a CIFs/NFS share from which to share files. I used Samba with this tutorial Note: If you get weird errors in the logs when starting SMB, you probably have a type-o in your smb.conf. I saw different things in different tutorials, but to get firewalld running I used firewall-cmd --permanent --add-service=samba && firewall-cmd --reload","title":"Install Ansible Modules"},{"location":"Dell%20Ansible%20Testing/#deploy-operating-system-incomplete","text":"Next you will need to create a \"Kickstartable\" ISO. I chose to use RHEL8. I used the free developer license of RHEL 8 I used this post to create the ISO. I used this site to generate my Kickstart config. Update I was going to use RHEL 8. However it looks like there is a bug that is a hard stop for RHEL 8 Kickstart so I'm swapping to CentOS.","title":"Deploy Operating System (Incomplete)"},{"location":"Elasticsearch%20Display%20Map%20Data/","text":"Elasticsearch Display Map Data Environment CentOS7 I tried this first with RHEL8 and the backend docker networking didn't work Installation Install docker with the following: yum install -y https://download.docker.com/linux/centos/7/x86_64/stable/Packages/containerd.io-1.2.6-3.3.el7.x86_64.rpm epel-release yum config-manager --add-repo=https://download.docker.com/linux/centos/docker-ce.repo yum install -y docker python-pip python36 curl -L \"https://github.com/docker/compose/releases/download/1.25.5/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose chmod +x /usr/local/bin/docker-compose systemctl enable docker systemctl start docker pip3.6 install geojson elasticsearch Running Elasticsearch with Docker Set VM Max Map In /etc/sysctl.conf add vm.max_map_count=262144 Turn off swap sudo swapoff -a Setup Data Directories If you are bind-mounting a local directory or file, it must be readable by the elasticsearch user. In addition, this user must have write access to the data and log dirs. A good strategy is to grant group access to gid 0 for the local directory. For example, to prepare a local directory for storing data through a bind-mount: mkdir /opt/ mkdir /opt/data01 mkdir /opt/data02 mkdir /opt/data03 mkdir /opt/data04 chmod g+rwx /opt/data* chgrp 0 /opt/data* chmod 777 -R /opt/data* ^ I got lazy. Not sure what the permissions issue is, but I couldn't get the volumes to mount so I gave up and set it to 777. My best guess is that the real problem is it isn't running as user 0 because that's root. It's probably something else. Running Elasticsearch Copy over the docker compose file Now, you must run docker-compose in the folder in which you have the directories . Otherwies you get a permissions error. cd /var/elasticsearch-data/ then run docker-compose up Importing the Data into Elasticsearch I wrote the code in csv2geojson.py to take a CSV I got from ACLED into geoJSON formatted data. The program format.py just formatted the 30 fields into the Python program for ease of use. 1.Modify the code as necessary and then run to get geoJSON formatted data. Next you'll need to upload the mapping file. 1.First you have to create the index with curl -X PUT \"localhost:9200/conflict-data?pretty\" -H 'Content-Type: application/json' -d' { \"settings\" : { \"index\" : { \"number_of_shards\" : 4, \"number_of_replicas\" : 3 } } } ' 2.Then you can upload the mapping with: curl -X PUT localhost:9200/conflict-data/_mapping?pretty -H \"Content-Type: application/json\" -d @mapping.json 3. Now you can import the data with index_data.py . NOTE Make sure you use python3.6 1.You may have to modify the code a bit to get it to ingest properly. Configuring Metricbeat in a container First double check the name of your elastic network with docker network ls It's probably opt_elastic. Docker compose prefixes everything with the directory from which you're running unless you specify the -p option. Pull the container and then run the setup cd /opt docker pull docker.elastic.co/beats/metricbeat:7.7.0 docker run --network opt_elastic docker.elastic.co/beats/metricbeat:7.7.0 setup -E setup.kibana.host=kib01:5601 -E output.elasticsearch.hosts=[\"es01:9200\"] Copy the metricbeat.yml to /opt Importing Map from Somewhere Else Online it will tell you that you need code to import and export objects. This is no longer the case. When I tested in 7.7.0 you could export saved objects from the saved objects menu in Kibana and then import them on the other side. I included the CPU load gauges, my custom queries, and the maps. Import the three ndjson files included in the repo. Helpful Commands Check Heap Size curl -sS -XGET \"localhost:9200/_cat/nodes?h=heap*&v\" Remove Exited Containers sudo docker rm $(docker ps -a -f status=exited -q) Running A Single Elasticsearch Instance docker run -v /opt/elasticsearch:/usr/share/elasticsearch/data --privileged -p 9200:9200 -p 9300:9300 -e \"discovery.type=single-node\" -e ES_JAVA_OPTS=\"-Xms28g -Xmx28g\" docker.elastic.co/elasticsearch/elasticsearch:7.7.0","title":"Elasticsearch Display Map Data"},{"location":"Elasticsearch%20Display%20Map%20Data/#elasticsearch-display-map-data","text":"","title":"Elasticsearch Display Map Data"},{"location":"Elasticsearch%20Display%20Map%20Data/#environment","text":"CentOS7 I tried this first with RHEL8 and the backend docker networking didn't work","title":"Environment"},{"location":"Elasticsearch%20Display%20Map%20Data/#installation","text":"Install docker with the following: yum install -y https://download.docker.com/linux/centos/7/x86_64/stable/Packages/containerd.io-1.2.6-3.3.el7.x86_64.rpm epel-release yum config-manager --add-repo=https://download.docker.com/linux/centos/docker-ce.repo yum install -y docker python-pip python36 curl -L \"https://github.com/docker/compose/releases/download/1.25.5/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose chmod +x /usr/local/bin/docker-compose systemctl enable docker systemctl start docker pip3.6 install geojson elasticsearch","title":"Installation"},{"location":"Elasticsearch%20Display%20Map%20Data/#running-elasticsearch-with-docker","text":"","title":"Running Elasticsearch with Docker"},{"location":"Elasticsearch%20Display%20Map%20Data/#set-vm-max-map","text":"In /etc/sysctl.conf add vm.max_map_count=262144","title":"Set VM Max Map"},{"location":"Elasticsearch%20Display%20Map%20Data/#turn-off-swap","text":"sudo swapoff -a","title":"Turn off swap"},{"location":"Elasticsearch%20Display%20Map%20Data/#setup-data-directories","text":"If you are bind-mounting a local directory or file, it must be readable by the elasticsearch user. In addition, this user must have write access to the data and log dirs. A good strategy is to grant group access to gid 0 for the local directory. For example, to prepare a local directory for storing data through a bind-mount: mkdir /opt/ mkdir /opt/data01 mkdir /opt/data02 mkdir /opt/data03 mkdir /opt/data04 chmod g+rwx /opt/data* chgrp 0 /opt/data* chmod 777 -R /opt/data* ^ I got lazy. Not sure what the permissions issue is, but I couldn't get the volumes to mount so I gave up and set it to 777. My best guess is that the real problem is it isn't running as user 0 because that's root. It's probably something else.","title":"Setup Data Directories"},{"location":"Elasticsearch%20Display%20Map%20Data/#running-elasticsearch","text":"Copy over the docker compose file Now, you must run docker-compose in the folder in which you have the directories . Otherwies you get a permissions error. cd /var/elasticsearch-data/ then run docker-compose up","title":"Running Elasticsearch"},{"location":"Elasticsearch%20Display%20Map%20Data/#importing-the-data-into-elasticsearch","text":"I wrote the code in csv2geojson.py to take a CSV I got from ACLED into geoJSON formatted data. The program format.py just formatted the 30 fields into the Python program for ease of use. 1.Modify the code as necessary and then run to get geoJSON formatted data. Next you'll need to upload the mapping file. 1.First you have to create the index with curl -X PUT \"localhost:9200/conflict-data?pretty\" -H 'Content-Type: application/json' -d' { \"settings\" : { \"index\" : { \"number_of_shards\" : 4, \"number_of_replicas\" : 3 } } } ' 2.Then you can upload the mapping with: curl -X PUT localhost:9200/conflict-data/_mapping?pretty -H \"Content-Type: application/json\" -d @mapping.json 3. Now you can import the data with index_data.py . NOTE Make sure you use python3.6 1.You may have to modify the code a bit to get it to ingest properly.","title":"Importing the Data into Elasticsearch"},{"location":"Elasticsearch%20Display%20Map%20Data/#configuring-metricbeat-in-a-container","text":"First double check the name of your elastic network with docker network ls It's probably opt_elastic. Docker compose prefixes everything with the directory from which you're running unless you specify the -p option. Pull the container and then run the setup cd /opt docker pull docker.elastic.co/beats/metricbeat:7.7.0 docker run --network opt_elastic docker.elastic.co/beats/metricbeat:7.7.0 setup -E setup.kibana.host=kib01:5601 -E output.elasticsearch.hosts=[\"es01:9200\"] Copy the metricbeat.yml to /opt","title":"Configuring Metricbeat in a container"},{"location":"Elasticsearch%20Display%20Map%20Data/#importing-map-from-somewhere-else","text":"Online it will tell you that you need code to import and export objects. This is no longer the case. When I tested in 7.7.0 you could export saved objects from the saved objects menu in Kibana and then import them on the other side. I included the CPU load gauges, my custom queries, and the maps. Import the three ndjson files included in the repo.","title":"Importing Map from Somewhere Else"},{"location":"Elasticsearch%20Display%20Map%20Data/#helpful-commands","text":"","title":"Helpful Commands"},{"location":"Elasticsearch%20Display%20Map%20Data/#check-heap-size","text":"curl -sS -XGET \"localhost:9200/_cat/nodes?h=heap*&v\"","title":"Check Heap Size"},{"location":"Elasticsearch%20Display%20Map%20Data/#remove-exited-containers","text":"sudo docker rm $(docker ps -a -f status=exited -q)","title":"Remove Exited Containers"},{"location":"Elasticsearch%20Display%20Map%20Data/#running-a-single-elasticsearch-instance","text":"docker run -v /opt/elasticsearch:/usr/share/elasticsearch/data --privileged -p 9200:9200 -p 9300:9300 -e \"discovery.type=single-node\" -e ES_JAVA_OPTS=\"-Xms28g -Xmx28g\" docker.elastic.co/elasticsearch/elasticsearch:7.7.0","title":"Running A Single Elasticsearch Instance"},{"location":"Elasticsearch%20Display%20Map%20Data/did_not_work/","text":"Stuff I tried that didn't work GDAL Attempt (Does not work) I originally tried to push the data with GDAL, but wasn't sure how to get the syntax to work. Kibana only allows you to upload 50 MB files so ingest must be done manually. I'm using GDAL. Install here wget https://github.com/OSGeo/gdal/releases/download/v3.1.0/gdal-3.1.0.tar.gz tar xf gdal* On RHEL8 run dnf install -y gcc-toolset-9 && scl enable gcc-toolset-9 bash . This will install gcc v.9 and will open a new bash shell using v9 with appropriate environment variables. On Ubuntu do the following: sudo add-apt-repository ppa:ubuntu-toolchain-r/test sudo apt update sudo apt install gcc-9 pkg-config You will need project 6 from https://download.osgeo.org/proj/proj-6.0.0.zip Run dnf install -y sqlite-devel && ./configure && make && make install You will need to install jasper and libcurl-devel with dnf install -y jasper-devel libcurl-devel Now run ./configure --with-proj=/usr/local && make -j <as many as possible> && make install The make takes an eternity if you don't give it more threads. For utility you may want to install EPEL with sudo dnf install https://dl.fedoraproject.org/pub/epel/epel-release-latest-8.noarch.rpm Creating a mapping with GDAL: ogr2ogr -overwrite -lco INDEX_NAME=gdal-data -lco NOT_ANALYZED_FIELDS={ALL} -lco WRITE_MAPPING=./mapping.json ES:http://localhost:9200 GeoObs.json Uploading with GDAL: ogr2ogr -lco INDEX_NAME=gdal-data -lco OVERWRITE_INDEX=YES -lco MAPPING=./mapping.json ES:http://localhost:9200 GeoObs.json You can run ogrinfo ES:http://localhost:9200 to check the indicies in your Elasticsearch instance and make sure that everything is connected and ready to go. Metricbeat Install Attempt (Does not work) If you want to make it easy don't set up security, but ! Run with podman run -v /opt/elasticsearch:/usr/share/elasticsearch/data --privileged -p 9200:9200 -p 9300:9300 -e \"discovery.type=single-node\" -e xpack.security.enabled=true -e xpack.monitoring.collection.enabled=true -e ES_JAVA_OPTS=\"-Xms16g [53/53]\" docker.elastic.co/elasticsearch/elasticsearch:7.7.0 to run Elasticsearch with security enabled. I ran chmod -R 774 /opt/elasticsearch to change the permissions to allow podman to run unpriveleged. If you need to, you can double check the heap size with: curl -sS -XGET \"localhost:9200/_cat/nodes?h=heap*&v\" If you want to use Metricbeat you'll also have to enable security and configure users. Do the following: Install Metricbeat from here Before starting Metricbeat, go in to the config file at /etc/metricbeat/metricbeat.yml and make sure you have the dashboard upload enabled. Next, you'll need to exec into your Elasticsearch container and setup passwords. 1.Run podman exec -it <CONTAINER_ID> /bin/bash to get a command line on the container. 2.Run /usr/share/elasticsearch/bin/elasticsearch-setup-passwords interactive to setup passwords. Run cd /etc/metricbeat then run metricbeat modules enable elasticsearch-xpack Then edit /etc/metricbeat/metricbeat.yml Make sure the output hosts are correct and then change username and password to what you set earlier. 1.Also make sure that the Kibana host is set correctly. You'll also need to set the Kibana user in /etc/kibana/kibana.yml Helpful Links Examples of usage https://gist.github.com/nickpeihl/1a8f9cbecc78e9e04a73a953b30da84d Ingest geospatial data into Elasticsearch with GDAL Elasticsearch Driver for GDAL Page","title":"Stuff I tried that didn't work"},{"location":"Elasticsearch%20Display%20Map%20Data/did_not_work/#stuff-i-tried-that-didnt-work","text":"","title":"Stuff I tried that didn't work"},{"location":"Elasticsearch%20Display%20Map%20Data/did_not_work/#gdal-attempt-does-not-work","text":"I originally tried to push the data with GDAL, but wasn't sure how to get the syntax to work. Kibana only allows you to upload 50 MB files so ingest must be done manually. I'm using GDAL. Install here wget https://github.com/OSGeo/gdal/releases/download/v3.1.0/gdal-3.1.0.tar.gz tar xf gdal* On RHEL8 run dnf install -y gcc-toolset-9 && scl enable gcc-toolset-9 bash . This will install gcc v.9 and will open a new bash shell using v9 with appropriate environment variables. On Ubuntu do the following: sudo add-apt-repository ppa:ubuntu-toolchain-r/test sudo apt update sudo apt install gcc-9 pkg-config You will need project 6 from https://download.osgeo.org/proj/proj-6.0.0.zip Run dnf install -y sqlite-devel && ./configure && make && make install You will need to install jasper and libcurl-devel with dnf install -y jasper-devel libcurl-devel Now run ./configure --with-proj=/usr/local && make -j <as many as possible> && make install The make takes an eternity if you don't give it more threads. For utility you may want to install EPEL with sudo dnf install https://dl.fedoraproject.org/pub/epel/epel-release-latest-8.noarch.rpm Creating a mapping with GDAL: ogr2ogr -overwrite -lco INDEX_NAME=gdal-data -lco NOT_ANALYZED_FIELDS={ALL} -lco WRITE_MAPPING=./mapping.json ES:http://localhost:9200 GeoObs.json Uploading with GDAL: ogr2ogr -lco INDEX_NAME=gdal-data -lco OVERWRITE_INDEX=YES -lco MAPPING=./mapping.json ES:http://localhost:9200 GeoObs.json You can run ogrinfo ES:http://localhost:9200 to check the indicies in your Elasticsearch instance and make sure that everything is connected and ready to go.","title":"GDAL Attempt (Does not work)"},{"location":"Elasticsearch%20Display%20Map%20Data/did_not_work/#metricbeat-install-attempt-does-not-work","text":"If you want to make it easy don't set up security, but ! Run with podman run -v /opt/elasticsearch:/usr/share/elasticsearch/data --privileged -p 9200:9200 -p 9300:9300 -e \"discovery.type=single-node\" -e xpack.security.enabled=true -e xpack.monitoring.collection.enabled=true -e ES_JAVA_OPTS=\"-Xms16g [53/53]\" docker.elastic.co/elasticsearch/elasticsearch:7.7.0 to run Elasticsearch with security enabled. I ran chmod -R 774 /opt/elasticsearch to change the permissions to allow podman to run unpriveleged. If you need to, you can double check the heap size with: curl -sS -XGET \"localhost:9200/_cat/nodes?h=heap*&v\" If you want to use Metricbeat you'll also have to enable security and configure users. Do the following: Install Metricbeat from here Before starting Metricbeat, go in to the config file at /etc/metricbeat/metricbeat.yml and make sure you have the dashboard upload enabled. Next, you'll need to exec into your Elasticsearch container and setup passwords. 1.Run podman exec -it <CONTAINER_ID> /bin/bash to get a command line on the container. 2.Run /usr/share/elasticsearch/bin/elasticsearch-setup-passwords interactive to setup passwords. Run cd /etc/metricbeat then run metricbeat modules enable elasticsearch-xpack Then edit /etc/metricbeat/metricbeat.yml Make sure the output hosts are correct and then change username and password to what you set earlier. 1.Also make sure that the Kibana host is set correctly. You'll also need to set the Kibana user in /etc/kibana/kibana.yml","title":"Metricbeat Install Attempt (Does not work)"},{"location":"Elasticsearch%20Display%20Map%20Data/did_not_work/#helpful-links","text":"Examples of usage https://gist.github.com/nickpeihl/1a8f9cbecc78e9e04a73a953b30da84d Ingest geospatial data into Elasticsearch with GDAL Elasticsearch Driver for GDAL Page","title":"Helpful Links"},{"location":"Elasticsearch%20Load%20Testing/","text":"Elasticsearch Load Testing Setup Download the chromedriver for Selenium from here . Place it in the same directory as run.py . Run yum update -y to make sure everything is up to date. Reboot if you have any kernel updates. Install python 3 with yum install -y python3 Run pip3 install selenium to install selenium Usage Run python3 run.py for usage information on CentOS. Run python run.py for usage information on Windows. Version I built and tested using Python v3.8.0 on Windows 10.","title":"Elasticsearch Load Testing"},{"location":"Elasticsearch%20Load%20Testing/#elasticsearch-load-testing","text":"","title":"Elasticsearch Load Testing"},{"location":"Elasticsearch%20Load%20Testing/#setup","text":"Download the chromedriver for Selenium from here . Place it in the same directory as run.py . Run yum update -y to make sure everything is up to date. Reboot if you have any kernel updates. Install python 3 with yum install -y python3 Run pip3 install selenium to install selenium","title":"Setup"},{"location":"Elasticsearch%20Load%20Testing/#usage","text":"Run python3 run.py for usage information on CentOS. Run python run.py for usage information on Windows.","title":"Usage"},{"location":"Elasticsearch%20Load%20Testing/#version","text":"I built and tested using Python v3.8.0 on Windows 10.","title":"Version"},{"location":"Estimating%20Compute%20Requirements%20for%20Machine%20Learning/","text":"Estimating Compute Requirements for Machine Learning System Performance Analysis Executive Summary Training Benchmarks Inference Benchmarks Extrapolating Performance Caveats Inference Benchmarking Retinanet Benchmark What is the retinanet benchmark? The Results Understanding the Results Constraints of the Benchmark: Benchmark Results: Executive Brief Interpretation System Description Training Benchmarking MLPerf Benchmark Rules Accuracy Targets for Mask R-CNN and SSD (RetinaNet) Mask R-CNN (Object Detection - Heavy Weight) SSD (RetinaNet) (Object Detection - Light Weight) What is mean Average Precision (mAP) Single Shot MultiBox Detector (SSD) What is SSD Hyperparameters Results Intersection over Union (IoU) Mask Region-based Convolutional Neural Network (Mask R-CNN) What is Mask R-CNN Hyperparameters Results System Description Executive Summary The focus of this analysis is on object detection in images with three algorithms; two for training and one for inference where training is the process of creating a model which can identify objects and inference is using that model to identify the objects. The subject of the analysis is the MLPerf (Machine Learning Performance) benchmark which is the industry standard for judging performance. We specifically examine the results for the Dell XE8960, a GPU-focused server with eight Nvidia H100 GPUs. Training Benchmarks It takes the XE9680 with 8 H100s 37 minutes 21 seconds to pass the Single Shot Multibox Detector (SSD) object detection benchmark where the input data came from OpenImages dataset SSD is generally used for live object detection. Think a Tesla recognizing a stop sign on the fly. The MLPerf benchmark specifies an overall mAP (mean Average Precision) score of 34%. mAP is a measurement is defined as both accurately identifying the existence of objects and correctly drawing a box around the object. Ex: Source The above image demonstrates precision. The other metric fed into the 34% value is correctly identifying all objects. Ex: The model could perfectly draw a box around one object but not notice that there were another 5 objects in the model. This is saying it takes 37 minutes to create a model that is correct 34% of the time. There is some nuance to this, but this is the high level. It takes the XE9680 19 minutes 50 seconds to pass the Mask Region-based Convolutional Neural Networ (Mask R-CNN) benchmark where the training data came from the COCO dataset Mask R-CNN is used for recognizing objects and classifying the parts of objects. It is typically less realtime focused than something like SSD. The MLPerf benchmark specifies that the model must reach Minimum Box mAP (mean Average Precision): 0.377 and Minimum Mask mAP (mean Average Precision): 0.339. The box mAP is described above for SSD and remains unchanged. That is to say, correctly drawing a box around an object. Mask R-CNN also requires the model to draw a mask as pictured below: Source Inference Benchmarks A detailed analysis of all the rules governing the inference benchmark is too complex to place here. The high level is that for the benchmark everyone's model must perform to a certain standard. The key stat is that the model must perform to at least a level of mAP=37.55% where mAP is as described above. The benchmark used for object detection is retinanet The XE9680 with 8 H100s is able to process 12484.05 images per second with a mean latency of 12199791 nanoseconds. Extrapolating Performance The case described above is for a single XE9680 with 8 H100s. In the ideal case, performance scales linearly. However, in the real world performance will scale linearly...ish. The factors affecting this are myriad and complex, but your main bottleneck will be software inefficiencies which prevent the hardware from being fully utilized. Caveats The viability of these numbers and using them to estimate performance hinge on your data being similar to the benchmark data. I have selected these algorithms because they are most relevant to our use case. The training data is an unknown. How many objects you want to detect, how different they are, how well the data is preprocessed, etc all play a massive role in performance. Hundreds of orders of magnitude easily. For example, if the data is preprocessed perfectly then you can expect stunningly accurate results. The opposite is also true. In all of these benchmarks the data has already been labeled by experts. One can easily spend months or years just prepping the data for consumption by a model. Inference Benchmarking Retinanet Benchmark See here for the original results. What is the retinanet benchmark? RetinaNet is a computer program designed to automatically find and identify objects in pictures or videos. Imagine you have a security camera that needs to recognize people, cars, and other objects. RetinaNet is the brain behind that camera. How it works: Object Detection : When you show RetinaNet an image or video frame, it looks for objects in it. Objects could be people, cars, animals, or anything you want it to find. Efficient and Fast : RetinaNet is really good at finding objects quickly. It doesn't waste time by looking at every tiny part of the image. It's like finding a needle in a haystack without checking every straw. Smart Learning : It learns from examples. You show it lots of pictures with objects marked, and it figures out how to recognize those objects in new pictures. Handles Different Sizes : RetinaNet can find both big and small objects. For example, it can spot a person standing far away or a small item up close. Works in Many Fields : People use RetinaNet in all sorts of places. In factories, it checks for defects in products. In self-driving cars, it helps them see other cars and pedestrians. It's even used in security cameras to identify intruders. The Results Metric Value Description Layman's Description SUT name LWIS_Server System Under Test name. The name of the hardware and software configuration used for the benchmark. Scenario Server Benchmark scenario. Indicates the scenario under which the test was conducted, in this case, a server-side inference scenario. Mode PerformanceOnly Benchmarking mode. Focus on achieving high throughput and efficiency. Scheduled samples per second 12484.77 Scheduled rate of inference samples per second. The planned rate at which inference samples are processed per second. Result is VALID Overall benchmark result. Indicates that the benchmark run meets the defined criteria and constraints. Performance constraints satisfied Yes Whether performance constraints are satisfied. Confirms that performance constraints have been met. Min duration satisfied Yes Whether the minimum duration requirement is met. The benchmark ran for at least the required minimum duration. Min queries satisfied Yes Whether the minimum query count requirement is met. The benchmark processed at least the required minimum number of queries. Early stopping satisfied Yes Whether early stopping criteria are met. Successful early stopping. Completed samples per second 12484.05 Actual rate of completed inference samples per second. The achieved rate of processing inference samples per second. Min latency (ns) 8611423 Minimum observed latency in nanoseconds. The shortest time taken for inference. Max latency (ns) 34852012 Maximum observed latency in nanoseconds. The longest time taken for inference. Mean latency (ns) 12199791 Average observed latency in nanoseconds. The typical time taken for inference. 50.00 percentile latency (ns) 12137988 Median latency at the 50th percentile. The middle value of latency observations. 90.00 percentile latency (ns) 14458213 Latency at the 90th percentile. The latency below which 90% of measurements fall. 95.00 percentile latency (ns) 15073964 Latency at the 95th percentile. The latency below which 95% of measurements fall. 97.00 percentile latency (ns) 15527007 Latency at the 97th percentile. The latency below which 97% of measurements fall. 99.00 percentile latency (ns) 16502552 Latency at the 99th percentile. The latency below which 99% of measurements fall. 99.90 percentile latency (ns) 18403063 Latency at the 99.90th percentile. A high percentile latency value. samples_per_query 1 Number of data samples processed per inference query. Each inference query processes one data sample. target_qps 12480 Target queries per second (throughput) for the benchmark. The desired rate of processing inference queries per second. target_latency (ns) 100000000 Target latency in nanoseconds. The desired maximum time allowed for inference. max_async_queries 0 Maximum number of asynchronous queries allowed. No asynchronous queries are allowed. min_duration (ms) 600000 Minimum duration of the benchmark run in milliseconds. The shortest time for the benchmark run. max_duration (ms) 0 Maximum duration of the benchmark run in milliseconds. No maximum duration is set. min_query_count 100 Minimum number of queries to be processed. At least 100 queries must be processed. max_query_count 0 Maximum number of queries to be processed. No maximum query count is set. qsl_rng_seed 148687905518835231 RNG seed for query set list. Seed value for randomizing the query set list. sample_index_rng_seed 520418551913322573 RNG seed for sample index. Seed value for randomizing sample indices. schedule_rng_seed 811580660758947900 RNG seed for scheduling. Seed value for scheduling-related randomization. accuracy_log_rng_seed 0 RNG seed for accuracy log entries. Seed value for generating accuracy log entries. accuracy_log_probability 0 Probability of logging accuracy information. The likelihood of logging accuracy information. accuracy_log_sampling_target 0 Target for accuracy log sampling. The desired level of sampling accuracy information. print_timestamps 0 Whether timestamps were printed. Indicates if timestamps were included in the output. performance_issue_unique 0 Flag for unique performance issue. Indicates the presence of a unique performance issue. performance_issue_same 0 Flag for the same performance issue. Indicates the presence of the same performance issue. performance_issue_same_index 0 Index of a performance issue. Identifies the specific index of a performance issue. performance_sample_count 64 Specific value not provided. The count of performance samples. Understanding the Results Constraints of the Benchmark: Performance Constraints: The benchmark sets certain performance expectations that the AI system needs to meet. In this case, the AI system was able to meet these performance standards. It performed efficiently and met the required speed and accuracy criteria. Minimum Duration: The benchmark specifies a minimum duration for the test, ensuring that the AI system runs for a specific amount of time to collect meaningful data. In this test, the AI system ran for at least 600,000 milliseconds (10 minutes). Minimum Query Count: To ensure thorough testing, the benchmark requires that a minimum number of queries (inquiries or requests) be processed. In this case, at least 100 queries needed to be processed to evaluate the system's performance. Early Stopping: The benchmark has a mechanism for early stopping, which means if the AI system performs exceptionally well before completing the full test, it can stop early. In this test, early stopping criteria were met successfully. Benchmark Results: Overall Result: The overall result of the benchmark is labeled as \"VALID,\" indicating that the AI system performed well and met the defined criteria and constraints. Essentially, it passed the test. Latency: Latency refers to the time it takes for the AI system to process a request or query. The benchmark measured various aspects of latency, including the fastest (8,611,423 nanoseconds) and slowest (34,852,012 nanoseconds) response times. On average, the AI system took approximately 12,199,791 nanoseconds to process a request. Throughput: Throughput measures how fast the AI system can handle requests. In this case, the system processed around 12,484 requests per second, indicating its ability to handle a high volume of requests efficiently. Additional Stats: The benchmark also provided additional statistics about the AI system's performance across different scenarios, such as different object sizes in object detection tasks. These statistics help assess how well the system can detect objects of various sizes in images. In summary, this benchmark rigorously tested the AI system's performance, ensuring it met speed and accuracy requirements, ran for a sufficient duration, and processed a minimum number of queries. The AI system successfully passed the test, demonstrating its efficiency in handling requests with varying levels of complexity and object sizes in image recognition tasks. Executive Brief Interpretation The benchmark results for the XE9680 with eight H100s system indicate outstanding performance in processing image-related tasks. The system achieved a remarkable rate of approximately 12,484.77 image tasks per second, demonstrating its efficiency in handling image-based workloads. When assessing response times, the system consistently delivered rapid results. The minimum response time observed during testing was 8,611,423 nanoseconds (ns), highlighting the system's ability to swiftly process image tasks. Even at higher percentiles, response times remained impressive, with the 99.90th percentile response time at approximately 18,403,063 ns. Importantly, the system met all specified requirements and criteria, including performance constraints, minimum duration, and query count. It also successfully met early stopping criteria, indicating a high level of performance reliability. The benchmark employed settings that align with the system's focus on efficiently handling image tasks. It aimed for a throughput of 12,480 image tasks per second with a target response time of 100,000,000 ns. In summary, the system showcased exceptional performance in processing image-related tasks, making it well-suited for demanding applications that require fast and efficient image processing capabilities. System Description Field Value accelerator_frequency accelerator_host_interconnect PCIe Gen5 x16 accelerator_interconnect TBD accelerator_interconnect_topology accelerator_memory_capacity 80 GB accelerator_memory_configuration HBM3 accelerator_model_name NVIDIA H100-SXM-80GB accelerator_on-chip_memories accelerators_per_node 8 boot_firmware_version cooling air-cooled disk_controllers disk_drives division closed filesystem framework TensorRT 9.0.0, CUDA 12.2 host_memory_capacity 2 TB host_memory_configuration TBD host_networking Infiniband host_networking_topology N/A host_network_card_count 8x 400Gb Infiniband system_type_detail TBD host_processor_caches host_processor_core_count 52 host_processor_frequency host_processor_interconnect host_processor_model_name Intel(R) Xeon(R) Platinum 8470 host_processors_per_node 2 host_storage_capacity 3 TB host_storage_type NVMe SSD hw_notes management_firmware_version network_speed_mbit nics_enabled_connected nics_enabled_firmware nics_enabled_os number_of_nodes 1 number_of_type_nics_installed operating_system Ubuntu 22.04 other_hardware other_software_stack TensorRT 9.0.0, CUDA 12.2, cuDNN 8.8.0, Driver 525.85.12, DALI 1.28.0 power_management power_supply_details power_supply_quantity_and_rating_watts status available submitter Dell sw_notes system_name Dell PowerEdge XE9680 (8x H100-SXM-80GB, TensorRT) system_type datacenter Training Benchmarking MLPerf Benchmark Rules The rules for the training models are available here . Accuracy Targets for Mask R-CNN and SSD (RetinaNet) Mask R-CNN (Object Detection - Heavy Weight) Minimum Box mAP (mean Average Precision): 0.377 Minimum Mask mAP (mean Average Precision): 0.339 Description : For Mask R-CNN, these accuracy targets represent the model's ability to identify and outline objects in images. The \"Box mAP\" target of 0.377 means that it should correctly draw bounding boxes around objects in images about 38% of the time. The \"Mask mAP\" target of 0.339 means that it should accurately outline the shapes of these objects about 34% of the time. SSD (RetinaNet) (Object Detection - Light Weight) Minimum mAP (mean Average Precision): 34.0% Description : In the case of SSD (RetinaNet), these accuracy targets signify the model's capability to detect objects in images. The mAP target of 34.0% means that it should correctly identify objects in images with an accuracy of at least 34%. For instance, when shown 100 images with objects, it should accurately locate those objects in about 34 of those images. What is mean Average Precision (mAP) A great description of mAP is available here The bottom line is it is a measurement of both how correctly the model draws a box around a known target object and how well does it identify all objects in an image. For example, a precise model with poor recall might accurately identify a single object in an image but not realize that there were ten objects total. However, for that single object, it did precisely draw a box around the object. An imprecise model with high recall might identify all ten objects but the boxes it draws are incorrect. If the model is both precise and has good recall then its map score should be closer to one. The goal of the training benchmark is to see how fast you can train a model to have the specified accuracy as defined my mAP. Single Shot MultiBox Detector (SSD) What is SSD The Single Shot MultiBox Detector (SSD) is a computer vision algorithm designed to facilitate object detection within images or video frames. It is engineered as a sophisticated visual analysis tool with the following key characteristics: Enhanced Visual Perception: SSD equips computational systems with the capability to comprehend and locate objects within visual content. Multi-Faceted Analysis: This algorithm performs multi-scale analysis, simultaneously examining both the comprehensive context and fine-grained details within an image. It then generates predictions regarding the potential locations of objects. Object Identification: For each prediction, SSD attempts to recognize the nature of the object present (e.g., labeling it as a \"car\" or \"dog\") and precisely determine its spatial coordinates within the image. Refined Predictions: SSD employs a sophisticated filtering process to refine and retain the most accurate predictions while discarding less reliable ones. This is akin to selecting the best answers from a pool of possibilities. Final Output: Upon completion of its analysis, SSD presents a detailed report of identified objects, accompanied by bounding boxes delineating their exact positions within the image. Key Advantages of SSD: - Rapid Processing: SSD is distinguished by its speed and efficiency in detecting objects within visual data. - Versatility: It is proficient at detecting objects of varying sizes within a single analysis. - Prudent Filtering: SSD employs intelligent filtering techniques to minimize false identifications. In essence, SSD empowers computer systems to comprehend visual content and efficiently discern objects within images, making it a valuable tool for a wide range of applications in business and technology. Hyperparameters These are defined by MLCommons here Model Optimizer Name Constraint Definition SSD Adam Global Batch Size Arbitrary constant Total number of input examples processed in a training batch. Optimal Learning Rate Warm-up Epochs Integer (>= 0) Number of epochs for learning rate to warm up. Optimal Learning Rate Warm-up Factor Unconstrained Constant factor applied during learning rate warm-up. Optimal Base Learning Rate Unconstrained Base learning rate after warm-up and before decay. Optimal Weight Decay 0 L2 weight decay. Results These results taken from here Metric Value Description Layman's Description Average Precision (AP) @ IoU=0.50:0.95 0.34562 Average precision over various IoU thresholds for all object sizes, with a limit of 100 detections per image. This measures the accuracy of object detection. This measures how well the model finds objects in images. A higher value is better. Average Precision (AP) @ IoU=0.50 0.49204 Average precision at IoU=0.50 for all object sizes, with a limit of 100 detections per image. This measures the accuracy of object detection when objects overlap by 50%. A higher value is better. Average Precision (AP) @ IoU=0.75 0.36934 Average precision at IoU=0.75 for all object sizes, with a limit of 100 detections per image. This measures the accuracy of object detection when objects overlap by 75%. A higher value is better. Average Precision (AP) @ IoU=0.50:0.95 0.00922 Average precision over various IoU thresholds for small objects, with a limit of 100 detections per image. This measures how well the model finds small objects in images. A higher value is better. Average Precision (AP) @ IoU=0.50:0.95 0.10076 Average precision over various IoU thresholds for medium-sized objects, with a limit of 100 detections per image. This measures how well the model finds medium-sized objects in images. A higher value is better. Average Precision (AP) @ IoU=0.50:0.95 0.38291 Average precision over various IoU thresholds for large objects, with a limit of 100 detections per image. This measures how well the model finds large objects in images. A higher value is better. Average Recall (AR) @ IoU=0.50:0.95 0.40965 (maxDets=1) Average recall over various IoU thresholds for all object sizes, with a limit of 1 detection per image. This measures how well the model recalls objects when considering only the most confident detection. A higher value is better. Average Recall (AR) @ IoU=0.50:0.95 0.58156 (maxDets=10) Average recall over various IoU thresholds for all object sizes, with a limit of 10 detections per image. This measures how well the model recalls objects when considering up to 10 detections per image. A higher value is better. Average Recall (AR) @ IoU=0.50:0.95 0.60825 (maxDets=100) Average recall over various IoU thresholds for all object sizes, with a limit of 100 detections per image. This measures how well the model recalls objects when considering up to 100 detections per image. A higher value is better. Average Recall (AR) @ IoU=0.50:0.95 0.03928 (maxDets=100) Average recall over various IoU thresholds for small objects, with a limit of 100 detections per image. This measures how well the model recalls small objects when considering up to 100 detections per image. A higher value is better. Average Recall (AR) @ IoU=0.50:0.95 0.24963 (maxDets=100) Average recall over various IoU thresholds for medium-sized objects, with a limit of 100 detections per image. This measures how well the model recalls medium-sized objects when considering up to 100 detections per image. A higher value is better. Average Recall (AR) @ IoU=0.50:0.95 0.66018 (maxDets=100) Average recall over various IoU thresholds for large objects, with a limit of 100 detections per image. This measures how well the model recalls large objects when considering up to 100 detections per image. A higher value is better. Training Duration 37 minutes 21 seconds Total time taken for training. The time it took to train the model. Throughput 287.7233 samples/s The number of samples processed per second during training. How fast the model can process images during training. MLPerf Metric Time 1322.8699 seconds The total time taken for the MLPerf benchmark. The overall time it took to run the benchmark. Intersection over Union (IoU) Intersection over Union (IoU) is a metric commonly used in object detection and image segmentation tasks to evaluate the accuracy of predicted object boundaries or masks. It quantifies the degree of overlap between the predicted region and the ground truth (actual) region of an object within an image. IoU is calculated as the ratio of the area of intersection between the predicted and ground truth regions to the area of their union. In simpler terms, IoU measures how well a predicted object's location aligns with the actual object's location. It provides a value between 0 and 1, where: IoU = 0 indicates no overlap, meaning the prediction and ground truth have completely different locations. IoU = 1 signifies a perfect match, where the predicted and ground truth regions are identical. IoU is particularly valuable in tasks where precise object localization is crucial, such as object detection and image segmentation, as it helps assess the quality of the predictions and the model's accuracy in delineating objects within images. Mask Region-based Convolutional Neural Network (Mask R-CNN) What is Mask R-CNN Mask R-CNN is a computer vision algorithm designed for advanced object detection and instance segmentation tasks in images or video frames. It is engineered to provide precise and detailed analysis of visual content with the following key characteristics: Object Detection and Segmentation: Mask R-CNN is capable of not only detecting objects within images but also precisely segmenting each object's pixels, providing a mask that outlines its exact shape. Multi-Task Approach: This algorithm simultaneously tackles multiple tasks, including object detection, object classification, and instance segmentation. It excels in providing a comprehensive understanding of visual scenes. Accurate Object Localization: For each detected object, Mask R-CNN not only identifies the object's class (e.g., \"car\" or \"dog\") but also delineates its precise boundaries with pixel-level accuracy. Semantic Segmentation: In addition to instance segmentation, Mask R-CNN can perform semantic segmentation by assigning each pixel in the image to a specific object class. Real-Time Capabilities: Mask R-CNN is designed for real-time or near-real-time performance, making it suitable for applications that require fast and accurate object detection and segmentation. Key Advantages of Mask R-CNN: - High Precision: It provides exceptionally precise object masks and localization, making it suitable for tasks that demand pixel-level accuracy. - Rich Information: The algorithm not only identifies objects but also provides detailed information about each object's shape and class. - Versatility: Mask R-CNN can handle a wide range of object classes and varying object sizes within a single image. In summary, Mask R-CNN is a powerful tool for computer vision tasks that involve object detection, instance segmentation, and semantic segmentation. Its ability to provide detailed and accurate information about objects within images makes it valuable for applications in diverse industries. Hyperparameters These are defined by MLCommons here Model Optimizer Name Constraint Definition Mask R-CNN SGD Max Image Size* Fixed to Reference Maximum size of the longer side Min Image Size* Fixed to Reference Maximum size of the shorter side Num Image Candidates* 1000 or 1000 * Batches per Chip Tunable number of region proposals for given batch size Optimal Learning Rate Warm-up Factor Unconstrained Constant factor applied during learning rate warm-up Optimal Learning Rate Warm-up Steps Unconstrained Number of steps for learning rate to warm up Results Pulled from these results Metric Value Description Layman's Description Average Precision (AP) @ IoU=0.50:0.95 0.34411 Average precision over various IoU thresholds for all object sizes, with a limit of 100 detections per image. This measures the accuracy of object detection. This measures how well the model finds objects in images. A higher value is better. Average Precision (AP) @ IoU=0.50 0.56214 Average precision at IoU=0.50 for all object sizes, with a limit of 100 detections per image. This measures the accuracy of object detection when objects overlap by 50%. A higher value is better. Average Precision (AP) @ IoU=0.75 0.36660 Average precision at IoU=0.75 for all object sizes, with a limit of 100 detections per image. This measures the accuracy of object detection when objects overlap by 75%. A higher value is better. Average Precision (AP) @ IoU=0.50:0.95 0.15656 Average precision over various IoU thresholds for small objects, with a limit of 100 detections per image. This measures how well the model finds small objects in images. A higher value is better. Average Precision (AP) @ IoU=0.50:0.95 0.36903 Average precision over various IoU thresholds for medium-sized objects, with a limit of 100 detections per image. This measures how well the model finds medium-sized objects in images. A higher value is better. Average Precision (AP) @ IoU=0.50:0.95 0.50665 Average precision over various IoU thresholds for large objects, with a limit of 100 detections per image. This measures how well the model finds large objects in images. A higher value is better. Average Recall (AR) @ IoU=0.50:0.95 0.29223 (maxDets=1) Average recall over various IoU thresholds for all object sizes, with a limit of 1 detection per image. This measures how well the model recalls objects when considering only the most confident detection. A higher value is better. Average Recall (AR) @ IoU=0.50:0.95 0.44859 (maxDets=10) Average recall over various IoU thresholds for all object sizes, with a limit of 10 detections per image. This measures how well the model recalls objects when considering up to 10 detections per image. A higher value is better. Average Recall (AR) @ IoU=0.50:0.95 0.46795 (maxDets=100) Average recall over various IoU thresholds for all object sizes, with a limit of 100 detections per image. This measures how well the model recalls objects when considering up to 100 detections per image. A higher value is better. Average Recall (AR) @ IoU=0.50:0.95 0.27618 (maxDets=100) Average recall over various IoU thresholds for small objects, with a limit of 100 detections per image. This measures how well the model recalls small objects when considering up to 100 detections per image. A higher value is better. Average Recall (AR) @ IoU=0.50:0.95 0.50280 (maxDets=100) Average recall over various IoU thresholds for medium-sized objects, with a limit of 100 detections per image. This measures how well the model recalls medium-sized objects when considering up to 100 detections per image. A higher value is better. Average Recall (AR) @ IoU=0.50:0.95 0.61762 (maxDets=100) Average recall over various IoU thresholds for large objects, with a limit of 100 detections per image. This measures how well the model recalls large objects when considering up to 100 detections per image. A higher value is better. Training Duration 19 minutes 50 seconds Total time taken for training. The time it took to train the model. Throughput 1388.8244 samples/s The number of samples processed per second during training. How fast the model can process images during training. MLPerf Metric Time 1322.8699 seconds The total time taken for the MLPerf benchmark. The overall time it took to run the benchmark. System Description Field Value submitter Dell division closed status onprem system_name XE9680x8H100-SXM-80GB number_of_nodes 1 host_processors_per_node 2 host_processor_model_name Intel(R) Xeon(R) Platinum 8470 host_processor_core_count 52 host_processor_vcpu_count 208 host_processor_frequency host_processor_caches N/A host_processor_interconnect host_memory_capacity 1.024 TB host_storage_type NVMe host_storage_capacity 4x6.4TB NVMe host_networking host_networking_topology N/A host_memory_configuration 32x 32GB DDR5 accelerators_per_node 8 accelerator_model_name NVIDIA H100-SXM5-80GB accelerator_host_interconnect PCIe 5.0x16 accelerator_frequency 1980MHz accelerator_on-chip_memories accelerator_memory_configuration HBM3 accelerator_memory_capacity 80 GB accelerator_interconnect 18xNVLink 25GB/s + 4xNVSwitch accelerator_interconnect_topology cooling hw_notes GPU TDP:700W framework NGC MXNet 23.04, NGC Pytorch 23.04, NGC HugeCTR 23.04 other_software_stack cuda_version: 12.0, cuda_driver_version: 530.30.02, cublas_version: 12.1.3, cudnn_version: 8.9.0, trt_version: 8.6.1, dali_version: 1.23.0, nccl_version: 2.17.1, openmpi_version: 4.1.4+, mofed_version: 5.4-rdmacore36.0 operating_system Red Hat Enterprise Linux 9.1 sw_notes N/A","title":"Estimating Compute Requirements for Machine Learning"},{"location":"Estimating%20Compute%20Requirements%20for%20Machine%20Learning/#estimating-compute-requirements-for-machine-learning","text":"System Performance Analysis Executive Summary Training Benchmarks Inference Benchmarks Extrapolating Performance Caveats Inference Benchmarking Retinanet Benchmark What is the retinanet benchmark? The Results Understanding the Results Constraints of the Benchmark: Benchmark Results: Executive Brief Interpretation System Description Training Benchmarking MLPerf Benchmark Rules Accuracy Targets for Mask R-CNN and SSD (RetinaNet) Mask R-CNN (Object Detection - Heavy Weight) SSD (RetinaNet) (Object Detection - Light Weight) What is mean Average Precision (mAP) Single Shot MultiBox Detector (SSD) What is SSD Hyperparameters Results Intersection over Union (IoU) Mask Region-based Convolutional Neural Network (Mask R-CNN) What is Mask R-CNN Hyperparameters Results System Description","title":"Estimating Compute Requirements for Machine Learning"},{"location":"Estimating%20Compute%20Requirements%20for%20Machine%20Learning/#executive-summary","text":"The focus of this analysis is on object detection in images with three algorithms; two for training and one for inference where training is the process of creating a model which can identify objects and inference is using that model to identify the objects. The subject of the analysis is the MLPerf (Machine Learning Performance) benchmark which is the industry standard for judging performance. We specifically examine the results for the Dell XE8960, a GPU-focused server with eight Nvidia H100 GPUs.","title":"Executive Summary"},{"location":"Estimating%20Compute%20Requirements%20for%20Machine%20Learning/#training-benchmarks","text":"It takes the XE9680 with 8 H100s 37 minutes 21 seconds to pass the Single Shot Multibox Detector (SSD) object detection benchmark where the input data came from OpenImages dataset SSD is generally used for live object detection. Think a Tesla recognizing a stop sign on the fly. The MLPerf benchmark specifies an overall mAP (mean Average Precision) score of 34%. mAP is a measurement is defined as both accurately identifying the existence of objects and correctly drawing a box around the object. Ex: Source The above image demonstrates precision. The other metric fed into the 34% value is correctly identifying all objects. Ex: The model could perfectly draw a box around one object but not notice that there were another 5 objects in the model. This is saying it takes 37 minutes to create a model that is correct 34% of the time. There is some nuance to this, but this is the high level. It takes the XE9680 19 minutes 50 seconds to pass the Mask Region-based Convolutional Neural Networ (Mask R-CNN) benchmark where the training data came from the COCO dataset Mask R-CNN is used for recognizing objects and classifying the parts of objects. It is typically less realtime focused than something like SSD. The MLPerf benchmark specifies that the model must reach Minimum Box mAP (mean Average Precision): 0.377 and Minimum Mask mAP (mean Average Precision): 0.339. The box mAP is described above for SSD and remains unchanged. That is to say, correctly drawing a box around an object. Mask R-CNN also requires the model to draw a mask as pictured below: Source","title":"Training Benchmarks"},{"location":"Estimating%20Compute%20Requirements%20for%20Machine%20Learning/#inference-benchmarks","text":"A detailed analysis of all the rules governing the inference benchmark is too complex to place here. The high level is that for the benchmark everyone's model must perform to a certain standard. The key stat is that the model must perform to at least a level of mAP=37.55% where mAP is as described above. The benchmark used for object detection is retinanet The XE9680 with 8 H100s is able to process 12484.05 images per second with a mean latency of 12199791 nanoseconds.","title":"Inference Benchmarks"},{"location":"Estimating%20Compute%20Requirements%20for%20Machine%20Learning/#extrapolating-performance","text":"The case described above is for a single XE9680 with 8 H100s. In the ideal case, performance scales linearly. However, in the real world performance will scale linearly...ish. The factors affecting this are myriad and complex, but your main bottleneck will be software inefficiencies which prevent the hardware from being fully utilized.","title":"Extrapolating Performance"},{"location":"Estimating%20Compute%20Requirements%20for%20Machine%20Learning/#caveats","text":"The viability of these numbers and using them to estimate performance hinge on your data being similar to the benchmark data. I have selected these algorithms because they are most relevant to our use case. The training data is an unknown. How many objects you want to detect, how different they are, how well the data is preprocessed, etc all play a massive role in performance. Hundreds of orders of magnitude easily. For example, if the data is preprocessed perfectly then you can expect stunningly accurate results. The opposite is also true. In all of these benchmarks the data has already been labeled by experts. One can easily spend months or years just prepping the data for consumption by a model.","title":"Caveats"},{"location":"Estimating%20Compute%20Requirements%20for%20Machine%20Learning/#inference-benchmarking","text":"","title":"Inference Benchmarking"},{"location":"Estimating%20Compute%20Requirements%20for%20Machine%20Learning/#retinanet-benchmark","text":"See here for the original results.","title":"Retinanet Benchmark"},{"location":"Estimating%20Compute%20Requirements%20for%20Machine%20Learning/#what-is-the-retinanet-benchmark","text":"RetinaNet is a computer program designed to automatically find and identify objects in pictures or videos. Imagine you have a security camera that needs to recognize people, cars, and other objects. RetinaNet is the brain behind that camera. How it works: Object Detection : When you show RetinaNet an image or video frame, it looks for objects in it. Objects could be people, cars, animals, or anything you want it to find. Efficient and Fast : RetinaNet is really good at finding objects quickly. It doesn't waste time by looking at every tiny part of the image. It's like finding a needle in a haystack without checking every straw. Smart Learning : It learns from examples. You show it lots of pictures with objects marked, and it figures out how to recognize those objects in new pictures. Handles Different Sizes : RetinaNet can find both big and small objects. For example, it can spot a person standing far away or a small item up close. Works in Many Fields : People use RetinaNet in all sorts of places. In factories, it checks for defects in products. In self-driving cars, it helps them see other cars and pedestrians. It's even used in security cameras to identify intruders.","title":"What is the retinanet benchmark?"},{"location":"Estimating%20Compute%20Requirements%20for%20Machine%20Learning/#the-results","text":"Metric Value Description Layman's Description SUT name LWIS_Server System Under Test name. The name of the hardware and software configuration used for the benchmark. Scenario Server Benchmark scenario. Indicates the scenario under which the test was conducted, in this case, a server-side inference scenario. Mode PerformanceOnly Benchmarking mode. Focus on achieving high throughput and efficiency. Scheduled samples per second 12484.77 Scheduled rate of inference samples per second. The planned rate at which inference samples are processed per second. Result is VALID Overall benchmark result. Indicates that the benchmark run meets the defined criteria and constraints. Performance constraints satisfied Yes Whether performance constraints are satisfied. Confirms that performance constraints have been met. Min duration satisfied Yes Whether the minimum duration requirement is met. The benchmark ran for at least the required minimum duration. Min queries satisfied Yes Whether the minimum query count requirement is met. The benchmark processed at least the required minimum number of queries. Early stopping satisfied Yes Whether early stopping criteria are met. Successful early stopping. Completed samples per second 12484.05 Actual rate of completed inference samples per second. The achieved rate of processing inference samples per second. Min latency (ns) 8611423 Minimum observed latency in nanoseconds. The shortest time taken for inference. Max latency (ns) 34852012 Maximum observed latency in nanoseconds. The longest time taken for inference. Mean latency (ns) 12199791 Average observed latency in nanoseconds. The typical time taken for inference. 50.00 percentile latency (ns) 12137988 Median latency at the 50th percentile. The middle value of latency observations. 90.00 percentile latency (ns) 14458213 Latency at the 90th percentile. The latency below which 90% of measurements fall. 95.00 percentile latency (ns) 15073964 Latency at the 95th percentile. The latency below which 95% of measurements fall. 97.00 percentile latency (ns) 15527007 Latency at the 97th percentile. The latency below which 97% of measurements fall. 99.00 percentile latency (ns) 16502552 Latency at the 99th percentile. The latency below which 99% of measurements fall. 99.90 percentile latency (ns) 18403063 Latency at the 99.90th percentile. A high percentile latency value. samples_per_query 1 Number of data samples processed per inference query. Each inference query processes one data sample. target_qps 12480 Target queries per second (throughput) for the benchmark. The desired rate of processing inference queries per second. target_latency (ns) 100000000 Target latency in nanoseconds. The desired maximum time allowed for inference. max_async_queries 0 Maximum number of asynchronous queries allowed. No asynchronous queries are allowed. min_duration (ms) 600000 Minimum duration of the benchmark run in milliseconds. The shortest time for the benchmark run. max_duration (ms) 0 Maximum duration of the benchmark run in milliseconds. No maximum duration is set. min_query_count 100 Minimum number of queries to be processed. At least 100 queries must be processed. max_query_count 0 Maximum number of queries to be processed. No maximum query count is set. qsl_rng_seed 148687905518835231 RNG seed for query set list. Seed value for randomizing the query set list. sample_index_rng_seed 520418551913322573 RNG seed for sample index. Seed value for randomizing sample indices. schedule_rng_seed 811580660758947900 RNG seed for scheduling. Seed value for scheduling-related randomization. accuracy_log_rng_seed 0 RNG seed for accuracy log entries. Seed value for generating accuracy log entries. accuracy_log_probability 0 Probability of logging accuracy information. The likelihood of logging accuracy information. accuracy_log_sampling_target 0 Target for accuracy log sampling. The desired level of sampling accuracy information. print_timestamps 0 Whether timestamps were printed. Indicates if timestamps were included in the output. performance_issue_unique 0 Flag for unique performance issue. Indicates the presence of a unique performance issue. performance_issue_same 0 Flag for the same performance issue. Indicates the presence of the same performance issue. performance_issue_same_index 0 Index of a performance issue. Identifies the specific index of a performance issue. performance_sample_count 64 Specific value not provided. The count of performance samples.","title":"The Results"},{"location":"Estimating%20Compute%20Requirements%20for%20Machine%20Learning/#understanding-the-results","text":"","title":"Understanding the Results"},{"location":"Estimating%20Compute%20Requirements%20for%20Machine%20Learning/#constraints-of-the-benchmark","text":"Performance Constraints: The benchmark sets certain performance expectations that the AI system needs to meet. In this case, the AI system was able to meet these performance standards. It performed efficiently and met the required speed and accuracy criteria. Minimum Duration: The benchmark specifies a minimum duration for the test, ensuring that the AI system runs for a specific amount of time to collect meaningful data. In this test, the AI system ran for at least 600,000 milliseconds (10 minutes). Minimum Query Count: To ensure thorough testing, the benchmark requires that a minimum number of queries (inquiries or requests) be processed. In this case, at least 100 queries needed to be processed to evaluate the system's performance. Early Stopping: The benchmark has a mechanism for early stopping, which means if the AI system performs exceptionally well before completing the full test, it can stop early. In this test, early stopping criteria were met successfully.","title":"Constraints of the Benchmark:"},{"location":"Estimating%20Compute%20Requirements%20for%20Machine%20Learning/#benchmark-results","text":"Overall Result: The overall result of the benchmark is labeled as \"VALID,\" indicating that the AI system performed well and met the defined criteria and constraints. Essentially, it passed the test. Latency: Latency refers to the time it takes for the AI system to process a request or query. The benchmark measured various aspects of latency, including the fastest (8,611,423 nanoseconds) and slowest (34,852,012 nanoseconds) response times. On average, the AI system took approximately 12,199,791 nanoseconds to process a request. Throughput: Throughput measures how fast the AI system can handle requests. In this case, the system processed around 12,484 requests per second, indicating its ability to handle a high volume of requests efficiently. Additional Stats: The benchmark also provided additional statistics about the AI system's performance across different scenarios, such as different object sizes in object detection tasks. These statistics help assess how well the system can detect objects of various sizes in images. In summary, this benchmark rigorously tested the AI system's performance, ensuring it met speed and accuracy requirements, ran for a sufficient duration, and processed a minimum number of queries. The AI system successfully passed the test, demonstrating its efficiency in handling requests with varying levels of complexity and object sizes in image recognition tasks.","title":"Benchmark Results:"},{"location":"Estimating%20Compute%20Requirements%20for%20Machine%20Learning/#executive-brief-interpretation","text":"The benchmark results for the XE9680 with eight H100s system indicate outstanding performance in processing image-related tasks. The system achieved a remarkable rate of approximately 12,484.77 image tasks per second, demonstrating its efficiency in handling image-based workloads. When assessing response times, the system consistently delivered rapid results. The minimum response time observed during testing was 8,611,423 nanoseconds (ns), highlighting the system's ability to swiftly process image tasks. Even at higher percentiles, response times remained impressive, with the 99.90th percentile response time at approximately 18,403,063 ns. Importantly, the system met all specified requirements and criteria, including performance constraints, minimum duration, and query count. It also successfully met early stopping criteria, indicating a high level of performance reliability. The benchmark employed settings that align with the system's focus on efficiently handling image tasks. It aimed for a throughput of 12,480 image tasks per second with a target response time of 100,000,000 ns. In summary, the system showcased exceptional performance in processing image-related tasks, making it well-suited for demanding applications that require fast and efficient image processing capabilities.","title":"Executive Brief Interpretation"},{"location":"Estimating%20Compute%20Requirements%20for%20Machine%20Learning/#system-description","text":"Field Value accelerator_frequency accelerator_host_interconnect PCIe Gen5 x16 accelerator_interconnect TBD accelerator_interconnect_topology accelerator_memory_capacity 80 GB accelerator_memory_configuration HBM3 accelerator_model_name NVIDIA H100-SXM-80GB accelerator_on-chip_memories accelerators_per_node 8 boot_firmware_version cooling air-cooled disk_controllers disk_drives division closed filesystem framework TensorRT 9.0.0, CUDA 12.2 host_memory_capacity 2 TB host_memory_configuration TBD host_networking Infiniband host_networking_topology N/A host_network_card_count 8x 400Gb Infiniband system_type_detail TBD host_processor_caches host_processor_core_count 52 host_processor_frequency host_processor_interconnect host_processor_model_name Intel(R) Xeon(R) Platinum 8470 host_processors_per_node 2 host_storage_capacity 3 TB host_storage_type NVMe SSD hw_notes management_firmware_version network_speed_mbit nics_enabled_connected nics_enabled_firmware nics_enabled_os number_of_nodes 1 number_of_type_nics_installed operating_system Ubuntu 22.04 other_hardware other_software_stack TensorRT 9.0.0, CUDA 12.2, cuDNN 8.8.0, Driver 525.85.12, DALI 1.28.0 power_management power_supply_details power_supply_quantity_and_rating_watts status available submitter Dell sw_notes system_name Dell PowerEdge XE9680 (8x H100-SXM-80GB, TensorRT) system_type datacenter","title":"System Description"},{"location":"Estimating%20Compute%20Requirements%20for%20Machine%20Learning/#training-benchmarking","text":"","title":"Training Benchmarking"},{"location":"Estimating%20Compute%20Requirements%20for%20Machine%20Learning/#mlperf-benchmark-rules","text":"The rules for the training models are available here .","title":"MLPerf Benchmark Rules"},{"location":"Estimating%20Compute%20Requirements%20for%20Machine%20Learning/#accuracy-targets-for-mask-r-cnn-and-ssd-retinanet","text":"","title":"Accuracy Targets for Mask R-CNN and SSD (RetinaNet)"},{"location":"Estimating%20Compute%20Requirements%20for%20Machine%20Learning/#mask-r-cnn-object-detection-heavy-weight","text":"Minimum Box mAP (mean Average Precision): 0.377 Minimum Mask mAP (mean Average Precision): 0.339 Description : For Mask R-CNN, these accuracy targets represent the model's ability to identify and outline objects in images. The \"Box mAP\" target of 0.377 means that it should correctly draw bounding boxes around objects in images about 38% of the time. The \"Mask mAP\" target of 0.339 means that it should accurately outline the shapes of these objects about 34% of the time.","title":"Mask R-CNN (Object Detection - Heavy Weight)"},{"location":"Estimating%20Compute%20Requirements%20for%20Machine%20Learning/#ssd-retinanet-object-detection-light-weight","text":"Minimum mAP (mean Average Precision): 34.0% Description : In the case of SSD (RetinaNet), these accuracy targets signify the model's capability to detect objects in images. The mAP target of 34.0% means that it should correctly identify objects in images with an accuracy of at least 34%. For instance, when shown 100 images with objects, it should accurately locate those objects in about 34 of those images.","title":"SSD (RetinaNet) (Object Detection - Light Weight)"},{"location":"Estimating%20Compute%20Requirements%20for%20Machine%20Learning/#what-is-mean-average-precision-map","text":"A great description of mAP is available here The bottom line is it is a measurement of both how correctly the model draws a box around a known target object and how well does it identify all objects in an image. For example, a precise model with poor recall might accurately identify a single object in an image but not realize that there were ten objects total. However, for that single object, it did precisely draw a box around the object. An imprecise model with high recall might identify all ten objects but the boxes it draws are incorrect. If the model is both precise and has good recall then its map score should be closer to one. The goal of the training benchmark is to see how fast you can train a model to have the specified accuracy as defined my mAP.","title":"What is mean Average Precision (mAP)"},{"location":"Estimating%20Compute%20Requirements%20for%20Machine%20Learning/#single-shot-multibox-detector-ssd","text":"","title":"Single Shot MultiBox Detector (SSD)"},{"location":"Estimating%20Compute%20Requirements%20for%20Machine%20Learning/#what-is-ssd","text":"The Single Shot MultiBox Detector (SSD) is a computer vision algorithm designed to facilitate object detection within images or video frames. It is engineered as a sophisticated visual analysis tool with the following key characteristics: Enhanced Visual Perception: SSD equips computational systems with the capability to comprehend and locate objects within visual content. Multi-Faceted Analysis: This algorithm performs multi-scale analysis, simultaneously examining both the comprehensive context and fine-grained details within an image. It then generates predictions regarding the potential locations of objects. Object Identification: For each prediction, SSD attempts to recognize the nature of the object present (e.g., labeling it as a \"car\" or \"dog\") and precisely determine its spatial coordinates within the image. Refined Predictions: SSD employs a sophisticated filtering process to refine and retain the most accurate predictions while discarding less reliable ones. This is akin to selecting the best answers from a pool of possibilities. Final Output: Upon completion of its analysis, SSD presents a detailed report of identified objects, accompanied by bounding boxes delineating their exact positions within the image. Key Advantages of SSD: - Rapid Processing: SSD is distinguished by its speed and efficiency in detecting objects within visual data. - Versatility: It is proficient at detecting objects of varying sizes within a single analysis. - Prudent Filtering: SSD employs intelligent filtering techniques to minimize false identifications. In essence, SSD empowers computer systems to comprehend visual content and efficiently discern objects within images, making it a valuable tool for a wide range of applications in business and technology.","title":"What is SSD"},{"location":"Estimating%20Compute%20Requirements%20for%20Machine%20Learning/#hyperparameters","text":"These are defined by MLCommons here Model Optimizer Name Constraint Definition SSD Adam Global Batch Size Arbitrary constant Total number of input examples processed in a training batch. Optimal Learning Rate Warm-up Epochs Integer (>= 0) Number of epochs for learning rate to warm up. Optimal Learning Rate Warm-up Factor Unconstrained Constant factor applied during learning rate warm-up. Optimal Base Learning Rate Unconstrained Base learning rate after warm-up and before decay. Optimal Weight Decay 0 L2 weight decay.","title":"Hyperparameters"},{"location":"Estimating%20Compute%20Requirements%20for%20Machine%20Learning/#results","text":"These results taken from here Metric Value Description Layman's Description Average Precision (AP) @ IoU=0.50:0.95 0.34562 Average precision over various IoU thresholds for all object sizes, with a limit of 100 detections per image. This measures the accuracy of object detection. This measures how well the model finds objects in images. A higher value is better. Average Precision (AP) @ IoU=0.50 0.49204 Average precision at IoU=0.50 for all object sizes, with a limit of 100 detections per image. This measures the accuracy of object detection when objects overlap by 50%. A higher value is better. Average Precision (AP) @ IoU=0.75 0.36934 Average precision at IoU=0.75 for all object sizes, with a limit of 100 detections per image. This measures the accuracy of object detection when objects overlap by 75%. A higher value is better. Average Precision (AP) @ IoU=0.50:0.95 0.00922 Average precision over various IoU thresholds for small objects, with a limit of 100 detections per image. This measures how well the model finds small objects in images. A higher value is better. Average Precision (AP) @ IoU=0.50:0.95 0.10076 Average precision over various IoU thresholds for medium-sized objects, with a limit of 100 detections per image. This measures how well the model finds medium-sized objects in images. A higher value is better. Average Precision (AP) @ IoU=0.50:0.95 0.38291 Average precision over various IoU thresholds for large objects, with a limit of 100 detections per image. This measures how well the model finds large objects in images. A higher value is better. Average Recall (AR) @ IoU=0.50:0.95 0.40965 (maxDets=1) Average recall over various IoU thresholds for all object sizes, with a limit of 1 detection per image. This measures how well the model recalls objects when considering only the most confident detection. A higher value is better. Average Recall (AR) @ IoU=0.50:0.95 0.58156 (maxDets=10) Average recall over various IoU thresholds for all object sizes, with a limit of 10 detections per image. This measures how well the model recalls objects when considering up to 10 detections per image. A higher value is better. Average Recall (AR) @ IoU=0.50:0.95 0.60825 (maxDets=100) Average recall over various IoU thresholds for all object sizes, with a limit of 100 detections per image. This measures how well the model recalls objects when considering up to 100 detections per image. A higher value is better. Average Recall (AR) @ IoU=0.50:0.95 0.03928 (maxDets=100) Average recall over various IoU thresholds for small objects, with a limit of 100 detections per image. This measures how well the model recalls small objects when considering up to 100 detections per image. A higher value is better. Average Recall (AR) @ IoU=0.50:0.95 0.24963 (maxDets=100) Average recall over various IoU thresholds for medium-sized objects, with a limit of 100 detections per image. This measures how well the model recalls medium-sized objects when considering up to 100 detections per image. A higher value is better. Average Recall (AR) @ IoU=0.50:0.95 0.66018 (maxDets=100) Average recall over various IoU thresholds for large objects, with a limit of 100 detections per image. This measures how well the model recalls large objects when considering up to 100 detections per image. A higher value is better. Training Duration 37 minutes 21 seconds Total time taken for training. The time it took to train the model. Throughput 287.7233 samples/s The number of samples processed per second during training. How fast the model can process images during training. MLPerf Metric Time 1322.8699 seconds The total time taken for the MLPerf benchmark. The overall time it took to run the benchmark.","title":"Results"},{"location":"Estimating%20Compute%20Requirements%20for%20Machine%20Learning/#intersection-over-union-iou","text":"Intersection over Union (IoU) is a metric commonly used in object detection and image segmentation tasks to evaluate the accuracy of predicted object boundaries or masks. It quantifies the degree of overlap between the predicted region and the ground truth (actual) region of an object within an image. IoU is calculated as the ratio of the area of intersection between the predicted and ground truth regions to the area of their union. In simpler terms, IoU measures how well a predicted object's location aligns with the actual object's location. It provides a value between 0 and 1, where: IoU = 0 indicates no overlap, meaning the prediction and ground truth have completely different locations. IoU = 1 signifies a perfect match, where the predicted and ground truth regions are identical. IoU is particularly valuable in tasks where precise object localization is crucial, such as object detection and image segmentation, as it helps assess the quality of the predictions and the model's accuracy in delineating objects within images.","title":"Intersection over Union (IoU)"},{"location":"Estimating%20Compute%20Requirements%20for%20Machine%20Learning/#mask-region-based-convolutional-neural-network-mask-r-cnn","text":"","title":"Mask Region-based Convolutional Neural Network (Mask R-CNN)"},{"location":"Estimating%20Compute%20Requirements%20for%20Machine%20Learning/#what-is-mask-r-cnn","text":"Mask R-CNN is a computer vision algorithm designed for advanced object detection and instance segmentation tasks in images or video frames. It is engineered to provide precise and detailed analysis of visual content with the following key characteristics: Object Detection and Segmentation: Mask R-CNN is capable of not only detecting objects within images but also precisely segmenting each object's pixels, providing a mask that outlines its exact shape. Multi-Task Approach: This algorithm simultaneously tackles multiple tasks, including object detection, object classification, and instance segmentation. It excels in providing a comprehensive understanding of visual scenes. Accurate Object Localization: For each detected object, Mask R-CNN not only identifies the object's class (e.g., \"car\" or \"dog\") but also delineates its precise boundaries with pixel-level accuracy. Semantic Segmentation: In addition to instance segmentation, Mask R-CNN can perform semantic segmentation by assigning each pixel in the image to a specific object class. Real-Time Capabilities: Mask R-CNN is designed for real-time or near-real-time performance, making it suitable for applications that require fast and accurate object detection and segmentation. Key Advantages of Mask R-CNN: - High Precision: It provides exceptionally precise object masks and localization, making it suitable for tasks that demand pixel-level accuracy. - Rich Information: The algorithm not only identifies objects but also provides detailed information about each object's shape and class. - Versatility: Mask R-CNN can handle a wide range of object classes and varying object sizes within a single image. In summary, Mask R-CNN is a powerful tool for computer vision tasks that involve object detection, instance segmentation, and semantic segmentation. Its ability to provide detailed and accurate information about objects within images makes it valuable for applications in diverse industries.","title":"What is Mask R-CNN"},{"location":"Estimating%20Compute%20Requirements%20for%20Machine%20Learning/#hyperparameters_1","text":"These are defined by MLCommons here Model Optimizer Name Constraint Definition Mask R-CNN SGD Max Image Size* Fixed to Reference Maximum size of the longer side Min Image Size* Fixed to Reference Maximum size of the shorter side Num Image Candidates* 1000 or 1000 * Batches per Chip Tunable number of region proposals for given batch size Optimal Learning Rate Warm-up Factor Unconstrained Constant factor applied during learning rate warm-up Optimal Learning Rate Warm-up Steps Unconstrained Number of steps for learning rate to warm up","title":"Hyperparameters"},{"location":"Estimating%20Compute%20Requirements%20for%20Machine%20Learning/#results_1","text":"Pulled from these results Metric Value Description Layman's Description Average Precision (AP) @ IoU=0.50:0.95 0.34411 Average precision over various IoU thresholds for all object sizes, with a limit of 100 detections per image. This measures the accuracy of object detection. This measures how well the model finds objects in images. A higher value is better. Average Precision (AP) @ IoU=0.50 0.56214 Average precision at IoU=0.50 for all object sizes, with a limit of 100 detections per image. This measures the accuracy of object detection when objects overlap by 50%. A higher value is better. Average Precision (AP) @ IoU=0.75 0.36660 Average precision at IoU=0.75 for all object sizes, with a limit of 100 detections per image. This measures the accuracy of object detection when objects overlap by 75%. A higher value is better. Average Precision (AP) @ IoU=0.50:0.95 0.15656 Average precision over various IoU thresholds for small objects, with a limit of 100 detections per image. This measures how well the model finds small objects in images. A higher value is better. Average Precision (AP) @ IoU=0.50:0.95 0.36903 Average precision over various IoU thresholds for medium-sized objects, with a limit of 100 detections per image. This measures how well the model finds medium-sized objects in images. A higher value is better. Average Precision (AP) @ IoU=0.50:0.95 0.50665 Average precision over various IoU thresholds for large objects, with a limit of 100 detections per image. This measures how well the model finds large objects in images. A higher value is better. Average Recall (AR) @ IoU=0.50:0.95 0.29223 (maxDets=1) Average recall over various IoU thresholds for all object sizes, with a limit of 1 detection per image. This measures how well the model recalls objects when considering only the most confident detection. A higher value is better. Average Recall (AR) @ IoU=0.50:0.95 0.44859 (maxDets=10) Average recall over various IoU thresholds for all object sizes, with a limit of 10 detections per image. This measures how well the model recalls objects when considering up to 10 detections per image. A higher value is better. Average Recall (AR) @ IoU=0.50:0.95 0.46795 (maxDets=100) Average recall over various IoU thresholds for all object sizes, with a limit of 100 detections per image. This measures how well the model recalls objects when considering up to 100 detections per image. A higher value is better. Average Recall (AR) @ IoU=0.50:0.95 0.27618 (maxDets=100) Average recall over various IoU thresholds for small objects, with a limit of 100 detections per image. This measures how well the model recalls small objects when considering up to 100 detections per image. A higher value is better. Average Recall (AR) @ IoU=0.50:0.95 0.50280 (maxDets=100) Average recall over various IoU thresholds for medium-sized objects, with a limit of 100 detections per image. This measures how well the model recalls medium-sized objects when considering up to 100 detections per image. A higher value is better. Average Recall (AR) @ IoU=0.50:0.95 0.61762 (maxDets=100) Average recall over various IoU thresholds for large objects, with a limit of 100 detections per image. This measures how well the model recalls large objects when considering up to 100 detections per image. A higher value is better. Training Duration 19 minutes 50 seconds Total time taken for training. The time it took to train the model. Throughput 1388.8244 samples/s The number of samples processed per second during training. How fast the model can process images during training. MLPerf Metric Time 1322.8699 seconds The total time taken for the MLPerf benchmark. The overall time it took to run the benchmark.","title":"Results"},{"location":"Estimating%20Compute%20Requirements%20for%20Machine%20Learning/#system-description_1","text":"Field Value submitter Dell division closed status onprem system_name XE9680x8H100-SXM-80GB number_of_nodes 1 host_processors_per_node 2 host_processor_model_name Intel(R) Xeon(R) Platinum 8470 host_processor_core_count 52 host_processor_vcpu_count 208 host_processor_frequency host_processor_caches N/A host_processor_interconnect host_memory_capacity 1.024 TB host_storage_type NVMe host_storage_capacity 4x6.4TB NVMe host_networking host_networking_topology N/A host_memory_configuration 32x 32GB DDR5 accelerators_per_node 8 accelerator_model_name NVIDIA H100-SXM5-80GB accelerator_host_interconnect PCIe 5.0x16 accelerator_frequency 1980MHz accelerator_on-chip_memories accelerator_memory_configuration HBM3 accelerator_memory_capacity 80 GB accelerator_interconnect 18xNVLink 25GB/s + 4xNVSwitch accelerator_interconnect_topology cooling hw_notes GPU TDP:700W framework NGC MXNet 23.04, NGC Pytorch 23.04, NGC HugeCTR 23.04 other_software_stack cuda_version: 12.0, cuda_driver_version: 530.30.02, cublas_version: 12.1.3, cudnn_version: 8.9.0, trt_version: 8.6.1, dali_version: 1.23.0, nccl_version: 2.17.1, openmpi_version: 4.1.4+, mofed_version: 5.4-rdmacore36.0 operating_system Red Hat Enterprise Linux 9.1 sw_notes N/A","title":"System Description"},{"location":"Finding%20Rare%20Logs%20with%20DBSCAN/","text":"Finding Rare Logs with DBSCAN Finding Rare Logs with DBSCAN Prototype What is Density-Based Clustering? Density Drop Around Cluster Borders Visualized What is Parametric / Non-Parametric What is an R-Tree R-Tree Visualized What is mean shift? Synopsis of DBSCAN Synopsis of DBSCAN w/Text DBSCAN for Rare Logs Visualized Why would rare logs be clustered? Example What is a hyperparameter? What is Term Frequency-Inverse Document Frequency (TF-IDF) Why Logarithm Key Points Example Visualize TF-IDF What is a core point? How are core points selected? What is Epsilon? Rare Logs w/Good Epsilon Visualized How to Pick a Good Epsilon Elbow Method Elbow Method Visualized Why the Sum of Squared Distances How to use the elbow curve Problems with Elbow Method in DBSCAN How to Use the Elbow Curve for DBSCAN Example of Sum of Squared Distances Average nearest neighbors Average nearest neighbors visualized Silhouette Score Silhouette Score Math Example Silhouette Score Visualized Grid Search Grid Search Visualized How is the distance from a core point calculated? Example How to Include Log Attributes that Aren't Message Example of One Hot Encoding for Severity The Problem of New Documents Normalizing Data How to Interpret a Q-Q Plot General Approach w/Decision Tree General Approach Rare Log Detection Prototype Output produced appears as below: Code is available at rare_log_finder.py The report template is available at report_template.html What is Density-Based Clustering? In density-based clustering, clusters are defined as areas of higher density than the remainder of the data set. Objects in sparse areas \u2013 that are required to separate clusters \u2013 are usually considered to be noise and border points. The most popular density based clustering method is DBSCAN. In contrast to many newer methods, it features a well-defined cluster model called \"density-reachability\". Similar to linkage based clustering, it is based on connecting points within certain distance thresholds. However, it only connects points that satisfy a density criterion, in the original variant defined as a minimum number of other objects within this radius. A cluster consists of all density-connected objects (which can form a cluster of an arbitrary shape, in contrast to many other methods) plus all objects that are within these objects' range. Another interesting property of DBSCAN is that its complexity is fairly low \u2013 it requires a linear number of range queries on the database \u2013 and that it will discover essentially the same results (it is deterministic for core and noise points, but not for border points) in each run, therefore there is no need to run it multiple times. OPTICS is a generalization of DBSCAN that removes the need to choose an appropriate value for the range parameter \u03b5, and produces a hierarchical result related to that of linkage clustering. DeLi-Clu, Density-Link-Clustering combines ideas from single-linkage clustering and OPTICS, eliminating the \u03b5 parameter entirely and offering performance improvements over OPTICS by using an R-tree index. The key drawback of DBSCAN and OPTICS is that they expect some kind of density drop to detect cluster borders. On data sets with, for example, overlapping Gaussian distributions \u2013 a common use case in artificial data \u2013 the cluster borders produced by these algorithms will often look arbitrary, because the cluster density decreases continuously. On a data set consisting of mixtures of Gaussians, these algorithms are nearly always outperformed by methods such as EM clustering that are able to precisely model this kind of data. Mean-shift is a clustering approach where each object is moved to the densest area in its vicinity, based on kernel density estimation. Eventually, objects converge to local maxima of density. Similar to k-means clustering, these \"density attractors\" can serve as representatives for the data set, but mean-shift can detect arbitrary-shaped clusters similar to DBSCAN. Due to the expensive iterative procedure and density estimation, mean-shift is usually slower than DBSCAN or k-Means. Besides that, the applicability of the mean-shift algorithm to multidimensional data is hindered by the unsmooth behaviour of the kernel density estimate, which results in over-fragmentation of cluster tails. Density Drop Around Cluster Borders Visualized See this code What is Parametric / Non-Parametric Non-parametric refers to a type of statistical analysis or machine learning algorithm that does not assume a particular probability distribution for the data being analyzed. In contrast, parametric methods assume a specific distribution, such as a normal distribution or a Poisson distribution, for the underlying population. Non-parametric methods can be useful when the data do not follow a known distribution or when there is not enough information to assume a particular distribution. These methods include techniques such as rank-based tests, kernel density estimation, and decision trees. What is an R-Tree An R-tree is a spatial index structure that is commonly used in spatial databases and geographic information systems (GIS). It is a tree data structure where each node represents a bounding box of a set of objects in the spatial domain, and the child nodes represent subdivisions of the bounding box. The R-tree is optimized for efficient search and retrieval of spatial objects, by organizing them hierarchically based on their spatial relationships. The R-tree is particularly useful for performing spatial queries such as nearest neighbor search, range search, and spatial join. It is commonly used in applications such as location-based services, geographic information systems, and image retrieval. R-Tree Visualized In the code I provided for R-tree visualization, the 5 points represent the objects that are inserted into the R-tree index. These objects are represented as rectangles in the final visualization, with the position and size of each rectangle determined by the R-tree index structure. The points are randomly generated in this example, but in practice they could represent any spatial data such as geographic locations or sensor readings. What is mean shift? Mean shift is a non-parametric clustering algorithm that seeks to find the modes or high-density areas in a dataset. In the context of clustering, a mode can be thought of as a cluster center or a representative of a group of similar data points. The algorithm works by iteratively shifting a window or kernel towards the highest density region in the dataset, thereby merging nearby data points until convergence is reached. The final position of each kernel represents the cluster centers, and each data point is assigned to the nearest kernel. Mean shift can be applied to datasets of any dimensionality and is particularly effective at identifying clusters with arbitrary shapes. Synopsis of DBSCAN Here are the basic steps to get started with DBSCAN: Choose the distance metric: DBSCAN requires a distance metric to measure the similarity between data points. For text data, you might consider using a cosine similarity metric, which measures the cosine of the angle between two vectors in high-dimensional space. This metric is often used for text data because it is based on the frequency of words in each document and can handle high-dimensional data well. Choose the neighborhood radius and minimum points: DBSCAN requires two parameters: the neighborhood radius (epsilon) and the minimum number of points required to form a dense region (minPts). The neighborhood radius determines the size of the neighborhood around each point, and minPts determines how many points must be in the neighborhood for a point to be considered part of a dense region. Compute the distance matrix: Use the chosen distance metric to compute a distance matrix between all pairs of data points. Identify core points: A core point is a data point that has at least minPts other points within its neighborhood. Identify all core points in the data. Identify non-core points: A non-core point is a point that is not a core point, but is within the neighborhood of a core point. Identify all non-core points in the data. Form clusters: For each core point, form a cluster by including all other core points and non-core points that are within its neighborhood. Repeat this process for all core points until all points are assigned to a cluster. Identify noise points: Any points that are not assigned to a cluster are considered noise points. Determine optimal parameters: Experiment with different values for epsilon and minPts to find the optimal parameters for your data. Overall, DBSCAN is a relatively simple algorithm to implement, but choosing the right distance metric and setting the optimal parameters can be challenging. Experimentation and iteration are key to finding the best parameters for your particular dataset. Synopsis of DBSCAN w/Text Step 1: Import the dataset and preprocess the data Assuming that the dataset is a collection of text documents, we will import the dataset into our code and preprocess the data to prepare it for clustering. This may involve removing stop words, stemming or lemmatizing the text, and converting the text to numerical vectors using techniques such as TF-IDF. For this example, let's assume that the data has already been preprocessed and is stored in a Pandas DataFrame called documents_df. Step 2: Choose the epsilon and min_samples hyperparameters As before, we need to choose the epsilon and min_samples hyperparameters. The choice of hyperparameters will depend on the specific dataset and the desired level of granularity in the clusters. Let's say we choose epsilon = 0.3 and min_samples = 5 for this example. Step 3: Calculate the distance matrix We will now calculate the distance matrix between all pairs of documents in the dataset. We will use a cosine distance as the distance metric since we are dealing with text data. The distance matrix is a square matrix where each element (i,j) is the distance between document i and document j. Let's assume that we have already calculated the distance matrix and stored it in a NumPy array called dist_matrix. Step 4: Identify core points Next, we will identify the core points in the dataset. A point is a core point if it has at least min_samples other points within a distance of epsilon. We can identify core points by counting the number of points within epsilon distance of each point and checking if it is greater than or equal to min_samples. Let's store the indices of the core points in a list called core_indices. Step 5: Identify border points and noise points We will now identify the border points and noise points. A border point is a point that is not a core point but is within epsilon distance of at least one core point. A noise point is a point that is not a core point and is not within epsilon distance of any core point. We can identify border points and noise points by iterating over all points and checking if they are in core_indices. If a point is not a core point, we can then check if it is within epsilon distance of any core point. If it is, it is a border point; if not, it is a noise point. Let's store the indices of the border points in a list called border_indices and the indices of the noise points in a list called noise_indices. Step 6: Create clusters We will now create the clusters. We will start with an empty list of clusters and iterate over the core points. For each core point, we will check if it has not already been assigned to a cluster. If it has not, we will create a new cluster and add the core point to it. We will then add all border points that are within epsilon distance of the core point to the cluster. We will then repeat this process for all new core points added to the cluster until there are no more points to add. Let's store the clusters in a list called clusters. Step 7: Visualize the clusters Finally, we can visualize the clusters by examining the documents in each cluster and identifying common themes or topics. We can also use techniques such as word clouds or topic modeling to gain insights into the clusters. Let's assume that we have already performed some exploratory analysis on the clusters and identified common themes or topics in each cluster. We can then present these findings in a report or dashboard for further analysis or action. DBSCAN for Rare Logs Visualized Why would rare logs be clustered? NOTE They don't have to be clustered together. See Rare Logs w/Good Epsilon Visualized for an example of what non-clustered rare logs look like. DBSCAN identifies clusters by finding regions of high point density in the feature space. Rare points can be clustered together if they are located in regions of low point density, and if the distance metric used by DBSCAN captures the similarity between these points. For example, if the rare points share certain characteristics or patterns in the feature space that distinguish them from other points, then DBSCAN may identify these points as a separate cluster. Additionally, rare points may be clustered together if they are located in a region of the feature space that is otherwise sparsely populated, and DBSCAN identifies this region as a cluster due to its low point density. In both cases, it is important to carefully choose the distance metric and parameter values for DBSCAN in order to identify clusters that correspond to rare events. Additionally, further analysis may be needed to understand the properties and potential causes of the identified rare events. Example An example of rare log features that could distinguish them from regular logs but cause the rare logs themselves to be similar could be a sudden increase or decrease in the magnitude of certain log features such as network traffic, user activity, or system performance. For instance, in the case of network traffic logs, a rare event could be a sudden spike in the number of failed connection attempts, which would cause the affected logs to have similar patterns of failed connection attempts. However, these rare logs would still be distinguishable from regular logs due to their significantly higher frequency of failed connection attempts. What is a hyperparameter? In machine learning, a hyperparameter is a configuration parameter that is set before training a model and affects the behavior and performance of the model. Hyperparameters are different from the model parameters, which are learned during training. Hyperparameters are set by the user or a tuning algorithm before the training process begins. Examples of hyperparameters include learning rate, regularization strength, batch size, number of hidden layers, and number of neurons per layer in a neural network. The values of hyperparameters can have a significant impact on the performance of a model. Therefore, selecting appropriate hyperparameters is an important step in building a successful machine learning model. Typically, hyperparameters are chosen based on a combination of domain expertise, intuition, and empirical experimentation. What is Term Frequency-Inverse Document Frequency (TF-IDF) TF-IDF stands for \"Term Frequency-Inverse Document Frequency\". It is a technique used to convert a collection of text documents into a numerical vector representation that can be used for machine learning algorithms such as clustering, classification, or recommendation systems. The basic idea behind TF-IDF is to weigh the importance of each word in a document by considering how often it appears in the document (term frequency) and how common or rare the word is across all documents in the dataset (inverse document frequency). The term frequency (TF) of a word in a document is simply the number of times the word appears in the document. However, this does not take into account the fact that some words may be more important than others even if they appear frequently. For example, in a document about cats, the word \"cat\" may appear frequently but the word \"whiskers\" may be more informative or distinctive. The inverse document frequency (IDF) of a word is a measure of how rare or common the word is across all documents in the dataset. It is calculated as the logarithm of the total number of documents in the dataset divided by the number of documents that contain the word. The TF-IDF score for a word in a document is simply the product of its term frequency and inverse document frequency. Words that appear frequently in a document but are rare across all documents in the dataset will have a high TF-IDF score and are likely to be more informative or distinctive. After calculating the TF-IDF scores for all words in all documents, we can represent each document as a numerical vector where each element of the vector represents the TF-IDF score of a particular word in the document. These vectors can then be used as input to machine learning algorithms for tasks such as clustering, classification, or recommendation systems. Why Logarithm We use logarithm in the IDF calculation of TF-IDF because the frequency distribution of words in a corpus typically follows a power law or Zipf's law, where a few words occur very frequently and most words occur rarely. For example, in a large corpus of English text, the word \"the\" is likely to occur very frequently while the word \"zygote\" may occur very rarely. Without logarithm, the IDF value for common words like \"the\" may become very small or even zero, which would lead to a distortion of the TF-IDF weights. Taking the logarithm of the IDF value helps to dampen the effect of the high frequency of common words and amplify the effect of the low frequency of rare words. It also helps to reduce the range of IDF values and make them more comparable across words. By using logarithm, we can obtain a more accurate and meaningful measure of the rarity or commonality of each word across the entire corpus, which can be used to weight the importance of each word in a document. Logarithm dampens the effect of common words because it compresses the range of IDF values. As the number of documents containing a given word increases, the IDF value decreases, and the rate of decrease slows down. By taking the logarithm of the IDF value, we can reduce the magnitude of this effect, effectively \"flattening\" the curve and reducing the range of IDF values. For example, let's say we have a corpus of 1000 documents and the word \"the\" appears in all 1000 documents. The IDF value for \"the\" would be log(1000/1000) = log(1) = 0. Now let's consider a less common word like \"zygote\" that appears in only 1 document. The IDF value for \"zygote\" would be log(1000/1) = log(1000) = 3. By taking the logarithm, we have compressed the range of IDF values from 3 orders of magnitude to just 1. This compression of the IDF values has the effect of reducing the weight of common words like \"the\" and increasing the weight of rare words like \"zygote\". This is desirable because common words tend to be less informative than rare words and may not be as useful for distinguishing between different documents or topics. By dampening the effect of common words, we can focus more attention on the rare words that are more likely to be distinctive or informative. Key Points In the context of IDF calculation, the log is used to scale down the importance of words that appear in many documents in the corpus. When a word appears in all documents, the IDF score will be 0 because log of 1 is 0. This reflects the idea that a word that appears in all documents is not very informative or useful for distinguishing between documents, and therefore should not be given a high weighting in a text analysis. The IDF term is $$IDF(t) = log \\frac{N}{n_t}$$ Example This is the general idea but ChatGPT's math was off Let $D_1$ to $D_{10}$ be a corpus of 10 documents and let $TF(d, t)$ be the term frequency of term $t$ in document $d$. The inverse document frequency of term $t$, $IDF(t)$, is defined as: $$IDF(t) = log \\frac{N}{n_t}$$ where $N$ is the total number of documents in the corpus and $n_t$ is the number of documents that contain term $t$. The TF-IDF score of term $t$ in document $d$, $TFIDF(d, t)$, is defined as: $$TFIDF(d, t) = TF(d, t) \\times IDF(t)$$ Let's say we want to calculate the TF-IDF score for the word \"cat\" in each document. Here are the 10 documents in our corpus: Document 1: \"The cat in the hat.\" Document 2: \"The cat and the dog.\" Document 3: \"The dog chased the cat.\" Document 4: \"The cat sat on the mat.\" Document 5: \"The dog and the cat played.\" Document 6: \"The cat and the mouse.\" Document 7: \"The cat was hungry.\" Document 8: \"The cat slept all day.\" Document 9: \"The cat and the bird.\" Document 10: \"The cat is black.\" We can calculate the TF-IDF score for \"cat\" in each document as follows: TF(\"cat\", $D_1$) = 1/5 = 0.2 TF(\"cat\", $D_2$) = 1/5 = 0.2 TF(\"cat\", $D_3$) = 1/5 = 0.2 TF(\"cat\", $D_4$) = 1/6 = 0.1667 TF(\"cat\", $D_5$) = 1/5 = 0.2 TF(\"cat\", $D_6$) = 1/4 = 0.25 TF(\"cat\", $D_7$) = 1/4 = 0.25 TF(\"cat\", $D_8$) = 1/4 = 0.25 TF(\"cat\", $D_9$) = 1/4 = 0.25 TF(\"cat\", $D_{10}$) = 1/4 = 0.25 Number of documents containing \"cat\": 9 IDF(\"cat\") = log(10/9) = 0.1054 TFIDF($D_1$, \"cat\") = 0.2 * 0.1054 = 0.0211 TFIDF($D_2$, \"cat\") = 0.2 * 0.1054 = 0.0211 TFIDF($D_3$, \"cat\") = 0.2 * 0.1054 = 0.0211 TFIDF($D_4$, \"cat\") = 0.1667 * 0.1054 = 0.0175 TFIDF($D_5$, \"cat\") = 0.2 * 0.1054 = 0.0211 TFIDF($D_6$, \"cat\") = 0.25 * 0.1054 = 0.0264 TFIDF($D_7$, \"cat\") = 0.25 * 0.1054 = 0.0264 TFIDF($D_8$, \"cat\") = 0.25 * 0.1054 = 0.0264 TFIDF($D_9$, \"cat\") = 0.25 * 0.1054 = 0.0264 TFIDF($D_{10}$, \"cat\") = 0.25 * 0.1054 = 0.0264 So, the TF-IDF score of \"cat\" in each document is as follows: Document 1: 0.0211 Document 2: 0.0211 Document 3: 0.0211 Document 4: 0.0175 Document 5: 0.0211 Document 6: 0.0264 Document 7: 0.0264 Document 8: 0.0264 Document 9: 0.0264 Document 10: 0.0264 Visualize TF-IDF The heatmap chart shows the TF-IDF score for each word in each document in the corpus. The color of each cell indicates the relative TF-IDF score of the corresponding word in the corresponding document. The darker red cells represent words that have higher TF-IDF scores, which means they are more important in the corresponding documents. Conversely, the lighter blue cells represent words that have lower TF-IDF scores, which means they are less important in the corresponding documents. For example, in document 1 (\"The cat in the hat.\"), the word \"cat\" has a relatively high TF-IDF score, which is why its cell is darker red compared to other cells in the same row. In contrast, the word \"the\" has a relatively low TF-IDF score in most documents, which is why its cells are lighter blue compared to other cells in the same column. Overall, the heatmap can give you a sense of the most important words in each document and across the corpus as a whole. What is a core point? In DBSCAN, a core point is a data point that has at least min_samples other data points within a distance of eps. In other words, a core point is a point that is at the center of a dense region of the dataset. More formally, a data point p is a core point if and only if there exist at least min_samples data points (including p itself) within a distance of eps of p. This means that a core point has at least min_samples neighboring data points within a radius of eps. Core points are important in the DBSCAN algorithm because they define the centers of clusters. Any data points that are not core points themselves but are within a distance of eps of a core point are considered to be part of the same cluster as that core point. This means that the clusters in the dataset are defined by the core points and the density of the data around them. How are core points selected? Core points are selected based on the value of the hyperparameters epsilon and min_samples. A core point is defined as a data point that has at least min_samples other data points within a distance of epsilon (i.e., the distance between any pair of core points must be less than or equal to epsilon). In other words, if a data point has at least min_samples other data points within a distance of epsilon, then it is a core point. Otherwise, it is not a core point. Note that the hyperparameters epsilon and min_samples are typically set based on domain knowledge or by using trial and error to determine the best values for a given dataset. The choice of hyperparameters will depend on the specific dataset and the desired level of granularity in the clusters. What is Epsilon? In the context of the DBSCAN algorithm, epsilon is a hyperparameter that defines the maximum distance between two data points in order for them to be considered part of the same cluster. Specifically, a data point is considered a core point if it has at least min_samples data points within a distance of epsilon from it. If a data point is not a core point but is within epsilon distance of a core point, it is considered a border point. Data points that are not core points or border points are considered noise points and are not part of any cluster. Choosing an appropriate value for epsilon depends on the density of the data and the desired level of granularity in the clusters. A small epsilon will result in dense clusters with more noise points, while a large epsilon will result in fewer, more spread-out clusters with fewer noise points. The choice of epsilon should be guided by domain knowledge and empirical experimentation. See this code Rare Logs w/Good Epsilon Visualized You can see in chart 2 how only a few points are highlighted - these would be our rare logs. How to Pick a Good Epsilon There are several approaches you can use to determine a good epsilon for your data: - Visual inspection: Plot a k-distance graph and visually inspect it to find the elbow point, which represents a good estimate of epsilon. - Average nearest neighbors: Plot the average distance to the k-nearest neighbors for each point and select the point with the highest change in distance as the estimate of epsilon. - Silhouette score: Calculate the silhouette score for different values of epsilon and select the value that maximizes the silhouette score. - Also often used as a tool for comparing different clustering algorithms or parameter settings to determine which produces the most well-defined clusters. - Grid search: Use a grid search approach to test different values of epsilon and select the value that results in the best clustering performance. Elbow Method Elbow Method Visualized Why is it called inertia? : In the elbow method, we are interested in minimizing the within-cluster sum of squares (also called the cluster inertia or simply inertia) as the number of clusters increases. This is because as the number of clusters increases, we can expect the inertia to decrease since the data is being divided into more and more smaller subsets. The y-axis was labeled as \"inertia\" since it represents the within-cluster sum of squares, which we are trying to minimize. See this code NOTE This code shows the elbow method for k means rather than DBSCAN see the problem with that in Problems with Elbow Method in DBSCAN The elbow method helps to determine the optimal number of clusters for a given dataset. The graph typically shows the number of clusters on the x-axis and the sum of squared distances (SSE) on the y-axis. The SSE measures how far the points within a cluster are from the center of the cluster. The goal is to minimize the SSE, which indicates that the points within each cluster are close together and well-separated from points in other clusters. In the graph, you want to look for the \"elbow\" point, which is the point of inflection where the SSE starts to level off or decrease at a slower rate. This point represents the optimal number of clusters for the given dataset. In the example graph above, the elbow point appears to be around 3 or 4 clusters, since adding more clusters beyond this point doesn't result in significant reduction of the SSE. However, the choice of the optimal number of clusters ultimately depends on the specific problem and the context of the data. Why the Sum of Squared Distances The sum of squared distances is helpful in clustering because it provides a way to measure how similar or dissimilar the points within a cluster are. If the sum of squared distances within a cluster is small, it suggests that the points in that cluster are tightly grouped together and similar to one another. On the other hand, if the sum of squared distances within a cluster is large, it suggests that the points in that cluster are more spread out and dissimilar to one another. Therefore, the sum of squared distances can help us to identify the optimal number of clusters by evaluating how well the data fits the given number of clusters. The goal is to find the smallest number of clusters that results in a low sum of squared distances within each cluster, while also avoiding overfitting the data. How to use the elbow curve The optimal number of clusters can be determined by looking at the elbow point on the curve. The elbow point is the point where the inertia starts to decrease at a slower rate. This point represents the trade-off between minimizing the sum of squared distances within each cluster and maximizing the number of clusters. In general, the elbow point represents the optimal number of clusters to use for the given data set. However, sometimes the elbow point may not be clearly defined or there may be multiple elbow points, in which case other methods such as silhouette analysis may be used to determine the optimal number of clusters. Problems with Elbow Method in DBSCAN One thing to note is that unlike KMeans, DBSCAN doesn't have a fixed number of clusters. So the elbow curve for DBSCAN may not have a clear \"elbow\" point like in the KMeans case. Instead, you may have to manually examine the plot and choose a reasonable value of eps based on the trade-off between the number of clusters and the amount of data within each cluster. How to Use the Elbow Curve for DBSCAN See this code So in the chart above we can see the inertia jumps up until right before .1. Which means we probably want to aim for something like 6 clusters with an epsilon of like .7 and an inertia of ~7.5. Example of Sum of Squared Distances (1, 2) (3, 4) (5, 6) We want to calculate the sum of squared distances for a cluster of two points. We randomly assign the first two points to the cluster and calculate the distance between each point and the centroid: Cluster 1: - (1, 2) -> distance to centroid (2, 3) = sqrt((1-2)^2 + (2-3)^2) = sqrt(2) - (3, 4) -> distance to centroid (2, 3) = sqrt((3-2)^2 + (4-3)^2) = sqrt(2) - Sum of squared distances = (sqrt(2))^2 + (sqrt(2))^2 = 4 Next, we calculate the sum of squared distances for a cluster of the other two points: Cluster 2: - (3, 4) -> distance to centroid (3, 5) = sqrt((3-3)^2 + (4-5)^2) = 1 - (5, 6) -> distance to centroid (3, 5) = sqrt((5-3)^2 + (6-5)^2) = sqrt(5) - Sum of squared distances = 1^2 + (sqrt(5))^2 = 6 We can see that the sum of squared distances for cluster 1 is 4, and the sum of squared distances for cluster 2 is 6. In this case, cluster 1 is the better choice since it has a lower sum of squared distances. Average nearest neighbors The average nearest neighbors (ANN) algorithm is a method for estimating the optimal value of the epsilon parameter in DBSCAN clustering. The idea behind ANN is to plot the distances between each data point and its kth nearest neighbor. The plot is then sorted in increasing order of distances, and the elbow point is identified as the point where the curve starts to level off, indicating a natural break in the distances. To use ANN to estimate the optimal epsilon value, one needs to choose a value for k, which is typically set to 4 or 5. The kth nearest neighbor can be found using a k-d tree or brute force search, and the distances can be sorted using a quicksort or similar algorithm. Once the distances are sorted, the elbow point can be identified by visual inspection or by fitting a curve to the data using a regression algorithm. The value of epsilon corresponding to the elbow point is then used as the input to the DBSCAN algorithm. The advantage of the ANN algorithm is that it can be applied to high-dimensional data and is relatively insensitive to the choice of k. However, it requires additional computation to find the kth nearest neighbor for each data point, which can be time-consuming for large datasets. Average nearest neighbors visualized See this code This code generates 100 random points and computes the distances to the 5th nearest neighbor for each point. It then sorts the average distances and plots them against the point index (just the index of the point in an array of 100). The resulting plot shows how the average nearest neighbor distance varies across the points. We would want a value around .14 right before the average distance rapidly ascends. Silhouette Score The Silhouette score is a measure of how well a data point fits into its assigned cluster in unsupervised learning algorithms such as clustering. It is calculated using the mean intra-cluster distance (a) and the mean nearest-cluster distance (b) for each data point, and ranges from -1 to 1. A Silhouette score of 1 indicates that the data point is well-matched to its own cluster and poorly-matched to neighboring clusters, while a score of -1 indicates the opposite. A score of 0 indicates that the data point is on the boundary between two clusters. The Silhouette score can be used to evaluate the quality of a clustering algorithm and to determine the optimal number of clusters. A higher Silhouette score indicates better clustering, and the optimal number of clusters can be determined by selecting the number of clusters that maximizes the Silhouette score. Silhouette Score Math The silhouette score measures how similar an object is to its own cluster (cohesion) compared to other clusters (separation). The formula for silhouette score is as follows: $s(i) = \\frac{b(i) - a(i)}{\\max{a(i), b(i)}}$ where $i$ is an individual data point, $a(i)$ is the average distance between $i$ and all other data points in the same cluster, and $b(i)$ is the minimum average distance between $i$ and all data points in any other cluster. The silhouette score ranges from -1 to 1, with a score of 1 indicating that the point is well-matched to its own cluster and poorly-matched to neighboring clusters, a score of 0 indicating that the point is equally similar to its own cluster and neighboring clusters, and a score of -1 indicating that the point is poorly-matched to its own cluster and well-matched to neighboring clusters. Example Sure, let's say we have a dataset with four data points, labeled A, B, C, and D. We have performed clustering using some clustering algorithm, and let's say that points A and B belong to cluster 1, while points C and D belong to cluster 2. We can calculate the silhouette score for each point as follows: For point A: Compute the average distance from point A to all other points in cluster 1. Let's call this value a = 0.4. Compute the average distance from point A to all points in the closest other cluster, cluster 2. Let's call this value b = 0.6. The silhouette score for point A is then $(b - a) / max(a, b) = (0.6 - 0.4) / 0.6 = 0.33$. For point B: Compute the average distance from point B to all other points in cluster 1. Let's call this value a = 0.2. Compute the average distance from point B to all points in the closest other cluster, cluster 2. Let's call this value b = 0.8. The silhouette score for point B is then $(b - a) / max(a, b) = (0.8 - 0.2) / 0.8 = 0.75$. For point C: Compute the average distance from point C to all other points in cluster 2. Let's call this value a = 0.3. Compute the average distance from point C to all points in the closest other cluster, cluster 1. Let's call this value b = 0.9. The silhouette score for point C is then $(b - a) / max(a, b) = (0.9 - 0.3) / 0.9 = 0.67$. For point D: Compute the average distance from point D to all other points in cluster 2. Let's call this value a = 0.5. Compute the average distance from point D to all points in the closest other cluster, cluster 1. Let's call this value b = 0.7. The silhouette score for point D is then $(b - a) / max(a, b) = (0.7 - 0.5) / 0.7 = 0.29$. The overall silhouette score for this clustering is the average of the silhouette scores for all points, which in this case is $(0.33 + 0.75 + 0.67 + 0.29) / 4 = 0.51$. Silhouette Score Visualized See this code Grid Search Grid search is a hyperparameter tuning technique in machine learning where a model is trained on various combinations of hyperparameters to find the optimal set of hyperparameters that yields the best performance for a specific evaluation metric. In grid search, the model is trained and evaluated on all possible combinations of hyperparameters within a predefined range or grid. This is done by creating a grid of hyperparameters, where each hyperparameter is assigned a set of values to try. The model is then trained and evaluated using each combination of hyperparameters in the grid. The combination of hyperparameters that yields the best evaluation metric is then chosen as the optimal set of hyperparameters for the model. Grid search is often used when there are a limited number of hyperparameters to tune, as it can be computationally expensive when the number of hyperparameters or the range of values for each hyperparameter is large. Grid Search Visualized See this code The chart shows the results of a grid search over a range of values for two hyperparameters of the DBSCAN clustering algorithm: eps (which controls the maximum distance between points in the same cluster) and min_samples (which controls the minimum number of points required to form a dense region). The x-axis of the chart shows the values of min_samples tested in the grid search, and the y-axis shows the values of eps tested in the grid search. The color of each cell in the chart represents the number of clusters found by DBSCAN for that combination of eps and min_samples. Darker colors indicate a higher number of clusters, while lighter colors indicate fewer clusters. The chart can be used to identify the combination of eps and min_samples that yields the best clustering results for the particular dataset being analyzed. For example, if the goal is to find a small number of well-defined clusters, the optimal combination of eps and min_samples might be found in a region of the chart with light colors. Conversely, if the goal is to find a larger number of clusters, a region of the chart with darker colors might be more suitable. How is the distance from a core point calculated? The distance between two data points in DBSCAN is typically calculated using a distance metric such as Euclidean distance or Manhattan distance. These distance metrics measure the distance between two points in n-dimensional space, where n is the number of features or dimensions in the dataset. For example, if we have a dataset with two features (i.e., two-dimensional data), the Euclidean distance between two data points p and q is calculated as: $$distance(p, q) = \\sqrt{(p_1 - q_1)^2 + (p_2 - q_2)^2}$$ where $p_1$ and $q_1$ are the values of the first feature in $p$ and $q$, and $p_2$ and $q_2$ are the values of the second feature in $p$ and $q$. Manhattan distance is another common distance metric used in DBSCAN. Manhattan distance measures the distance between two points by summing the absolute differences of their coordinates. For example, the Manhattan distance between two data points p and q with two features is calculated as: $$distance(p, q) = |p_1 - q_1| + |p_2 - q_2|$$ where $p_1$ and $q_1$ are the values of the first feature in $p$ and $q$, and $p_2$ and $q_2$ are the values of the second feature in $p$ and $q$. DBSCAN is flexible enough to allow different distance metrics to be used depending on the nature of the data and the problem being solved. Example Certainly! Let's say we have a dataset with two features, \"age\" and \"income\", and we want to calculate the distance between two data points, $p$ and $q$. Here are the values of these two features for $p$ and $q$: $p = (30, 50000)$ $q = (35, 60000)$ We can calculate the Euclidean distance between $p$ and $q$ as follows: $$distance(p, q) = \\sqrt{(p_1 - q_1)^2 + (p_2 - q_2)^2}$$ (based on Pythagoreans Theorem) where $p_1$ and $q_1$ are the values of the \"age\" feature in $p$ and $q$, and $p_2$ and $q_2$ are the values of the \"income\" feature in $p$ and $q$. Substituting the values of $p$ and $q$, we get: $$distance(p, q) = \\sqrt{(30 - 35)^2 + (50000 - 60000)^2} = \\sqrt{25^2 + 10000^2} \\approx 10000.25$$ So the Euclidean distance between $p$ and $q$ is approximately 10000.25. Why $$distance(p, q) = \\sqrt{(p_1 - q_1)^2 + (p_2 - q_2)^2}$$ The distance formula is derived from the Pythagorean theorem, which states that the square of the hypotenuse of a right triangle is equal to the sum of the squares of the other two sides. In this case, the sides of the triangle are the horizontal and vertical distances between the two points, and the hypotenuse is the straight line distance between them. The formula uses the square root of the sum of the squares of the horizontal and vertical distances to find the length of the hypotenuse, which is the Euclidean distance between the two points. This formula can be extended to higher dimensions by adding the squares of the differences in each dimension and taking the square root of the sum. How to Include Log Attributes that Aren't Message The problem with the above is that TF-IDF accounts for the message field but not how to combine it with other log attributes. To combine additional fields with the text data represented by TF-IDF, you can concatenate the TF-IDF vectors with vectors representing the additional fields. These additional fields can be transformed into numerical representations that are appropriate for clustering using methods such as one-hot encoding or normalization. Then, the resulting vectors can be used as input to the clustering algorithm, allowing you to perform rare log detection based on multiple features. For example, let's say you have a set of log messages and two additional fields: \"source\" and \"severity\". To combine these fields with the text data, you could create a new vector for each log message that contains the TF-IDF representation of the message text, the one-hot encoded representation of the source field, and the normalized representation of the severity field. These vectors could then be used as input to DBSCAN or other clustering algorithms to identify clusters of logs with low point density, which may correspond to rare events. Example of One Hot Encoding for Severity Suppose we have a log entry with the following fields: { \"timestamp\": \"2022-04-12T10:15:30\", \"severity\": \"warning\", \"message\": \"Disk usage is high on /dev/sda1\" } To encode this log entry using a combination of one-hot encoding and TF-IDF, we can follow these steps: One-hot encode the severity field. Suppose we have three possible values for the severity field: \"informational\", \"warning\", and \"error\". We can create a one-hot encoding vector for the severity field as follows: { \"informational\": [1, 0, 0], \"warning\": [0, 1, 0], \"error\": [0, 0, 1] } For this log entry, the one-hot encoding vector for the severity field would be $[0, 1, 0]$, since the severity is \"warning\". Apply TF-IDF encoding to the message field. Suppose we have a vocabulary of 10,000 unique words that appear in our log messages. We can create a TF-IDF vector for the message field as follows: Tokenize the message field into a list of words. In this case, the message field would be tokenized into the list [\"disk\", \"usage\", \"is\", \"high\", \"on\", \"/dev/sda1\"]. Calculate the TF-IDF score for each word in the message field using the following formula: $tf-idf(w, d, D) = tf(w, d) * idf(w, D)$ where $tf(w, d)$ is the frequency of the word $w$ in the document $d$, and $idf(w, D)$ is the inverse document frequency of the word $w$ in the set of all documents $D$. The inverse document frequency can be calculated as follows: $idf(w, D) = log(N / (1 + df(w, D)))$ where $N$ is the total number of documents in the corpus, and $df(w, D)$ is the number of documents in the corpus that contain the word $w$. For each word in the message field, multiply its TF-IDF score by the one-hot encoding vector for the severity field to create a combined feature vector. For example, suppose the TF-IDF scores for the words in the message field are as follows: { \"disk\": 0.1, \"usage\": 0.05, \"is\": 0.02, \"high\": 0.15, \"on\": 0.03, \"/dev/sda1\": 0.08 } The combined feature vector for this log entry would be (in a real case it would be as long as the corpus of words in all documents): [0, 1, 0, 0.01, 0.005, 0.002, 0.015, 0.03, 0.008] The first three elements of the vector represent the one-hot encoding for the severity field, and the remaining elements represent the TF-IDF scores for the words in the message field. By using one-hot encoding for categorical fields and TF-IDF encoding for text fields, we can create a single feature vector that captures information from multiple fields in a log entry. The Problem of New Documents If a new document is added to the corpus that contains words not seen in the original vocabulary, then the TF-IDF vectors for all other documents in the corpus would need to be recomputed to include the new term in their vector representations. This can be computationally expensive, especially for large corpora with many documents. One way to mitigate this issue is to use incremental learning algorithms that can update the existing vector representations with new data without recomputing everything from scratch. Another approach is to periodically retrain the model on the entire corpus, which can be time-consuming but ensures that the vector representations are up-to-date. To incrementally update the feature vectors when new documents are added to the corpus, you would need to update the inverse document frequency (idf) values for each word in the vocabulary. Suppose you have a current corpus of N documents, and you add a new document to the corpus. You would first update the idf values for each word in the vocabulary using the new document. Then, for each existing document in the corpus, you would update the tf-idf scores for any words in the new document that also appear in the existing document. This incremental update can be more efficient than recalculating the tf-idf scores for all documents in the corpus every time a new document is added, especially if the corpus is large. Normalizing Data The problem is that some of the data is going to have very large magnitude and we have to normalize it before feeding it into the algorithm. Ex: the time epoch is very large relative to everything else. Scatter plot of data before standardization: This plot shows the distribution of the data in its original scale. The two variables have different means and variances, and the scale of the variables are not the same. There is a clear positive correlation between the two variables, indicating that when one variable is high, the other variable is likely to be high as well. Scatter plot of data after standardization: This plot shows the same data as the first plot, but after applying standardization using StandardScaler. The data is now centered around the origin (0, 0), and the scale of the two variables are now the same. The correlation between the two variables is still present. Q-Q plot of variable 1 before standardization: This plot shows the distribution of the first variable before standardization. The blue points represent the ordered values of the variable, and the red line represents the theoretical quantiles of a normal distribution. If the distribution of the variable was perfectly normal, the points would lie on the red line. However, we can see that the distribution deviates from normality, as indicated by the non-linear shape of the Q-Q plot. Q-Q plot of variable 1 after standardization: This plot shows the same variable as in the third plot, but after applying standardization. The data is now distributed closer to normal, as indicated by the straight line in the Q-Q plot. This means that the standardization process has reduced the deviation from normality in the distribution of this variable. Overall, the scatter plots show the relationship between the two variables and how standardization can bring them onto the same scale. The Q-Q plots show how standardization can improve the normality of the distribution of a variable. How to Interpret a Q-Q Plot A Q-Q plot (quantile-quantile plot) is a graphical technique used to compare the distribution of a sample of data to a theoretical probability distribution. It is a commonly used diagnostic tool in statistics to assess whether a sample of data comes from a certain distribution, such as a normal distribution, and to identify any departures from this distribution. Here's how to interpret a Q-Q plot: Theoretical quantiles: The x-axis of the Q-Q plot represents the theoretical quantiles of the theoretical distribution being compared to the observed data. Ordered values: The y-axis of the Q-Q plot represents the ordered values of the sample data. Perfect fit: If the sample data follows the theoretical distribution perfectly, the Q-Q plot will show a straight line that passes through the origin. Departures from the theoretical distribution: Any departures from a straight line indicate departures from the theoretical distribution. Outliers: Outliers in the sample data may appear as points that are far away from the straight line in the Q-Q plot. Overall, a Q-Q plot is a useful tool for assessing whether a sample of data follows a certain distribution and identifying any departures from that distribution. It can also be used to identify outliers and other features of the data that may require further investigation. General Approach w/Decision Tree I decided not to go this way General Approach Collect and preprocess the log data: Before you can apply decision trees to log files, you need to collect and preprocess the data. This may involve extracting the relevant log files from different sources, aggregating them into a single data set, and cleaning the data to remove any noise or inconsistencies. Define the problem and the variables: The next step is to define the problem you want to solve using decision trees and identify the variables you will use to build the decision tree. For example, you might want to detect anomalies in system log files, in which case the variables might include the timestamp, the type of log event, the source IP address, and other relevant attributes. Choose the decision tree algorithm and parameters: Once you have defined the problem and variables, you need to choose the decision tree algorithm you will use and set the parameters. There are several decision tree algorithms available, including ID3, C4.5, CART, and Random Forest. The choice of algorithm will depend on the specific problem you are trying to solve and the characteristics of the log data. Train the decision tree: The next step is to train the decision tree using the log data. This involves splitting the data into a training set and a test set, and using the training set to build the decision tree. The algorithm will recursively partition the data into subsets based on the values of the variables, and choose the best variable to split the data at each internal node. Test and evaluate the decision tree: Once the decision tree has been trained, you need to test it on the test set and evaluate its performance. This involves measuring metrics such as accuracy, precision, recall, and F1 score, and tuning the algorithm parameters as needed to improve performance. Deploy and monitor the decision tree: Finally, you need to deploy the decision tree in a production environment and monitor its performance over time. This may involve setting up alerts or notifications to notify you when anomalies are detected, or integrating the decision tree with other systems or applications. Keep in mind that this is a high-level overview, and there may be additional steps or considerations depending on the specific problem you are trying to solve and the characteristics of the log data. It's also important to have a good understanding of machine learning concepts and techniques, as well as the tools and technologies used to preprocess, analyze, and visualize log data. Rare Log Detection If your goal is to identify rare logs or events in your log data using decision trees, you can approach the problem as an anomaly detection problem. In this case, you would define the variables you will use to build the decision tree, and then train the decision tree on a dataset of logs that are representative of normal behavior. The decision tree will learn to recognize patterns that are typical of normal logs, and any logs that deviate from these patterns will be flagged as anomalous or rare. Here are some steps you can take to get started with this approach: Define the problem: Determine what types of rare or anomalous logs you want to identify, and why they are important. For example, you might want to identify logs that indicate potential security threats, system failures, or other unusual behavior. Identify the variables: Choose the variables that you will use to build the decision tree. These may include the timestamp, the type of log event, the source IP address, the destination IP address, the user ID, and other relevant attributes. Collect and preprocess the data: Collect a representative dataset of logs and preprocess the data to remove any noise or inconsistencies. This may involve filtering out irrelevant logs, removing duplicates, and transforming the data into a suitable format. Split the data: Split the data into a training set and a test set. The training set should include logs that are representative of normal behavior, while the test set should include logs that may contain rare or anomalous events. Train the decision tree: Train the decision tree on the training set of logs. The algorithm will recursively partition the data into subsets based on the values of the variables, and choose the best variable to split the data at each internal node. Test the decision tree: Test the decision tree on the test set of logs. The decision tree will flag any logs that deviate from the patterns learned during training as anomalous or rare. Evaluate and refine the decision tree: Evaluate the performance of the decision tree using metrics such as precision, recall, and F1 score. Refine the decision tree by adjusting the algorithm parameters or adding new variables as needed to improve performance. By following these steps, you can build a decision tree that can identify rare or anomalous logs in your data. Keep in mind that the performance of the decision tree may depend on the characteristics of the log data, and you may need to experiment with different algorithms and parameters to achieve the best results.","title":"Finding Rare Logs with DBSCAN"},{"location":"Finding%20Rare%20Logs%20with%20DBSCAN/#finding-rare-logs-with-dbscan","text":"Finding Rare Logs with DBSCAN Prototype What is Density-Based Clustering? Density Drop Around Cluster Borders Visualized What is Parametric / Non-Parametric What is an R-Tree R-Tree Visualized What is mean shift? Synopsis of DBSCAN Synopsis of DBSCAN w/Text DBSCAN for Rare Logs Visualized Why would rare logs be clustered? Example What is a hyperparameter? What is Term Frequency-Inverse Document Frequency (TF-IDF) Why Logarithm Key Points Example Visualize TF-IDF What is a core point? How are core points selected? What is Epsilon? Rare Logs w/Good Epsilon Visualized How to Pick a Good Epsilon Elbow Method Elbow Method Visualized Why the Sum of Squared Distances How to use the elbow curve Problems with Elbow Method in DBSCAN How to Use the Elbow Curve for DBSCAN Example of Sum of Squared Distances Average nearest neighbors Average nearest neighbors visualized Silhouette Score Silhouette Score Math Example Silhouette Score Visualized Grid Search Grid Search Visualized How is the distance from a core point calculated? Example How to Include Log Attributes that Aren't Message Example of One Hot Encoding for Severity The Problem of New Documents Normalizing Data How to Interpret a Q-Q Plot General Approach w/Decision Tree General Approach Rare Log Detection","title":"Finding Rare Logs with DBSCAN"},{"location":"Finding%20Rare%20Logs%20with%20DBSCAN/#prototype","text":"Output produced appears as below: Code is available at rare_log_finder.py The report template is available at report_template.html","title":"Prototype"},{"location":"Finding%20Rare%20Logs%20with%20DBSCAN/#what-is-density-based-clustering","text":"In density-based clustering, clusters are defined as areas of higher density than the remainder of the data set. Objects in sparse areas \u2013 that are required to separate clusters \u2013 are usually considered to be noise and border points. The most popular density based clustering method is DBSCAN. In contrast to many newer methods, it features a well-defined cluster model called \"density-reachability\". Similar to linkage based clustering, it is based on connecting points within certain distance thresholds. However, it only connects points that satisfy a density criterion, in the original variant defined as a minimum number of other objects within this radius. A cluster consists of all density-connected objects (which can form a cluster of an arbitrary shape, in contrast to many other methods) plus all objects that are within these objects' range. Another interesting property of DBSCAN is that its complexity is fairly low \u2013 it requires a linear number of range queries on the database \u2013 and that it will discover essentially the same results (it is deterministic for core and noise points, but not for border points) in each run, therefore there is no need to run it multiple times. OPTICS is a generalization of DBSCAN that removes the need to choose an appropriate value for the range parameter \u03b5, and produces a hierarchical result related to that of linkage clustering. DeLi-Clu, Density-Link-Clustering combines ideas from single-linkage clustering and OPTICS, eliminating the \u03b5 parameter entirely and offering performance improvements over OPTICS by using an R-tree index. The key drawback of DBSCAN and OPTICS is that they expect some kind of density drop to detect cluster borders. On data sets with, for example, overlapping Gaussian distributions \u2013 a common use case in artificial data \u2013 the cluster borders produced by these algorithms will often look arbitrary, because the cluster density decreases continuously. On a data set consisting of mixtures of Gaussians, these algorithms are nearly always outperformed by methods such as EM clustering that are able to precisely model this kind of data. Mean-shift is a clustering approach where each object is moved to the densest area in its vicinity, based on kernel density estimation. Eventually, objects converge to local maxima of density. Similar to k-means clustering, these \"density attractors\" can serve as representatives for the data set, but mean-shift can detect arbitrary-shaped clusters similar to DBSCAN. Due to the expensive iterative procedure and density estimation, mean-shift is usually slower than DBSCAN or k-Means. Besides that, the applicability of the mean-shift algorithm to multidimensional data is hindered by the unsmooth behaviour of the kernel density estimate, which results in over-fragmentation of cluster tails.","title":"What is Density-Based Clustering?"},{"location":"Finding%20Rare%20Logs%20with%20DBSCAN/#density-drop-around-cluster-borders-visualized","text":"See this code","title":"Density Drop Around Cluster Borders Visualized"},{"location":"Finding%20Rare%20Logs%20with%20DBSCAN/#what-is-parametric-non-parametric","text":"Non-parametric refers to a type of statistical analysis or machine learning algorithm that does not assume a particular probability distribution for the data being analyzed. In contrast, parametric methods assume a specific distribution, such as a normal distribution or a Poisson distribution, for the underlying population. Non-parametric methods can be useful when the data do not follow a known distribution or when there is not enough information to assume a particular distribution. These methods include techniques such as rank-based tests, kernel density estimation, and decision trees.","title":"What is Parametric / Non-Parametric"},{"location":"Finding%20Rare%20Logs%20with%20DBSCAN/#what-is-an-r-tree","text":"An R-tree is a spatial index structure that is commonly used in spatial databases and geographic information systems (GIS). It is a tree data structure where each node represents a bounding box of a set of objects in the spatial domain, and the child nodes represent subdivisions of the bounding box. The R-tree is optimized for efficient search and retrieval of spatial objects, by organizing them hierarchically based on their spatial relationships. The R-tree is particularly useful for performing spatial queries such as nearest neighbor search, range search, and spatial join. It is commonly used in applications such as location-based services, geographic information systems, and image retrieval.","title":"What is an R-Tree"},{"location":"Finding%20Rare%20Logs%20with%20DBSCAN/#r-tree-visualized","text":"In the code I provided for R-tree visualization, the 5 points represent the objects that are inserted into the R-tree index. These objects are represented as rectangles in the final visualization, with the position and size of each rectangle determined by the R-tree index structure. The points are randomly generated in this example, but in practice they could represent any spatial data such as geographic locations or sensor readings.","title":"R-Tree Visualized"},{"location":"Finding%20Rare%20Logs%20with%20DBSCAN/#what-is-mean-shift","text":"Mean shift is a non-parametric clustering algorithm that seeks to find the modes or high-density areas in a dataset. In the context of clustering, a mode can be thought of as a cluster center or a representative of a group of similar data points. The algorithm works by iteratively shifting a window or kernel towards the highest density region in the dataset, thereby merging nearby data points until convergence is reached. The final position of each kernel represents the cluster centers, and each data point is assigned to the nearest kernel. Mean shift can be applied to datasets of any dimensionality and is particularly effective at identifying clusters with arbitrary shapes.","title":"What is mean shift?"},{"location":"Finding%20Rare%20Logs%20with%20DBSCAN/#synopsis-of-dbscan","text":"Here are the basic steps to get started with DBSCAN: Choose the distance metric: DBSCAN requires a distance metric to measure the similarity between data points. For text data, you might consider using a cosine similarity metric, which measures the cosine of the angle between two vectors in high-dimensional space. This metric is often used for text data because it is based on the frequency of words in each document and can handle high-dimensional data well. Choose the neighborhood radius and minimum points: DBSCAN requires two parameters: the neighborhood radius (epsilon) and the minimum number of points required to form a dense region (minPts). The neighborhood radius determines the size of the neighborhood around each point, and minPts determines how many points must be in the neighborhood for a point to be considered part of a dense region. Compute the distance matrix: Use the chosen distance metric to compute a distance matrix between all pairs of data points. Identify core points: A core point is a data point that has at least minPts other points within its neighborhood. Identify all core points in the data. Identify non-core points: A non-core point is a point that is not a core point, but is within the neighborhood of a core point. Identify all non-core points in the data. Form clusters: For each core point, form a cluster by including all other core points and non-core points that are within its neighborhood. Repeat this process for all core points until all points are assigned to a cluster. Identify noise points: Any points that are not assigned to a cluster are considered noise points. Determine optimal parameters: Experiment with different values for epsilon and minPts to find the optimal parameters for your data. Overall, DBSCAN is a relatively simple algorithm to implement, but choosing the right distance metric and setting the optimal parameters can be challenging. Experimentation and iteration are key to finding the best parameters for your particular dataset.","title":"Synopsis of DBSCAN"},{"location":"Finding%20Rare%20Logs%20with%20DBSCAN/#synopsis-of-dbscan-wtext","text":"Step 1: Import the dataset and preprocess the data Assuming that the dataset is a collection of text documents, we will import the dataset into our code and preprocess the data to prepare it for clustering. This may involve removing stop words, stemming or lemmatizing the text, and converting the text to numerical vectors using techniques such as TF-IDF. For this example, let's assume that the data has already been preprocessed and is stored in a Pandas DataFrame called documents_df. Step 2: Choose the epsilon and min_samples hyperparameters As before, we need to choose the epsilon and min_samples hyperparameters. The choice of hyperparameters will depend on the specific dataset and the desired level of granularity in the clusters. Let's say we choose epsilon = 0.3 and min_samples = 5 for this example. Step 3: Calculate the distance matrix We will now calculate the distance matrix between all pairs of documents in the dataset. We will use a cosine distance as the distance metric since we are dealing with text data. The distance matrix is a square matrix where each element (i,j) is the distance between document i and document j. Let's assume that we have already calculated the distance matrix and stored it in a NumPy array called dist_matrix. Step 4: Identify core points Next, we will identify the core points in the dataset. A point is a core point if it has at least min_samples other points within a distance of epsilon. We can identify core points by counting the number of points within epsilon distance of each point and checking if it is greater than or equal to min_samples. Let's store the indices of the core points in a list called core_indices. Step 5: Identify border points and noise points We will now identify the border points and noise points. A border point is a point that is not a core point but is within epsilon distance of at least one core point. A noise point is a point that is not a core point and is not within epsilon distance of any core point. We can identify border points and noise points by iterating over all points and checking if they are in core_indices. If a point is not a core point, we can then check if it is within epsilon distance of any core point. If it is, it is a border point; if not, it is a noise point. Let's store the indices of the border points in a list called border_indices and the indices of the noise points in a list called noise_indices. Step 6: Create clusters We will now create the clusters. We will start with an empty list of clusters and iterate over the core points. For each core point, we will check if it has not already been assigned to a cluster. If it has not, we will create a new cluster and add the core point to it. We will then add all border points that are within epsilon distance of the core point to the cluster. We will then repeat this process for all new core points added to the cluster until there are no more points to add. Let's store the clusters in a list called clusters. Step 7: Visualize the clusters Finally, we can visualize the clusters by examining the documents in each cluster and identifying common themes or topics. We can also use techniques such as word clouds or topic modeling to gain insights into the clusters. Let's assume that we have already performed some exploratory analysis on the clusters and identified common themes or topics in each cluster. We can then present these findings in a report or dashboard for further analysis or action.","title":"Synopsis of DBSCAN w/Text"},{"location":"Finding%20Rare%20Logs%20with%20DBSCAN/#dbscan-for-rare-logs-visualized","text":"","title":"DBSCAN for Rare Logs Visualized"},{"location":"Finding%20Rare%20Logs%20with%20DBSCAN/#why-would-rare-logs-be-clustered","text":"NOTE They don't have to be clustered together. See Rare Logs w/Good Epsilon Visualized for an example of what non-clustered rare logs look like. DBSCAN identifies clusters by finding regions of high point density in the feature space. Rare points can be clustered together if they are located in regions of low point density, and if the distance metric used by DBSCAN captures the similarity between these points. For example, if the rare points share certain characteristics or patterns in the feature space that distinguish them from other points, then DBSCAN may identify these points as a separate cluster. Additionally, rare points may be clustered together if they are located in a region of the feature space that is otherwise sparsely populated, and DBSCAN identifies this region as a cluster due to its low point density. In both cases, it is important to carefully choose the distance metric and parameter values for DBSCAN in order to identify clusters that correspond to rare events. Additionally, further analysis may be needed to understand the properties and potential causes of the identified rare events.","title":"Why would rare logs be clustered?"},{"location":"Finding%20Rare%20Logs%20with%20DBSCAN/#example","text":"An example of rare log features that could distinguish them from regular logs but cause the rare logs themselves to be similar could be a sudden increase or decrease in the magnitude of certain log features such as network traffic, user activity, or system performance. For instance, in the case of network traffic logs, a rare event could be a sudden spike in the number of failed connection attempts, which would cause the affected logs to have similar patterns of failed connection attempts. However, these rare logs would still be distinguishable from regular logs due to their significantly higher frequency of failed connection attempts.","title":"Example"},{"location":"Finding%20Rare%20Logs%20with%20DBSCAN/#what-is-a-hyperparameter","text":"In machine learning, a hyperparameter is a configuration parameter that is set before training a model and affects the behavior and performance of the model. Hyperparameters are different from the model parameters, which are learned during training. Hyperparameters are set by the user or a tuning algorithm before the training process begins. Examples of hyperparameters include learning rate, regularization strength, batch size, number of hidden layers, and number of neurons per layer in a neural network. The values of hyperparameters can have a significant impact on the performance of a model. Therefore, selecting appropriate hyperparameters is an important step in building a successful machine learning model. Typically, hyperparameters are chosen based on a combination of domain expertise, intuition, and empirical experimentation.","title":"What is a hyperparameter?"},{"location":"Finding%20Rare%20Logs%20with%20DBSCAN/#what-is-term-frequency-inverse-document-frequency-tf-idf","text":"TF-IDF stands for \"Term Frequency-Inverse Document Frequency\". It is a technique used to convert a collection of text documents into a numerical vector representation that can be used for machine learning algorithms such as clustering, classification, or recommendation systems. The basic idea behind TF-IDF is to weigh the importance of each word in a document by considering how often it appears in the document (term frequency) and how common or rare the word is across all documents in the dataset (inverse document frequency). The term frequency (TF) of a word in a document is simply the number of times the word appears in the document. However, this does not take into account the fact that some words may be more important than others even if they appear frequently. For example, in a document about cats, the word \"cat\" may appear frequently but the word \"whiskers\" may be more informative or distinctive. The inverse document frequency (IDF) of a word is a measure of how rare or common the word is across all documents in the dataset. It is calculated as the logarithm of the total number of documents in the dataset divided by the number of documents that contain the word. The TF-IDF score for a word in a document is simply the product of its term frequency and inverse document frequency. Words that appear frequently in a document but are rare across all documents in the dataset will have a high TF-IDF score and are likely to be more informative or distinctive. After calculating the TF-IDF scores for all words in all documents, we can represent each document as a numerical vector where each element of the vector represents the TF-IDF score of a particular word in the document. These vectors can then be used as input to machine learning algorithms for tasks such as clustering, classification, or recommendation systems.","title":"What is Term Frequency-Inverse Document Frequency (TF-IDF)"},{"location":"Finding%20Rare%20Logs%20with%20DBSCAN/#why-logarithm","text":"We use logarithm in the IDF calculation of TF-IDF because the frequency distribution of words in a corpus typically follows a power law or Zipf's law, where a few words occur very frequently and most words occur rarely. For example, in a large corpus of English text, the word \"the\" is likely to occur very frequently while the word \"zygote\" may occur very rarely. Without logarithm, the IDF value for common words like \"the\" may become very small or even zero, which would lead to a distortion of the TF-IDF weights. Taking the logarithm of the IDF value helps to dampen the effect of the high frequency of common words and amplify the effect of the low frequency of rare words. It also helps to reduce the range of IDF values and make them more comparable across words. By using logarithm, we can obtain a more accurate and meaningful measure of the rarity or commonality of each word across the entire corpus, which can be used to weight the importance of each word in a document. Logarithm dampens the effect of common words because it compresses the range of IDF values. As the number of documents containing a given word increases, the IDF value decreases, and the rate of decrease slows down. By taking the logarithm of the IDF value, we can reduce the magnitude of this effect, effectively \"flattening\" the curve and reducing the range of IDF values. For example, let's say we have a corpus of 1000 documents and the word \"the\" appears in all 1000 documents. The IDF value for \"the\" would be log(1000/1000) = log(1) = 0. Now let's consider a less common word like \"zygote\" that appears in only 1 document. The IDF value for \"zygote\" would be log(1000/1) = log(1000) = 3. By taking the logarithm, we have compressed the range of IDF values from 3 orders of magnitude to just 1. This compression of the IDF values has the effect of reducing the weight of common words like \"the\" and increasing the weight of rare words like \"zygote\". This is desirable because common words tend to be less informative than rare words and may not be as useful for distinguishing between different documents or topics. By dampening the effect of common words, we can focus more attention on the rare words that are more likely to be distinctive or informative.","title":"Why Logarithm"},{"location":"Finding%20Rare%20Logs%20with%20DBSCAN/#key-points","text":"In the context of IDF calculation, the log is used to scale down the importance of words that appear in many documents in the corpus. When a word appears in all documents, the IDF score will be 0 because log of 1 is 0. This reflects the idea that a word that appears in all documents is not very informative or useful for distinguishing between documents, and therefore should not be given a high weighting in a text analysis. The IDF term is $$IDF(t) = log \\frac{N}{n_t}$$","title":"Key Points"},{"location":"Finding%20Rare%20Logs%20with%20DBSCAN/#example_1","text":"This is the general idea but ChatGPT's math was off Let $D_1$ to $D_{10}$ be a corpus of 10 documents and let $TF(d, t)$ be the term frequency of term $t$ in document $d$. The inverse document frequency of term $t$, $IDF(t)$, is defined as: $$IDF(t) = log \\frac{N}{n_t}$$ where $N$ is the total number of documents in the corpus and $n_t$ is the number of documents that contain term $t$. The TF-IDF score of term $t$ in document $d$, $TFIDF(d, t)$, is defined as: $$TFIDF(d, t) = TF(d, t) \\times IDF(t)$$ Let's say we want to calculate the TF-IDF score for the word \"cat\" in each document. Here are the 10 documents in our corpus: Document 1: \"The cat in the hat.\" Document 2: \"The cat and the dog.\" Document 3: \"The dog chased the cat.\" Document 4: \"The cat sat on the mat.\" Document 5: \"The dog and the cat played.\" Document 6: \"The cat and the mouse.\" Document 7: \"The cat was hungry.\" Document 8: \"The cat slept all day.\" Document 9: \"The cat and the bird.\" Document 10: \"The cat is black.\" We can calculate the TF-IDF score for \"cat\" in each document as follows: TF(\"cat\", $D_1$) = 1/5 = 0.2 TF(\"cat\", $D_2$) = 1/5 = 0.2 TF(\"cat\", $D_3$) = 1/5 = 0.2 TF(\"cat\", $D_4$) = 1/6 = 0.1667 TF(\"cat\", $D_5$) = 1/5 = 0.2 TF(\"cat\", $D_6$) = 1/4 = 0.25 TF(\"cat\", $D_7$) = 1/4 = 0.25 TF(\"cat\", $D_8$) = 1/4 = 0.25 TF(\"cat\", $D_9$) = 1/4 = 0.25 TF(\"cat\", $D_{10}$) = 1/4 = 0.25 Number of documents containing \"cat\": 9 IDF(\"cat\") = log(10/9) = 0.1054 TFIDF($D_1$, \"cat\") = 0.2 * 0.1054 = 0.0211 TFIDF($D_2$, \"cat\") = 0.2 * 0.1054 = 0.0211 TFIDF($D_3$, \"cat\") = 0.2 * 0.1054 = 0.0211 TFIDF($D_4$, \"cat\") = 0.1667 * 0.1054 = 0.0175 TFIDF($D_5$, \"cat\") = 0.2 * 0.1054 = 0.0211 TFIDF($D_6$, \"cat\") = 0.25 * 0.1054 = 0.0264 TFIDF($D_7$, \"cat\") = 0.25 * 0.1054 = 0.0264 TFIDF($D_8$, \"cat\") = 0.25 * 0.1054 = 0.0264 TFIDF($D_9$, \"cat\") = 0.25 * 0.1054 = 0.0264 TFIDF($D_{10}$, \"cat\") = 0.25 * 0.1054 = 0.0264 So, the TF-IDF score of \"cat\" in each document is as follows: Document 1: 0.0211 Document 2: 0.0211 Document 3: 0.0211 Document 4: 0.0175 Document 5: 0.0211 Document 6: 0.0264 Document 7: 0.0264 Document 8: 0.0264 Document 9: 0.0264 Document 10: 0.0264","title":"Example"},{"location":"Finding%20Rare%20Logs%20with%20DBSCAN/#visualize-tf-idf","text":"The heatmap chart shows the TF-IDF score for each word in each document in the corpus. The color of each cell indicates the relative TF-IDF score of the corresponding word in the corresponding document. The darker red cells represent words that have higher TF-IDF scores, which means they are more important in the corresponding documents. Conversely, the lighter blue cells represent words that have lower TF-IDF scores, which means they are less important in the corresponding documents. For example, in document 1 (\"The cat in the hat.\"), the word \"cat\" has a relatively high TF-IDF score, which is why its cell is darker red compared to other cells in the same row. In contrast, the word \"the\" has a relatively low TF-IDF score in most documents, which is why its cells are lighter blue compared to other cells in the same column. Overall, the heatmap can give you a sense of the most important words in each document and across the corpus as a whole.","title":"Visualize TF-IDF"},{"location":"Finding%20Rare%20Logs%20with%20DBSCAN/#what-is-a-core-point","text":"In DBSCAN, a core point is a data point that has at least min_samples other data points within a distance of eps. In other words, a core point is a point that is at the center of a dense region of the dataset. More formally, a data point p is a core point if and only if there exist at least min_samples data points (including p itself) within a distance of eps of p. This means that a core point has at least min_samples neighboring data points within a radius of eps. Core points are important in the DBSCAN algorithm because they define the centers of clusters. Any data points that are not core points themselves but are within a distance of eps of a core point are considered to be part of the same cluster as that core point. This means that the clusters in the dataset are defined by the core points and the density of the data around them.","title":"What is a core point?"},{"location":"Finding%20Rare%20Logs%20with%20DBSCAN/#how-are-core-points-selected","text":"Core points are selected based on the value of the hyperparameters epsilon and min_samples. A core point is defined as a data point that has at least min_samples other data points within a distance of epsilon (i.e., the distance between any pair of core points must be less than or equal to epsilon). In other words, if a data point has at least min_samples other data points within a distance of epsilon, then it is a core point. Otherwise, it is not a core point. Note that the hyperparameters epsilon and min_samples are typically set based on domain knowledge or by using trial and error to determine the best values for a given dataset. The choice of hyperparameters will depend on the specific dataset and the desired level of granularity in the clusters.","title":"How are core points selected?"},{"location":"Finding%20Rare%20Logs%20with%20DBSCAN/#what-is-epsilon","text":"In the context of the DBSCAN algorithm, epsilon is a hyperparameter that defines the maximum distance between two data points in order for them to be considered part of the same cluster. Specifically, a data point is considered a core point if it has at least min_samples data points within a distance of epsilon from it. If a data point is not a core point but is within epsilon distance of a core point, it is considered a border point. Data points that are not core points or border points are considered noise points and are not part of any cluster. Choosing an appropriate value for epsilon depends on the density of the data and the desired level of granularity in the clusters. A small epsilon will result in dense clusters with more noise points, while a large epsilon will result in fewer, more spread-out clusters with fewer noise points. The choice of epsilon should be guided by domain knowledge and empirical experimentation. See this code","title":"What is Epsilon?"},{"location":"Finding%20Rare%20Logs%20with%20DBSCAN/#rare-logs-wgood-epsilon-visualized","text":"You can see in chart 2 how only a few points are highlighted - these would be our rare logs.","title":"Rare Logs w/Good Epsilon Visualized"},{"location":"Finding%20Rare%20Logs%20with%20DBSCAN/#how-to-pick-a-good-epsilon","text":"There are several approaches you can use to determine a good epsilon for your data: - Visual inspection: Plot a k-distance graph and visually inspect it to find the elbow point, which represents a good estimate of epsilon. - Average nearest neighbors: Plot the average distance to the k-nearest neighbors for each point and select the point with the highest change in distance as the estimate of epsilon. - Silhouette score: Calculate the silhouette score for different values of epsilon and select the value that maximizes the silhouette score. - Also often used as a tool for comparing different clustering algorithms or parameter settings to determine which produces the most well-defined clusters. - Grid search: Use a grid search approach to test different values of epsilon and select the value that results in the best clustering performance.","title":"How to Pick a Good Epsilon"},{"location":"Finding%20Rare%20Logs%20with%20DBSCAN/#elbow-method","text":"","title":"Elbow Method"},{"location":"Finding%20Rare%20Logs%20with%20DBSCAN/#elbow-method-visualized","text":"Why is it called inertia? : In the elbow method, we are interested in minimizing the within-cluster sum of squares (also called the cluster inertia or simply inertia) as the number of clusters increases. This is because as the number of clusters increases, we can expect the inertia to decrease since the data is being divided into more and more smaller subsets. The y-axis was labeled as \"inertia\" since it represents the within-cluster sum of squares, which we are trying to minimize. See this code NOTE This code shows the elbow method for k means rather than DBSCAN see the problem with that in Problems with Elbow Method in DBSCAN The elbow method helps to determine the optimal number of clusters for a given dataset. The graph typically shows the number of clusters on the x-axis and the sum of squared distances (SSE) on the y-axis. The SSE measures how far the points within a cluster are from the center of the cluster. The goal is to minimize the SSE, which indicates that the points within each cluster are close together and well-separated from points in other clusters. In the graph, you want to look for the \"elbow\" point, which is the point of inflection where the SSE starts to level off or decrease at a slower rate. This point represents the optimal number of clusters for the given dataset. In the example graph above, the elbow point appears to be around 3 or 4 clusters, since adding more clusters beyond this point doesn't result in significant reduction of the SSE. However, the choice of the optimal number of clusters ultimately depends on the specific problem and the context of the data.","title":"Elbow Method Visualized"},{"location":"Finding%20Rare%20Logs%20with%20DBSCAN/#why-the-sum-of-squared-distances","text":"The sum of squared distances is helpful in clustering because it provides a way to measure how similar or dissimilar the points within a cluster are. If the sum of squared distances within a cluster is small, it suggests that the points in that cluster are tightly grouped together and similar to one another. On the other hand, if the sum of squared distances within a cluster is large, it suggests that the points in that cluster are more spread out and dissimilar to one another. Therefore, the sum of squared distances can help us to identify the optimal number of clusters by evaluating how well the data fits the given number of clusters. The goal is to find the smallest number of clusters that results in a low sum of squared distances within each cluster, while also avoiding overfitting the data.","title":"Why the Sum of Squared Distances"},{"location":"Finding%20Rare%20Logs%20with%20DBSCAN/#how-to-use-the-elbow-curve","text":"The optimal number of clusters can be determined by looking at the elbow point on the curve. The elbow point is the point where the inertia starts to decrease at a slower rate. This point represents the trade-off between minimizing the sum of squared distances within each cluster and maximizing the number of clusters. In general, the elbow point represents the optimal number of clusters to use for the given data set. However, sometimes the elbow point may not be clearly defined or there may be multiple elbow points, in which case other methods such as silhouette analysis may be used to determine the optimal number of clusters.","title":"How to use the elbow curve"},{"location":"Finding%20Rare%20Logs%20with%20DBSCAN/#problems-with-elbow-method-in-dbscan","text":"One thing to note is that unlike KMeans, DBSCAN doesn't have a fixed number of clusters. So the elbow curve for DBSCAN may not have a clear \"elbow\" point like in the KMeans case. Instead, you may have to manually examine the plot and choose a reasonable value of eps based on the trade-off between the number of clusters and the amount of data within each cluster.","title":"Problems with Elbow Method in DBSCAN"},{"location":"Finding%20Rare%20Logs%20with%20DBSCAN/#how-to-use-the-elbow-curve-for-dbscan","text":"See this code So in the chart above we can see the inertia jumps up until right before .1. Which means we probably want to aim for something like 6 clusters with an epsilon of like .7 and an inertia of ~7.5.","title":"How to Use the Elbow Curve for DBSCAN"},{"location":"Finding%20Rare%20Logs%20with%20DBSCAN/#example-of-sum-of-squared-distances","text":"(1, 2) (3, 4) (5, 6) We want to calculate the sum of squared distances for a cluster of two points. We randomly assign the first two points to the cluster and calculate the distance between each point and the centroid: Cluster 1: - (1, 2) -> distance to centroid (2, 3) = sqrt((1-2)^2 + (2-3)^2) = sqrt(2) - (3, 4) -> distance to centroid (2, 3) = sqrt((3-2)^2 + (4-3)^2) = sqrt(2) - Sum of squared distances = (sqrt(2))^2 + (sqrt(2))^2 = 4 Next, we calculate the sum of squared distances for a cluster of the other two points: Cluster 2: - (3, 4) -> distance to centroid (3, 5) = sqrt((3-3)^2 + (4-5)^2) = 1 - (5, 6) -> distance to centroid (3, 5) = sqrt((5-3)^2 + (6-5)^2) = sqrt(5) - Sum of squared distances = 1^2 + (sqrt(5))^2 = 6 We can see that the sum of squared distances for cluster 1 is 4, and the sum of squared distances for cluster 2 is 6. In this case, cluster 1 is the better choice since it has a lower sum of squared distances.","title":"Example of Sum of Squared Distances"},{"location":"Finding%20Rare%20Logs%20with%20DBSCAN/#average-nearest-neighbors","text":"The average nearest neighbors (ANN) algorithm is a method for estimating the optimal value of the epsilon parameter in DBSCAN clustering. The idea behind ANN is to plot the distances between each data point and its kth nearest neighbor. The plot is then sorted in increasing order of distances, and the elbow point is identified as the point where the curve starts to level off, indicating a natural break in the distances. To use ANN to estimate the optimal epsilon value, one needs to choose a value for k, which is typically set to 4 or 5. The kth nearest neighbor can be found using a k-d tree or brute force search, and the distances can be sorted using a quicksort or similar algorithm. Once the distances are sorted, the elbow point can be identified by visual inspection or by fitting a curve to the data using a regression algorithm. The value of epsilon corresponding to the elbow point is then used as the input to the DBSCAN algorithm. The advantage of the ANN algorithm is that it can be applied to high-dimensional data and is relatively insensitive to the choice of k. However, it requires additional computation to find the kth nearest neighbor for each data point, which can be time-consuming for large datasets.","title":"Average nearest neighbors"},{"location":"Finding%20Rare%20Logs%20with%20DBSCAN/#average-nearest-neighbors-visualized","text":"See this code This code generates 100 random points and computes the distances to the 5th nearest neighbor for each point. It then sorts the average distances and plots them against the point index (just the index of the point in an array of 100). The resulting plot shows how the average nearest neighbor distance varies across the points. We would want a value around .14 right before the average distance rapidly ascends.","title":"Average nearest neighbors visualized"},{"location":"Finding%20Rare%20Logs%20with%20DBSCAN/#silhouette-score","text":"The Silhouette score is a measure of how well a data point fits into its assigned cluster in unsupervised learning algorithms such as clustering. It is calculated using the mean intra-cluster distance (a) and the mean nearest-cluster distance (b) for each data point, and ranges from -1 to 1. A Silhouette score of 1 indicates that the data point is well-matched to its own cluster and poorly-matched to neighboring clusters, while a score of -1 indicates the opposite. A score of 0 indicates that the data point is on the boundary between two clusters. The Silhouette score can be used to evaluate the quality of a clustering algorithm and to determine the optimal number of clusters. A higher Silhouette score indicates better clustering, and the optimal number of clusters can be determined by selecting the number of clusters that maximizes the Silhouette score.","title":"Silhouette Score"},{"location":"Finding%20Rare%20Logs%20with%20DBSCAN/#silhouette-score-math","text":"The silhouette score measures how similar an object is to its own cluster (cohesion) compared to other clusters (separation). The formula for silhouette score is as follows: $s(i) = \\frac{b(i) - a(i)}{\\max{a(i), b(i)}}$ where $i$ is an individual data point, $a(i)$ is the average distance between $i$ and all other data points in the same cluster, and $b(i)$ is the minimum average distance between $i$ and all data points in any other cluster. The silhouette score ranges from -1 to 1, with a score of 1 indicating that the point is well-matched to its own cluster and poorly-matched to neighboring clusters, a score of 0 indicating that the point is equally similar to its own cluster and neighboring clusters, and a score of -1 indicating that the point is poorly-matched to its own cluster and well-matched to neighboring clusters.","title":"Silhouette Score Math"},{"location":"Finding%20Rare%20Logs%20with%20DBSCAN/#example_2","text":"Sure, let's say we have a dataset with four data points, labeled A, B, C, and D. We have performed clustering using some clustering algorithm, and let's say that points A and B belong to cluster 1, while points C and D belong to cluster 2. We can calculate the silhouette score for each point as follows: For point A: Compute the average distance from point A to all other points in cluster 1. Let's call this value a = 0.4. Compute the average distance from point A to all points in the closest other cluster, cluster 2. Let's call this value b = 0.6. The silhouette score for point A is then $(b - a) / max(a, b) = (0.6 - 0.4) / 0.6 = 0.33$. For point B: Compute the average distance from point B to all other points in cluster 1. Let's call this value a = 0.2. Compute the average distance from point B to all points in the closest other cluster, cluster 2. Let's call this value b = 0.8. The silhouette score for point B is then $(b - a) / max(a, b) = (0.8 - 0.2) / 0.8 = 0.75$. For point C: Compute the average distance from point C to all other points in cluster 2. Let's call this value a = 0.3. Compute the average distance from point C to all points in the closest other cluster, cluster 1. Let's call this value b = 0.9. The silhouette score for point C is then $(b - a) / max(a, b) = (0.9 - 0.3) / 0.9 = 0.67$. For point D: Compute the average distance from point D to all other points in cluster 2. Let's call this value a = 0.5. Compute the average distance from point D to all points in the closest other cluster, cluster 1. Let's call this value b = 0.7. The silhouette score for point D is then $(b - a) / max(a, b) = (0.7 - 0.5) / 0.7 = 0.29$. The overall silhouette score for this clustering is the average of the silhouette scores for all points, which in this case is $(0.33 + 0.75 + 0.67 + 0.29) / 4 = 0.51$.","title":"Example"},{"location":"Finding%20Rare%20Logs%20with%20DBSCAN/#silhouette-score-visualized","text":"See this code","title":"Silhouette Score Visualized"},{"location":"Finding%20Rare%20Logs%20with%20DBSCAN/#grid-search","text":"Grid search is a hyperparameter tuning technique in machine learning where a model is trained on various combinations of hyperparameters to find the optimal set of hyperparameters that yields the best performance for a specific evaluation metric. In grid search, the model is trained and evaluated on all possible combinations of hyperparameters within a predefined range or grid. This is done by creating a grid of hyperparameters, where each hyperparameter is assigned a set of values to try. The model is then trained and evaluated using each combination of hyperparameters in the grid. The combination of hyperparameters that yields the best evaluation metric is then chosen as the optimal set of hyperparameters for the model. Grid search is often used when there are a limited number of hyperparameters to tune, as it can be computationally expensive when the number of hyperparameters or the range of values for each hyperparameter is large.","title":"Grid Search"},{"location":"Finding%20Rare%20Logs%20with%20DBSCAN/#grid-search-visualized","text":"See this code The chart shows the results of a grid search over a range of values for two hyperparameters of the DBSCAN clustering algorithm: eps (which controls the maximum distance between points in the same cluster) and min_samples (which controls the minimum number of points required to form a dense region). The x-axis of the chart shows the values of min_samples tested in the grid search, and the y-axis shows the values of eps tested in the grid search. The color of each cell in the chart represents the number of clusters found by DBSCAN for that combination of eps and min_samples. Darker colors indicate a higher number of clusters, while lighter colors indicate fewer clusters. The chart can be used to identify the combination of eps and min_samples that yields the best clustering results for the particular dataset being analyzed. For example, if the goal is to find a small number of well-defined clusters, the optimal combination of eps and min_samples might be found in a region of the chart with light colors. Conversely, if the goal is to find a larger number of clusters, a region of the chart with darker colors might be more suitable.","title":"Grid Search Visualized"},{"location":"Finding%20Rare%20Logs%20with%20DBSCAN/#how-is-the-distance-from-a-core-point-calculated","text":"The distance between two data points in DBSCAN is typically calculated using a distance metric such as Euclidean distance or Manhattan distance. These distance metrics measure the distance between two points in n-dimensional space, where n is the number of features or dimensions in the dataset. For example, if we have a dataset with two features (i.e., two-dimensional data), the Euclidean distance between two data points p and q is calculated as: $$distance(p, q) = \\sqrt{(p_1 - q_1)^2 + (p_2 - q_2)^2}$$ where $p_1$ and $q_1$ are the values of the first feature in $p$ and $q$, and $p_2$ and $q_2$ are the values of the second feature in $p$ and $q$. Manhattan distance is another common distance metric used in DBSCAN. Manhattan distance measures the distance between two points by summing the absolute differences of their coordinates. For example, the Manhattan distance between two data points p and q with two features is calculated as: $$distance(p, q) = |p_1 - q_1| + |p_2 - q_2|$$ where $p_1$ and $q_1$ are the values of the first feature in $p$ and $q$, and $p_2$ and $q_2$ are the values of the second feature in $p$ and $q$. DBSCAN is flexible enough to allow different distance metrics to be used depending on the nature of the data and the problem being solved.","title":"How is the distance from a core point calculated?"},{"location":"Finding%20Rare%20Logs%20with%20DBSCAN/#example_3","text":"Certainly! Let's say we have a dataset with two features, \"age\" and \"income\", and we want to calculate the distance between two data points, $p$ and $q$. Here are the values of these two features for $p$ and $q$: $p = (30, 50000)$ $q = (35, 60000)$ We can calculate the Euclidean distance between $p$ and $q$ as follows: $$distance(p, q) = \\sqrt{(p_1 - q_1)^2 + (p_2 - q_2)^2}$$ (based on Pythagoreans Theorem) where $p_1$ and $q_1$ are the values of the \"age\" feature in $p$ and $q$, and $p_2$ and $q_2$ are the values of the \"income\" feature in $p$ and $q$. Substituting the values of $p$ and $q$, we get: $$distance(p, q) = \\sqrt{(30 - 35)^2 + (50000 - 60000)^2} = \\sqrt{25^2 + 10000^2} \\approx 10000.25$$ So the Euclidean distance between $p$ and $q$ is approximately 10000.25. Why $$distance(p, q) = \\sqrt{(p_1 - q_1)^2 + (p_2 - q_2)^2}$$ The distance formula is derived from the Pythagorean theorem, which states that the square of the hypotenuse of a right triangle is equal to the sum of the squares of the other two sides. In this case, the sides of the triangle are the horizontal and vertical distances between the two points, and the hypotenuse is the straight line distance between them. The formula uses the square root of the sum of the squares of the horizontal and vertical distances to find the length of the hypotenuse, which is the Euclidean distance between the two points. This formula can be extended to higher dimensions by adding the squares of the differences in each dimension and taking the square root of the sum.","title":"Example"},{"location":"Finding%20Rare%20Logs%20with%20DBSCAN/#how-to-include-log-attributes-that-arent-message","text":"The problem with the above is that TF-IDF accounts for the message field but not how to combine it with other log attributes. To combine additional fields with the text data represented by TF-IDF, you can concatenate the TF-IDF vectors with vectors representing the additional fields. These additional fields can be transformed into numerical representations that are appropriate for clustering using methods such as one-hot encoding or normalization. Then, the resulting vectors can be used as input to the clustering algorithm, allowing you to perform rare log detection based on multiple features. For example, let's say you have a set of log messages and two additional fields: \"source\" and \"severity\". To combine these fields with the text data, you could create a new vector for each log message that contains the TF-IDF representation of the message text, the one-hot encoded representation of the source field, and the normalized representation of the severity field. These vectors could then be used as input to DBSCAN or other clustering algorithms to identify clusters of logs with low point density, which may correspond to rare events.","title":"How to Include Log Attributes that Aren't Message"},{"location":"Finding%20Rare%20Logs%20with%20DBSCAN/#example-of-one-hot-encoding-for-severity","text":"Suppose we have a log entry with the following fields: { \"timestamp\": \"2022-04-12T10:15:30\", \"severity\": \"warning\", \"message\": \"Disk usage is high on /dev/sda1\" } To encode this log entry using a combination of one-hot encoding and TF-IDF, we can follow these steps: One-hot encode the severity field. Suppose we have three possible values for the severity field: \"informational\", \"warning\", and \"error\". We can create a one-hot encoding vector for the severity field as follows: { \"informational\": [1, 0, 0], \"warning\": [0, 1, 0], \"error\": [0, 0, 1] } For this log entry, the one-hot encoding vector for the severity field would be $[0, 1, 0]$, since the severity is \"warning\". Apply TF-IDF encoding to the message field. Suppose we have a vocabulary of 10,000 unique words that appear in our log messages. We can create a TF-IDF vector for the message field as follows: Tokenize the message field into a list of words. In this case, the message field would be tokenized into the list [\"disk\", \"usage\", \"is\", \"high\", \"on\", \"/dev/sda1\"]. Calculate the TF-IDF score for each word in the message field using the following formula: $tf-idf(w, d, D) = tf(w, d) * idf(w, D)$ where $tf(w, d)$ is the frequency of the word $w$ in the document $d$, and $idf(w, D)$ is the inverse document frequency of the word $w$ in the set of all documents $D$. The inverse document frequency can be calculated as follows: $idf(w, D) = log(N / (1 + df(w, D)))$ where $N$ is the total number of documents in the corpus, and $df(w, D)$ is the number of documents in the corpus that contain the word $w$. For each word in the message field, multiply its TF-IDF score by the one-hot encoding vector for the severity field to create a combined feature vector. For example, suppose the TF-IDF scores for the words in the message field are as follows: { \"disk\": 0.1, \"usage\": 0.05, \"is\": 0.02, \"high\": 0.15, \"on\": 0.03, \"/dev/sda1\": 0.08 } The combined feature vector for this log entry would be (in a real case it would be as long as the corpus of words in all documents): [0, 1, 0, 0.01, 0.005, 0.002, 0.015, 0.03, 0.008] The first three elements of the vector represent the one-hot encoding for the severity field, and the remaining elements represent the TF-IDF scores for the words in the message field. By using one-hot encoding for categorical fields and TF-IDF encoding for text fields, we can create a single feature vector that captures information from multiple fields in a log entry.","title":"Example of One Hot Encoding for Severity"},{"location":"Finding%20Rare%20Logs%20with%20DBSCAN/#the-problem-of-new-documents","text":"If a new document is added to the corpus that contains words not seen in the original vocabulary, then the TF-IDF vectors for all other documents in the corpus would need to be recomputed to include the new term in their vector representations. This can be computationally expensive, especially for large corpora with many documents. One way to mitigate this issue is to use incremental learning algorithms that can update the existing vector representations with new data without recomputing everything from scratch. Another approach is to periodically retrain the model on the entire corpus, which can be time-consuming but ensures that the vector representations are up-to-date. To incrementally update the feature vectors when new documents are added to the corpus, you would need to update the inverse document frequency (idf) values for each word in the vocabulary. Suppose you have a current corpus of N documents, and you add a new document to the corpus. You would first update the idf values for each word in the vocabulary using the new document. Then, for each existing document in the corpus, you would update the tf-idf scores for any words in the new document that also appear in the existing document. This incremental update can be more efficient than recalculating the tf-idf scores for all documents in the corpus every time a new document is added, especially if the corpus is large.","title":"The Problem of New Documents"},{"location":"Finding%20Rare%20Logs%20with%20DBSCAN/#normalizing-data","text":"The problem is that some of the data is going to have very large magnitude and we have to normalize it before feeding it into the algorithm. Ex: the time epoch is very large relative to everything else. Scatter plot of data before standardization: This plot shows the distribution of the data in its original scale. The two variables have different means and variances, and the scale of the variables are not the same. There is a clear positive correlation between the two variables, indicating that when one variable is high, the other variable is likely to be high as well. Scatter plot of data after standardization: This plot shows the same data as the first plot, but after applying standardization using StandardScaler. The data is now centered around the origin (0, 0), and the scale of the two variables are now the same. The correlation between the two variables is still present. Q-Q plot of variable 1 before standardization: This plot shows the distribution of the first variable before standardization. The blue points represent the ordered values of the variable, and the red line represents the theoretical quantiles of a normal distribution. If the distribution of the variable was perfectly normal, the points would lie on the red line. However, we can see that the distribution deviates from normality, as indicated by the non-linear shape of the Q-Q plot. Q-Q plot of variable 1 after standardization: This plot shows the same variable as in the third plot, but after applying standardization. The data is now distributed closer to normal, as indicated by the straight line in the Q-Q plot. This means that the standardization process has reduced the deviation from normality in the distribution of this variable. Overall, the scatter plots show the relationship between the two variables and how standardization can bring them onto the same scale. The Q-Q plots show how standardization can improve the normality of the distribution of a variable.","title":"Normalizing Data"},{"location":"Finding%20Rare%20Logs%20with%20DBSCAN/#how-to-interpret-a-q-q-plot","text":"A Q-Q plot (quantile-quantile plot) is a graphical technique used to compare the distribution of a sample of data to a theoretical probability distribution. It is a commonly used diagnostic tool in statistics to assess whether a sample of data comes from a certain distribution, such as a normal distribution, and to identify any departures from this distribution. Here's how to interpret a Q-Q plot: Theoretical quantiles: The x-axis of the Q-Q plot represents the theoretical quantiles of the theoretical distribution being compared to the observed data. Ordered values: The y-axis of the Q-Q plot represents the ordered values of the sample data. Perfect fit: If the sample data follows the theoretical distribution perfectly, the Q-Q plot will show a straight line that passes through the origin. Departures from the theoretical distribution: Any departures from a straight line indicate departures from the theoretical distribution. Outliers: Outliers in the sample data may appear as points that are far away from the straight line in the Q-Q plot. Overall, a Q-Q plot is a useful tool for assessing whether a sample of data follows a certain distribution and identifying any departures from that distribution. It can also be used to identify outliers and other features of the data that may require further investigation.","title":"How to Interpret a Q-Q Plot"},{"location":"Finding%20Rare%20Logs%20with%20DBSCAN/#general-approach-wdecision-tree","text":"I decided not to go this way","title":"General Approach w/Decision Tree"},{"location":"Finding%20Rare%20Logs%20with%20DBSCAN/#general-approach","text":"Collect and preprocess the log data: Before you can apply decision trees to log files, you need to collect and preprocess the data. This may involve extracting the relevant log files from different sources, aggregating them into a single data set, and cleaning the data to remove any noise or inconsistencies. Define the problem and the variables: The next step is to define the problem you want to solve using decision trees and identify the variables you will use to build the decision tree. For example, you might want to detect anomalies in system log files, in which case the variables might include the timestamp, the type of log event, the source IP address, and other relevant attributes. Choose the decision tree algorithm and parameters: Once you have defined the problem and variables, you need to choose the decision tree algorithm you will use and set the parameters. There are several decision tree algorithms available, including ID3, C4.5, CART, and Random Forest. The choice of algorithm will depend on the specific problem you are trying to solve and the characteristics of the log data. Train the decision tree: The next step is to train the decision tree using the log data. This involves splitting the data into a training set and a test set, and using the training set to build the decision tree. The algorithm will recursively partition the data into subsets based on the values of the variables, and choose the best variable to split the data at each internal node. Test and evaluate the decision tree: Once the decision tree has been trained, you need to test it on the test set and evaluate its performance. This involves measuring metrics such as accuracy, precision, recall, and F1 score, and tuning the algorithm parameters as needed to improve performance. Deploy and monitor the decision tree: Finally, you need to deploy the decision tree in a production environment and monitor its performance over time. This may involve setting up alerts or notifications to notify you when anomalies are detected, or integrating the decision tree with other systems or applications. Keep in mind that this is a high-level overview, and there may be additional steps or considerations depending on the specific problem you are trying to solve and the characteristics of the log data. It's also important to have a good understanding of machine learning concepts and techniques, as well as the tools and technologies used to preprocess, analyze, and visualize log data.","title":"General Approach"},{"location":"Finding%20Rare%20Logs%20with%20DBSCAN/#rare-log-detection","text":"If your goal is to identify rare logs or events in your log data using decision trees, you can approach the problem as an anomaly detection problem. In this case, you would define the variables you will use to build the decision tree, and then train the decision tree on a dataset of logs that are representative of normal behavior. The decision tree will learn to recognize patterns that are typical of normal logs, and any logs that deviate from these patterns will be flagged as anomalous or rare. Here are some steps you can take to get started with this approach: Define the problem: Determine what types of rare or anomalous logs you want to identify, and why they are important. For example, you might want to identify logs that indicate potential security threats, system failures, or other unusual behavior. Identify the variables: Choose the variables that you will use to build the decision tree. These may include the timestamp, the type of log event, the source IP address, the destination IP address, the user ID, and other relevant attributes. Collect and preprocess the data: Collect a representative dataset of logs and preprocess the data to remove any noise or inconsistencies. This may involve filtering out irrelevant logs, removing duplicates, and transforming the data into a suitable format. Split the data: Split the data into a training set and a test set. The training set should include logs that are representative of normal behavior, while the test set should include logs that may contain rare or anomalous events. Train the decision tree: Train the decision tree on the training set of logs. The algorithm will recursively partition the data into subsets based on the values of the variables, and choose the best variable to split the data at each internal node. Test the decision tree: Test the decision tree on the test set of logs. The decision tree will flag any logs that deviate from the patterns learned during training as anomalous or rare. Evaluate and refine the decision tree: Evaluate the performance of the decision tree using metrics such as precision, recall, and F1 score. Refine the decision tree by adjusting the algorithm parameters or adding new variables as needed to improve performance. By following these steps, you can build a decision tree that can identify rare or anomalous logs in your data. Keep in mind that the performance of the decision tree may depend on the characteristics of the log data, and you may need to experiment with different algorithms and parameters to achieve the best results.","title":"Rare Log Detection"},{"location":"Get%20NVMe%20Drives%20from%20iDRAC%20Redfish/","text":"Get NVMe Drives from iDRAC Redfish Get NVMe Drives from iDRAC Redfish Exploring iDRAC Detected Storage Understand the Behavior of Unqualified Drives Getting a Drive's Stats Exploring iDRAC Detected Storage I used the Storage API endpoint to accomplish this. From my host I received: {\"@odata.context\":\"/redfish/v1/$metadata#StorageCollection.StorageCollection\",\"@odata.id\":\"/redfish/v1/Systems/System.Embedded.1/Storage/\",\"@odata.type\":\"#StorageCollection.StorageCollection\",\"Description\":\"Collection Of Storage entities\",\"Members\":[{\"@odata.id\":\"/redfish/v1/Systems/System.Embedded.1/Storage/RAID.Slot.4-1\"},{\"@odata.id\":\"/redfish/v1/Systems/System.Embedded.1/Storage/RAID.Embedded.1-1\"},{\"@odata.id\":\"/redfish/v1/Systems/System.Embedded.1/Storage/CPU.1\"},{\"@odata.id\":\"/redfish/v1/Systems/System.Embedded.1/Storage/AHCI.Slot.2-1\"}],\"Members@odata.count\":4,\"Name\":\"Storage Collection\"} I'm running an R840 which is Dell 14G which I know does not have NVMe RAID controllers as an option so I know my NVMe drives must be hanging off the CPU. IE: /redfish/v1/Systems/System.Embedded.1/Storage/CPU.1 . I can expect that the BOSS card is hanging off of AHCI and that any SAS/SATA drives are likely on the RAID controller. The results above also imply that the host above runs a mixed backplane given the presence of RAID and CPU.1. Checking CPU.1 gets me: { \"@odata.context\": \"/redfish/v1/$metadata#Storage.Storage\", \"@odata.id\": \"/redfish/v1/Systems/System.Embedded.1/Storage/CPU.1\", \"@odata.type\": \"#Storage.v1_8_0.Storage\", \"Description\": \"CPU.1\", \"Drives\": [ { \"@odata.id\": \"/redfish/v1/Systems/System.Embedded.1/Storage/CPU.1/Drives/Disk.Bay.21:Enclosure.Internal.0-1\" }, { \"@odata.id\": \"/redfish/v1/Systems/System.Embedded.1/Storage/CPU.1/Drives/Disk.Bay.19:Enclosure.Internal.0-1\" }, { \"@odata.id\": \"/redfish/v1/Systems/System.Embedded.1/Storage/CPU.1/Drives/Disk.Bay.20:Enclosure.Internal.0-1\" }, { \"@odata.id\": \"/redfish/v1/Systems/System.Embedded.1/Storage/CPU.1/Drives/Disk.Bay.23:Enclosure.Internal.0-1\" }, { \"@odata.id\": \"/redfish/v1/Systems/System.Embedded.1/Storage/CPU.1/Drives/Disk.Bay.18:Enclosure.Internal.0-1\" }, { \"@odata.id\": \"/redfish/v1/Systems/System.Embedded.1/Storage/CPU.1/Drives/Disk.Bay.22:Enclosure.Internal.0-1\" } ], \"Drives@odata.count\": 6, \"Id\": \"CPU.1\", \"Links\": { \"Enclosures\": [ { \"@odata.id\": \"/redfish/v1/Chassis/Enclosure.Internal.0-1\" }, { \"@odata.id\": \"/redfish/v1/Chassis/System.Embedded.1\" } ], \"Enclosures@odata.count\": 2 }, \"Name\": \"CPU.1\", \"Status\": { \"Health\": \"OK\", \"HealthRollup\": \"OK\", \"State\": \"Enabled\" }, \"Volumes\": { \"@odata.id\": \"/redfish/v1/Systems/System.Embedded.1/Storage/CPU.1/Volumes\" } } From the above I can deduce that CPU 1 has six drives attached to it. Or does it? Understand the Behavior of Unqualified Drives Here is a picture of the front of my server: Here is the front of my server. You might say, \"Wait, there are 7 drives!?\" The problem is this 7th drive isn't qualified by Dell. It will still work just fine however, iDRAC won't know how to talk to it so it won't show up: You can confirm this is the case by checking the Storage->Physical Disks tab inside the iDRAC itself: Here you can see that I only have the 6 NVMe drives plus two SATA SSDs. While the iDRAC's personality module won't be able to properly sort the drive into Storage it will detect it as a PCIe device and accurately read the vendor information: Getting a Drive's Stats We can select one of them with /redfish/v1/Systems/System.Embedded.1/Storage/CPU.1/Drives/Disk.Bay.21:Enclosure.Internal.0-1 . This achieves the desired result and gets a dump of that drive's data. The size is available under the field CapacityBytes. { \"@odata.context\": \"/redfish/v1/$metadata#Drive.Drive\", \"@odata.id\": \"/redfish/v1/Systems/System.Embedded.1/Storage/CPU.1/Drives/Disk.Bay.21:Enclosure.Internal.0-1\", \"@odata.type\": \"#Drive.v1_9_0.Drive\", \"Actions\": { \"#Drive.SecureErase\": { \"@Redfish.OperationApplyTimeSupport\": { \"@odata.type\": \"#Settings.v1_3_0.OperationApplyTimeSupport\", \"SupportedValues\": [ \"Immediate\", \"OnReset\" ] }, \"target\": \"/redfish/v1/Systems/System.Embedded.1/Storage/CPU.1/Drives/Disk.Bay.21:Enclosure.Internal.0-1/Actions/Drive.SecureErase\" } }, \"Assembly\": { \"@odata.id\": \"/redfish/v1/Chassis/System.Embedded.1/Assembly\" }, \"BlockSizeBytes\": 0, \"CapableSpeedGbs\": 7.876923076923077, \"CapacityBytes\": 3200631791616, \"Description\": \"PCIe SSD in Slot 21 in Bay 1\", \"EncryptionAbility\": \"None\", \"EncryptionStatus\": \"Unencrypted\", \"FailurePredicted\": false, \"HotspareType\": \"None\", \"Id\": \"Disk.Bay.21:Enclosure.Internal.0-1\", \"Identifiers\": [ { \"DurableName\": null, \"DurableNameFormat\": null } ], \"Identifiers@odata.count\": 1, \"Links\": { \"Chassis\": { \"@odata.id\": \"/redfish/v1/Chassis/Enclosure.Internal.0-1\" }, \"PCIeFunctions\": [], \"PCIeFunctions@odata.count\": 0, \"Volumes\": [ { \"@odata.id\": \"/redfish/v1/Systems/System.Embedded.1/Storage/CPU.1/Volumes/Disk.Bay.21:Enclosure.Internal.0-1\" } ], \"Volumes@odata.count\": 1 }, \"Location\": [], \"Manufacturer\": \"Intel Corporation \", \"MediaType\": \"SSD\", \"Model\": \"Dell Express Flash NVMe P4610 3.2TB SFF\", \"Name\": \"PCIe SSD in Slot 21 in Bay 1\", \"NegotiatedSpeedGbs\": 7.876923076923077, \"Oem\": { \"Dell\": { \"@odata.type\": \"#DellOem.v1_1_0.DellOemResources\", \"DellDriveSMARTAttributes\": null, \"DellNVMeSMARTAttributes\": { \"@odata.id\": \"/redfish/v1/Systems/System.Embedded.1/Storage/CPU.1/Drives/Disk.Bay.21:Enclosure.Internal.0-1/Oem/Dell/DellNVMeSMARTAttributes\" }, \"DellPCIeSSD\": { \"@odata.context\": \"/redfish/v1/$metadata#DellPCIeSSD.DellPCIeSSD\", \"@odata.id\": \"/redfish/v1/Systems/System.Embedded.1/Storage/CPU.1/Drives/Disk.Bay.21:Enclosure.Internal.0-1/Oem/Dell/DellPCIeSSDs/Disk.Bay.21:Enclosure.Internal.0-1\", \"@odata.type\": \"#DellPCIeSSD.v1_4_0.DellPCIeSSD\", \"Bus\": \"CA\", \"BusProtocol\": \"PCIE\", \"Description\": \"An instance of DellPCIeSSD will have PCIe Solid State Drive specific data.\", \"Device\": \"0\", \"DeviceProtocol\": \"NVMe-MI1.0\", \"DriveFormFactor\": \"2.5Inch\", \"FreeSizeInBytes\": null, \"Function\": \"0\", \"HotSpareStatus\": null, \"Id\": \"Disk.Bay.21:Enclosure.Internal.0-1\", \"MediaType\": \"SolidStateDrive\", \"Name\": \"DellPCIeSSD\", \"PCIeCapableLinkWidth\": \"x4\", \"PCIeNegotiatedLinkWidth\": \"x4\", \"ProductID\": \"a54\", \"RAIDType\": \"Unknown\", \"RaidStatus\": null, \"Slot\": 21, \"SystemEraseCapability\": \"CryptographicErasePD\", \"UsedSizeInBytes\": 0 }, \"DellPhysicalDisk\": { \"@odata.context\": \"/redfish/v1/$metadata#DellPhysicalDisk.DellPhysicalDisk\", \"@odata.id\": \"/redfish/v1/Systems/System.Embedded.1/Storage/CPU.1/Drives/Disk.Bay.21:Enclosure.Internal.0-1/Oem/Dell/DellDrives/Disk.Bay.21:Enclosure.Internal.0-1\", \"@odata.type\": \"#DellPhysicalDisk.v1_3_0.DellPhysicalDisk\", \"Certified\": null, \"Connector\": null, \"Description\": \"An instance of DellPhysicalDisk will have Physical Disk specific data.\", \"DeviceProtocol\": \"NVMe-MI1.0\", \"DriveFormFactor\": \"2.5Inch\", \"EncryptionProtocol\": null, \"ForeignKeyIdentifier\": null, \"FreeSizeInBytes\": null, \"Id\": \"Disk.Bay.21:Enclosure.Internal.0-1\", \"LastSystemInventoryTime\": null, \"LastUpdateTime\": null, \"ManufacturingDay\": null, \"ManufacturingWeek\": null, \"ManufacturingYear\": null, \"Name\": \"DellPhysicalDisk\", \"NonRAIDDiskCachePolicy\": null, \"OperationName\": null, \"OperationPercentCompletePercent\": null, \"PCIeCapableLinkWidth\": \"x4\", \"PCIeNegotiatedLinkWidth\": \"x4\", \"PPID\": null, \"PowerStatus\": null, \"PredictiveFailureState\": null, \"ProductID\": \"a54\", \"RAIDType\": \"Unknown\", \"RaidStatus\": null, \"SASAddress\": null, \"Slot\": 21, \"SystemEraseCapability\": \"CryptographicErasePD\", \"T10PICapability\": null, \"UsedSizeInBytes\": 0, \"WWN\": null } } }, \"Operations\": [], \"Operations@odata.count\": 0, \"PartNumber\": \"TW02CN1TPIHIT9A9013TA02\", \"PhysicalLocation\": { \"PartLocation\": { \"LocationOrdinalValue\": 21, \"LocationType\": \"Slot\" } }, \"PredictedMediaLifeLeftPercent\": 100, \"Protocol\": \"PCIe\", \"Revision\": \"VDV1DP23\", \"RotationSpeedRPM\": null, \"SerialNumber\": \"PHLN9396002Q3P2BGN\", \"Status\": { \"Health\": \"OK\", \"HealthRollup\": \"OK\", \"State\": \"Enabled\" }, \"WriteCacheEnabled\": false }","title":"Get NVMe Drives from iDRAC Redfish"},{"location":"Get%20NVMe%20Drives%20from%20iDRAC%20Redfish/#get-nvme-drives-from-idrac-redfish","text":"Get NVMe Drives from iDRAC Redfish Exploring iDRAC Detected Storage Understand the Behavior of Unqualified Drives Getting a Drive's Stats","title":"Get NVMe Drives from iDRAC Redfish"},{"location":"Get%20NVMe%20Drives%20from%20iDRAC%20Redfish/#exploring-idrac-detected-storage","text":"I used the Storage API endpoint to accomplish this. From my host I received: {\"@odata.context\":\"/redfish/v1/$metadata#StorageCollection.StorageCollection\",\"@odata.id\":\"/redfish/v1/Systems/System.Embedded.1/Storage/\",\"@odata.type\":\"#StorageCollection.StorageCollection\",\"Description\":\"Collection Of Storage entities\",\"Members\":[{\"@odata.id\":\"/redfish/v1/Systems/System.Embedded.1/Storage/RAID.Slot.4-1\"},{\"@odata.id\":\"/redfish/v1/Systems/System.Embedded.1/Storage/RAID.Embedded.1-1\"},{\"@odata.id\":\"/redfish/v1/Systems/System.Embedded.1/Storage/CPU.1\"},{\"@odata.id\":\"/redfish/v1/Systems/System.Embedded.1/Storage/AHCI.Slot.2-1\"}],\"Members@odata.count\":4,\"Name\":\"Storage Collection\"} I'm running an R840 which is Dell 14G which I know does not have NVMe RAID controllers as an option so I know my NVMe drives must be hanging off the CPU. IE: /redfish/v1/Systems/System.Embedded.1/Storage/CPU.1 . I can expect that the BOSS card is hanging off of AHCI and that any SAS/SATA drives are likely on the RAID controller. The results above also imply that the host above runs a mixed backplane given the presence of RAID and CPU.1. Checking CPU.1 gets me: { \"@odata.context\": \"/redfish/v1/$metadata#Storage.Storage\", \"@odata.id\": \"/redfish/v1/Systems/System.Embedded.1/Storage/CPU.1\", \"@odata.type\": \"#Storage.v1_8_0.Storage\", \"Description\": \"CPU.1\", \"Drives\": [ { \"@odata.id\": \"/redfish/v1/Systems/System.Embedded.1/Storage/CPU.1/Drives/Disk.Bay.21:Enclosure.Internal.0-1\" }, { \"@odata.id\": \"/redfish/v1/Systems/System.Embedded.1/Storage/CPU.1/Drives/Disk.Bay.19:Enclosure.Internal.0-1\" }, { \"@odata.id\": \"/redfish/v1/Systems/System.Embedded.1/Storage/CPU.1/Drives/Disk.Bay.20:Enclosure.Internal.0-1\" }, { \"@odata.id\": \"/redfish/v1/Systems/System.Embedded.1/Storage/CPU.1/Drives/Disk.Bay.23:Enclosure.Internal.0-1\" }, { \"@odata.id\": \"/redfish/v1/Systems/System.Embedded.1/Storage/CPU.1/Drives/Disk.Bay.18:Enclosure.Internal.0-1\" }, { \"@odata.id\": \"/redfish/v1/Systems/System.Embedded.1/Storage/CPU.1/Drives/Disk.Bay.22:Enclosure.Internal.0-1\" } ], \"Drives@odata.count\": 6, \"Id\": \"CPU.1\", \"Links\": { \"Enclosures\": [ { \"@odata.id\": \"/redfish/v1/Chassis/Enclosure.Internal.0-1\" }, { \"@odata.id\": \"/redfish/v1/Chassis/System.Embedded.1\" } ], \"Enclosures@odata.count\": 2 }, \"Name\": \"CPU.1\", \"Status\": { \"Health\": \"OK\", \"HealthRollup\": \"OK\", \"State\": \"Enabled\" }, \"Volumes\": { \"@odata.id\": \"/redfish/v1/Systems/System.Embedded.1/Storage/CPU.1/Volumes\" } } From the above I can deduce that CPU 1 has six drives attached to it. Or does it?","title":"Exploring iDRAC Detected Storage"},{"location":"Get%20NVMe%20Drives%20from%20iDRAC%20Redfish/#understand-the-behavior-of-unqualified-drives","text":"Here is a picture of the front of my server: Here is the front of my server. You might say, \"Wait, there are 7 drives!?\" The problem is this 7th drive isn't qualified by Dell. It will still work just fine however, iDRAC won't know how to talk to it so it won't show up: You can confirm this is the case by checking the Storage->Physical Disks tab inside the iDRAC itself: Here you can see that I only have the 6 NVMe drives plus two SATA SSDs. While the iDRAC's personality module won't be able to properly sort the drive into Storage it will detect it as a PCIe device and accurately read the vendor information:","title":"Understand the Behavior of Unqualified Drives"},{"location":"Get%20NVMe%20Drives%20from%20iDRAC%20Redfish/#getting-a-drives-stats","text":"We can select one of them with /redfish/v1/Systems/System.Embedded.1/Storage/CPU.1/Drives/Disk.Bay.21:Enclosure.Internal.0-1 . This achieves the desired result and gets a dump of that drive's data. The size is available under the field CapacityBytes. { \"@odata.context\": \"/redfish/v1/$metadata#Drive.Drive\", \"@odata.id\": \"/redfish/v1/Systems/System.Embedded.1/Storage/CPU.1/Drives/Disk.Bay.21:Enclosure.Internal.0-1\", \"@odata.type\": \"#Drive.v1_9_0.Drive\", \"Actions\": { \"#Drive.SecureErase\": { \"@Redfish.OperationApplyTimeSupport\": { \"@odata.type\": \"#Settings.v1_3_0.OperationApplyTimeSupport\", \"SupportedValues\": [ \"Immediate\", \"OnReset\" ] }, \"target\": \"/redfish/v1/Systems/System.Embedded.1/Storage/CPU.1/Drives/Disk.Bay.21:Enclosure.Internal.0-1/Actions/Drive.SecureErase\" } }, \"Assembly\": { \"@odata.id\": \"/redfish/v1/Chassis/System.Embedded.1/Assembly\" }, \"BlockSizeBytes\": 0, \"CapableSpeedGbs\": 7.876923076923077, \"CapacityBytes\": 3200631791616, \"Description\": \"PCIe SSD in Slot 21 in Bay 1\", \"EncryptionAbility\": \"None\", \"EncryptionStatus\": \"Unencrypted\", \"FailurePredicted\": false, \"HotspareType\": \"None\", \"Id\": \"Disk.Bay.21:Enclosure.Internal.0-1\", \"Identifiers\": [ { \"DurableName\": null, \"DurableNameFormat\": null } ], \"Identifiers@odata.count\": 1, \"Links\": { \"Chassis\": { \"@odata.id\": \"/redfish/v1/Chassis/Enclosure.Internal.0-1\" }, \"PCIeFunctions\": [], \"PCIeFunctions@odata.count\": 0, \"Volumes\": [ { \"@odata.id\": \"/redfish/v1/Systems/System.Embedded.1/Storage/CPU.1/Volumes/Disk.Bay.21:Enclosure.Internal.0-1\" } ], \"Volumes@odata.count\": 1 }, \"Location\": [], \"Manufacturer\": \"Intel Corporation \", \"MediaType\": \"SSD\", \"Model\": \"Dell Express Flash NVMe P4610 3.2TB SFF\", \"Name\": \"PCIe SSD in Slot 21 in Bay 1\", \"NegotiatedSpeedGbs\": 7.876923076923077, \"Oem\": { \"Dell\": { \"@odata.type\": \"#DellOem.v1_1_0.DellOemResources\", \"DellDriveSMARTAttributes\": null, \"DellNVMeSMARTAttributes\": { \"@odata.id\": \"/redfish/v1/Systems/System.Embedded.1/Storage/CPU.1/Drives/Disk.Bay.21:Enclosure.Internal.0-1/Oem/Dell/DellNVMeSMARTAttributes\" }, \"DellPCIeSSD\": { \"@odata.context\": \"/redfish/v1/$metadata#DellPCIeSSD.DellPCIeSSD\", \"@odata.id\": \"/redfish/v1/Systems/System.Embedded.1/Storage/CPU.1/Drives/Disk.Bay.21:Enclosure.Internal.0-1/Oem/Dell/DellPCIeSSDs/Disk.Bay.21:Enclosure.Internal.0-1\", \"@odata.type\": \"#DellPCIeSSD.v1_4_0.DellPCIeSSD\", \"Bus\": \"CA\", \"BusProtocol\": \"PCIE\", \"Description\": \"An instance of DellPCIeSSD will have PCIe Solid State Drive specific data.\", \"Device\": \"0\", \"DeviceProtocol\": \"NVMe-MI1.0\", \"DriveFormFactor\": \"2.5Inch\", \"FreeSizeInBytes\": null, \"Function\": \"0\", \"HotSpareStatus\": null, \"Id\": \"Disk.Bay.21:Enclosure.Internal.0-1\", \"MediaType\": \"SolidStateDrive\", \"Name\": \"DellPCIeSSD\", \"PCIeCapableLinkWidth\": \"x4\", \"PCIeNegotiatedLinkWidth\": \"x4\", \"ProductID\": \"a54\", \"RAIDType\": \"Unknown\", \"RaidStatus\": null, \"Slot\": 21, \"SystemEraseCapability\": \"CryptographicErasePD\", \"UsedSizeInBytes\": 0 }, \"DellPhysicalDisk\": { \"@odata.context\": \"/redfish/v1/$metadata#DellPhysicalDisk.DellPhysicalDisk\", \"@odata.id\": \"/redfish/v1/Systems/System.Embedded.1/Storage/CPU.1/Drives/Disk.Bay.21:Enclosure.Internal.0-1/Oem/Dell/DellDrives/Disk.Bay.21:Enclosure.Internal.0-1\", \"@odata.type\": \"#DellPhysicalDisk.v1_3_0.DellPhysicalDisk\", \"Certified\": null, \"Connector\": null, \"Description\": \"An instance of DellPhysicalDisk will have Physical Disk specific data.\", \"DeviceProtocol\": \"NVMe-MI1.0\", \"DriveFormFactor\": \"2.5Inch\", \"EncryptionProtocol\": null, \"ForeignKeyIdentifier\": null, \"FreeSizeInBytes\": null, \"Id\": \"Disk.Bay.21:Enclosure.Internal.0-1\", \"LastSystemInventoryTime\": null, \"LastUpdateTime\": null, \"ManufacturingDay\": null, \"ManufacturingWeek\": null, \"ManufacturingYear\": null, \"Name\": \"DellPhysicalDisk\", \"NonRAIDDiskCachePolicy\": null, \"OperationName\": null, \"OperationPercentCompletePercent\": null, \"PCIeCapableLinkWidth\": \"x4\", \"PCIeNegotiatedLinkWidth\": \"x4\", \"PPID\": null, \"PowerStatus\": null, \"PredictiveFailureState\": null, \"ProductID\": \"a54\", \"RAIDType\": \"Unknown\", \"RaidStatus\": null, \"SASAddress\": null, \"Slot\": 21, \"SystemEraseCapability\": \"CryptographicErasePD\", \"T10PICapability\": null, \"UsedSizeInBytes\": 0, \"WWN\": null } } }, \"Operations\": [], \"Operations@odata.count\": 0, \"PartNumber\": \"TW02CN1TPIHIT9A9013TA02\", \"PhysicalLocation\": { \"PartLocation\": { \"LocationOrdinalValue\": 21, \"LocationType\": \"Slot\" } }, \"PredictedMediaLifeLeftPercent\": 100, \"Protocol\": \"PCIe\", \"Revision\": \"VDV1DP23\", \"RotationSpeedRPM\": null, \"SerialNumber\": \"PHLN9396002Q3P2BGN\", \"Status\": { \"Health\": \"OK\", \"HealthRollup\": \"OK\", \"State\": \"Enabled\" }, \"WriteCacheEnabled\": false }","title":"Getting a Drive's Stats"},{"location":"High%20Speed%20Packet%20Capture/","text":"High Speed Packet Capture See https://github.com/grantcurell/packet_capture For testing notes see: DPDK on ESXi with CentOS 7 (Incomplete) DPDK on FC640 with RHEL 8 DPDK on R940 with Napatech ntop ntop on R940 with Napatech","title":"High Speed Packet Capture"},{"location":"High%20Speed%20Packet%20Capture/#high-speed-packet-capture","text":"See https://github.com/grantcurell/packet_capture For testing notes see: DPDK on ESXi with CentOS 7 (Incomplete) DPDK on FC640 with RHEL 8 DPDK on R940 with Napatech ntop ntop on R940 with Napatech","title":"High Speed Packet Capture"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20ESXi%20with%20CentOS%207%20%28INCOMPLETE%29/","text":"How to Get DPDK with pdump Running The purpose of this experiment is to get Intel's DPDK framework up and running on a virtual machine. Useful Materials VMWare info on Intel DPDK How to Compile DPDK Info on Linux Drivers for DPDK My Environment I am running this inital test on ESXi 6.7 in a virtual machine CentOS Release Info NAME=\"CentOS Linux\" VERSION=\"7 (Core)\" ID=\"centos\" ID_LIKE=\"rhel fedora\" VERSION_ID=\"7\" PRETTY_NAME=\"CentOS Linux 7 (Core)\" ANSI_COLOR=\"0;31\" CPE_NAME=\"cpe:/o:centos:centos:7\" HOME_URL=\"https://www.centos.org/\" BUG_REPORT_URL=\"https://bugs.centos.org/\" CENTOS_MANTISBT_PROJECT=\"CentOS-7\" CENTOS_MANTISBT_PROJECT_VERSION=\"7\" REDHAT_SUPPORT_PRODUCT=\"centos\" REDHAT_SUPPORT_PRODUCT_VERSION=\"7\" CentOS Linux release 7.7.1908 (Core) CentOS Linux release 7.7.1908 (Core) Kernel Info Linux centos.lan 3.10.0-1062.4.1.el7.x86_64 #1 SMP Fri Oct 18 17:15:30 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux Installation Configure GRUB Command Line for Virtualized DPDK In order for DPDK to work in a virtual environment you must disable memory protection. The reason for this is that with memory protection enabled CentOS will block read/write/execution to the DMA'd portion of memory. See this post. Do the following: cd /etc/default vim grub Edit GRUB-CMDLINE and Add \u201cnopku\u201d GRUB_CMDLINE_LINUX=\"crashkernel=auto rd.lvm.lv=centos/root rd.lvm.lv=centos/swap rhgb quiet nopku transparent_hugepage=never log_buf_len=8M\" Recompile grub: sudo grub2-mkconfig -o /boot/grub2/grub.cfg reboot Install DPDK Download from https://core.dpdk.org/download/ Run yum install -y gcc numactl-devel kernel-devel pciutils elfutils-libelf-devel make libpcap python3 tar vim wget tmux vim mlocate hwloc libpcap-devel Extract dpdk and cd into its directory set the environment variable RTE_SDK. It is the directory in which you extracted all the DPDK files. export RTE_SDK=<YOUR_DIR> Run make install T=x86_64-native-linux-gcc DESTDIR=<INSTALL_DIR> to build dpdk. Ensure your install directory exists. Once an DPDK target environment directory has been created (such as x86_64-native-linux-gcc), it contains all libraries and header files required to build an application. When compiling an application in the Linux* environment on the DPDK, the following variables must be exported: RTE_TARGET - Points to the DPDK target environment directory. export RTE_TARGET=/opt/dpdk-19.08/x86_64-native-linux-gcc You may want to add this variable and RTE_SDK to ~/.bash_profile Configuration Update Ulimits Edit the security limits with vim /etc/security/limits.conf Add the following lines at the end of the file: root hard memlock unlimited root soft memlock unlimited Reboot and see if the system has the newly updated value Configure vfio-pci to Load on Boot Go to /etc/modules-load.d/ cd /etc/modules-load.d Run echo vfio-pci > vfio-pci.conf If you don't reboot you will need to run modprobe vfio-pci Configure Ports Move to your INSTALL_DIR and run ./usertools/dpdk-setup.sh . This should give you a menu with all available DPDK options. The menu is setup in such a way that you must perform each step listed in the menu. If things have gone correctly to this point your Step 1 should look like the following: ---------------------------------------------------------- Step 1: Select the DPDK environment to build ---------------------------------------------------------- [1] * My menu looks like this: ---------------------------------------------------------- Step 1: Select the DPDK environment to build ---------------------------------------------------------- [1] * ---------------------------------------------------------- Step 2: Setup linux environment ---------------------------------------------------------- [2] Insert IGB UIO module [3] Insert VFIO module [4] Insert KNI module [5] Setup hugepage mappings for non-NUMA systems [6] Setup hugepage mappings for NUMA systems [7] Display current Ethernet/Baseband/Crypto device settings [8] Bind Ethernet/Baseband/Crypto device to IGB UIO module [9] Bind Ethernet/Baseband/Crypto device to VFIO module [10] Setup VFIO permissions ---------------------------------------------------------- Step 3: Run test application for linux environment ---------------------------------------------------------- [11] Run test application ($RTE_TARGET/app/test) [12] Run testpmd application in interactive mode ($RTE_TARGET/app/testpmd) ---------------------------------------------------------- Step 4: Other tools ---------------------------------------------------------- [13] List hugepage info from /proc/meminfo ---------------------------------------------------------- Step 5: Uninstall and system cleanup ---------------------------------------------------------- [14] Unbind devices from IGB UIO or VFIO driver [15] Remove IGB UIO module [16] Remove VFIO module [17] Remove KNI module [18] Remove hugepage mappings [19] Exit Script WARNING : If you run Option 3 to insert the VFIO module I found that it actually caused DPDK to stop working. Next run option 6 to instert huge pages for NUMA systems. Notice you will be prompted to select an amount of memory on a per processor basis. This is because there are pages associated with each individual processor to increase performance via locality. Run option 7 and make sure you receive output and that network devices are listed. My output looks like this: Network devices using kernel driver =================================== 0000:0b:00.0 'VMXNET3 Ethernet Controller 07b0' if=ens192 drv=vmxnet3 unused=vfio-pci *Active* 0000:13:00.0 'VMXNET3 Ethernet Controller 07b0' if=ens224 drv=vmxnet3 unused=vfio-pci *Active* No 'Baseband' devices detected ============================== No 'Crypto' devices detected ============================ No 'Eventdev' devices detected ============================== No 'Mempool' devices detected ============================= No 'Compress' devices detected ============================== No 'Misc (rawdev)' devices detected =================================== NOTE : The active keyword means that DPDK thinks the interface is under active use. This typically means it has an IP address assigned to it. Run option 9 to bind an interface to DPDK Notes You can use PCI passthrough on the x520 and x710 Run ethtool -i <your_interface> to figure out what kind of driver you have Run bash /opt/dpdk-19.08/usertools/dpdk-setup.sh Run option 47 and then enter 64 when prompted Run option 44 to insert the VRIO module Run ulimit -u unlimited to increase the memlock limit (NOTE: I don't think this did what I needed it to.) add iommu=pt intel_iommu=on grub2-mkconfig -o /boot/grub2/grub.cfg","title":"How to Get DPDK with pdump Running"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20ESXi%20with%20CentOS%207%20%28INCOMPLETE%29/#how-to-get-dpdk-with-pdump-running","text":"The purpose of this experiment is to get Intel's DPDK framework up and running on a virtual machine.","title":"How to Get DPDK with pdump Running"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20ESXi%20with%20CentOS%207%20%28INCOMPLETE%29/#useful-materials","text":"VMWare info on Intel DPDK How to Compile DPDK Info on Linux Drivers for DPDK","title":"Useful Materials"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20ESXi%20with%20CentOS%207%20%28INCOMPLETE%29/#my-environment","text":"I am running this inital test on ESXi 6.7 in a virtual machine","title":"My Environment"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20ESXi%20with%20CentOS%207%20%28INCOMPLETE%29/#centos-release-info","text":"NAME=\"CentOS Linux\" VERSION=\"7 (Core)\" ID=\"centos\" ID_LIKE=\"rhel fedora\" VERSION_ID=\"7\" PRETTY_NAME=\"CentOS Linux 7 (Core)\" ANSI_COLOR=\"0;31\" CPE_NAME=\"cpe:/o:centos:centos:7\" HOME_URL=\"https://www.centos.org/\" BUG_REPORT_URL=\"https://bugs.centos.org/\" CENTOS_MANTISBT_PROJECT=\"CentOS-7\" CENTOS_MANTISBT_PROJECT_VERSION=\"7\" REDHAT_SUPPORT_PRODUCT=\"centos\" REDHAT_SUPPORT_PRODUCT_VERSION=\"7\" CentOS Linux release 7.7.1908 (Core) CentOS Linux release 7.7.1908 (Core)","title":"CentOS Release Info"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20ESXi%20with%20CentOS%207%20%28INCOMPLETE%29/#kernel-info","text":"Linux centos.lan 3.10.0-1062.4.1.el7.x86_64 #1 SMP Fri Oct 18 17:15:30 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux","title":"Kernel Info"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20ESXi%20with%20CentOS%207%20%28INCOMPLETE%29/#installation","text":"","title":"Installation"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20ESXi%20with%20CentOS%207%20%28INCOMPLETE%29/#configure-grub-command-line-for-virtualized-dpdk","text":"In order for DPDK to work in a virtual environment you must disable memory protection. The reason for this is that with memory protection enabled CentOS will block read/write/execution to the DMA'd portion of memory. See this post. Do the following: cd /etc/default vim grub Edit GRUB-CMDLINE and Add \u201cnopku\u201d GRUB_CMDLINE_LINUX=\"crashkernel=auto rd.lvm.lv=centos/root rd.lvm.lv=centos/swap rhgb quiet nopku transparent_hugepage=never log_buf_len=8M\" Recompile grub: sudo grub2-mkconfig -o /boot/grub2/grub.cfg reboot","title":"Configure GRUB Command Line for Virtualized DPDK"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20ESXi%20with%20CentOS%207%20%28INCOMPLETE%29/#install-dpdk","text":"Download from https://core.dpdk.org/download/ Run yum install -y gcc numactl-devel kernel-devel pciutils elfutils-libelf-devel make libpcap python3 tar vim wget tmux vim mlocate hwloc libpcap-devel Extract dpdk and cd into its directory set the environment variable RTE_SDK. It is the directory in which you extracted all the DPDK files. export RTE_SDK=<YOUR_DIR> Run make install T=x86_64-native-linux-gcc DESTDIR=<INSTALL_DIR> to build dpdk. Ensure your install directory exists. Once an DPDK target environment directory has been created (such as x86_64-native-linux-gcc), it contains all libraries and header files required to build an application. When compiling an application in the Linux* environment on the DPDK, the following variables must be exported: RTE_TARGET - Points to the DPDK target environment directory. export RTE_TARGET=/opt/dpdk-19.08/x86_64-native-linux-gcc You may want to add this variable and RTE_SDK to ~/.bash_profile","title":"Install DPDK"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20ESXi%20with%20CentOS%207%20%28INCOMPLETE%29/#configuration","text":"","title":"Configuration"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20ESXi%20with%20CentOS%207%20%28INCOMPLETE%29/#update-ulimits","text":"Edit the security limits with vim /etc/security/limits.conf Add the following lines at the end of the file: root hard memlock unlimited root soft memlock unlimited Reboot and see if the system has the newly updated value","title":"Update Ulimits"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20ESXi%20with%20CentOS%207%20%28INCOMPLETE%29/#configure-vfio-pci-to-load-on-boot","text":"Go to /etc/modules-load.d/ cd /etc/modules-load.d Run echo vfio-pci > vfio-pci.conf If you don't reboot you will need to run modprobe vfio-pci","title":"Configure vfio-pci to Load on Boot"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20ESXi%20with%20CentOS%207%20%28INCOMPLETE%29/#configure-ports","text":"Move to your INSTALL_DIR and run ./usertools/dpdk-setup.sh . This should give you a menu with all available DPDK options. The menu is setup in such a way that you must perform each step listed in the menu. If things have gone correctly to this point your Step 1 should look like the following: ---------------------------------------------------------- Step 1: Select the DPDK environment to build ---------------------------------------------------------- [1] * My menu looks like this: ---------------------------------------------------------- Step 1: Select the DPDK environment to build ---------------------------------------------------------- [1] * ---------------------------------------------------------- Step 2: Setup linux environment ---------------------------------------------------------- [2] Insert IGB UIO module [3] Insert VFIO module [4] Insert KNI module [5] Setup hugepage mappings for non-NUMA systems [6] Setup hugepage mappings for NUMA systems [7] Display current Ethernet/Baseband/Crypto device settings [8] Bind Ethernet/Baseband/Crypto device to IGB UIO module [9] Bind Ethernet/Baseband/Crypto device to VFIO module [10] Setup VFIO permissions ---------------------------------------------------------- Step 3: Run test application for linux environment ---------------------------------------------------------- [11] Run test application ($RTE_TARGET/app/test) [12] Run testpmd application in interactive mode ($RTE_TARGET/app/testpmd) ---------------------------------------------------------- Step 4: Other tools ---------------------------------------------------------- [13] List hugepage info from /proc/meminfo ---------------------------------------------------------- Step 5: Uninstall and system cleanup ---------------------------------------------------------- [14] Unbind devices from IGB UIO or VFIO driver [15] Remove IGB UIO module [16] Remove VFIO module [17] Remove KNI module [18] Remove hugepage mappings [19] Exit Script WARNING : If you run Option 3 to insert the VFIO module I found that it actually caused DPDK to stop working. Next run option 6 to instert huge pages for NUMA systems. Notice you will be prompted to select an amount of memory on a per processor basis. This is because there are pages associated with each individual processor to increase performance via locality. Run option 7 and make sure you receive output and that network devices are listed. My output looks like this: Network devices using kernel driver =================================== 0000:0b:00.0 'VMXNET3 Ethernet Controller 07b0' if=ens192 drv=vmxnet3 unused=vfio-pci *Active* 0000:13:00.0 'VMXNET3 Ethernet Controller 07b0' if=ens224 drv=vmxnet3 unused=vfio-pci *Active* No 'Baseband' devices detected ============================== No 'Crypto' devices detected ============================ No 'Eventdev' devices detected ============================== No 'Mempool' devices detected ============================= No 'Compress' devices detected ============================== No 'Misc (rawdev)' devices detected =================================== NOTE : The active keyword means that DPDK thinks the interface is under active use. This typically means it has an IP address assigned to it. Run option 9 to bind an interface to DPDK","title":"Configure Ports"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20ESXi%20with%20CentOS%207%20%28INCOMPLETE%29/#notes","text":"You can use PCI passthrough on the x520 and x710 Run ethtool -i <your_interface> to figure out what kind of driver you have Run bash /opt/dpdk-19.08/usertools/dpdk-setup.sh Run option 47 and then enter 64 when prompted Run option 44 to insert the VRIO module Run ulimit -u unlimited to increase the memlock limit (NOTE: I don't think this did what I needed it to.) add iommu=pt intel_iommu=on grub2-mkconfig -o /boot/grub2/grub.cfg","title":"Notes"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20FC640%20with%20RHEL%208/","text":"How to Get DPDK with pdump Running The purpose of this experiment is to get Intel's DPDK framework up and running on a server. Useful Materials How to Compile DPDK Info on Linux Drivers for DPDK Description of the TestPMD Program DPDK Testpmd Application User Guide My Environment I am running a Dell FC640 on a TFX2HE chassis. Red Hat Release Info NAME=\"Red Hat Enterprise Linux\" VERSION=\"8.1 (Ootpa)\" ID=\"rhel\" ID_LIKE=\"fedora\" VERSION_ID=\"8.1\" PLATFORM_ID=\"platform:el8\" PRETTY_NAME=\"Red Hat Enterprise Linux 8.1 (Ootpa)\" ANSI_COLOR=\"0;31\" CPE_NAME=\"cpe:/o:redhat:enterprise_linux:8.1:GA\" HOME_URL=\"https://www.redhat.com/\" BUG_REPORT_URL=\"https://bugzilla.redhat.com/\" REDHAT_BUGZILLA_PRODUCT=\"Red Hat Enterprise Linux 8\" REDHAT_BUGZILLA_PRODUCT_VERSION=8.1 REDHAT_SUPPORT_PRODUCT=\"Red Hat Enterprise Linux\" REDHAT_SUPPORT_PRODUCT_VERSION=\"8.1\" Red Hat Enterprise Linux release 8.1 (Ootpa) Red Hat Enterprise Linux release 8.1 (Ootpa) Kernel Info Linux dpdkdemo.lan 4.18.0-147.el8.x86_64 #1 SMP Thu Sep 26 15:52:44 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux Physical Setup I ran the traffic generator to port 5 of the passthrough module which maps to port 1 my internal x710 card. Installation Enable Red Hat Repos Run subscription-manager list --available | less and find the subscription which provides CodeReady for x86. Note the pool number associated with the subscription. Run subscription-manager attach --pool=<POOL_NUMBER> to enable the subscription. Then run: subscription-manager repos --enable=codeready-builder-for-rhel-8-x86_64-rpms to enable the repo. Install DPDK Download from https://core.dpdk.org/download/ Edit the security limits with vim /etc/security/limits.conf 1.Add the following lines at the end of the file. This assumes you are running as root: root hard memlock unlimited root soft memlock unlimited Make sure your kernel is up to date with dnf update -y && reboot . Run dnf install -y gcc numactl-devel kernel-devel pciutils elfutils-libelf-devel make libpcap python3 tar vim wget tmux vim mlocate hwloc Extract dpdk and cd into its directory set the environment variable RTE_SDK. It is the directory in which you extracted all the DPDK files. export RTE_SDK=<YOUR_DIR> Run make install T=x86_64-native-linux-gcc CONFIG_RTE_LIBRTE_PMD_PCAP=y CONFIG_RTE_LIBRTE_PDUMP=y DESTDIR=<INSTALL_DIR> to build dpdk. Ensure your install directory exists. NOTE : The option CONFIG_RTE_LIBRTE_PMD_PCAP=y enabled libpcap support in DPDK. This is required for pdump to work. Once an DPDK target environment directory has been created (such as x86_64-native-linux-gcc), it contains all libraries and header files required to build an application. When compiling an application in the Linux* environment on the DPDK, the following variables must be exported: RTE_TARGET - Points to the DPDK target environment directory. export RTE_TARGET=/opt/dpdk-19.08/x86_64-native-linux-gcc You may want to add this variable and RTE_SDK to ~/.bash_profile Install ELF Tools Run the following: dnf install -y python36-devel pip3 install numpy pip3 install elftools pip3 install pyelftools Configuration TODO: Need to update the instructions with this. Why can't the setup tool find it? I had to run modprobe uio && insmod /opt/dpdk-19.08/install/lib/modules/4.18.0-147.el8.x86_64/extra/dpdk/igb_uio.ko to get this to load. The modprobe uio is necessary because the uio module is a depency of igb_uio. Configure vfio-pci to Load on Boot (TODO REMOVE) Go to /etc/modules-load.d/ cd /etc/modules-load.d Run echo vfio-pci > vfio-pci.conf If you don't reboot you will need to run modprobe vfio-pci Configure Ports Move to your dpdk dir and run ./install/share/dpdk/usertools/dpdk-setup.sh . This should give you a menu with all available DPDK options. The menu is setup in such a way that you must perform each step listed in the menu. If things have gone correctly to this point your Step 1 should look like the following: ---------------------------------------------------------- Step 1: Select the DPDK environment to build ---------------------------------------------------------- [1] * My menu looks like this: ---------------------------------------------------------- Step 1: Select the DPDK environment to build ---------------------------------------------------------- [1] * ---------------------------------------------------------- Step 2: Setup linux environment ---------------------------------------------------------- [2] Insert IGB UIO module [3] Insert VFIO module [4] Insert KNI module [5] Setup hugepage mappings for non-NUMA systems [6] Setup hugepage mappings for NUMA systems [7] Display current Ethernet/Baseband/Crypto device settings [8] Bind Ethernet/Baseband/Crypto device to IGB UIO module [9] Bind Ethernet/Baseband/Crypto device to VFIO module [10] Setup VFIO permissions ---------------------------------------------------------- Step 3: Run test application for linux environment ---------------------------------------------------------- [11] Run test application ($RTE_TARGET/app/test) [12] Run testpmd application in interactive mode ($RTE_TARGET/app/testpmd) ---------------------------------------------------------- Step 4: Other tools ---------------------------------------------------------- [13] List hugepage info from /proc/meminfo ---------------------------------------------------------- Step 5: Uninstall and system cleanup ---------------------------------------------------------- [14] Unbind devices from IGB UIO or VFIO driver [15] Remove IGB UIO module [16] Remove VFIO module [17] Remove KNI module [18] Remove hugepage mappings [19] Exit Script Next run option 6 to instert huge pages for NUMA systems. Notice you will be prompted to select an amount of memory on a per processor basis. This is because there are pages associated with each individual processor to increase performance via locality. Run option 7 and make sure you receive output and that network devices are listed. My output looks like this: Network devices using kernel driver =================================== 0000:0b:00.0 'VMXNET3 Ethernet Controller 07b0' if=ens192 drv=vmxnet3 unused=vfio-pci *Active* 0000:13:00.0 'VMXNET3 Ethernet Controller 07b0' if=ens224 drv=vmxnet3 unused=vfio-pci *Active* No 'Baseband' devices detected ============================== No 'Crypto' devices detected ============================ No 'Eventdev' devices detected ============================== No 'Mempool' devices detected ============================= No 'Compress' devices detected ============================== No 'Misc (rawdev)' devices detected =================================== NOTE : The active keyword means that DPDK thinks the interface is under active use. This means that the interface has routes installed in the routing table. Run option 8 to bind an interface to DPDK using the IGB UIO driver. Performing Packet Capture Initial Setup Get the core layout with ./install/share/dpdk/usertools/cpu_layout.py View your port layout with ./install/share/dpdk/usertools/dpdk-devbind.py -s If you haven't already load the right kernel modules with: modprobe uio && insmod /opt/dpdk-19.08/install/lib/modules/4.18.0-147.el8.x86_64/extra/dpdk/igb_uio.ko Starting testpmd NOTE : The -- separates the argumentsn for the EAL vs TestPMD. ./install/bin/testpmd -l 4,8,10,12 -n 4 -- -i --forward-mode=rxonly After testpmd has started don't forget to run the start command on the testpmd command line. pdump ./install/bin/dpdk-pdump -- --pdump 'port=0,queue=*,rx-dev=/tmp/capture.pcap' Helpful Tips Getting CPU Info DPDK provides a tool for seeing the CPU layout with ./install/share/dpdk/usertools/cpu_layout.py You can see the logical layout of the cores with cat /proc/cpuinfo You can alse run lstopo-no-graphics Notice that the cores alternate back and forth. Process Types in DPDK DPDK runs two different types of processes. There are as follows: primary processes, which can initialize and which have full permissions on shared memory secondary processes, which cannot initialize shared memory, but can attach to pre- initialized shared memory and create objects in it. Standalone DPDK processes are primary processes, while secondary processes can only run alongside a primary process or after a primary process has already configured the hugepage shared memory for them. TestPMD The test pmd manual is available here Interactive Commands Starting transmit start Get Port Info show port info all Forwarding Modes TestPMD has different forwarding modes that can be used within the application. Input/output mode: This mode is generally referred to as IO mode. It is the most common forwarding mode and is the default mode when TestPMD is started. In IO mode a CPU core receives packets from one port (Rx) and transmits them to another port (Tx). The same port can be used for reception and transmission if required. Rx-only mode: In this mode the application polls packets from the Rx ports and frees them without transmitting them. In this way it acts as a packet sink. Tx-only mode: In this mode the application generates 64-byte IP packets and transmits them from the Tx ports. It doesn\u2019t handle the reception of packets and as such acts as a packet source. These latter two modes (Rx-only and Tx-only) are useful for checking packet reception and transmission separately. Apart from these three modes there are other forwarding modes that are explained in the TestPMD documentation . Port Topology Modes In paired mode, the forwarding is between pairs of ports, for example: (0,1), (2,3), (4,5). In chained mode, the forwarding is to the next available port in the port mask, for example: (0,1), (1,2), (2,0). The ordering of the ports can be changed using the portlist testpmd runtime function. In loop mode, ingress traffic is simply transmitted back on the same interface. Receive Side Scaling =Receive-Side Scaling (RSS), also known as multi-queue receive, distributes network receive processing across several hardware-based receive queues, allowing inbound network traffic to be processed by multiple CPUs. RSS can be used to relieve bottlenecks in receive interrupt processing caused by overloading a single CPU, and to reduce network latency. To determine whether your network interface card supports RSS, check whether multiple interrupt request queues are associated with the interface in /proc/interrupts. For example, if you are interested in the p1p1 interface: # egrep 'CPU|p1p1' /proc/interrupts CPU0 CPU1 CPU2 CPU3 CPU4 CPU5 89: 40187 0 0 0 0 0 IR-PCI-MSI-edge p1p1-0 90: 0 790 0 0 0 0 IR-PCI-MSI-edge p1p1-1 91: 0 0 959 0 0 0 IR-PCI-MSI-edge p1p1-2 92: 0 0 0 3310 0 0 IR-PCI-MSI-edge p1p1-3 93: 0 0 0 0 622 0 IR-PCI-MSI-edge p1p1-4 94: 0 0 0 0 0 2475 IR-PCI-MSI-edge p1p1-5 huge pages https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/performance_tuning_guide/sect-red_hat_enterprise_linux-performance_tuning_guide-memory-configuring-huge-pages","title":"How to Get DPDK with pdump Running"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20FC640%20with%20RHEL%208/#how-to-get-dpdk-with-pdump-running","text":"The purpose of this experiment is to get Intel's DPDK framework up and running on a server.","title":"How to Get DPDK with pdump Running"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20FC640%20with%20RHEL%208/#useful-materials","text":"How to Compile DPDK Info on Linux Drivers for DPDK Description of the TestPMD Program DPDK Testpmd Application User Guide","title":"Useful Materials"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20FC640%20with%20RHEL%208/#my-environment","text":"I am running a Dell FC640 on a TFX2HE chassis.","title":"My Environment"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20FC640%20with%20RHEL%208/#red-hat-release-info","text":"NAME=\"Red Hat Enterprise Linux\" VERSION=\"8.1 (Ootpa)\" ID=\"rhel\" ID_LIKE=\"fedora\" VERSION_ID=\"8.1\" PLATFORM_ID=\"platform:el8\" PRETTY_NAME=\"Red Hat Enterprise Linux 8.1 (Ootpa)\" ANSI_COLOR=\"0;31\" CPE_NAME=\"cpe:/o:redhat:enterprise_linux:8.1:GA\" HOME_URL=\"https://www.redhat.com/\" BUG_REPORT_URL=\"https://bugzilla.redhat.com/\" REDHAT_BUGZILLA_PRODUCT=\"Red Hat Enterprise Linux 8\" REDHAT_BUGZILLA_PRODUCT_VERSION=8.1 REDHAT_SUPPORT_PRODUCT=\"Red Hat Enterprise Linux\" REDHAT_SUPPORT_PRODUCT_VERSION=\"8.1\" Red Hat Enterprise Linux release 8.1 (Ootpa) Red Hat Enterprise Linux release 8.1 (Ootpa)","title":"Red Hat Release Info"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20FC640%20with%20RHEL%208/#kernel-info","text":"Linux dpdkdemo.lan 4.18.0-147.el8.x86_64 #1 SMP Thu Sep 26 15:52:44 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux","title":"Kernel Info"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20FC640%20with%20RHEL%208/#physical-setup","text":"I ran the traffic generator to port 5 of the passthrough module which maps to port 1 my internal x710 card.","title":"Physical Setup"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20FC640%20with%20RHEL%208/#installation","text":"","title":"Installation"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20FC640%20with%20RHEL%208/#enable-red-hat-repos","text":"Run subscription-manager list --available | less and find the subscription which provides CodeReady for x86. Note the pool number associated with the subscription. Run subscription-manager attach --pool=<POOL_NUMBER> to enable the subscription. Then run: subscription-manager repos --enable=codeready-builder-for-rhel-8-x86_64-rpms to enable the repo.","title":"Enable Red Hat Repos"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20FC640%20with%20RHEL%208/#install-dpdk","text":"Download from https://core.dpdk.org/download/ Edit the security limits with vim /etc/security/limits.conf 1.Add the following lines at the end of the file. This assumes you are running as root: root hard memlock unlimited root soft memlock unlimited Make sure your kernel is up to date with dnf update -y && reboot . Run dnf install -y gcc numactl-devel kernel-devel pciutils elfutils-libelf-devel make libpcap python3 tar vim wget tmux vim mlocate hwloc Extract dpdk and cd into its directory set the environment variable RTE_SDK. It is the directory in which you extracted all the DPDK files. export RTE_SDK=<YOUR_DIR> Run make install T=x86_64-native-linux-gcc CONFIG_RTE_LIBRTE_PMD_PCAP=y CONFIG_RTE_LIBRTE_PDUMP=y DESTDIR=<INSTALL_DIR> to build dpdk. Ensure your install directory exists. NOTE : The option CONFIG_RTE_LIBRTE_PMD_PCAP=y enabled libpcap support in DPDK. This is required for pdump to work. Once an DPDK target environment directory has been created (such as x86_64-native-linux-gcc), it contains all libraries and header files required to build an application. When compiling an application in the Linux* environment on the DPDK, the following variables must be exported: RTE_TARGET - Points to the DPDK target environment directory. export RTE_TARGET=/opt/dpdk-19.08/x86_64-native-linux-gcc You may want to add this variable and RTE_SDK to ~/.bash_profile","title":"Install DPDK"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20FC640%20with%20RHEL%208/#install-elf-tools","text":"Run the following: dnf install -y python36-devel pip3 install numpy pip3 install elftools pip3 install pyelftools","title":"Install ELF Tools"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20FC640%20with%20RHEL%208/#configuration","text":"TODO: Need to update the instructions with this. Why can't the setup tool find it? I had to run modprobe uio && insmod /opt/dpdk-19.08/install/lib/modules/4.18.0-147.el8.x86_64/extra/dpdk/igb_uio.ko to get this to load. The modprobe uio is necessary because the uio module is a depency of igb_uio.","title":"Configuration"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20FC640%20with%20RHEL%208/#configure-vfio-pci-to-load-on-boot-todo-remove","text":"Go to /etc/modules-load.d/ cd /etc/modules-load.d Run echo vfio-pci > vfio-pci.conf If you don't reboot you will need to run modprobe vfio-pci","title":"Configure vfio-pci to Load on Boot (TODO REMOVE)"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20FC640%20with%20RHEL%208/#configure-ports","text":"Move to your dpdk dir and run ./install/share/dpdk/usertools/dpdk-setup.sh . This should give you a menu with all available DPDK options. The menu is setup in such a way that you must perform each step listed in the menu. If things have gone correctly to this point your Step 1 should look like the following: ---------------------------------------------------------- Step 1: Select the DPDK environment to build ---------------------------------------------------------- [1] * My menu looks like this: ---------------------------------------------------------- Step 1: Select the DPDK environment to build ---------------------------------------------------------- [1] * ---------------------------------------------------------- Step 2: Setup linux environment ---------------------------------------------------------- [2] Insert IGB UIO module [3] Insert VFIO module [4] Insert KNI module [5] Setup hugepage mappings for non-NUMA systems [6] Setup hugepage mappings for NUMA systems [7] Display current Ethernet/Baseband/Crypto device settings [8] Bind Ethernet/Baseband/Crypto device to IGB UIO module [9] Bind Ethernet/Baseband/Crypto device to VFIO module [10] Setup VFIO permissions ---------------------------------------------------------- Step 3: Run test application for linux environment ---------------------------------------------------------- [11] Run test application ($RTE_TARGET/app/test) [12] Run testpmd application in interactive mode ($RTE_TARGET/app/testpmd) ---------------------------------------------------------- Step 4: Other tools ---------------------------------------------------------- [13] List hugepage info from /proc/meminfo ---------------------------------------------------------- Step 5: Uninstall and system cleanup ---------------------------------------------------------- [14] Unbind devices from IGB UIO or VFIO driver [15] Remove IGB UIO module [16] Remove VFIO module [17] Remove KNI module [18] Remove hugepage mappings [19] Exit Script Next run option 6 to instert huge pages for NUMA systems. Notice you will be prompted to select an amount of memory on a per processor basis. This is because there are pages associated with each individual processor to increase performance via locality. Run option 7 and make sure you receive output and that network devices are listed. My output looks like this: Network devices using kernel driver =================================== 0000:0b:00.0 'VMXNET3 Ethernet Controller 07b0' if=ens192 drv=vmxnet3 unused=vfio-pci *Active* 0000:13:00.0 'VMXNET3 Ethernet Controller 07b0' if=ens224 drv=vmxnet3 unused=vfio-pci *Active* No 'Baseband' devices detected ============================== No 'Crypto' devices detected ============================ No 'Eventdev' devices detected ============================== No 'Mempool' devices detected ============================= No 'Compress' devices detected ============================== No 'Misc (rawdev)' devices detected =================================== NOTE : The active keyword means that DPDK thinks the interface is under active use. This means that the interface has routes installed in the routing table. Run option 8 to bind an interface to DPDK using the IGB UIO driver.","title":"Configure Ports"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20FC640%20with%20RHEL%208/#performing-packet-capture","text":"","title":"Performing Packet Capture"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20FC640%20with%20RHEL%208/#initial-setup","text":"Get the core layout with ./install/share/dpdk/usertools/cpu_layout.py View your port layout with ./install/share/dpdk/usertools/dpdk-devbind.py -s If you haven't already load the right kernel modules with: modprobe uio && insmod /opt/dpdk-19.08/install/lib/modules/4.18.0-147.el8.x86_64/extra/dpdk/igb_uio.ko","title":"Initial Setup"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20FC640%20with%20RHEL%208/#starting-testpmd","text":"NOTE : The -- separates the argumentsn for the EAL vs TestPMD. ./install/bin/testpmd -l 4,8,10,12 -n 4 -- -i --forward-mode=rxonly After testpmd has started don't forget to run the start command on the testpmd command line.","title":"Starting testpmd"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20FC640%20with%20RHEL%208/#pdump","text":"./install/bin/dpdk-pdump -- --pdump 'port=0,queue=*,rx-dev=/tmp/capture.pcap'","title":"pdump"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20FC640%20with%20RHEL%208/#helpful-tips","text":"","title":"Helpful Tips"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20FC640%20with%20RHEL%208/#getting-cpu-info","text":"DPDK provides a tool for seeing the CPU layout with ./install/share/dpdk/usertools/cpu_layout.py You can see the logical layout of the cores with cat /proc/cpuinfo You can alse run lstopo-no-graphics Notice that the cores alternate back and forth.","title":"Getting CPU Info"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20FC640%20with%20RHEL%208/#process-types-in-dpdk","text":"DPDK runs two different types of processes. There are as follows: primary processes, which can initialize and which have full permissions on shared memory secondary processes, which cannot initialize shared memory, but can attach to pre- initialized shared memory and create objects in it. Standalone DPDK processes are primary processes, while secondary processes can only run alongside a primary process or after a primary process has already configured the hugepage shared memory for them.","title":"Process Types in DPDK"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20FC640%20with%20RHEL%208/#testpmd","text":"The test pmd manual is available here","title":"TestPMD"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20FC640%20with%20RHEL%208/#interactive-commands","text":"","title":"Interactive Commands"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20FC640%20with%20RHEL%208/#starting-transmit","text":"start","title":"Starting transmit"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20FC640%20with%20RHEL%208/#get-port-info","text":"show port info all","title":"Get Port Info"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20FC640%20with%20RHEL%208/#forwarding-modes","text":"TestPMD has different forwarding modes that can be used within the application. Input/output mode: This mode is generally referred to as IO mode. It is the most common forwarding mode and is the default mode when TestPMD is started. In IO mode a CPU core receives packets from one port (Rx) and transmits them to another port (Tx). The same port can be used for reception and transmission if required. Rx-only mode: In this mode the application polls packets from the Rx ports and frees them without transmitting them. In this way it acts as a packet sink. Tx-only mode: In this mode the application generates 64-byte IP packets and transmits them from the Tx ports. It doesn\u2019t handle the reception of packets and as such acts as a packet source. These latter two modes (Rx-only and Tx-only) are useful for checking packet reception and transmission separately. Apart from these three modes there are other forwarding modes that are explained in the TestPMD documentation .","title":"Forwarding Modes"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20FC640%20with%20RHEL%208/#port-topology-modes","text":"In paired mode, the forwarding is between pairs of ports, for example: (0,1), (2,3), (4,5). In chained mode, the forwarding is to the next available port in the port mask, for example: (0,1), (1,2), (2,0). The ordering of the ports can be changed using the portlist testpmd runtime function. In loop mode, ingress traffic is simply transmitted back on the same interface.","title":"Port Topology Modes"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20FC640%20with%20RHEL%208/#receive-side-scaling","text":"=Receive-Side Scaling (RSS), also known as multi-queue receive, distributes network receive processing across several hardware-based receive queues, allowing inbound network traffic to be processed by multiple CPUs. RSS can be used to relieve bottlenecks in receive interrupt processing caused by overloading a single CPU, and to reduce network latency. To determine whether your network interface card supports RSS, check whether multiple interrupt request queues are associated with the interface in /proc/interrupts. For example, if you are interested in the p1p1 interface: # egrep 'CPU|p1p1' /proc/interrupts CPU0 CPU1 CPU2 CPU3 CPU4 CPU5 89: 40187 0 0 0 0 0 IR-PCI-MSI-edge p1p1-0 90: 0 790 0 0 0 0 IR-PCI-MSI-edge p1p1-1 91: 0 0 959 0 0 0 IR-PCI-MSI-edge p1p1-2 92: 0 0 0 3310 0 0 IR-PCI-MSI-edge p1p1-3 93: 0 0 0 0 622 0 IR-PCI-MSI-edge p1p1-4 94: 0 0 0 0 0 2475 IR-PCI-MSI-edge p1p1-5 huge pages https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/performance_tuning_guide/sect-red_hat_enterprise_linux-performance_tuning_guide-memory-configuring-huge-pages","title":"Receive Side Scaling"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20R940%20with%20Napatech/","text":"How to Get DPDK with pdump Running The purpose of this experiment is to get Intel's DPDK framework up and running on a server. Useful Materials How to Compile DPDK Info on Linux Drivers for DPDK Description of the TestPMD Program My Environment I am running a Dell R940 with Napatech Card NT200A02 CentOS Release Info NAME=\"CentOS Linux\" VERSION=\"7 (Core)\" ID=\"centos\" ID_LIKE=\"rhel fedora\" VERSION_ID=\"7\" PRETTY_NAME=\"CentOS Linux 7 (Core)\" ANSI_COLOR=\"0;31\" CPE_NAME=\"cpe:/o:centos:centos:7\" HOME_URL=\"https://www.centos.org/\" BUG_REPORT_URL=\"https://bugs.centos.org/\" CENTOS_MANTISBT_PROJECT=\"CentOS-7\" CENTOS_MANTISBT_PROJECT_VERSION=\"7\" REDHAT_SUPPORT_PRODUCT=\"centos\" REDHAT_SUPPORT_PRODUCT_VERSION=\"7\" CentOS Linux release 7.7.1908 (Core) CentOS Linux release 7.7.1908 (Core) Kernel Info Linux r940.lan 3.10.0-1062.4.3.el7.x86_64 #1 SMP Wed Nov 13 23:58:53 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux Physical Setup Installation NapaTech Driver Install Driver Download the Napatech software from here Run yum groupinstall \"Development Tools\" && yum install -y kernel-devel gettext-devel openssl-devel perl-CPAN perl-devel zlib-devel pciutils && yum install -y https://centos7.iuscommunity.org/ius-release.rpm && yum remove -y git && yum install -y git2u-all Unzip and run package_install_3gd.sh Install DPDK export NAPATECH3_PATH=/opt/napatech3 Download with git clone https://github.com/napatech/dpdk.git Edit the security limits with vim /etc/security/limits.conf . After making the below edits you will need to log out and log back in for them to take effect. 1.Add the following lines at the end of the file. This assumes you are running as root: root hard memlock unlimited root soft memlock unlimited Run yum install -y gcc numactl-devel kernel-devel pciutils elfutils-libelf-devel make libpcap python3 tar vim wget tmux vim mlocate hwloc libpcap-devel python36-devel Extract dpdk and cd into its directory set the environment variable RTE_SDK. It is the directory in which you extracted all the DPDK files. export RTE_SDK=<YOUR_DIR> Run make config T=x86_64-native-linuxapp-gcc install CONFIG_RTE_LIBRTE_PMD_PCAP=y CONFIG_RTE_LIBRTE_PDUMP=y DESTDIR=install CONFIG_RTE_LIBRTE_PMD_NTACC=y to build dpdk. Ensure your install directory exists. make -j NOTE : The option CONFIG_RTE_LIBRTE_PMD_PCAP=y enabled libpcap support in DPDK. This is required for pdump to work. Once an DPDK target environment directory has been created (such as x86_64-native-linux-gcc), it contains all libraries and header files required to build an application. When compiling an application in the Linux* environment on the DPDK, the following variables must be exported: RTE_TARGET - Points to the DPDK target environment directory. export RTE_TARGET=/opt/dpdk-19.08/x86_64-native-linux-gcc You may want to add this variable and RTE_SDK to ~/.bash_profile Update Your Bash Profile Add: export RTE_SDK=/opt/dpdk export NAPATECH3_PATH=/opt/napatech3 export RTE_TARGET=/opt/dpdk/x86_64-native-linuxapp-gcc to ~/.bash_profile Install ELF Tools Run the following: pip3 install numpy pip3 install elftools pip3 install pyelftools Configuration Configure Ports Move to your dpdk dir and run ./install/share/dpdk/usertools/dpdk-setup.sh . This should give you a menu with all available DPDK options. The menu is setup in such a way that you must perform each step listed in the menu. If things have gone correctly to this point your Step 1 should look like the following: ---------------------------------------------------------- Step 1: Select the DPDK environment to build ---------------------------------------------------------- [1] * My menu looks like this: ---------------------------------------------------------- Step 1: Select the DPDK environment to build ---------------------------------------------------------- [1] * ---------------------------------------------------------- Step 2: Setup linux environment ---------------------------------------------------------- [2] Insert IGB UIO module [3] Insert VFIO module [4] Insert KNI module [5] Setup hugepage mappings for non-NUMA systems [6] Setup hugepage mappings for NUMA systems [7] Display current Ethernet/Baseband/Crypto device settings [8] Bind Ethernet/Baseband/Crypto device to IGB UIO module [9] Bind Ethernet/Baseband/Crypto device to VFIO module [10] Setup VFIO permissions ---------------------------------------------------------- Step 3: Run test application for linux environment ---------------------------------------------------------- [11] Run test application ($RTE_TARGET/app/test) [12] Run testpmd application in interactive mode ($RTE_TARGET/app/testpmd) ---------------------------------------------------------- Step 4: Other tools ---------------------------------------------------------- [13] List hugepage info from /proc/meminfo ---------------------------------------------------------- Step 5: Uninstall and system cleanup ---------------------------------------------------------- [14] Unbind devices from IGB UIO or VFIO driver [15] Remove IGB UIO module [16] Remove VFIO module [17] Remove KNI module [18] Remove hugepage mappings [19] Exit Script Next run option 6 to instert huge pages for NUMA systems. Notice you will be prompted to select an amount of memory on a per processor basis. This is because there are pages associated with each individual processor to increase performance via locality. I used a value of 4096 for each NUMA node. Performing Packet Capture Setup the NapaTech Card Initial Setup Get the core layout with ./install/share/dpdk/usertools/cpu_layout.py View your port layout with ./install/share/dpdk/usertools/dpdk-devbind.py -s Starting testpmd NOTE : The -- separates the argumentsn for the EAL vs TestPMD. ./install/bin/testpmd -l 0,4,8,12,16,20,24,28,32,36 -n 4 -- -i After testpmd has started don't forget to run the start command on the testpmd command line. pdump ./install/bin/dpdk-pdump -- --pdump 'device_id=0000:5b:00.0,queue=*,rx-dev=/tmp/capture.pcap' Helpful Tips NapaTech Detecting Installed Cards /opt/napatech3/bin/imgctrl -q Load the Driver /opt/napatech3/bin/ntload.sh Run the ntserver /opt/napatech3/bin/ntstart.sh Show Interface Info /opt/napatech3/bin/adapterinfo Getting CPU Info DPDK provides a tool for seeing the CPU layout with ./install/share/dpdk/usertools/cpu_layout.py You can see the logical layout of the cores with cat /proc/cpuinfo You can alse run lstopo-no-graphics Notice that the cores alternate back and forth. Process Types in DPDK DPDK runs two different types of processes. There are as follows: primary processes, which can initialize and which have full permissions on shared memory secondary processes, which cannot initialize shared memory, but can attach to pre- initialized shared memory and create objects in it. Standalone DPDK processes are primary processes, while secondary processes can only run alongside a primary process or after a primary process has already configured the hugepage shared memory for them. TestPMD The test pmd manual is available here Interactive Commands Starting transmit start Get Port Info show port info all Forwarding Modes TestPMD has different forwarding modes that can be used within the application. Input/output mode: This mode is generally referred to as IO mode. It is the most common forwarding mode and is the default mode when TestPMD is started. In IO mode a CPU core receives packets from one port (Rx) and transmits them to another port (Tx). The same port can be used for reception and transmission if required. Rx-only mode: In this mode the application polls packets from the Rx ports and frees them without transmitting them. In this way it acts as a packet sink. Tx-only mode: In this mode the application generates 64-byte IP packets and transmits them from the Tx ports. It doesn\u2019t handle the reception of packets and as such acts as a packet source. These latter two modes (Rx-only and Tx-only) are useful for checking packet reception and transmission separately. Apart from these three modes there are other forwarding modes that are explained in the TestPMD documentation . Port Topology Modes In paired mode, the forwarding is between pairs of ports, for example: (0,1), (2,3), (4,5). In chained mode, the forwarding is to the next available port in the port mask, for example: (0,1), (1,2), (2,0). The ordering of the ports can be changed using the portlist testpmd runtime function. In loop mode, ingress traffic is simply transmitted back on the same interface. Receive Side Scaling =Receive-Side Scaling (RSS), also known as multi-queue receive, distributes network receive processing across several hardware-based receive queues, allowing inbound network traffic to be processed by multiple CPUs. RSS can be used to relieve bottlenecks in receive interrupt processing caused by overloading a single CPU, and to reduce network latency. To determine whether your network interface card supports RSS, check whether multiple interrupt request queues are associated with the interface in /proc/interrupts. For example, if you are interested in the p1p1 interface: # egrep 'CPU|p1p1' /proc/interrupts CPU0 CPU1 CPU2 CPU3 CPU4 CPU5 89: 40187 0 0 0 0 0 IR-PCI-MSI-edge p1p1-0 90: 0 790 0 0 0 0 IR-PCI-MSI-edge p1p1-1 91: 0 0 959 0 0 0 IR-PCI-MSI-edge p1p1-2 92: 0 0 0 3310 0 0 IR-PCI-MSI-edge p1p1-3 93: 0 0 0 0 622 0 IR-PCI-MSI-edge p1p1-4 94: 0 0 0 0 0 2475 IR-PCI-MSI-edge p1p1-5 huge pages https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/performance_tuning_guide/sect-red_hat_enterprise_linux-performance_tuning_guide-memory-configuring-huge-pages","title":"How to Get DPDK with pdump Running"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20R940%20with%20Napatech/#how-to-get-dpdk-with-pdump-running","text":"The purpose of this experiment is to get Intel's DPDK framework up and running on a server.","title":"How to Get DPDK with pdump Running"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20R940%20with%20Napatech/#useful-materials","text":"How to Compile DPDK Info on Linux Drivers for DPDK Description of the TestPMD Program","title":"Useful Materials"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20R940%20with%20Napatech/#my-environment","text":"I am running a Dell R940 with Napatech Card NT200A02","title":"My Environment"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20R940%20with%20Napatech/#centos-release-info","text":"NAME=\"CentOS Linux\" VERSION=\"7 (Core)\" ID=\"centos\" ID_LIKE=\"rhel fedora\" VERSION_ID=\"7\" PRETTY_NAME=\"CentOS Linux 7 (Core)\" ANSI_COLOR=\"0;31\" CPE_NAME=\"cpe:/o:centos:centos:7\" HOME_URL=\"https://www.centos.org/\" BUG_REPORT_URL=\"https://bugs.centos.org/\" CENTOS_MANTISBT_PROJECT=\"CentOS-7\" CENTOS_MANTISBT_PROJECT_VERSION=\"7\" REDHAT_SUPPORT_PRODUCT=\"centos\" REDHAT_SUPPORT_PRODUCT_VERSION=\"7\" CentOS Linux release 7.7.1908 (Core) CentOS Linux release 7.7.1908 (Core)","title":"CentOS Release Info"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20R940%20with%20Napatech/#kernel-info","text":"Linux r940.lan 3.10.0-1062.4.3.el7.x86_64 #1 SMP Wed Nov 13 23:58:53 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux","title":"Kernel Info"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20R940%20with%20Napatech/#physical-setup","text":"","title":"Physical Setup"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20R940%20with%20Napatech/#installation-napatech-driver","text":"","title":"Installation NapaTech Driver"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20R940%20with%20Napatech/#install-driver","text":"Download the Napatech software from here Run yum groupinstall \"Development Tools\" && yum install -y kernel-devel gettext-devel openssl-devel perl-CPAN perl-devel zlib-devel pciutils && yum install -y https://centos7.iuscommunity.org/ius-release.rpm && yum remove -y git && yum install -y git2u-all Unzip and run package_install_3gd.sh","title":"Install Driver"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20R940%20with%20Napatech/#install-dpdk","text":"export NAPATECH3_PATH=/opt/napatech3 Download with git clone https://github.com/napatech/dpdk.git Edit the security limits with vim /etc/security/limits.conf . After making the below edits you will need to log out and log back in for them to take effect. 1.Add the following lines at the end of the file. This assumes you are running as root: root hard memlock unlimited root soft memlock unlimited Run yum install -y gcc numactl-devel kernel-devel pciutils elfutils-libelf-devel make libpcap python3 tar vim wget tmux vim mlocate hwloc libpcap-devel python36-devel Extract dpdk and cd into its directory set the environment variable RTE_SDK. It is the directory in which you extracted all the DPDK files. export RTE_SDK=<YOUR_DIR> Run make config T=x86_64-native-linuxapp-gcc install CONFIG_RTE_LIBRTE_PMD_PCAP=y CONFIG_RTE_LIBRTE_PDUMP=y DESTDIR=install CONFIG_RTE_LIBRTE_PMD_NTACC=y to build dpdk. Ensure your install directory exists. make -j NOTE : The option CONFIG_RTE_LIBRTE_PMD_PCAP=y enabled libpcap support in DPDK. This is required for pdump to work. Once an DPDK target environment directory has been created (such as x86_64-native-linux-gcc), it contains all libraries and header files required to build an application. When compiling an application in the Linux* environment on the DPDK, the following variables must be exported: RTE_TARGET - Points to the DPDK target environment directory. export RTE_TARGET=/opt/dpdk-19.08/x86_64-native-linux-gcc You may want to add this variable and RTE_SDK to ~/.bash_profile","title":"Install DPDK"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20R940%20with%20Napatech/#update-your-bash-profile","text":"Add: export RTE_SDK=/opt/dpdk export NAPATECH3_PATH=/opt/napatech3 export RTE_TARGET=/opt/dpdk/x86_64-native-linuxapp-gcc to ~/.bash_profile","title":"Update Your Bash Profile"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20R940%20with%20Napatech/#install-elf-tools","text":"Run the following: pip3 install numpy pip3 install elftools pip3 install pyelftools","title":"Install ELF Tools"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20R940%20with%20Napatech/#configuration","text":"","title":"Configuration"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20R940%20with%20Napatech/#configure-ports","text":"Move to your dpdk dir and run ./install/share/dpdk/usertools/dpdk-setup.sh . This should give you a menu with all available DPDK options. The menu is setup in such a way that you must perform each step listed in the menu. If things have gone correctly to this point your Step 1 should look like the following: ---------------------------------------------------------- Step 1: Select the DPDK environment to build ---------------------------------------------------------- [1] * My menu looks like this: ---------------------------------------------------------- Step 1: Select the DPDK environment to build ---------------------------------------------------------- [1] * ---------------------------------------------------------- Step 2: Setup linux environment ---------------------------------------------------------- [2] Insert IGB UIO module [3] Insert VFIO module [4] Insert KNI module [5] Setup hugepage mappings for non-NUMA systems [6] Setup hugepage mappings for NUMA systems [7] Display current Ethernet/Baseband/Crypto device settings [8] Bind Ethernet/Baseband/Crypto device to IGB UIO module [9] Bind Ethernet/Baseband/Crypto device to VFIO module [10] Setup VFIO permissions ---------------------------------------------------------- Step 3: Run test application for linux environment ---------------------------------------------------------- [11] Run test application ($RTE_TARGET/app/test) [12] Run testpmd application in interactive mode ($RTE_TARGET/app/testpmd) ---------------------------------------------------------- Step 4: Other tools ---------------------------------------------------------- [13] List hugepage info from /proc/meminfo ---------------------------------------------------------- Step 5: Uninstall and system cleanup ---------------------------------------------------------- [14] Unbind devices from IGB UIO or VFIO driver [15] Remove IGB UIO module [16] Remove VFIO module [17] Remove KNI module [18] Remove hugepage mappings [19] Exit Script Next run option 6 to instert huge pages for NUMA systems. Notice you will be prompted to select an amount of memory on a per processor basis. This is because there are pages associated with each individual processor to increase performance via locality. I used a value of 4096 for each NUMA node.","title":"Configure Ports"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20R940%20with%20Napatech/#performing-packet-capture","text":"","title":"Performing Packet Capture"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20R940%20with%20Napatech/#setup-the-napatech-card","text":"","title":"Setup the NapaTech Card"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20R940%20with%20Napatech/#initial-setup","text":"Get the core layout with ./install/share/dpdk/usertools/cpu_layout.py View your port layout with ./install/share/dpdk/usertools/dpdk-devbind.py -s","title":"Initial Setup"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20R940%20with%20Napatech/#starting-testpmd","text":"NOTE : The -- separates the argumentsn for the EAL vs TestPMD. ./install/bin/testpmd -l 0,4,8,12,16,20,24,28,32,36 -n 4 -- -i After testpmd has started don't forget to run the start command on the testpmd command line.","title":"Starting testpmd"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20R940%20with%20Napatech/#pdump","text":"./install/bin/dpdk-pdump -- --pdump 'device_id=0000:5b:00.0,queue=*,rx-dev=/tmp/capture.pcap'","title":"pdump"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20R940%20with%20Napatech/#helpful-tips","text":"","title":"Helpful Tips"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20R940%20with%20Napatech/#napatech","text":"","title":"NapaTech"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20R940%20with%20Napatech/#detecting-installed-cards","text":"/opt/napatech3/bin/imgctrl -q","title":"Detecting Installed Cards"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20R940%20with%20Napatech/#load-the-driver","text":"/opt/napatech3/bin/ntload.sh","title":"Load the Driver"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20R940%20with%20Napatech/#run-the-ntserver","text":"/opt/napatech3/bin/ntstart.sh","title":"Run the ntserver"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20R940%20with%20Napatech/#show-interface-info","text":"/opt/napatech3/bin/adapterinfo","title":"Show Interface Info"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20R940%20with%20Napatech/#getting-cpu-info","text":"DPDK provides a tool for seeing the CPU layout with ./install/share/dpdk/usertools/cpu_layout.py You can see the logical layout of the cores with cat /proc/cpuinfo You can alse run lstopo-no-graphics Notice that the cores alternate back and forth.","title":"Getting CPU Info"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20R940%20with%20Napatech/#process-types-in-dpdk","text":"DPDK runs two different types of processes. There are as follows: primary processes, which can initialize and which have full permissions on shared memory secondary processes, which cannot initialize shared memory, but can attach to pre- initialized shared memory and create objects in it. Standalone DPDK processes are primary processes, while secondary processes can only run alongside a primary process or after a primary process has already configured the hugepage shared memory for them.","title":"Process Types in DPDK"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20R940%20with%20Napatech/#testpmd","text":"The test pmd manual is available here","title":"TestPMD"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20R940%20with%20Napatech/#interactive-commands","text":"","title":"Interactive Commands"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20R940%20with%20Napatech/#starting-transmit","text":"start","title":"Starting transmit"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20R940%20with%20Napatech/#get-port-info","text":"show port info all","title":"Get Port Info"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20R940%20with%20Napatech/#forwarding-modes","text":"TestPMD has different forwarding modes that can be used within the application. Input/output mode: This mode is generally referred to as IO mode. It is the most common forwarding mode and is the default mode when TestPMD is started. In IO mode a CPU core receives packets from one port (Rx) and transmits them to another port (Tx). The same port can be used for reception and transmission if required. Rx-only mode: In this mode the application polls packets from the Rx ports and frees them without transmitting them. In this way it acts as a packet sink. Tx-only mode: In this mode the application generates 64-byte IP packets and transmits them from the Tx ports. It doesn\u2019t handle the reception of packets and as such acts as a packet source. These latter two modes (Rx-only and Tx-only) are useful for checking packet reception and transmission separately. Apart from these three modes there are other forwarding modes that are explained in the TestPMD documentation .","title":"Forwarding Modes"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20R940%20with%20Napatech/#port-topology-modes","text":"In paired mode, the forwarding is between pairs of ports, for example: (0,1), (2,3), (4,5). In chained mode, the forwarding is to the next available port in the port mask, for example: (0,1), (1,2), (2,0). The ordering of the ports can be changed using the portlist testpmd runtime function. In loop mode, ingress traffic is simply transmitted back on the same interface.","title":"Port Topology Modes"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20R940%20with%20Napatech/#receive-side-scaling","text":"=Receive-Side Scaling (RSS), also known as multi-queue receive, distributes network receive processing across several hardware-based receive queues, allowing inbound network traffic to be processed by multiple CPUs. RSS can be used to relieve bottlenecks in receive interrupt processing caused by overloading a single CPU, and to reduce network latency. To determine whether your network interface card supports RSS, check whether multiple interrupt request queues are associated with the interface in /proc/interrupts. For example, if you are interested in the p1p1 interface: # egrep 'CPU|p1p1' /proc/interrupts CPU0 CPU1 CPU2 CPU3 CPU4 CPU5 89: 40187 0 0 0 0 0 IR-PCI-MSI-edge p1p1-0 90: 0 790 0 0 0 0 IR-PCI-MSI-edge p1p1-1 91: 0 0 959 0 0 0 IR-PCI-MSI-edge p1p1-2 92: 0 0 0 3310 0 0 IR-PCI-MSI-edge p1p1-3 93: 0 0 0 0 622 0 IR-PCI-MSI-edge p1p1-4 94: 0 0 0 0 0 2475 IR-PCI-MSI-edge p1p1-5 huge pages https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/performance_tuning_guide/sect-red_hat_enterprise_linux-performance_tuning_guide-memory-configuring-huge-pages","title":"Receive Side Scaling"},{"location":"High%20Speed%20Packet%20Capture/ntop/","text":"Helpful Materials Drill Down Deeper: Using ntopng to Zoom In, Filter Out and Go Straight to the Packets Traffic Recording Manual Configuration Hardware Tracewell TFX2HE with 1 passthrough module, 1 switch module. Operating System Version CentOS Linux release 7.7.1908 (Core) NAME=\"CentOS Linux\" VERSION=\"7 (Core)\" ID=\"centos\" ID_LIKE=\"rhel fedora\" VERSION_ID=\"7\" PRETTY_NAME=\"CentOS Linux 7 (Core)\" ANSI_COLOR=\"0;31\" CPE_NAME=\"cpe:/o:centos:centos:7\" HOME_URL=\"https://www.centos.org/\" BUG_REPORT_URL=\"https://bugs.centos.org/\" CENTOS_MANTISBT_PROJECT=\"CentOS-7\" CENTOS_MANTISBT_PROJECT_VERSION=\"7\" REDHAT_SUPPORT_PRODUCT=\"centos\" REDHAT_SUPPORT_PRODUCT_VERSION=\"7\" CentOS Linux release 7.7.1908 (Core) CentOS Linux release 7.7.1908 (Core) Kernel Version Linux ntopdemo.lan 3.10.0-1062.4.1.el7.x86_64 #1 SMP Fri Oct 18 17:15:30 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux Install n2disk and ntop Perform Installation Install epel with yum install -y epel-release Erase the zeromq3 package with yum erase zeromq && yum clean all && yum update -y && reboot Pull ntop repo with wget http://packages.ntop.org/centos-stable/ntop.repo -O /etc/yum.repos.d/ntop.repo Install required packages with yum install pfring-dkms pfring n2disk nprobe ntopng ntopng-data cento pfring-drivers-zc-dkms redis hiredis-devel Perform Configuration Zero Copy Driver List interfaces with pf_ringcfg --list-interfaces Configure the driver with pf_ringcfg --configure-driver i40e Set promiscuous mode on the interface in question with /sbin/ip link set em1 promisc on Edit the pfring configuration file with vim /etc/pf_ring/interfaces.conf and add your configuration. MANAGEMENT_INTERFACES=\"<YOUR_MANAGEMENT_INTERFACE>\" CAPTURE_INTERFACES=\"<YOUR_CAPTURE_INTERFACE>\" Open the file etc/ntopng/ntopng.conf . If you do not have a license add --community to the end Configure the firewall to accept connections to ntopng with: firewall-cmd --zone=public --permanent --add-port=3000/tcp && firewall-cmd --reload Enable and start services with: systemctl enable redis.service systemctl enable ntopng.service systemctl enable pf_ring systemctl start redis.service systemctl start ntopng.service systemctl start pf_ring Make sure the services are running correctly with: systemctl status redis.service systemctl status ntopng.service systemctl status pf_ring Performing Filtering","title":"Helpful Materials"},{"location":"High%20Speed%20Packet%20Capture/ntop/#helpful-materials","text":"Drill Down Deeper: Using ntopng to Zoom In, Filter Out and Go Straight to the Packets Traffic Recording Manual","title":"Helpful Materials"},{"location":"High%20Speed%20Packet%20Capture/ntop/#configuration","text":"","title":"Configuration"},{"location":"High%20Speed%20Packet%20Capture/ntop/#hardware","text":"Tracewell TFX2HE with 1 passthrough module, 1 switch module.","title":"Hardware"},{"location":"High%20Speed%20Packet%20Capture/ntop/#operating-system-version","text":"CentOS Linux release 7.7.1908 (Core) NAME=\"CentOS Linux\" VERSION=\"7 (Core)\" ID=\"centos\" ID_LIKE=\"rhel fedora\" VERSION_ID=\"7\" PRETTY_NAME=\"CentOS Linux 7 (Core)\" ANSI_COLOR=\"0;31\" CPE_NAME=\"cpe:/o:centos:centos:7\" HOME_URL=\"https://www.centos.org/\" BUG_REPORT_URL=\"https://bugs.centos.org/\" CENTOS_MANTISBT_PROJECT=\"CentOS-7\" CENTOS_MANTISBT_PROJECT_VERSION=\"7\" REDHAT_SUPPORT_PRODUCT=\"centos\" REDHAT_SUPPORT_PRODUCT_VERSION=\"7\" CentOS Linux release 7.7.1908 (Core) CentOS Linux release 7.7.1908 (Core)","title":"Operating System Version"},{"location":"High%20Speed%20Packet%20Capture/ntop/#kernel-version","text":"Linux ntopdemo.lan 3.10.0-1062.4.1.el7.x86_64 #1 SMP Fri Oct 18 17:15:30 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux","title":"Kernel Version"},{"location":"High%20Speed%20Packet%20Capture/ntop/#install-n2disk-and-ntop","text":"","title":"Install n2disk and ntop"},{"location":"High%20Speed%20Packet%20Capture/ntop/#perform-installation","text":"Install epel with yum install -y epel-release Erase the zeromq3 package with yum erase zeromq && yum clean all && yum update -y && reboot Pull ntop repo with wget http://packages.ntop.org/centos-stable/ntop.repo -O /etc/yum.repos.d/ntop.repo Install required packages with yum install pfring-dkms pfring n2disk nprobe ntopng ntopng-data cento pfring-drivers-zc-dkms redis hiredis-devel","title":"Perform Installation"},{"location":"High%20Speed%20Packet%20Capture/ntop/#perform-configuration-zero-copy-driver","text":"List interfaces with pf_ringcfg --list-interfaces Configure the driver with pf_ringcfg --configure-driver i40e Set promiscuous mode on the interface in question with /sbin/ip link set em1 promisc on Edit the pfring configuration file with vim /etc/pf_ring/interfaces.conf and add your configuration. MANAGEMENT_INTERFACES=\"<YOUR_MANAGEMENT_INTERFACE>\" CAPTURE_INTERFACES=\"<YOUR_CAPTURE_INTERFACE>\" Open the file etc/ntopng/ntopng.conf . If you do not have a license add --community to the end Configure the firewall to accept connections to ntopng with: firewall-cmd --zone=public --permanent --add-port=3000/tcp && firewall-cmd --reload Enable and start services with: systemctl enable redis.service systemctl enable ntopng.service systemctl enable pf_ring systemctl start redis.service systemctl start ntopng.service systemctl start pf_ring Make sure the services are running correctly with: systemctl status redis.service systemctl status ntopng.service systemctl status pf_ring","title":"Perform Configuration Zero Copy Driver"},{"location":"High%20Speed%20Packet%20Capture/ntop/#performing-filtering","text":"","title":"Performing Filtering"},{"location":"High%20Speed%20Packet%20Capture/ntop%20on%20R940%20with%20Napatech/","text":"Useful Materials NapaTech Installation Instructions (Creatinga EXT4 Filesystem)[https://thelastmaimou.wordpress.com/2013/05/04/magic-soup-ext4-with-ssd-stripes-and-strides/] My Environment I am running a Dell FC640 on a Dell R940 See Server Specs for hardware details. Hard Drive Layout: I had a RAID of 12 SAS SSDs in RAID0 on the PERC740. I had 7 NVMe drives I used. I couldn't get the 8th NVMe drive working. [root@r940 /]# lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT sda 8:0 0 10.5T 0 disk /raiddata sdb 8:16 0 223.5G 0 disk \u251c\u2500sdb1 8:17 0 200M 0 part /boot/efi \u251c\u2500sdb2 8:18 0 1G 0 part /boot \u2514\u2500sdb3 8:19 0 222.3G 0 part \u251c\u2500centos-root 253:0 0 218.3G 0 lvm / \u2514\u2500centos-swap 253:1 0 4G 0 lvm [SWAP] sdc 8:32 0 59.8G 0 disk nvme0n1 259:6 0 1.5T 0 disk \u2514\u2500nvme0n1p1 259:8 0 1.5T 0 part \u2514\u2500data-data 253:2 0 10.2T 0 lvm /data nvme1n1 259:0 0 1.5T 0 disk \u2514\u2500nvme1n1p1 259:1 0 1.5T 0 part \u2514\u2500data-data 253:2 0 10.2T 0 lvm /data nvme2n1 259:2 0 1.5T 0 disk \u2514\u2500nvme2n1p1 259:3 0 1.5T 0 part \u2514\u2500data-data 253:2 0 10.2T 0 lvm /data nvme3n1 259:4 0 1.5T 0 disk \u2514\u2500nvme3n1p1 259:5 0 1.5T 0 part \u2514\u2500data-data 253:2 0 10.2T 0 lvm /data nvme4n1 259:10 0 1.5T 0 disk \u2514\u2500nvme4n1p1 259:13 0 1.5T 0 part \u2514\u2500data-data 253:2 0 10.2T 0 lvm /data nvme5n1 259:11 0 1.5T 0 disk nvme6n1 259:7 0 1.5T 0 disk \u2514\u2500nvme6n1p1 259:9 0 1.5T 0 part \u2514\u2500data-data 253:2 0 10.2T 0 lvm /data nvme7n1 259:12 0 1.5T 0 disk \u2514\u2500nvme7n1p1 259:14 0 1.5T 0 part \u2514\u2500data-data 253:2 0 10.2T 0 lvm /data [root@r940 /]# pvs PV VG Fmt Attr PSize PFree /dev/nvme0n1p1 data lvm2 a-- <1.46t 0 /dev/nvme1n1p1 data lvm2 a-- <1.46t 0 /dev/nvme2n1p1 data lvm2 a-- <1.46t 0 /dev/nvme3n1p1 data lvm2 a-- <1.46t 0 /dev/nvme4n1p1 data lvm2 a-- <1.46t 0 /dev/nvme6n1p1 data lvm2 a-- <1.46t 0 /dev/nvme7n1p1 data lvm2 a-- <1.46t 0 /dev/sdb3 centos lvm2 a-- <222.31g 0 [root@r940 /]# vgs VG #PV #LV #SN Attr VSize VFree centos 1 2 0 wz--n- <222.31g 0 data 7 1 0 wz--n- <10.19t 0 [root@r940 /]# lvs LV VG Attr LSize Pool Origin Data% Meta% Move Log Cpy%Sync Convert root centos -wi-ao---- <218.31g swap centos -wi-ao---- 4.00g data data -wi-ao---- <10.19t WARNING: fdisk GPT support is currently new, and therefore in an experimental phase. Use at your own discretion. Disk /dev/nvme1n1: 1600.3 GB, 1600321314816 bytes, 3125627568 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk label type: gpt Disk identifier: 759A1CF7-125F-469B-981E-149EBDBE3456 # Start End Size Type Name 1 2048 3125626879 1.5T Linux LVM WARNING: fdisk GPT support is currently new, and therefore in an experimental phase. Use at your own discretion. Disk /dev/nvme2n1: 1600.3 GB, 1600321314816 bytes, 3125627568 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk label type: gpt Disk identifier: 57898F4D-5B5E-4495-B695-E48EA3FCFA01 # Start End Size Type Name 1 2048 3125626879 1.5T Linux LVM WARNING: fdisk GPT support is currently new, and therefore in an experimental phase. Use at your own discretion. Disk /dev/nvme3n1: 1600.3 GB, 1600321314816 bytes, 3125627568 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk label type: gpt Disk identifier: DB46E8E3-5E96-4228-A148-E6C8F0187DF4 # Start End Size Type Name 1 2048 3125626879 1.5T Linux LVM WARNING: fdisk GPT support is currently new, and therefore in an experimental phase. Use at your own discretion. Disk /dev/nvme0n1: 1600.3 GB, 1600321314816 bytes, 3125627568 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk label type: gpt Disk identifier: 4066CDE1-E36E-41FB-8277-5E3FFDB55B4A # Start End Size Type Name 1 2048 3125626879 1.5T Linux LVM WARNING: fdisk GPT support is currently new, and therefore in an experimental phase. Use at your own discretion. Disk /dev/nvme6n1: 1600.3 GB, 1600321314816 bytes, 3125627568 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk label type: gpt Disk identifier: 0360D46A-9A41-4A8D-A449-94CCDC01FA8B # Start End Size Type Name 1 2048 3125626879 1.5T Linux LVM WARNING: fdisk GPT support is currently new, and therefore in an experimental phase. Use at your own discretion. Disk /dev/nvme4n1: 1600.3 GB, 1600321314816 bytes, 3125627568 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk label type: gpt Disk identifier: A5D3EDDF-C93D-4B33-92B0-D25B009EEB9D # Start End Size Type Name 1 2048 3125626879 1.5T Linux LVM Disk /dev/nvme5n1: 1600.3 GB, 1600321314816 bytes, 3125627568 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes WARNING: fdisk GPT support is currently new, and therefore in an experimental phase. Use at your own discretion. Disk /dev/nvme7n1: 1600.3 GB, 1600321314816 bytes, 3125627568 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk label type: gpt Disk identifier: 71A0B583-E727-45B6-A1AB-791D341709B6 # Start End Size Type Name 1 2048 3125626879 1.5T Linux LVM Disk /dev/sda: 11515.9 GB, 11515881062400 bytes, 22491955200 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 1048576 bytes / 1048576 bytes WARNING: fdisk GPT support is currently new, and therefore in an experimental phase. Use at your own discretion. Disk /dev/sdb: 240.0 GB, 239990276096 bytes, 468731008 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 4096 bytes I/O size (minimum/optimal): 4096 bytes / 4096 bytes Disk label type: gpt Disk identifier: F1E6A9A1-C85F-46C1-A27E-DBC41C9260AC # Start End Size Type Name 1 2048 411647 200M EFI System EFI System Partition 2 411648 2508799 1G Microsoft basic 3 2508800 468729855 222.3G Linux LVM Disk /dev/sdc: 64.2 GB, 64239960064 bytes, 125468672 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk /dev/mapper/centos-root: 234.4 GB, 234407067648 bytes, 457826304 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 4096 bytes I/O size (minimum/optimal): 4096 bytes / 4096 bytes Disk /dev/mapper/centos-swap: 4294 MB, 4294967296 bytes, 8388608 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 4096 bytes I/O size (minimum/optimal): 4096 bytes / 4096 bytes Disk /dev/mapper/data-data: 11202.2 GB, 11202210037760 bytes, 21879316480 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 131072 bytes / 917504 bytes CPU Layout [root@r940 data]# lscpu Architecture: x86_64 CPU op-mode(s): 32-bit, 64-bit Byte Order: Little Endian CPU(s): 176 On-line CPU(s) list: 0-175 Thread(s) per core: 2 Core(s) per socket: 22 Socket(s): 4 NUMA node(s): 4 Vendor ID: GenuineIntel CPU family: 6 Model: 85 Model name: Intel(R) Xeon(R) Gold 6152 CPU @ 2.10GHz Stepping: 4 CPU MHz: 1394.146 CPU max MHz: 3700.0000 CPU min MHz: 1000.0000 BogoMIPS: 4200.00 Virtualization: VT-x L1d cache: 32K L1i cache: 32K L2 cache: 1024K L3 cache: 30976K NUMA node0 CPU(s): 0,4,8,12,16,20,24,28,32,36,40,44,48,52,56,60,64,68,72,76,80,84,88,92,96,100,104,108,112,116,120,124,128,132,136,140,144,148,152,156,160,164,168,172 NUMA node1 CPU(s): 1,5,9,13,17,21,25,29,33,37,41,45,49,53,57,61,65,69,73,77,81,85,89,93,97,101,105,109,113,117,121,125,129,133,137,141,145,149,153,157,161,165,169,173 NUMA node2 CPU(s): 2,6,10,14,18,22,26,30,34,38,42,46,50,54,58,62,66,70,74,78,82,86,90,94,98,102,106,110,114,118,122,126,130,134,138,142,146,150,154,158,162,166,170,174 NUMA node3 CPU(s): 3,7,11,15,19,23,27,31,35,39,43,47,51,55,59,63,67,71,75,79,83,87,91,95,99,103,107,111,115,119,123,127,131,135,139,143,147,151,155,159,163,167,171,175 Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 invpcid_single intel_ppin intel_pt ssbd mba ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm mpx rdt_a avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts pku ospke md_clear spec_ctrl intel_stibp flush_l1d [root@r940 data]# cpu_layout.py ====================================================================== Core and Socket Information (as reported by '/sys/devices/system/cpu') ====================================================================== cores = [0, 5, 1, 4, 2, 3, 8, 12, 9, 11, 10, 21, 16, 20, 17, 19, 18, 28, 24, 27, 25, 26] sockets = [0, 1, 2, 3] Socket 0 Socket 1 Socket 2 Socket 3 -------- -------- -------- -------- Core 0 [0, 88] [1, 89] [2, 90] [3, 91] Core 5 [4, 92] [5, 93] [6, 94] [7, 95] Core 1 [8, 96] [9, 97] [10, 98] [11, 99] Core 4 [12, 100] [13, 101] [14, 102] [15, 103] Core 2 [16, 104] [17, 105] [18, 106] [19, 107] Core 3 [20, 108] [21, 109] [22, 110] [23, 111] Core 8 [24, 112] [25, 113] [26, 114] [27, 115] Core 12 [28, 116] [29, 117] [30, 118] [31, 119] Core 9 [32, 120] [33, 121] [34, 122] [35, 123] Core 11 [36, 124] [37, 125] [38, 126] [39, 127] Core 10 [40, 128] [41, 129] [42, 130] [43, 131] Core 21 [44, 132] [45, 133] [46, 134] [47, 135] Core 16 [48, 136] [49, 137] [50, 138] [51, 139] Core 20 [52, 140] [53, 141] [54, 142] [55, 143] Core 17 [56, 144] [57, 145] [58, 146] [59, 147] Core 19 [60, 148] [61, 149] [62, 150] [63, 151] Core 18 [64, 152] [65, 153] [66, 154] [67, 155] Core 28 [68, 156] [69, 157] [70, 158] [71, 159] Core 24 [72, 160] [73, 161] [74, 162] [75, 163] Core 27 [76, 164] [77, 165] [78, 166] [79, 167] Core 25 [80, 168] [81, 169] [82, 170] [83, 171] Core 26 [84, 172] [85, 173] [86, 174] [87, 175] CentOS 7 Release Info NAME=\"CentOS Linux\" VERSION=\"7 (Core)\" ID=\"centos\" ID_LIKE=\"rhel fedora\" VERSION_ID=\"7\" PRETTY_NAME=\"CentOS Linux 7 (Core)\" ANSI_COLOR=\"0;31\" CPE_NAME=\"cpe:/o:centos:centos:7\" HOME_URL=\"https://www.centos.org/\" BUG_REPORT_URL=\"https://bugs.centos.org/\" CENTOS_MANTISBT_PROJECT=\"CentOS-7\" CENTOS_MANTISBT_PROJECT_VERSION=\"7\" REDHAT_SUPPORT_PRODUCT=\"centos\" REDHAT_SUPPORT_PRODUCT_VERSION=\"7\" CentOS Linux release 7.7.1908 (Core) CentOS Linux release 7.7.1908 (Core) Kernel Info Linux r940.lan 3.10.0-1062.4.3.el7.x86_64 #1 SMP Wed Nov 13 23:58:53 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux Format Your Data Partitions Create physical volumes and volume groups with: pvcreate <NVMe drive> vgcreate data <List of NVMe Drives> NOTE: I only did this on the NVMe drives. To create the logical volume I used: lvcreate -l 100%FREE -i7 -I128 -n data data To format the drive I used: mkfs.ext4 -F -b 4096 -E discard,stride=16,stripe-width=256 # This was the 12 SAS SSDs I had in a RAID mkfs.ext4 -F -b 4096 -E discard,stride=16,stripe-width=256 /dev/mapper/data-data # This was the NVMes I tied together with LVM mkfs.ext4 -F -b 4096 -E discard,stride=16,stripe-width=256 /dev/mapper/data2-data2 mount -o rw,auto,discard /dev/mapper/data-data /data mount -o rw,auto,discard /dev/sda /raiddata/ mount -o rw,auto,discard /dev/mapper/data2-data2 /data2 echo noop > /sys/block/sda/queue/scheduler Install NapaTech Driver Install Driver Download the Napatech software from here Run yum groupinstall \"Development Tools\" && yum install -y kernel-devel wget gettext-devel openssl-devel perl-CPAN perl-devel zlib-devel pciutils && yum install -y https://centos7.iuscommunity.org/ius-release.rpm && yum remove -y git && yum install -y git2u-all Unzip and run package_install_3gd.sh Run the following commands: /opt/napatech3/bin/ntload.sh /opt/napatech3/bin/ntstart.sh Install n2disk and ntop Perform Installation Install epel with yum install -y epel-release Erase the zeromq3 package with yum erase zeromq && yum clean all && yum update -y && reboot Pull ntop repo with wget http://packages.ntop.org/centos-stable/ntop.repo -O /etc/yum.repos.d/ntop.repo Install required packages with yum install -y pfring-dkms pfring n2disk nprobe ntopng ntopng-data cento pfring-drivers-zc-dkms redis hiredis-devel Configure n2disk Create backup of the the NapaTech ini file cp /opt/napatech3/config/ntservice.ini /opt/napatech3/config/ntservice.ini.bak Update /opt/napatech3/config/ntservice.ini with the following values: TimestampFormat = PCAP_NS PacketDescriptor = PCAP HostBufferSegmentSizeRx = 4 TODO change these lines see notes for help HostBuffersRx = [16,16,0],[16,16,1] HostBuffersTx = [16,16,0],[16,16,1] You will need to start and stop the ntservice for these changes to take effect with: /opt/napatech3/bin/ntstop.sh /opt/napatech3/bin/ntstart.sh Perform Configuration Zero Copy Driver Edit the pfring configuration file with vim /etc/pf_ring/interfaces.conf and add your configuration. MANAGEMENT_INTERFACES=\"em1\" CAPTURE_INTERFACES=\"nt:0\" Open the file /etc/ntopng/ntopng.conf . If you do not have a license add --community to the end Configure the firewall to accept connections to ntopng with: firewall-cmd --zone=public --permanent --add-port=3000/tcp && firewall-cmd --reload Enable and start services with: systemctl enable redis.service systemctl enable ntopng.service systemctl enable pf_ring systemctl start redis.service systemctl start ntopng.service systemctl start pf_ring Make sure the services are running correctly with: systemctl status redis.service systemctl status ntopng.service systemctl status pf_ring Configure License Run zcount -i nt:0 and note the serial number Output n2disk license to /etc/n2disk.license Output ntopng license to /etc/ntopng.license Useful Tips Hardware Filtering Napatech NICs support full-blown hardware filtering out of the box. Thanks to nBPF we convert BPF expressions to hardware filters. This feature is supported transparently, and thus all PF_RING/libpcap-over-PF_RING can benefit from it. Example: pfcount -i nt:3 -f \"tcp and port 80 and src host 192.168.1.1\" Hostbuffer Notes HostBuffersRx = [16,2048,0],[16,2048,1],[16,2048,2],[16,2048,3] HostBuffersTx = [16,2048,0],[16,2048,1],[16,2048,2],[16,2048,3] First number is the number of host buffers Second number is the size of the host buffers Third number is the NUMA node You have to have one set of numbers for each NUMA node. Testing Transmit Speed ./pktgen -p 1 -r 10G Testing the PCAP transmit speed To test to see if the Napatech card is up and running run this command: ./pfcount -i nt:0 ./monitoring You can press t to switch stats between receive and transmit. Testing Receive chmod -R 777 /data rm /tmp/*-none\\.* 2>/dev/null; while true; do grep 'Dropped:\\|Slow.*:' -C50 /proc/net/pf_ring/stats/* 2>/dev/null; cp /proc/net/pf_ring/stats/*none* /tmp 2>/dev/null; sleep 1; done NUMA Lookup Run lscpu Things we tried: We used the to list the cpu layout and determine where we wanted to run what threads. Tests 1-6 were with the default settings for a RAID0 partition on Linux at setup time. Test 1 This is running at 100Gb/s generation n2disk -a -v -l -o /<Storage path> -x $(date +%s.) -i nt:0 This seemed to dump everything to one thread. We maxed out and had ~82% packet loss. Test 2 This is running at 100Gb/s generation n2disk -a -v -l -o /data/ -x $(date +%s.) -i nt:0 -n 5000 -m 10000 -p $((4*1024)) -b $((12*1024)) -C 4096 -c 32 -z 2,3,4,5,6,7 -Z -w 16,17,18,19,20,20,21 -S 22 Throughput results: [root@r940 bin]# cat /proc/net/pf_ring/stats/*none* Duration: 0:00:00:53:022 Throughput: 2.12 Mpps 17.65 Gbps Packets: 78780625 Filtered: 78780625 Dropped: 518512807 Bytes: 81931850000 DumpedBytes: 71137406724 DumpedFiles: 17 SlowSlavesLoops: 0 SlowStorageLoops: 432580 CaptureLoops: 19532 FirstDumpedEpoch: 0 LastDumpedEpoch: 1574185087 Worked better but we got a warning saying the time thread was on a different core than the reader/writer threads. Test 3 This is running at 100Gb/s generation n2disk -a -v -l -o /data/ -x $(date +%s.) -i nt:0 -n 5000 -m 10000 -p $((4*1024)) -b $((12*1024)) -C 4096 -c 32 -z 2,3,4,5,6,7 -Z -w 16,17,18,19,20,20,21 -S 56 Worked better: 19/Nov/2019 12:41:59 [n2disk.c:1109] Average Capture Throughput: 22.49 Gbit / 2.69 Mpps Test 4 This is running at 100Gb/s generation n2disk -a -v -l -o /data/ -x $(date +%s.) -i nt:0 -n 5000 -m 10000 -p $((4*1024)) -b $((12*1024)) -C 4096 -c 0 -z 8,12,16,20,24,28,32,26,40,44,48,52,56,60,64,68,72,76,80,84,88 -Z -w 92,96,100,104,108,112,116,120,124,128,132,136,140,144,148,152,156,160,164,168,172 -S 4 Test 5 This is running at 20Gb/s generation n2disk -a -v -l -o /data/ -x $(date +%s.) -i nt:0 -n 5000 -m 10000 -p $((4*1024)) -b $((12*1024)) -C 4096 -c 0 -z 8,12,16,20,24,28,32,26,40,44,48,52,56,60,64,68,72,76,80,84,88 -Z -w 92,96,100,104,108,112,116,120,124,128,132,136,140,144,148,152,156,160,164,168,172 -S 4 Failed. We got a lot of packet loss. Test 6 This is running at 10Gb/s generation n2disk -a -v -l -o /data/ -x $(date +%s.) -i nt:0 -n 5000 -m 10000 -p $((4*1024)) -b $((12*1024)) -C 4096 -c 0 -z 8,12,16,20,24,28,32,26,40,44,48,52,56,60,64,68,72,76,80,84,88 -Z -w 92,96,100,104,108,112,116,120,124,128,132,136,140,144,148,152,156,160,164,168,172 -S 4 No packet drops. Test 7 This is running at 20Gb/s generation n2disk -a -v -l -o /data/ -x $(date +%s.) -i nt:0 -n 5000 -m 10000 -p $((4*1024)) -b $((12*1024)) -C 4096 -c 0 -z 8,12,16,20,24,28,32,26,40,44,48,52,56,60,64,68,72,76,80,84,88 -Z -w 92,96,100,104,108,112,116,120,124,128,132,136,140,144,148,152,156,160,164,168,172 -S 4 This was with a data partition formatting according to the above. Test 8 /opt/napatech3/bin/ntpl -e \"Delete = All\" /opt/napatech3/bin/ntpl -e \"Setup[NumaNode=2] = StreamID == 0\" # Repeat this for each stream ID. In our case 0-4 ./profiling # use this to see traffic being received # N is the NUMA node that the profiler detects the traffic on /opt/napatech3/bin/ntpl -e \"HashMode = Hash2TupleSorted\" /opt/napatech3/bin/ntpl -e \"Assign[StreamId=(0..3)] = port == 0\" The problem with this was that we couldn't get n2disk to listen on multiple interfaces. It also didn't really give us a way to split the traffic across multiple reader threads. Test 9 I made four directories, one for each n2disk process I did the following: /opt/napatech3/bin/ntpl -e \"Delete = All\" /opt/napatech3/bin/ntpl -e \"Setup[NumaNode=2] = StreamID == 0\" # Repeat this for each stream ID. In our case 0-4 /opt/napatech3/bin/ntpl -e \"HashMode = Hash2TupleSorted\" /opt/napatech3/bin/ntpl -e \"Assign[StreamId=(0..3)] = port == 0\" ./profiling # use this to see traffic being received # N is the NUMA node that the profiler detects the traffic on Run the following commands: n2disk -a -v -l -o /data/data0 -x $(date +%s.) -i nt:stream0 -n 5000 -m 10000 -p $((20*1024)) -b $((40*1024)) -C 4096 -c 120 -z 0,4,8,12,16,20,24,28 -Z -w 88,92,96,100,104,108,112,116 -S 32 n2disk -a -v -l -o /data/data1 -x $(date +%s.) -i nt:stream1 -n 5000 -m 10000 -p $((20*1024)) -b $((40*1024)) -C 4096 -c 121 -z 1,5,9,13,17,21,25,29 -Z -w 89,93,97,101,105,109,113,117 -S 33 n2disk -a -v -l -o /data/data2 -x $(date +%s.) -i nt:stream2 -n 5000 -m 10000 -p $((20*1024)) -b $((40*1024)) -C 4096 -c 156 -z 36,40,44,48,52,56,60,64 -Z -w 124,128,132,136,140,144,148,152 -S 68 n2disk -a -v -l -o /data/data3 -x $(date +%s.) -i nt:stream3 -n 5000 -m 10000 -p $((20*1024)) -b $((40*1024)) -C 4096 -c 157 -z 37,41,45,49,53,57,61,65 -Z -w 125,129,133,137,141,145,149,153 -S 69 -a : Archive pcap file (rename to .old) instead of overwriting if already present on disk. -v : Verbose. -o : Directory where dump files will be saved (multiple -o can be specified) -x : Dump file prefix. -i : Ingress packet device. -n : Max number of nested dump sub-directories. -m : Max number of files before restarting file name. -p : Max pcap file length (MBytes). -b : Buffer length (MBytes). -C : Size (KB) of the chunk written to disk (must be multiple of 4096). Default: 64 KB. -c : Bind the reader thread to the specified core. -z : Enable multithread compression and/or indexing and bind thread(s) to the specified core ids (e.g. 0,1,2,3) (mandatory with indexing on Napatech cards) -Z : Compute index on the thread(s) used for compression (-z) instead of using the capture thread(s). -w : Bind the writer thread(s) to the specified core ids. A comma-separated list of cores (e.g. 0,1,2,3) should be specified in case of multiple dump directories (-o). -S : Enable time pulse thread (optimise sw packet timestamping) and bind it to the specified core. With this setup I was able to get 70Gb/s. I tested 100Gb/s and got drops. Did not perform further testing to narrow down exactly how much traffic I could push before moving on to the next test. Test 10 I made seven directories, one for each n2disk process. 4 assigned to the NVMe drives and 3 assigned to the SAS SSD RAID I did the following: /opt/napatech3/bin/ntpl -e \"Delete = All\" /opt/napatech3/bin/ntpl -e \"Setup[NumaNode=2] = StreamID == 0\" # Repeat this for each stream ID. In our case 0-6 /opt/napatech3/bin/ntpl -e \"HashMode = Hash2TupleSorted\" /opt/napatech3/bin/ntpl -e \"Assign[StreamId=(0..6)] = port == 0\" ./profiling # use this to see traffic being received # N is the NUMA node that the profiler detects the traffic on watch cat /proc/net/pf_ring/stats/*none* Run the following commands: n2disk -a -v -l -o /data/data0 -x $(date +%s.) -i nt:stream0 -n 5000 -m 10000 -p $((20*1024)) -b $((40*1024)) -C 4096 -c 120 -z 0,4,8,12,16,20,24,28 -Z -w 88,92,96,100,104,108,112,116 -S 32 n2disk -a -v -l -o /data/data1 -x $(date +%s.) -i nt:stream1 -n 5000 -m 10000 -p $((20*1024)) -b $((40*1024)) -C 4096 -c 121 -z 1,5,9,13,17,21,25,29 -Z -w 89,93,97,101,105,109,113,117 -S 33 n2disk -a -v -l -o /data/data2 -x $(date +%s.) -i nt:stream2 -n 5000 -m 10000 -p $((20*1024)) -b $((40*1024)) -C 4096 -c 156 -z 36,40,44,48,52,56,60,64 -Z -w 124,128,132,136,140,144,148,152 -S 68 n2disk -a -v -l -o /data/data3 -x $(date +%s.) -i nt:stream3 -n 5000 -m 10000 -p $((20*1024)) -b $((40*1024)) -C 4096 -c 157 -z 37,41,45,49,53,57,61,65 -Z -w 125,129,133,137,141,145,149,153 -S 69 n2disk -a -v -l -o /raiddata/data0 -x $(date +%s.) -i nt:stream4 -n 5000 -m 10000 -p $((20*1024)) -b $((40*1024)) -C 4096 -c 123 -z 3,7,11,15,19,23,27,31 -Z -w 91,95,99,103,107,111,115,119 -S 35 n2disk -a -v -l -o /raiddata/data1 -x $(date +%s.) -i nt:stream5 -n 5000 -m 10000 -p $((20*1024)) -b $((40*1024)) -C 4096 -c 159 -z 39,43,47,51,55,59,63,67 -Z -w 127,131,135,139,143,147,151,155 -S 71 n2disk -a -v -l -o /raiddata/data2 -x $(date +%s.) -i nt:stream6 -n 5000 -m 10000 -p $((20*1024)) -b $((40*1024)) -C 4096 -c 174 -z 54,58,62,66,70,74,78,82 -Z -w 142,146,150,154,158,162,166,170 -S 86 -a : Archive pcap file (rename to .old) instead of overwriting if already present on disk. -v : Verbose. -o : Directory where dump files will be saved (multiple -o can be specified) -x : Dump file prefix. -i : Ingress packet device. -n : Max number of nested dump sub-directories. -m : Max number of files before restarting file name. -p : Max pcap file length (MBytes). -b : Buffer length (MBytes). -C : Size (KB) of the chunk written to disk (must be multiple of 4096). Default: 64 KB. -c : Bind the reader thread to the specified core. -z : Enable multithread compression and/or indexing and bind thread(s) to the specified core ids (e.g. 0,1,2,3) (mandatory with indexing on Napatech cards) -Z : Compute index on the thread(s) used for compression (-z) instead of using the capture thread(s). -w : Bind the writer thread(s) to the specified core ids. A comma-separated list of cores (e.g. 0,1,2,3) should be specified in case of multiple dump directories (-o). -S : Enable time pulse thread (optimise sw packet timestamping) and bind it to the specified core. With this setup I was able to get 70Gb/s. I tested 100Gb/s and got drops. Did not perform further testing to narrow down exactly how much traffic I could push before moving on to the next test. Compile PF_RING Driver (PROBABLY NOT NECESSARY) NOTE There is a note in the documentation saying that installing from repository comes with NapaTech support. Move to opt run git clone https://github.com/ntop/PF_RING.git Move into the directory and run cd PF_RING/kernel && make && sudo insmod pf_ring.ko Next run cd ../userland/lib && ./configure && make Next run cd ../libpcap && ./configure && make Next run cd ../examples && make","title":"Useful Materials"},{"location":"High%20Speed%20Packet%20Capture/ntop%20on%20R940%20with%20Napatech/#useful-materials","text":"NapaTech Installation Instructions (Creatinga EXT4 Filesystem)[https://thelastmaimou.wordpress.com/2013/05/04/magic-soup-ext4-with-ssd-stripes-and-strides/]","title":"Useful Materials"},{"location":"High%20Speed%20Packet%20Capture/ntop%20on%20R940%20with%20Napatech/#my-environment","text":"I am running a Dell FC640 on a Dell R940 See Server Specs for hardware details.","title":"My Environment"},{"location":"High%20Speed%20Packet%20Capture/ntop%20on%20R940%20with%20Napatech/#hard-drive-layout","text":"I had a RAID of 12 SAS SSDs in RAID0 on the PERC740. I had 7 NVMe drives I used. I couldn't get the 8th NVMe drive working. [root@r940 /]# lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT sda 8:0 0 10.5T 0 disk /raiddata sdb 8:16 0 223.5G 0 disk \u251c\u2500sdb1 8:17 0 200M 0 part /boot/efi \u251c\u2500sdb2 8:18 0 1G 0 part /boot \u2514\u2500sdb3 8:19 0 222.3G 0 part \u251c\u2500centos-root 253:0 0 218.3G 0 lvm / \u2514\u2500centos-swap 253:1 0 4G 0 lvm [SWAP] sdc 8:32 0 59.8G 0 disk nvme0n1 259:6 0 1.5T 0 disk \u2514\u2500nvme0n1p1 259:8 0 1.5T 0 part \u2514\u2500data-data 253:2 0 10.2T 0 lvm /data nvme1n1 259:0 0 1.5T 0 disk \u2514\u2500nvme1n1p1 259:1 0 1.5T 0 part \u2514\u2500data-data 253:2 0 10.2T 0 lvm /data nvme2n1 259:2 0 1.5T 0 disk \u2514\u2500nvme2n1p1 259:3 0 1.5T 0 part \u2514\u2500data-data 253:2 0 10.2T 0 lvm /data nvme3n1 259:4 0 1.5T 0 disk \u2514\u2500nvme3n1p1 259:5 0 1.5T 0 part \u2514\u2500data-data 253:2 0 10.2T 0 lvm /data nvme4n1 259:10 0 1.5T 0 disk \u2514\u2500nvme4n1p1 259:13 0 1.5T 0 part \u2514\u2500data-data 253:2 0 10.2T 0 lvm /data nvme5n1 259:11 0 1.5T 0 disk nvme6n1 259:7 0 1.5T 0 disk \u2514\u2500nvme6n1p1 259:9 0 1.5T 0 part \u2514\u2500data-data 253:2 0 10.2T 0 lvm /data nvme7n1 259:12 0 1.5T 0 disk \u2514\u2500nvme7n1p1 259:14 0 1.5T 0 part \u2514\u2500data-data 253:2 0 10.2T 0 lvm /data [root@r940 /]# pvs PV VG Fmt Attr PSize PFree /dev/nvme0n1p1 data lvm2 a-- <1.46t 0 /dev/nvme1n1p1 data lvm2 a-- <1.46t 0 /dev/nvme2n1p1 data lvm2 a-- <1.46t 0 /dev/nvme3n1p1 data lvm2 a-- <1.46t 0 /dev/nvme4n1p1 data lvm2 a-- <1.46t 0 /dev/nvme6n1p1 data lvm2 a-- <1.46t 0 /dev/nvme7n1p1 data lvm2 a-- <1.46t 0 /dev/sdb3 centos lvm2 a-- <222.31g 0 [root@r940 /]# vgs VG #PV #LV #SN Attr VSize VFree centos 1 2 0 wz--n- <222.31g 0 data 7 1 0 wz--n- <10.19t 0 [root@r940 /]# lvs LV VG Attr LSize Pool Origin Data% Meta% Move Log Cpy%Sync Convert root centos -wi-ao---- <218.31g swap centos -wi-ao---- 4.00g data data -wi-ao---- <10.19t WARNING: fdisk GPT support is currently new, and therefore in an experimental phase. Use at your own discretion. Disk /dev/nvme1n1: 1600.3 GB, 1600321314816 bytes, 3125627568 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk label type: gpt Disk identifier: 759A1CF7-125F-469B-981E-149EBDBE3456 # Start End Size Type Name 1 2048 3125626879 1.5T Linux LVM WARNING: fdisk GPT support is currently new, and therefore in an experimental phase. Use at your own discretion. Disk /dev/nvme2n1: 1600.3 GB, 1600321314816 bytes, 3125627568 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk label type: gpt Disk identifier: 57898F4D-5B5E-4495-B695-E48EA3FCFA01 # Start End Size Type Name 1 2048 3125626879 1.5T Linux LVM WARNING: fdisk GPT support is currently new, and therefore in an experimental phase. Use at your own discretion. Disk /dev/nvme3n1: 1600.3 GB, 1600321314816 bytes, 3125627568 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk label type: gpt Disk identifier: DB46E8E3-5E96-4228-A148-E6C8F0187DF4 # Start End Size Type Name 1 2048 3125626879 1.5T Linux LVM WARNING: fdisk GPT support is currently new, and therefore in an experimental phase. Use at your own discretion. Disk /dev/nvme0n1: 1600.3 GB, 1600321314816 bytes, 3125627568 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk label type: gpt Disk identifier: 4066CDE1-E36E-41FB-8277-5E3FFDB55B4A # Start End Size Type Name 1 2048 3125626879 1.5T Linux LVM WARNING: fdisk GPT support is currently new, and therefore in an experimental phase. Use at your own discretion. Disk /dev/nvme6n1: 1600.3 GB, 1600321314816 bytes, 3125627568 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk label type: gpt Disk identifier: 0360D46A-9A41-4A8D-A449-94CCDC01FA8B # Start End Size Type Name 1 2048 3125626879 1.5T Linux LVM WARNING: fdisk GPT support is currently new, and therefore in an experimental phase. Use at your own discretion. Disk /dev/nvme4n1: 1600.3 GB, 1600321314816 bytes, 3125627568 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk label type: gpt Disk identifier: A5D3EDDF-C93D-4B33-92B0-D25B009EEB9D # Start End Size Type Name 1 2048 3125626879 1.5T Linux LVM Disk /dev/nvme5n1: 1600.3 GB, 1600321314816 bytes, 3125627568 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes WARNING: fdisk GPT support is currently new, and therefore in an experimental phase. Use at your own discretion. Disk /dev/nvme7n1: 1600.3 GB, 1600321314816 bytes, 3125627568 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk label type: gpt Disk identifier: 71A0B583-E727-45B6-A1AB-791D341709B6 # Start End Size Type Name 1 2048 3125626879 1.5T Linux LVM Disk /dev/sda: 11515.9 GB, 11515881062400 bytes, 22491955200 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 1048576 bytes / 1048576 bytes WARNING: fdisk GPT support is currently new, and therefore in an experimental phase. Use at your own discretion. Disk /dev/sdb: 240.0 GB, 239990276096 bytes, 468731008 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 4096 bytes I/O size (minimum/optimal): 4096 bytes / 4096 bytes Disk label type: gpt Disk identifier: F1E6A9A1-C85F-46C1-A27E-DBC41C9260AC # Start End Size Type Name 1 2048 411647 200M EFI System EFI System Partition 2 411648 2508799 1G Microsoft basic 3 2508800 468729855 222.3G Linux LVM Disk /dev/sdc: 64.2 GB, 64239960064 bytes, 125468672 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk /dev/mapper/centos-root: 234.4 GB, 234407067648 bytes, 457826304 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 4096 bytes I/O size (minimum/optimal): 4096 bytes / 4096 bytes Disk /dev/mapper/centos-swap: 4294 MB, 4294967296 bytes, 8388608 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 4096 bytes I/O size (minimum/optimal): 4096 bytes / 4096 bytes Disk /dev/mapper/data-data: 11202.2 GB, 11202210037760 bytes, 21879316480 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 131072 bytes / 917504 bytes","title":"Hard Drive Layout:"},{"location":"High%20Speed%20Packet%20Capture/ntop%20on%20R940%20with%20Napatech/#cpu-layout","text":"[root@r940 data]# lscpu Architecture: x86_64 CPU op-mode(s): 32-bit, 64-bit Byte Order: Little Endian CPU(s): 176 On-line CPU(s) list: 0-175 Thread(s) per core: 2 Core(s) per socket: 22 Socket(s): 4 NUMA node(s): 4 Vendor ID: GenuineIntel CPU family: 6 Model: 85 Model name: Intel(R) Xeon(R) Gold 6152 CPU @ 2.10GHz Stepping: 4 CPU MHz: 1394.146 CPU max MHz: 3700.0000 CPU min MHz: 1000.0000 BogoMIPS: 4200.00 Virtualization: VT-x L1d cache: 32K L1i cache: 32K L2 cache: 1024K L3 cache: 30976K NUMA node0 CPU(s): 0,4,8,12,16,20,24,28,32,36,40,44,48,52,56,60,64,68,72,76,80,84,88,92,96,100,104,108,112,116,120,124,128,132,136,140,144,148,152,156,160,164,168,172 NUMA node1 CPU(s): 1,5,9,13,17,21,25,29,33,37,41,45,49,53,57,61,65,69,73,77,81,85,89,93,97,101,105,109,113,117,121,125,129,133,137,141,145,149,153,157,161,165,169,173 NUMA node2 CPU(s): 2,6,10,14,18,22,26,30,34,38,42,46,50,54,58,62,66,70,74,78,82,86,90,94,98,102,106,110,114,118,122,126,130,134,138,142,146,150,154,158,162,166,170,174 NUMA node3 CPU(s): 3,7,11,15,19,23,27,31,35,39,43,47,51,55,59,63,67,71,75,79,83,87,91,95,99,103,107,111,115,119,123,127,131,135,139,143,147,151,155,159,163,167,171,175 Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 invpcid_single intel_ppin intel_pt ssbd mba ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm mpx rdt_a avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts pku ospke md_clear spec_ctrl intel_stibp flush_l1d [root@r940 data]# cpu_layout.py ====================================================================== Core and Socket Information (as reported by '/sys/devices/system/cpu') ====================================================================== cores = [0, 5, 1, 4, 2, 3, 8, 12, 9, 11, 10, 21, 16, 20, 17, 19, 18, 28, 24, 27, 25, 26] sockets = [0, 1, 2, 3] Socket 0 Socket 1 Socket 2 Socket 3 -------- -------- -------- -------- Core 0 [0, 88] [1, 89] [2, 90] [3, 91] Core 5 [4, 92] [5, 93] [6, 94] [7, 95] Core 1 [8, 96] [9, 97] [10, 98] [11, 99] Core 4 [12, 100] [13, 101] [14, 102] [15, 103] Core 2 [16, 104] [17, 105] [18, 106] [19, 107] Core 3 [20, 108] [21, 109] [22, 110] [23, 111] Core 8 [24, 112] [25, 113] [26, 114] [27, 115] Core 12 [28, 116] [29, 117] [30, 118] [31, 119] Core 9 [32, 120] [33, 121] [34, 122] [35, 123] Core 11 [36, 124] [37, 125] [38, 126] [39, 127] Core 10 [40, 128] [41, 129] [42, 130] [43, 131] Core 21 [44, 132] [45, 133] [46, 134] [47, 135] Core 16 [48, 136] [49, 137] [50, 138] [51, 139] Core 20 [52, 140] [53, 141] [54, 142] [55, 143] Core 17 [56, 144] [57, 145] [58, 146] [59, 147] Core 19 [60, 148] [61, 149] [62, 150] [63, 151] Core 18 [64, 152] [65, 153] [66, 154] [67, 155] Core 28 [68, 156] [69, 157] [70, 158] [71, 159] Core 24 [72, 160] [73, 161] [74, 162] [75, 163] Core 27 [76, 164] [77, 165] [78, 166] [79, 167] Core 25 [80, 168] [81, 169] [82, 170] [83, 171] Core 26 [84, 172] [85, 173] [86, 174] [87, 175]","title":"CPU Layout"},{"location":"High%20Speed%20Packet%20Capture/ntop%20on%20R940%20with%20Napatech/#centos-7-release-info","text":"NAME=\"CentOS Linux\" VERSION=\"7 (Core)\" ID=\"centos\" ID_LIKE=\"rhel fedora\" VERSION_ID=\"7\" PRETTY_NAME=\"CentOS Linux 7 (Core)\" ANSI_COLOR=\"0;31\" CPE_NAME=\"cpe:/o:centos:centos:7\" HOME_URL=\"https://www.centos.org/\" BUG_REPORT_URL=\"https://bugs.centos.org/\" CENTOS_MANTISBT_PROJECT=\"CentOS-7\" CENTOS_MANTISBT_PROJECT_VERSION=\"7\" REDHAT_SUPPORT_PRODUCT=\"centos\" REDHAT_SUPPORT_PRODUCT_VERSION=\"7\" CentOS Linux release 7.7.1908 (Core) CentOS Linux release 7.7.1908 (Core)","title":"CentOS 7 Release Info"},{"location":"High%20Speed%20Packet%20Capture/ntop%20on%20R940%20with%20Napatech/#kernel-info","text":"Linux r940.lan 3.10.0-1062.4.3.el7.x86_64 #1 SMP Wed Nov 13 23:58:53 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux","title":"Kernel Info"},{"location":"High%20Speed%20Packet%20Capture/ntop%20on%20R940%20with%20Napatech/#format-your-data-partitions","text":"Create physical volumes and volume groups with: pvcreate <NVMe drive> vgcreate data <List of NVMe Drives> NOTE: I only did this on the NVMe drives. To create the logical volume I used: lvcreate -l 100%FREE -i7 -I128 -n data data To format the drive I used: mkfs.ext4 -F -b 4096 -E discard,stride=16,stripe-width=256 # This was the 12 SAS SSDs I had in a RAID mkfs.ext4 -F -b 4096 -E discard,stride=16,stripe-width=256 /dev/mapper/data-data # This was the NVMes I tied together with LVM mkfs.ext4 -F -b 4096 -E discard,stride=16,stripe-width=256 /dev/mapper/data2-data2 mount -o rw,auto,discard /dev/mapper/data-data /data mount -o rw,auto,discard /dev/sda /raiddata/ mount -o rw,auto,discard /dev/mapper/data2-data2 /data2 echo noop > /sys/block/sda/queue/scheduler","title":"Format Your Data Partitions"},{"location":"High%20Speed%20Packet%20Capture/ntop%20on%20R940%20with%20Napatech/#install-napatech-driver","text":"","title":"Install NapaTech Driver"},{"location":"High%20Speed%20Packet%20Capture/ntop%20on%20R940%20with%20Napatech/#install-driver","text":"Download the Napatech software from here Run yum groupinstall \"Development Tools\" && yum install -y kernel-devel wget gettext-devel openssl-devel perl-CPAN perl-devel zlib-devel pciutils && yum install -y https://centos7.iuscommunity.org/ius-release.rpm && yum remove -y git && yum install -y git2u-all Unzip and run package_install_3gd.sh Run the following commands: /opt/napatech3/bin/ntload.sh /opt/napatech3/bin/ntstart.sh","title":"Install Driver"},{"location":"High%20Speed%20Packet%20Capture/ntop%20on%20R940%20with%20Napatech/#install-n2disk-and-ntop","text":"","title":"Install n2disk and ntop"},{"location":"High%20Speed%20Packet%20Capture/ntop%20on%20R940%20with%20Napatech/#perform-installation","text":"Install epel with yum install -y epel-release Erase the zeromq3 package with yum erase zeromq && yum clean all && yum update -y && reboot Pull ntop repo with wget http://packages.ntop.org/centos-stable/ntop.repo -O /etc/yum.repos.d/ntop.repo Install required packages with yum install -y pfring-dkms pfring n2disk nprobe ntopng ntopng-data cento pfring-drivers-zc-dkms redis hiredis-devel","title":"Perform Installation"},{"location":"High%20Speed%20Packet%20Capture/ntop%20on%20R940%20with%20Napatech/#configure-n2disk","text":"Create backup of the the NapaTech ini file cp /opt/napatech3/config/ntservice.ini /opt/napatech3/config/ntservice.ini.bak Update /opt/napatech3/config/ntservice.ini with the following values: TimestampFormat = PCAP_NS PacketDescriptor = PCAP HostBufferSegmentSizeRx = 4 TODO change these lines see notes for help HostBuffersRx = [16,16,0],[16,16,1] HostBuffersTx = [16,16,0],[16,16,1] You will need to start and stop the ntservice for these changes to take effect with: /opt/napatech3/bin/ntstop.sh /opt/napatech3/bin/ntstart.sh","title":"Configure n2disk"},{"location":"High%20Speed%20Packet%20Capture/ntop%20on%20R940%20with%20Napatech/#perform-configuration-zero-copy-driver","text":"Edit the pfring configuration file with vim /etc/pf_ring/interfaces.conf and add your configuration. MANAGEMENT_INTERFACES=\"em1\" CAPTURE_INTERFACES=\"nt:0\" Open the file /etc/ntopng/ntopng.conf . If you do not have a license add --community to the end Configure the firewall to accept connections to ntopng with: firewall-cmd --zone=public --permanent --add-port=3000/tcp && firewall-cmd --reload Enable and start services with: systemctl enable redis.service systemctl enable ntopng.service systemctl enable pf_ring systemctl start redis.service systemctl start ntopng.service systemctl start pf_ring Make sure the services are running correctly with: systemctl status redis.service systemctl status ntopng.service systemctl status pf_ring","title":"Perform Configuration Zero Copy Driver"},{"location":"High%20Speed%20Packet%20Capture/ntop%20on%20R940%20with%20Napatech/#configure-license","text":"Run zcount -i nt:0 and note the serial number Output n2disk license to /etc/n2disk.license Output ntopng license to /etc/ntopng.license","title":"Configure License"},{"location":"High%20Speed%20Packet%20Capture/ntop%20on%20R940%20with%20Napatech/#useful-tips","text":"","title":"Useful Tips"},{"location":"High%20Speed%20Packet%20Capture/ntop%20on%20R940%20with%20Napatech/#hardware-filtering","text":"Napatech NICs support full-blown hardware filtering out of the box. Thanks to nBPF we convert BPF expressions to hardware filters. This feature is supported transparently, and thus all PF_RING/libpcap-over-PF_RING can benefit from it. Example: pfcount -i nt:3 -f \"tcp and port 80 and src host 192.168.1.1\"","title":"Hardware Filtering"},{"location":"High%20Speed%20Packet%20Capture/ntop%20on%20R940%20with%20Napatech/#hostbuffer-notes","text":"HostBuffersRx = [16,2048,0],[16,2048,1],[16,2048,2],[16,2048,3] HostBuffersTx = [16,2048,0],[16,2048,1],[16,2048,2],[16,2048,3] First number is the number of host buffers Second number is the size of the host buffers Third number is the NUMA node You have to have one set of numbers for each NUMA node.","title":"Hostbuffer Notes"},{"location":"High%20Speed%20Packet%20Capture/ntop%20on%20R940%20with%20Napatech/#testing-transmit-speed","text":"./pktgen -p 1 -r 10G","title":"Testing Transmit Speed"},{"location":"High%20Speed%20Packet%20Capture/ntop%20on%20R940%20with%20Napatech/#testing-the-pcap-transmit-speed","text":"To test to see if the Napatech card is up and running run this command: ./pfcount -i nt:0 ./monitoring You can press t to switch stats between receive and transmit.","title":"Testing the PCAP transmit speed"},{"location":"High%20Speed%20Packet%20Capture/ntop%20on%20R940%20with%20Napatech/#testing-receive","text":"chmod -R 777 /data rm /tmp/*-none\\.* 2>/dev/null; while true; do grep 'Dropped:\\|Slow.*:' -C50 /proc/net/pf_ring/stats/* 2>/dev/null; cp /proc/net/pf_ring/stats/*none* /tmp 2>/dev/null; sleep 1; done","title":"Testing Receive"},{"location":"High%20Speed%20Packet%20Capture/ntop%20on%20R940%20with%20Napatech/#numa-lookup","text":"Run lscpu","title":"NUMA Lookup"},{"location":"High%20Speed%20Packet%20Capture/ntop%20on%20R940%20with%20Napatech/#things-we-tried","text":"We used the to list the cpu layout and determine where we wanted to run what threads. Tests 1-6 were with the default settings for a RAID0 partition on Linux at setup time.","title":"Things we tried:"},{"location":"High%20Speed%20Packet%20Capture/ntop%20on%20R940%20with%20Napatech/#test-1","text":"This is running at 100Gb/s generation n2disk -a -v -l -o /<Storage path> -x $(date +%s.) -i nt:0 This seemed to dump everything to one thread. We maxed out and had ~82% packet loss.","title":"Test 1"},{"location":"High%20Speed%20Packet%20Capture/ntop%20on%20R940%20with%20Napatech/#test-2","text":"This is running at 100Gb/s generation n2disk -a -v -l -o /data/ -x $(date +%s.) -i nt:0 -n 5000 -m 10000 -p $((4*1024)) -b $((12*1024)) -C 4096 -c 32 -z 2,3,4,5,6,7 -Z -w 16,17,18,19,20,20,21 -S 22 Throughput results: [root@r940 bin]# cat /proc/net/pf_ring/stats/*none* Duration: 0:00:00:53:022 Throughput: 2.12 Mpps 17.65 Gbps Packets: 78780625 Filtered: 78780625 Dropped: 518512807 Bytes: 81931850000 DumpedBytes: 71137406724 DumpedFiles: 17 SlowSlavesLoops: 0 SlowStorageLoops: 432580 CaptureLoops: 19532 FirstDumpedEpoch: 0 LastDumpedEpoch: 1574185087 Worked better but we got a warning saying the time thread was on a different core than the reader/writer threads.","title":"Test 2"},{"location":"High%20Speed%20Packet%20Capture/ntop%20on%20R940%20with%20Napatech/#test-3","text":"This is running at 100Gb/s generation n2disk -a -v -l -o /data/ -x $(date +%s.) -i nt:0 -n 5000 -m 10000 -p $((4*1024)) -b $((12*1024)) -C 4096 -c 32 -z 2,3,4,5,6,7 -Z -w 16,17,18,19,20,20,21 -S 56 Worked better: 19/Nov/2019 12:41:59 [n2disk.c:1109] Average Capture Throughput: 22.49 Gbit / 2.69 Mpps","title":"Test 3"},{"location":"High%20Speed%20Packet%20Capture/ntop%20on%20R940%20with%20Napatech/#test-4","text":"This is running at 100Gb/s generation n2disk -a -v -l -o /data/ -x $(date +%s.) -i nt:0 -n 5000 -m 10000 -p $((4*1024)) -b $((12*1024)) -C 4096 -c 0 -z 8,12,16,20,24,28,32,26,40,44,48,52,56,60,64,68,72,76,80,84,88 -Z -w 92,96,100,104,108,112,116,120,124,128,132,136,140,144,148,152,156,160,164,168,172 -S 4","title":"Test 4"},{"location":"High%20Speed%20Packet%20Capture/ntop%20on%20R940%20with%20Napatech/#test-5","text":"This is running at 20Gb/s generation n2disk -a -v -l -o /data/ -x $(date +%s.) -i nt:0 -n 5000 -m 10000 -p $((4*1024)) -b $((12*1024)) -C 4096 -c 0 -z 8,12,16,20,24,28,32,26,40,44,48,52,56,60,64,68,72,76,80,84,88 -Z -w 92,96,100,104,108,112,116,120,124,128,132,136,140,144,148,152,156,160,164,168,172 -S 4 Failed. We got a lot of packet loss.","title":"Test 5"},{"location":"High%20Speed%20Packet%20Capture/ntop%20on%20R940%20with%20Napatech/#test-6","text":"This is running at 10Gb/s generation n2disk -a -v -l -o /data/ -x $(date +%s.) -i nt:0 -n 5000 -m 10000 -p $((4*1024)) -b $((12*1024)) -C 4096 -c 0 -z 8,12,16,20,24,28,32,26,40,44,48,52,56,60,64,68,72,76,80,84,88 -Z -w 92,96,100,104,108,112,116,120,124,128,132,136,140,144,148,152,156,160,164,168,172 -S 4 No packet drops.","title":"Test 6"},{"location":"High%20Speed%20Packet%20Capture/ntop%20on%20R940%20with%20Napatech/#test-7","text":"This is running at 20Gb/s generation n2disk -a -v -l -o /data/ -x $(date +%s.) -i nt:0 -n 5000 -m 10000 -p $((4*1024)) -b $((12*1024)) -C 4096 -c 0 -z 8,12,16,20,24,28,32,26,40,44,48,52,56,60,64,68,72,76,80,84,88 -Z -w 92,96,100,104,108,112,116,120,124,128,132,136,140,144,148,152,156,160,164,168,172 -S 4 This was with a data partition formatting according to the above.","title":"Test 7"},{"location":"High%20Speed%20Packet%20Capture/ntop%20on%20R940%20with%20Napatech/#test-8","text":"/opt/napatech3/bin/ntpl -e \"Delete = All\" /opt/napatech3/bin/ntpl -e \"Setup[NumaNode=2] = StreamID == 0\" # Repeat this for each stream ID. In our case 0-4 ./profiling # use this to see traffic being received # N is the NUMA node that the profiler detects the traffic on /opt/napatech3/bin/ntpl -e \"HashMode = Hash2TupleSorted\" /opt/napatech3/bin/ntpl -e \"Assign[StreamId=(0..3)] = port == 0\" The problem with this was that we couldn't get n2disk to listen on multiple interfaces. It also didn't really give us a way to split the traffic across multiple reader threads.","title":"Test 8"},{"location":"High%20Speed%20Packet%20Capture/ntop%20on%20R940%20with%20Napatech/#test-9","text":"I made four directories, one for each n2disk process I did the following: /opt/napatech3/bin/ntpl -e \"Delete = All\" /opt/napatech3/bin/ntpl -e \"Setup[NumaNode=2] = StreamID == 0\" # Repeat this for each stream ID. In our case 0-4 /opt/napatech3/bin/ntpl -e \"HashMode = Hash2TupleSorted\" /opt/napatech3/bin/ntpl -e \"Assign[StreamId=(0..3)] = port == 0\" ./profiling # use this to see traffic being received # N is the NUMA node that the profiler detects the traffic on Run the following commands: n2disk -a -v -l -o /data/data0 -x $(date +%s.) -i nt:stream0 -n 5000 -m 10000 -p $((20*1024)) -b $((40*1024)) -C 4096 -c 120 -z 0,4,8,12,16,20,24,28 -Z -w 88,92,96,100,104,108,112,116 -S 32 n2disk -a -v -l -o /data/data1 -x $(date +%s.) -i nt:stream1 -n 5000 -m 10000 -p $((20*1024)) -b $((40*1024)) -C 4096 -c 121 -z 1,5,9,13,17,21,25,29 -Z -w 89,93,97,101,105,109,113,117 -S 33 n2disk -a -v -l -o /data/data2 -x $(date +%s.) -i nt:stream2 -n 5000 -m 10000 -p $((20*1024)) -b $((40*1024)) -C 4096 -c 156 -z 36,40,44,48,52,56,60,64 -Z -w 124,128,132,136,140,144,148,152 -S 68 n2disk -a -v -l -o /data/data3 -x $(date +%s.) -i nt:stream3 -n 5000 -m 10000 -p $((20*1024)) -b $((40*1024)) -C 4096 -c 157 -z 37,41,45,49,53,57,61,65 -Z -w 125,129,133,137,141,145,149,153 -S 69 -a : Archive pcap file (rename to .old) instead of overwriting if already present on disk. -v : Verbose. -o : Directory where dump files will be saved (multiple -o can be specified) -x : Dump file prefix. -i : Ingress packet device. -n : Max number of nested dump sub-directories. -m : Max number of files before restarting file name. -p : Max pcap file length (MBytes). -b : Buffer length (MBytes). -C : Size (KB) of the chunk written to disk (must be multiple of 4096). Default: 64 KB. -c : Bind the reader thread to the specified core. -z : Enable multithread compression and/or indexing and bind thread(s) to the specified core ids (e.g. 0,1,2,3) (mandatory with indexing on Napatech cards) -Z : Compute index on the thread(s) used for compression (-z) instead of using the capture thread(s). -w : Bind the writer thread(s) to the specified core ids. A comma-separated list of cores (e.g. 0,1,2,3) should be specified in case of multiple dump directories (-o). -S : Enable time pulse thread (optimise sw packet timestamping) and bind it to the specified core. With this setup I was able to get 70Gb/s. I tested 100Gb/s and got drops. Did not perform further testing to narrow down exactly how much traffic I could push before moving on to the next test.","title":"Test 9"},{"location":"High%20Speed%20Packet%20Capture/ntop%20on%20R940%20with%20Napatech/#test-10","text":"I made seven directories, one for each n2disk process. 4 assigned to the NVMe drives and 3 assigned to the SAS SSD RAID I did the following: /opt/napatech3/bin/ntpl -e \"Delete = All\" /opt/napatech3/bin/ntpl -e \"Setup[NumaNode=2] = StreamID == 0\" # Repeat this for each stream ID. In our case 0-6 /opt/napatech3/bin/ntpl -e \"HashMode = Hash2TupleSorted\" /opt/napatech3/bin/ntpl -e \"Assign[StreamId=(0..6)] = port == 0\" ./profiling # use this to see traffic being received # N is the NUMA node that the profiler detects the traffic on watch cat /proc/net/pf_ring/stats/*none* Run the following commands: n2disk -a -v -l -o /data/data0 -x $(date +%s.) -i nt:stream0 -n 5000 -m 10000 -p $((20*1024)) -b $((40*1024)) -C 4096 -c 120 -z 0,4,8,12,16,20,24,28 -Z -w 88,92,96,100,104,108,112,116 -S 32 n2disk -a -v -l -o /data/data1 -x $(date +%s.) -i nt:stream1 -n 5000 -m 10000 -p $((20*1024)) -b $((40*1024)) -C 4096 -c 121 -z 1,5,9,13,17,21,25,29 -Z -w 89,93,97,101,105,109,113,117 -S 33 n2disk -a -v -l -o /data/data2 -x $(date +%s.) -i nt:stream2 -n 5000 -m 10000 -p $((20*1024)) -b $((40*1024)) -C 4096 -c 156 -z 36,40,44,48,52,56,60,64 -Z -w 124,128,132,136,140,144,148,152 -S 68 n2disk -a -v -l -o /data/data3 -x $(date +%s.) -i nt:stream3 -n 5000 -m 10000 -p $((20*1024)) -b $((40*1024)) -C 4096 -c 157 -z 37,41,45,49,53,57,61,65 -Z -w 125,129,133,137,141,145,149,153 -S 69 n2disk -a -v -l -o /raiddata/data0 -x $(date +%s.) -i nt:stream4 -n 5000 -m 10000 -p $((20*1024)) -b $((40*1024)) -C 4096 -c 123 -z 3,7,11,15,19,23,27,31 -Z -w 91,95,99,103,107,111,115,119 -S 35 n2disk -a -v -l -o /raiddata/data1 -x $(date +%s.) -i nt:stream5 -n 5000 -m 10000 -p $((20*1024)) -b $((40*1024)) -C 4096 -c 159 -z 39,43,47,51,55,59,63,67 -Z -w 127,131,135,139,143,147,151,155 -S 71 n2disk -a -v -l -o /raiddata/data2 -x $(date +%s.) -i nt:stream6 -n 5000 -m 10000 -p $((20*1024)) -b $((40*1024)) -C 4096 -c 174 -z 54,58,62,66,70,74,78,82 -Z -w 142,146,150,154,158,162,166,170 -S 86 -a : Archive pcap file (rename to .old) instead of overwriting if already present on disk. -v : Verbose. -o : Directory where dump files will be saved (multiple -o can be specified) -x : Dump file prefix. -i : Ingress packet device. -n : Max number of nested dump sub-directories. -m : Max number of files before restarting file name. -p : Max pcap file length (MBytes). -b : Buffer length (MBytes). -C : Size (KB) of the chunk written to disk (must be multiple of 4096). Default: 64 KB. -c : Bind the reader thread to the specified core. -z : Enable multithread compression and/or indexing and bind thread(s) to the specified core ids (e.g. 0,1,2,3) (mandatory with indexing on Napatech cards) -Z : Compute index on the thread(s) used for compression (-z) instead of using the capture thread(s). -w : Bind the writer thread(s) to the specified core ids. A comma-separated list of cores (e.g. 0,1,2,3) should be specified in case of multiple dump directories (-o). -S : Enable time pulse thread (optimise sw packet timestamping) and bind it to the specified core. With this setup I was able to get 70Gb/s. I tested 100Gb/s and got drops. Did not perform further testing to narrow down exactly how much traffic I could push before moving on to the next test.","title":"Test 10"},{"location":"High%20Speed%20Packet%20Capture/ntop%20on%20R940%20with%20Napatech/#compile-pf_ring-driver-probably-not-necessary","text":"NOTE There is a note in the documentation saying that installing from repository comes with NapaTech support. Move to opt run git clone https://github.com/ntop/PF_RING.git Move into the directory and run cd PF_RING/kernel && make && sudo insmod pf_ring.ko Next run cd ../userland/lib && ./configure && make Next run cd ../libpcap && ./configure && make Next run cd ../examples && make","title":"Compile PF_RING Driver (PROBABLY NOT NECESSARY)"},{"location":"How%20Bitcoin-Blockchain%20Works%20-%20Notes/","text":"How Bitcoin-Blockchain Works - Notes How Bitcoin-Blockchain Works - Notes Helpful Resources Azure Solution with Active Directory Bitcoin Unspent Transaction Output (UTXO) Blockchain What does each block have What is a permissionless blockchain? Consensus Algorithm What is a permissions blockchain? What is a block Genesis Block Creating a new transaction Khan Academy Notes Bitcoin What is It Bitcoin: Overview Bitcoin: Cryptographic Hash Functions Bitcoin: Digital Signatures Bitcoin: Transaction records Bitcoin: Proof of Work Bitcoin: Transaction Block Chains Bitcoin: The Bitcoin Money Supply Bitcoin: The security of transaction block chains Using Blockchain for Voting How the Keys are Generated Helpful Resources IBM Lecture: https://mediacenter.ibm.com/media/Blockchain%20Explained/1_e34h0ey8 The Bitcoin Protocol Explained: How it Actually Works: https://komodoplatform.com/en/academy/bitcoin-protocol/ Blockchain Learning Resources: https://github.com/mikeroyal/Blockchain-Guide Demo of Azure Solution: https://learn.microsoft.com/en-us/shows/azure-friday/issue-and-accept-verifiable-credentials-using-azure-active-directory?culture=en-us&country=US OpenID Connect: https://openid.net/connect/ Khan Academy Video Series: https://www.khanacademy.org/economics-finance-domain/core-finance/money-and-banking/bitcoin/v/bitcoin-what-is-it Follow My Vote (Blockchain used for voting): https://github.com/FollowMyVote Why Using Bitcoin/Blockchain for Voting is a Bad Idea: https://www.coindesk.com/tech/2020/11/16/new-mit-paper-roundly-rejects-blockchain-voting-as-solution-to-election-woes/ MIT Paper on Why Bitcoin/Blockchain is Bad for Voting: link Azure Solution with Active Directory See https://learn.microsoft.com/en-us/shows/azure-friday/issue-and-accept-verifiable-credentials-using-azure-active-directory?culture=en-us&country=US Under the hood, when a 3rd party attempts to validate a credential it is using OpenID connect. Microsoft created a client API where a single API call will allow you to validate user credentials. The return URL shown above is what is encoded in the QR code the person scans. The way this will work with an app is that the app (like authenticator in this case) can register itself as the protocol handler so whenever someone scans something with their camera it will automatically open with that app. Bitcoin Unspent Transaction Output (UTXO) See: https://komodoplatform.com/en/academy/bitcoin-protocol/ Blockchain What does each block have A hash A list of transactions that have occurred on that block Previous block's hash What is a permissionless blockchain? These are what is used by cryptocurrencies. Everyone can see all transactions that have ever taken place. You will see the transactions by each person's address. Any time it gets updated and new transactions are made you get a new block. Consensus Algorithm You have all these transactions coming in how do you decide which transactions will make up the next block. A client will first submit a transaction and that transaction will join a list of other transactions that have been made on the network. A node will start picking up the transactions, look through all the previous transactions on the blockchain and know those are valid. It will kind of emulate a block and then start a proof of work algorithm. The proof of work algorithm is a complex crypto-hash algorithm everyone works together to solve. Once one node resolves it, it will broadcast the position of that next block to all the other nodes in the network. What is a permissioned blockchain? There is an idea called pluggable consensus algorithms. You can use this when the nodes in the blockchain are trusted. In addition, the nodes may not be just users but entire organizations. Privacy here is important. Scenario: You have a retailer who buys 100 pounds of something at $1000. Then you have a shipper who wants $100 for shipping. The shipper should know when the order was placed, for what, and how much they charged, but they wouldn't know how much the total original cost was. Smart contracts: This allows you to do something like make sure the warehouse has enough goods at the manufacturer, the retailer has enough money to pay the shipper, and you can take automated action (like refunding money when goods aren't available) based on this information. What is a block From: https://followmyvote.com/blockchain-technology/ The blockchain further requires that an audit trail of all changes to the database is preserved, which allows anyone to audit that the database is correct at any time. This audit trail is composed to the individual changes to the database, which are called transactions. A group of transactions which were all added by a single node on its turn is called a block. Each block contains a reference to the block which preceded it, which establishes an ordering of the blocks. This is the origin of the term \u201cblockchain\u201d: it is a chain of blocks, each one containing a link to the previous block and a list of new transactions since that previous block. Genesis Block An empty block that is the beginning of the block chain. Creating a new transaction From: https://followmyvote.com/blockchain-technology/ The most obvious example of blockchain technology in use today is Bitcoin. Bitcoin is a digital currency system which uses a blockchain to keep track of ownership of the currency. Whenever someone wishes to spend their bitcoins, they create a transaction which states that they are sending a certain number of their bitcoins to someone else. Then they digitally sign this transaction to authorize it, and broadcast it to all of the nodes in the Bitcoin network. When the next node creates a block, it will check that the new transaction is valid, and include it in the new block, which is then propagated to all other nodes in the network, which adjust their databases to deduct the transferred bitcoins from the sender and credit them to the recipient. Khan Academy Notes Bitcoin What is It Bitcoin transactions can use a thick client or a 3rd party service. Bitcoin: Overview Transactions are tracked on the global ledger Transaction fees are used to motivated other nodes to validate the transaction To create the transaction, person 1 signs with private key and broadcasts the details of the transaction to other nodes How we avoid the double spending problem: We use bitcoin miners. They take all the transactions we see, take those transactions and compile them into a transaction block. This is like the entire page of a ledger block. The miners will include in the block an additional transaction which gives themselves a reward for completing this mining task. The miner will also include a proof of work (some sort of hash problem but he hasn't gotten to that) The transaction block will have a hash of the block's transactions / the hash of the previous block. This is what creates the chain. As soon as a node finishes creating the transaction block, it broadcasts it. The nodes will only consider the longest block chain - that is to say, the node with the most work which has been accomplished Bitcoin: Cryptographic Hash Functions Properties of cryptographic hash functions Computationally efficient Hard to find hash collision (colision resistant) Hide information about the original input Output should be well distributed Bitcoin: Digital Signatures Private key may also be referred to as signing key (SK) and public key can be verification key (VK) Usually you will hash then sign the message Bitcoin: Transaction records A transaction is just one party's intent to transfer some of their bitcoins to another party You can verify that a party possesses the Bitcoins they say they possess by searching for transactions showing that the party in question received that quantity of bitcoins See UTXO for a description of how transactions really work under the hood The idea here is that for each UTXO, there is a message digest of those transaction records and anyone can check those message digests and that the target was indeed the recipient In the case of a new transaction, the sender will create new transactions and then sign all of them with the private key. This will also include the transaction fees. If there is a double spend, remember that the order of transactions is included in the ledger which will prevent someone from committing a double spend to the ledger (because it would become obvious to the calculating node they have insufficient funds) Bitcoin: Proof of Work Other uses of Proof of Work: preventing DOS or SPAM In general, they begin with a challenge string (c) In the case of SPAM it might be an e-mail message In response there is a response or proof response (p) In example of the challenge might be to combine the challenge string and the response string such that a hash output of the two has a fixed number of leading zeros and then the other bits are whatever you want. If you said you want 40 bits of leading zeros this would require effectively 2^40 consecutive heads coin flips. If you want to increase the difficulty, you can simply require more leading zeros effectively doubling the effort. Someone just has to find the proof string to prove they have done the work. Bitcoin: Transaction Block Chains Nodes (Bitcoin miners) will take all these transactions they have received that have not yet been registered in the block chain. The first thing they will do is collate all these transactions. What they will do is take all the transactions they have collected and split them up into pairs. Then they will hash each pair, then take the hashes of the pairs, hash those, and so on and so forth until they have a single value. Ex: Then, they will combine that hash with the hash of the previous block in the blockchain. Then, after the node has created the new blockchain including their new block, they will take all that and hash it into the challenge as mentioned in Bitcoin: Proof of Work . Then, they must generate the proof, which when hashed will have some fixed number of leading zeros. The Bitcoin protocol is designed such that the average time to creating a new block is about every ten minutes The reward for miners is that in the first transaction block, they are allowed to insert a reward for themselves. This first transaction record's reward will change over time. These are called coinbase generation. This is how new coins are generated and placed into the system. In addition to the coinbase reward, you also get all the transaction fees. If there is a tie, then the chain with the most amount of work is selected Bitcoin: The Bitcoin Money Supply The bitcoin system is designed such that the maximum number of coins is 21,000,000 BTC The smallest possible bitcoin is .00000001 BTC called a Satoshi Bitcoin started around Jan 2009 and the first reward was 50 BTC After about 210,000 blocks the reward halves. This takes approximately 4 years. Around 2140 the entire Bitcoin supply will have been generated For every 2,016 blocks, the network estimates the amount of time it took to generate the blocks. It should take about two weeks. If it took far less time, than the protocol will make things harder and on the flip side it will make it easier This works out to about every 10 minutes per block. It may seem like all nodes are simultaneously working on many of the same transactions. While this may be true, recall that each node inserts its own transaction to reward itself with the coinbase transaction and that transaction includes information unique to the node. Subsequently each node has a different challenge string. Bitcoin: The security of transaction block chains TLDR: It's a synopsis of the other sections. You would have to fork the chain, add your own new block, plus add other blocks to make their chain longer, such that they then have the chain with the most work. Effectively, you would need at least 51% of the compute power in the network Using Blockchain for Voting From https://followmyvote.com/blockchain-technology/ Another application for blockchain technology is voting. By casting votes as transactions, we can create a blockchain which keeps track of the tallies of the votes. This way, everyone can agree on the final count because they can count the votes themselves, and because of the blockchain audit trail, they can verify that no votes were changed or removed, and no illegitimate votes were added. From: https://followmyvote.com/cryptographically-secure-voting-2/ How the Keys are Generated From: https://followmyvote.com/elliptic-curve-cryptography/ At Follow My Vote, we use this technology to create votes. During the registration process, voters create two ECC key-pairs. The voter reveals her identity to a verifier, who certifies the first key-pair (the identity key-pair) as belonging to that voter, then the voter anonymously registers her second key-pair (the voting key-pair) as belonging to one of the identity keys, but the way this is done, no one can determine which identity key owns her voting key. She can then create transactions which state her votes on the contests in an election, and use her voting private key to sign those transactions. Once these are published, everyone participating in the Follow My Vote network can verify that the signature is valid and adjust the tally accordingly. This way the votes are public and anonymous, but each voter can verify that her vote was correctly recorded and counted. Furthermore, all participants can verify that none of the votes were tampered with by validating the signatures. In this way, Follow My Vote software performs transparent, end-to-end verifiable online elections without compromising on security or voter anonymity.","title":"How Bitcoin-Blockchain Works - Notes"},{"location":"How%20Bitcoin-Blockchain%20Works%20-%20Notes/#how-bitcoin-blockchain-works-notes","text":"How Bitcoin-Blockchain Works - Notes Helpful Resources Azure Solution with Active Directory Bitcoin Unspent Transaction Output (UTXO) Blockchain What does each block have What is a permissionless blockchain? Consensus Algorithm What is a permissions blockchain? What is a block Genesis Block Creating a new transaction Khan Academy Notes Bitcoin What is It Bitcoin: Overview Bitcoin: Cryptographic Hash Functions Bitcoin: Digital Signatures Bitcoin: Transaction records Bitcoin: Proof of Work Bitcoin: Transaction Block Chains Bitcoin: The Bitcoin Money Supply Bitcoin: The security of transaction block chains Using Blockchain for Voting How the Keys are Generated","title":"How Bitcoin-Blockchain Works - Notes"},{"location":"How%20Bitcoin-Blockchain%20Works%20-%20Notes/#helpful-resources","text":"IBM Lecture: https://mediacenter.ibm.com/media/Blockchain%20Explained/1_e34h0ey8 The Bitcoin Protocol Explained: How it Actually Works: https://komodoplatform.com/en/academy/bitcoin-protocol/ Blockchain Learning Resources: https://github.com/mikeroyal/Blockchain-Guide Demo of Azure Solution: https://learn.microsoft.com/en-us/shows/azure-friday/issue-and-accept-verifiable-credentials-using-azure-active-directory?culture=en-us&country=US OpenID Connect: https://openid.net/connect/ Khan Academy Video Series: https://www.khanacademy.org/economics-finance-domain/core-finance/money-and-banking/bitcoin/v/bitcoin-what-is-it Follow My Vote (Blockchain used for voting): https://github.com/FollowMyVote Why Using Bitcoin/Blockchain for Voting is a Bad Idea: https://www.coindesk.com/tech/2020/11/16/new-mit-paper-roundly-rejects-blockchain-voting-as-solution-to-election-woes/ MIT Paper on Why Bitcoin/Blockchain is Bad for Voting: link","title":"Helpful Resources"},{"location":"How%20Bitcoin-Blockchain%20Works%20-%20Notes/#azure-solution-with-active-directory","text":"See https://learn.microsoft.com/en-us/shows/azure-friday/issue-and-accept-verifiable-credentials-using-azure-active-directory?culture=en-us&country=US Under the hood, when a 3rd party attempts to validate a credential it is using OpenID connect. Microsoft created a client API where a single API call will allow you to validate user credentials. The return URL shown above is what is encoded in the QR code the person scans. The way this will work with an app is that the app (like authenticator in this case) can register itself as the protocol handler so whenever someone scans something with their camera it will automatically open with that app.","title":"Azure Solution with Active Directory"},{"location":"How%20Bitcoin-Blockchain%20Works%20-%20Notes/#bitcoin","text":"","title":"Bitcoin"},{"location":"How%20Bitcoin-Blockchain%20Works%20-%20Notes/#unspent-transaction-output-utxo","text":"See: https://komodoplatform.com/en/academy/bitcoin-protocol/","title":"Unspent Transaction Output (UTXO)"},{"location":"How%20Bitcoin-Blockchain%20Works%20-%20Notes/#blockchain","text":"","title":"Blockchain"},{"location":"How%20Bitcoin-Blockchain%20Works%20-%20Notes/#what-does-each-block-have","text":"A hash A list of transactions that have occurred on that block Previous block's hash","title":"What does each block have"},{"location":"How%20Bitcoin-Blockchain%20Works%20-%20Notes/#what-is-a-permissionless-blockchain","text":"These are what is used by cryptocurrencies. Everyone can see all transactions that have ever taken place. You will see the transactions by each person's address. Any time it gets updated and new transactions are made you get a new block.","title":"What is a permissionless blockchain?"},{"location":"How%20Bitcoin-Blockchain%20Works%20-%20Notes/#consensus-algorithm","text":"You have all these transactions coming in how do you decide which transactions will make up the next block. A client will first submit a transaction and that transaction will join a list of other transactions that have been made on the network. A node will start picking up the transactions, look through all the previous transactions on the blockchain and know those are valid. It will kind of emulate a block and then start a proof of work algorithm. The proof of work algorithm is a complex crypto-hash algorithm everyone works together to solve. Once one node resolves it, it will broadcast the position of that next block to all the other nodes in the network.","title":"Consensus Algorithm"},{"location":"How%20Bitcoin-Blockchain%20Works%20-%20Notes/#what-is-a-permissioned-blockchain","text":"There is an idea called pluggable consensus algorithms. You can use this when the nodes in the blockchain are trusted. In addition, the nodes may not be just users but entire organizations. Privacy here is important. Scenario: You have a retailer who buys 100 pounds of something at $1000. Then you have a shipper who wants $100 for shipping. The shipper should know when the order was placed, for what, and how much they charged, but they wouldn't know how much the total original cost was. Smart contracts: This allows you to do something like make sure the warehouse has enough goods at the manufacturer, the retailer has enough money to pay the shipper, and you can take automated action (like refunding money when goods aren't available) based on this information.","title":"What is a permissioned blockchain?"},{"location":"How%20Bitcoin-Blockchain%20Works%20-%20Notes/#what-is-a-block","text":"From: https://followmyvote.com/blockchain-technology/ The blockchain further requires that an audit trail of all changes to the database is preserved, which allows anyone to audit that the database is correct at any time. This audit trail is composed to the individual changes to the database, which are called transactions. A group of transactions which were all added by a single node on its turn is called a block. Each block contains a reference to the block which preceded it, which establishes an ordering of the blocks. This is the origin of the term \u201cblockchain\u201d: it is a chain of blocks, each one containing a link to the previous block and a list of new transactions since that previous block.","title":"What is a block"},{"location":"How%20Bitcoin-Blockchain%20Works%20-%20Notes/#genesis-block","text":"An empty block that is the beginning of the block chain.","title":"Genesis Block"},{"location":"How%20Bitcoin-Blockchain%20Works%20-%20Notes/#creating-a-new-transaction","text":"From: https://followmyvote.com/blockchain-technology/ The most obvious example of blockchain technology in use today is Bitcoin. Bitcoin is a digital currency system which uses a blockchain to keep track of ownership of the currency. Whenever someone wishes to spend their bitcoins, they create a transaction which states that they are sending a certain number of their bitcoins to someone else. Then they digitally sign this transaction to authorize it, and broadcast it to all of the nodes in the Bitcoin network. When the next node creates a block, it will check that the new transaction is valid, and include it in the new block, which is then propagated to all other nodes in the network, which adjust their databases to deduct the transferred bitcoins from the sender and credit them to the recipient.","title":"Creating a new transaction"},{"location":"How%20Bitcoin-Blockchain%20Works%20-%20Notes/#khan-academy-notes","text":"","title":"Khan Academy Notes"},{"location":"How%20Bitcoin-Blockchain%20Works%20-%20Notes/#bitcoin-what-is-it","text":"Bitcoin transactions can use a thick client or a 3rd party service.","title":"Bitcoin What is It"},{"location":"How%20Bitcoin-Blockchain%20Works%20-%20Notes/#bitcoin-overview","text":"Transactions are tracked on the global ledger Transaction fees are used to motivated other nodes to validate the transaction To create the transaction, person 1 signs with private key and broadcasts the details of the transaction to other nodes How we avoid the double spending problem: We use bitcoin miners. They take all the transactions we see, take those transactions and compile them into a transaction block. This is like the entire page of a ledger block. The miners will include in the block an additional transaction which gives themselves a reward for completing this mining task. The miner will also include a proof of work (some sort of hash problem but he hasn't gotten to that) The transaction block will have a hash of the block's transactions / the hash of the previous block. This is what creates the chain. As soon as a node finishes creating the transaction block, it broadcasts it. The nodes will only consider the longest block chain - that is to say, the node with the most work which has been accomplished","title":"Bitcoin: Overview"},{"location":"How%20Bitcoin-Blockchain%20Works%20-%20Notes/#bitcoin-cryptographic-hash-functions","text":"Properties of cryptographic hash functions Computationally efficient Hard to find hash collision (colision resistant) Hide information about the original input Output should be well distributed","title":"Bitcoin: Cryptographic Hash Functions"},{"location":"How%20Bitcoin-Blockchain%20Works%20-%20Notes/#bitcoin-digital-signatures","text":"Private key may also be referred to as signing key (SK) and public key can be verification key (VK) Usually you will hash then sign the message","title":"Bitcoin: Digital Signatures"},{"location":"How%20Bitcoin-Blockchain%20Works%20-%20Notes/#bitcoin-transaction-records","text":"A transaction is just one party's intent to transfer some of their bitcoins to another party You can verify that a party possesses the Bitcoins they say they possess by searching for transactions showing that the party in question received that quantity of bitcoins See UTXO for a description of how transactions really work under the hood The idea here is that for each UTXO, there is a message digest of those transaction records and anyone can check those message digests and that the target was indeed the recipient In the case of a new transaction, the sender will create new transactions and then sign all of them with the private key. This will also include the transaction fees. If there is a double spend, remember that the order of transactions is included in the ledger which will prevent someone from committing a double spend to the ledger (because it would become obvious to the calculating node they have insufficient funds)","title":"Bitcoin: Transaction records"},{"location":"How%20Bitcoin-Blockchain%20Works%20-%20Notes/#bitcoin-proof-of-work","text":"Other uses of Proof of Work: preventing DOS or SPAM In general, they begin with a challenge string (c) In the case of SPAM it might be an e-mail message In response there is a response or proof response (p) In example of the challenge might be to combine the challenge string and the response string such that a hash output of the two has a fixed number of leading zeros and then the other bits are whatever you want. If you said you want 40 bits of leading zeros this would require effectively 2^40 consecutive heads coin flips. If you want to increase the difficulty, you can simply require more leading zeros effectively doubling the effort. Someone just has to find the proof string to prove they have done the work.","title":"Bitcoin: Proof of Work"},{"location":"How%20Bitcoin-Blockchain%20Works%20-%20Notes/#bitcoin-transaction-block-chains","text":"Nodes (Bitcoin miners) will take all these transactions they have received that have not yet been registered in the block chain. The first thing they will do is collate all these transactions. What they will do is take all the transactions they have collected and split them up into pairs. Then they will hash each pair, then take the hashes of the pairs, hash those, and so on and so forth until they have a single value. Ex: Then, they will combine that hash with the hash of the previous block in the blockchain. Then, after the node has created the new blockchain including their new block, they will take all that and hash it into the challenge as mentioned in Bitcoin: Proof of Work . Then, they must generate the proof, which when hashed will have some fixed number of leading zeros. The Bitcoin protocol is designed such that the average time to creating a new block is about every ten minutes The reward for miners is that in the first transaction block, they are allowed to insert a reward for themselves. This first transaction record's reward will change over time. These are called coinbase generation. This is how new coins are generated and placed into the system. In addition to the coinbase reward, you also get all the transaction fees. If there is a tie, then the chain with the most amount of work is selected","title":"Bitcoin: Transaction Block Chains"},{"location":"How%20Bitcoin-Blockchain%20Works%20-%20Notes/#bitcoin-the-bitcoin-money-supply","text":"The bitcoin system is designed such that the maximum number of coins is 21,000,000 BTC The smallest possible bitcoin is .00000001 BTC called a Satoshi Bitcoin started around Jan 2009 and the first reward was 50 BTC After about 210,000 blocks the reward halves. This takes approximately 4 years. Around 2140 the entire Bitcoin supply will have been generated For every 2,016 blocks, the network estimates the amount of time it took to generate the blocks. It should take about two weeks. If it took far less time, than the protocol will make things harder and on the flip side it will make it easier This works out to about every 10 minutes per block. It may seem like all nodes are simultaneously working on many of the same transactions. While this may be true, recall that each node inserts its own transaction to reward itself with the coinbase transaction and that transaction includes information unique to the node. Subsequently each node has a different challenge string.","title":"Bitcoin: The Bitcoin Money Supply"},{"location":"How%20Bitcoin-Blockchain%20Works%20-%20Notes/#bitcoin-the-security-of-transaction-block-chains","text":"TLDR: It's a synopsis of the other sections. You would have to fork the chain, add your own new block, plus add other blocks to make their chain longer, such that they then have the chain with the most work. Effectively, you would need at least 51% of the compute power in the network","title":"Bitcoin: The security of transaction block chains"},{"location":"How%20Bitcoin-Blockchain%20Works%20-%20Notes/#using-blockchain-for-voting","text":"From https://followmyvote.com/blockchain-technology/ Another application for blockchain technology is voting. By casting votes as transactions, we can create a blockchain which keeps track of the tallies of the votes. This way, everyone can agree on the final count because they can count the votes themselves, and because of the blockchain audit trail, they can verify that no votes were changed or removed, and no illegitimate votes were added. From: https://followmyvote.com/cryptographically-secure-voting-2/","title":"Using Blockchain for Voting"},{"location":"How%20Bitcoin-Blockchain%20Works%20-%20Notes/#how-the-keys-are-generated","text":"From: https://followmyvote.com/elliptic-curve-cryptography/ At Follow My Vote, we use this technology to create votes. During the registration process, voters create two ECC key-pairs. The voter reveals her identity to a verifier, who certifies the first key-pair (the identity key-pair) as belonging to that voter, then the voter anonymously registers her second key-pair (the voting key-pair) as belonging to one of the identity keys, but the way this is done, no one can determine which identity key owns her voting key. She can then create transactions which state her votes on the contests in an election, and use her voting private key to sign those transactions. Once these are published, everyone participating in the Follow My Vote network can verify that the signature is valid and adjust the tally accordingly. This way the votes are public and anonymous, but each voter can verify that her vote was correctly recorded and counted. Furthermore, all participants can verify that none of the votes were tampered with by validating the signatures. In this way, Follow My Vote software performs transparent, end-to-end verifiable online elections without compromising on security or voter anonymity.","title":"How the Keys are Generated"},{"location":"How%20OS10%20Installer%20Works/","text":"How OS10 Installer Works The OS10 installer is a binary file with a bash stub: #!/bin/sh ####################################################################### # Dell OS10 Installer ####################################################################### ####################################################################### # OS10 Data export OS_NAME=\"Dell EMC Networking OS10 Enterprise\" export OS_VERSION=\"10.5.2.7\" export PLATFORM=\"generic-x86_64\" export ARCHITECTURE=\"x86_64\" export INTERNAL_BUILD_ID=\"Dell EMC OS10 Enterprise Edition Blueprint 1.0.0\" export BUILD_VERSION=\"10.5.2.7.374\" export BUILD_DATE=\"2021-07-28T04:48:06+0000\" ####################################################################### # Magic cookies for OS10 feature detection. DO NOT CHANGE! # !OS10!1PART! # Enable error handling set -e INSTALLER=$(realpath \"$0\") TMP_DIR=$(mktemp -d) cd $TMP_DIR # Extract installer scripts echo -n \"Initializing installer ... \" sed -e '1,/^__INSTALLER__$/d;/^__IMAGE__$/,$d' \"$INSTALLER\" | base64 -d | tar xzf - echo \"OK\" # Load the installer library files cd installer . install_support.sh install_main \"$@\" rc=\"$?\" exit $rc __INSTALLER__ <BASE_64_ENCODED_INSTALLER> __IMAGE__ <BINARY_IMAGE_HERE> What this does is grab the installer's name with INSTALLER=$(realpath \"$0\") and then extracts itself with sed -e '1,/^__INSTALLER__$/d;/^__IMAGE__$/,$d' \"$INSTALLER\" | base64 -d | tar xzf - . This grabs everything between the INSTALLER and IMAGE tags, base64 decodes it, and then extracts it with tar.","title":"How OS10 Installer Works"},{"location":"How%20OS10%20Installer%20Works/#how-os10-installer-works","text":"The OS10 installer is a binary file with a bash stub: #!/bin/sh ####################################################################### # Dell OS10 Installer ####################################################################### ####################################################################### # OS10 Data export OS_NAME=\"Dell EMC Networking OS10 Enterprise\" export OS_VERSION=\"10.5.2.7\" export PLATFORM=\"generic-x86_64\" export ARCHITECTURE=\"x86_64\" export INTERNAL_BUILD_ID=\"Dell EMC OS10 Enterprise Edition Blueprint 1.0.0\" export BUILD_VERSION=\"10.5.2.7.374\" export BUILD_DATE=\"2021-07-28T04:48:06+0000\" ####################################################################### # Magic cookies for OS10 feature detection. DO NOT CHANGE! # !OS10!1PART! # Enable error handling set -e INSTALLER=$(realpath \"$0\") TMP_DIR=$(mktemp -d) cd $TMP_DIR # Extract installer scripts echo -n \"Initializing installer ... \" sed -e '1,/^__INSTALLER__$/d;/^__IMAGE__$/,$d' \"$INSTALLER\" | base64 -d | tar xzf - echo \"OK\" # Load the installer library files cd installer . install_support.sh install_main \"$@\" rc=\"$?\" exit $rc __INSTALLER__ <BASE_64_ENCODED_INSTALLER> __IMAGE__ <BINARY_IMAGE_HERE> What this does is grab the installer's name with INSTALLER=$(realpath \"$0\") and then extracts itself with sed -e '1,/^__INSTALLER__$/d;/^__IMAGE__$/,$d' \"$INSTALLER\" | base64 -d | tar xzf - . This grabs everything between the INSTALLER and IMAGE tags, base64 decodes it, and then extracts it with tar.","title":"How OS10 Installer Works"},{"location":"How%20to%20ONIE%20Install%20and%20ZTP%20Config%20Dell%20SONiC/","text":"How to ONIE Install and ZTP Config Dell SONiC How to ONIE Install and ZTP Config Dell SONiC My Test Platform Switch Server OS Prepare to Install Operating System Manually Install SONiC via ONIE Fully Automated Installation from OS10 to SONiC Overview Configure DHCP Server for SONiC ZTP Configure DHCP for ZTP Configure Your HTTP Server More Configuration Options Running ZTP Get Command Line See Install Workflow for an overview of how this process flows. Video of the full sequence: https://youtu.be/Xm4stcPvnUc My Test Platform Switch OS10# show version Dell EMC Networking OS10 Enterprise Copyright (c) 1999-2021 by Dell Inc. All Rights Reserved. OS Version: 10.5.3.0 Build Version: 10.5.3.0.44 Build Time: 2021-10-06T23:03:55+0000 System Type: S5212F-ON Architecture: x86_64 Up Time: 00:34:10 Server OS Fedora release 35 (Thirty Five) NAME=\"Fedora Linux\" VERSION=\"35 (Workstation Edition)\" ID=fedora VERSION_ID=35 VERSION_CODENAME=\"\" PLATFORM_ID=\"platform:f35\" PRETTY_NAME=\"Fedora Linux 35 (Workstation Edition)\" ANSI_COLOR=\"0;38;2;60;110;180\" LOGO=fedora-logo-icon CPE_NAME=\"cpe:/o:fedoraproject:fedora:35\" HOME_URL=\"https://fedoraproject.org/\" DOCUMENTATION_URL=\"https://docs.fedoraproject.org/en-US/fedora/f35/system-administrators-guide/\" SUPPORT_URL=\"https://ask.fedoraproject.org/\" BUG_REPORT_URL=\"https://bugzilla.redhat.com/\" REDHAT_BUGZILLA_PRODUCT=\"Fedora\" REDHAT_BUGZILLA_PRODUCT_VERSION=35 REDHAT_SUPPORT_PRODUCT=\"Fedora\" REDHAT_SUPPORT_PRODUCT_VERSION=35 PRIVACY_POLICY_URL=\"https://fedoraproject.org/wiki/Legal:PrivacyPolicy\" VARIANT=\"Workstation Edition\" VARIANT_ID=workstation Fedora release 35 (Thirty Five) Fedora release 35 (Thirty Five) Prepare to Install Operating System This first section discusses operating system installation. If you manually install the OS, you will start the next section of the instructions with the OS already installed. If you are performing the fully automated setup, you will install the OS later on in the instructions. Manually Install SONiC via ONIE Download SONiC OS for your manufacturer. In my case I pulled Dell's stable image. Follow the instructions for setting up and running ONIE Use SONiC binary instead of OS10 Fully Automated Installation from OS10 to SONiC Overview On a separate host, set up an HTTP server to host your operating system files You could also use TFTP but for demonstration here I am using HTTP On a separate host, set up a DHCP server to feed the ZTD/ZTP options Force OS10 to reboot in ONIE uninstall mode and then allow it to boot in ONIE install mode Install SONiC Dell switches are often shipped with OS10 by default (many times to meet TAA compliance requirements). In this scenario you may have just purchased several switches and you want to fully automate their out-of-the-box installation by replacing OS10 with SONiC. By default, OS10 will boot for the first time in Zero Touch Deployment (ZTD) mode. Entering configuration mode or rebooting the switch disables it. You can re-enable it with reload ztd . WARNING This will delete your startup configuration. You can check ztd status with show ztd-status . At time of writing this is the current OS10 manual. See page 93 for information on ZTD. While you can ONIe boot / ZTD with the switch's frontpanel ports, I tested this configuration using the management interface. These instructions are written assuming you have the management interface plugged in. If you need to console into the management interface the settings are: Baud Rate: 115200 Data Bits: 8 Stop Bits: 1 Parity: None Flow Control: None Configure HTTP server On your Fedora box run dnf install httpd Start it with systemctl start httpd Upload the SONiC binary to var/www/html Run ln -s Enterprise_SONiC_OS_4.0.1_Enterprise_Premium.bin onie-installer to create a symbolic link with the name onie-installer. This is the name ONIE will automatically search for. Confirm your web server is working by browsing to onie-installer at http://<your_ip>/onie-installer . On your Fedora box run dnf install -y dhcp-server Edit your DHCP server configuration with vim /etc/dhcp/dhcpd.conf . Replace the IP addresses with your IP addresses. Note: You will modify the line option bootfile-name \"http://192.168.1.186:80/initial.json\"; # For Option 67 (HTTP) further on in the instructions # # DHCP Server Configuration file. # see /usr/share/doc/dhcp-server/dhcpd.conf.example # see dhcpd.conf(5) man page # ddns-update-style interim; option ztd-provision-url code 240 = text; subnet 192.168.1.0 netmask 255.255.255.0 { max-lease-time 86400; min-lease-time 60; default-lease-time 86400; option netbios-node-type 8; host OS10 { hardware ethernet b0:4f:13:37:b8:c0; fixed-address 192.168.1.90; option default-url \"http://192.168.1.186/ztd/onie-installer\"; # For Option 114 (Default URL) option bootfile-name \"http://192.168.1.186:80/initial.json\"; # For Option 67 (HTTP) } } Run systemctl restart dhcpd to apply the configuration and ensure that dhcpd is running. You are now ready to move on to the ZTP configuration Configure DHCP Server for SONiC ZTP The official ZTP documentation for SONiC is here . Configure DHCP for ZTP The first thing you will need to do is configure the DHCP server servicing the devices to provide option 67 which will point to initial boot file used by SONiC's ZTP agent. The options for URL are defined here . As shown above, I used the following DHCP configuration file (located at /etc/dhcp/dhcpd.conf on Fedora): # # DHCP Server Configuration file. # see /usr/share/doc/dhcp-server/dhcpd.conf.example # see dhcpd.conf(5) man page # ddns-update-style interim; option ztd-provision-url code 240 = text; subnet 192.168.1.0 netmask 255.255.255.0 { max-lease-time 86400; min-lease-time 60; default-lease-time 86400; option netbios-node-type 8; host OS10 { hardware ethernet b0:4f:13:37:b8:c0; fixed-address 192.168.1.90; option default-url \"http://192.168.1.186/ztd/onie-installer\"; # For Option 114 (Default URL) option bootfile-name \"http://192.168.1.186:80/initial.json\"; # For Option 67 (HTTP) } } Notice the line option bootfile-name \"http://192.168.1.186:80/initial.json\"; # For Option 67 (HTTP) . If you want to use TFTP you can replace the http:// with tftp:// . This line tells SONiC where to look for a configuration pointer. The name initial.json does not matter - you can name this file whatever you want. This file will contain a web address pointing to config_db.json which is the file SONiC gives ZTP to configure itself. Run systemctl restart dhcpd to ensure your changes take effect. Configure Your HTTP Server As mentioned above, you have to create a file which points to config_db.json. A copy of mine is below. { \"ztp\": { \"configdb-json\" : { \"url\": { \"source\": \"http://192.168.1.186:80/config_db.json\" } } } } The source field points to the actual configuration file you want to deploy to the networking device. On your web server you will also need to provide this configuration file. In my case I only had it update the device's management IP address but you can have it configure any aspect of the switch: { \"MGMT_INTERFACE\": { \"eth0|192.168.1.96/24\": { \"gwaddr\": \"192.168.1.1\" } } } This file's sections are identical to what is in /etc/sonic/config_db.json . If you want to see what something should look like you can look there and then copy/paste. More Configuration Options The file initial.json has a myriad of options allowing you to do things like set firmware options, set the password, perform connection tests on completion, etc. I only demonstrated one here just to get started. See Dell's SONiC manual for additional options. Running ZTP If you have already installed SONiC manually, at this point you can leave things running and SONiC's ZTP should automatically start. If not, you can make sure it is on with ztp enable and then ztp run to force it to run. If you are performing a fully automated install, then at this point, if your DHCP server config is correct, you should see OS10 reboot into ONIE uninstall mode. It will uninstall OS10, reboot in ONIE install mode, and install SONiC. Note: The way this works is OS10 checks the onie-installer argument provided in the DHCP configuration. If the installer is not OS10, it reboots in ONIE uninstall mode. See https://youtu.be/Xm4stcPvnUc for a depiction of what the entire process looks like from start to finish. Get Command Line Run sonic-cli to get an OS-10 style command line.","title":"How to ONIE Install and ZTP Config Dell SONiC"},{"location":"How%20to%20ONIE%20Install%20and%20ZTP%20Config%20Dell%20SONiC/#how-to-onie-install-and-ztp-config-dell-sonic","text":"How to ONIE Install and ZTP Config Dell SONiC My Test Platform Switch Server OS Prepare to Install Operating System Manually Install SONiC via ONIE Fully Automated Installation from OS10 to SONiC Overview Configure DHCP Server for SONiC ZTP Configure DHCP for ZTP Configure Your HTTP Server More Configuration Options Running ZTP Get Command Line See Install Workflow for an overview of how this process flows. Video of the full sequence: https://youtu.be/Xm4stcPvnUc","title":"How to ONIE Install and ZTP Config Dell SONiC"},{"location":"How%20to%20ONIE%20Install%20and%20ZTP%20Config%20Dell%20SONiC/#my-test-platform","text":"","title":"My Test Platform"},{"location":"How%20to%20ONIE%20Install%20and%20ZTP%20Config%20Dell%20SONiC/#switch","text":"OS10# show version Dell EMC Networking OS10 Enterprise Copyright (c) 1999-2021 by Dell Inc. All Rights Reserved. OS Version: 10.5.3.0 Build Version: 10.5.3.0.44 Build Time: 2021-10-06T23:03:55+0000 System Type: S5212F-ON Architecture: x86_64 Up Time: 00:34:10","title":"Switch"},{"location":"How%20to%20ONIE%20Install%20and%20ZTP%20Config%20Dell%20SONiC/#server-os","text":"Fedora release 35 (Thirty Five) NAME=\"Fedora Linux\" VERSION=\"35 (Workstation Edition)\" ID=fedora VERSION_ID=35 VERSION_CODENAME=\"\" PLATFORM_ID=\"platform:f35\" PRETTY_NAME=\"Fedora Linux 35 (Workstation Edition)\" ANSI_COLOR=\"0;38;2;60;110;180\" LOGO=fedora-logo-icon CPE_NAME=\"cpe:/o:fedoraproject:fedora:35\" HOME_URL=\"https://fedoraproject.org/\" DOCUMENTATION_URL=\"https://docs.fedoraproject.org/en-US/fedora/f35/system-administrators-guide/\" SUPPORT_URL=\"https://ask.fedoraproject.org/\" BUG_REPORT_URL=\"https://bugzilla.redhat.com/\" REDHAT_BUGZILLA_PRODUCT=\"Fedora\" REDHAT_BUGZILLA_PRODUCT_VERSION=35 REDHAT_SUPPORT_PRODUCT=\"Fedora\" REDHAT_SUPPORT_PRODUCT_VERSION=35 PRIVACY_POLICY_URL=\"https://fedoraproject.org/wiki/Legal:PrivacyPolicy\" VARIANT=\"Workstation Edition\" VARIANT_ID=workstation Fedora release 35 (Thirty Five) Fedora release 35 (Thirty Five)","title":"Server OS"},{"location":"How%20to%20ONIE%20Install%20and%20ZTP%20Config%20Dell%20SONiC/#prepare-to-install-operating-system","text":"This first section discusses operating system installation. If you manually install the OS, you will start the next section of the instructions with the OS already installed. If you are performing the fully automated setup, you will install the OS later on in the instructions.","title":"Prepare to Install Operating System"},{"location":"How%20to%20ONIE%20Install%20and%20ZTP%20Config%20Dell%20SONiC/#manually-install-sonic-via-onie","text":"Download SONiC OS for your manufacturer. In my case I pulled Dell's stable image. Follow the instructions for setting up and running ONIE Use SONiC binary instead of OS10","title":"Manually Install SONiC via ONIE"},{"location":"How%20to%20ONIE%20Install%20and%20ZTP%20Config%20Dell%20SONiC/#fully-automated-installation-from-os10-to-sonic","text":"","title":"Fully Automated Installation from OS10 to SONiC"},{"location":"How%20to%20ONIE%20Install%20and%20ZTP%20Config%20Dell%20SONiC/#overview","text":"On a separate host, set up an HTTP server to host your operating system files You could also use TFTP but for demonstration here I am using HTTP On a separate host, set up a DHCP server to feed the ZTD/ZTP options Force OS10 to reboot in ONIE uninstall mode and then allow it to boot in ONIE install mode Install SONiC Dell switches are often shipped with OS10 by default (many times to meet TAA compliance requirements). In this scenario you may have just purchased several switches and you want to fully automate their out-of-the-box installation by replacing OS10 with SONiC. By default, OS10 will boot for the first time in Zero Touch Deployment (ZTD) mode. Entering configuration mode or rebooting the switch disables it. You can re-enable it with reload ztd . WARNING This will delete your startup configuration. You can check ztd status with show ztd-status . At time of writing this is the current OS10 manual. See page 93 for information on ZTD. While you can ONIe boot / ZTD with the switch's frontpanel ports, I tested this configuration using the management interface. These instructions are written assuming you have the management interface plugged in. If you need to console into the management interface the settings are: Baud Rate: 115200 Data Bits: 8 Stop Bits: 1 Parity: None Flow Control: None Configure HTTP server On your Fedora box run dnf install httpd Start it with systemctl start httpd Upload the SONiC binary to var/www/html Run ln -s Enterprise_SONiC_OS_4.0.1_Enterprise_Premium.bin onie-installer to create a symbolic link with the name onie-installer. This is the name ONIE will automatically search for. Confirm your web server is working by browsing to onie-installer at http://<your_ip>/onie-installer . On your Fedora box run dnf install -y dhcp-server Edit your DHCP server configuration with vim /etc/dhcp/dhcpd.conf . Replace the IP addresses with your IP addresses. Note: You will modify the line option bootfile-name \"http://192.168.1.186:80/initial.json\"; # For Option 67 (HTTP) further on in the instructions # # DHCP Server Configuration file. # see /usr/share/doc/dhcp-server/dhcpd.conf.example # see dhcpd.conf(5) man page # ddns-update-style interim; option ztd-provision-url code 240 = text; subnet 192.168.1.0 netmask 255.255.255.0 { max-lease-time 86400; min-lease-time 60; default-lease-time 86400; option netbios-node-type 8; host OS10 { hardware ethernet b0:4f:13:37:b8:c0; fixed-address 192.168.1.90; option default-url \"http://192.168.1.186/ztd/onie-installer\"; # For Option 114 (Default URL) option bootfile-name \"http://192.168.1.186:80/initial.json\"; # For Option 67 (HTTP) } } Run systemctl restart dhcpd to apply the configuration and ensure that dhcpd is running. You are now ready to move on to the ZTP configuration","title":"Overview"},{"location":"How%20to%20ONIE%20Install%20and%20ZTP%20Config%20Dell%20SONiC/#configure-dhcp-server-for-sonic-ztp","text":"The official ZTP documentation for SONiC is here .","title":"Configure DHCP Server for SONiC ZTP"},{"location":"How%20to%20ONIE%20Install%20and%20ZTP%20Config%20Dell%20SONiC/#configure-dhcp-for-ztp","text":"The first thing you will need to do is configure the DHCP server servicing the devices to provide option 67 which will point to initial boot file used by SONiC's ZTP agent. The options for URL are defined here . As shown above, I used the following DHCP configuration file (located at /etc/dhcp/dhcpd.conf on Fedora): # # DHCP Server Configuration file. # see /usr/share/doc/dhcp-server/dhcpd.conf.example # see dhcpd.conf(5) man page # ddns-update-style interim; option ztd-provision-url code 240 = text; subnet 192.168.1.0 netmask 255.255.255.0 { max-lease-time 86400; min-lease-time 60; default-lease-time 86400; option netbios-node-type 8; host OS10 { hardware ethernet b0:4f:13:37:b8:c0; fixed-address 192.168.1.90; option default-url \"http://192.168.1.186/ztd/onie-installer\"; # For Option 114 (Default URL) option bootfile-name \"http://192.168.1.186:80/initial.json\"; # For Option 67 (HTTP) } } Notice the line option bootfile-name \"http://192.168.1.186:80/initial.json\"; # For Option 67 (HTTP) . If you want to use TFTP you can replace the http:// with tftp:// . This line tells SONiC where to look for a configuration pointer. The name initial.json does not matter - you can name this file whatever you want. This file will contain a web address pointing to config_db.json which is the file SONiC gives ZTP to configure itself. Run systemctl restart dhcpd to ensure your changes take effect.","title":"Configure DHCP for ZTP"},{"location":"How%20to%20ONIE%20Install%20and%20ZTP%20Config%20Dell%20SONiC/#configure-your-http-server","text":"As mentioned above, you have to create a file which points to config_db.json. A copy of mine is below. { \"ztp\": { \"configdb-json\" : { \"url\": { \"source\": \"http://192.168.1.186:80/config_db.json\" } } } } The source field points to the actual configuration file you want to deploy to the networking device. On your web server you will also need to provide this configuration file. In my case I only had it update the device's management IP address but you can have it configure any aspect of the switch: { \"MGMT_INTERFACE\": { \"eth0|192.168.1.96/24\": { \"gwaddr\": \"192.168.1.1\" } } } This file's sections are identical to what is in /etc/sonic/config_db.json . If you want to see what something should look like you can look there and then copy/paste.","title":"Configure Your HTTP Server"},{"location":"How%20to%20ONIE%20Install%20and%20ZTP%20Config%20Dell%20SONiC/#more-configuration-options","text":"The file initial.json has a myriad of options allowing you to do things like set firmware options, set the password, perform connection tests on completion, etc. I only demonstrated one here just to get started. See Dell's SONiC manual for additional options.","title":"More Configuration Options"},{"location":"How%20to%20ONIE%20Install%20and%20ZTP%20Config%20Dell%20SONiC/#running-ztp","text":"If you have already installed SONiC manually, at this point you can leave things running and SONiC's ZTP should automatically start. If not, you can make sure it is on with ztp enable and then ztp run to force it to run. If you are performing a fully automated install, then at this point, if your DHCP server config is correct, you should see OS10 reboot into ONIE uninstall mode. It will uninstall OS10, reboot in ONIE install mode, and install SONiC. Note: The way this works is OS10 checks the onie-installer argument provided in the DHCP configuration. If the installer is not OS10, it reboots in ONIE uninstall mode. See https://youtu.be/Xm4stcPvnUc for a depiction of what the entire process looks like from start to finish.","title":"Running ZTP"},{"location":"How%20to%20ONIE%20Install%20and%20ZTP%20Config%20Dell%20SONiC/#get-command-line","text":"Run sonic-cli to get an OS-10 style command line.","title":"Get Command Line"},{"location":"How%20to%20ONIE%20Install%20and%20ZTP%20Config%20Dell%20SONiC/webserver/","text":"Configuring the Webserver Requirements: - Python 3 - flask (install with pip install flask ) Run the config server from this directory with python config_server.py --host 0.0.0.0 --port 80","title":"Configuring the Webserver"},{"location":"How%20to%20ONIE%20Install%20and%20ZTP%20Config%20Dell%20SONiC/webserver/#configuring-the-webserver","text":"Requirements: - Python 3 - flask (install with pip install flask ) Run the config server from this directory with python config_server.py --host 0.0.0.0 --port 80","title":"Configuring the Webserver"},{"location":"How%20to%20Read%20lstopo%20and%20a%20PCIe%20Overview/","text":"How to Read lstopo and a PCIe Overview The first time I looked at lstopo , I found the output rather overwhelming so I wrote this guide to break down what I was looking at. Subject Platform Dell R840 Package The word package is synonymous with the word socket. In the R840's case there are four separate, physical, sockets. Caches (LXi, LXd, and L3) This in particular confused me at first as the nomenclature is specific to your architecture (ex: Intel or AMD). Here you see three caches LXi, LXd, and L3. L i refers to an instruction cache , L d refers to a data cache , and L3 is a mixed cache including data and instructions. In the Intel architecture the first couple of levels may be dedicated to data or instruction caches but higher levels are mixed. Cores It helps here to look at the output of lscpu [root@r8402 ~]# lscpu Architecture: x86_64 CPU op-mode(s): 32-bit, 64-bit Byte Order: Little Endian CPU(s): 80 On-line CPU(s) list: 0-79 Thread(s) per core: 2 Core(s) per socket: 10 Socket(s): 4 NUMA node(s): 4 Vendor ID: GenuineIntel BIOS Vendor ID: Intel CPU family: 6 Model: 85 Model name: Intel(R) Xeon(R) Gold 5215 CPU @ 2.50GHz BIOS Model name: Intel(R) Xeon(R) Gold 5215 CPU @ 2.50GHz Stepping: 7 CPU MHz: 2643.471 CPU max MHz: 3400.0000 CPU min MHz: 1000.0000 BogoMIPS: 5000.00 Virtualization: VT-x L1d cache: 32K L1i cache: 32K L2 cache: 1024K L3 cache: 14080K NUMA node0 CPU(s): 0,4,8,12,16,20,24,28,32,36,40,44,48,52,56,60,64,68,72,76 NUMA node1 CPU(s): 1,5,9,13,17,21,25,29,33,37,41,45,49,53,57,61,65,69,73,77 NUMA node2 CPU(s): 2,6,10,14,18,22,26,30,34,38,42,46,50,54,58,62,66,70,74,78 NUMA node3 CPU(s): 3,7,11,15,19,23,27,31,35,39,43,47,51,55,59,63,67,71,75,79 First we need to understand a bit about computer architecture. In modern servers, you have the physical processors, cores within those processors, and finally logical cores (due to HyperThreading in Intel architectures ar Simultaneous Multi-Threading [SMT] in AMD architectures). AS you can see from the output of lscpu , in my server, each processor has 10 cores and each of those cores, due to hyperthreading, has 20 logical cores (as indicated by the NUMA node field). What you are seeing in this part: are the 10 cores in each proc. Notice that the numbers are in ascending order starting with package L#0 which has cores 0-9, then L#1 has 10-19, on up to 39 in package L#3. Also notice that the output omits some of the procs for space reasons and simply says \"10x total\" to indicate that there are actually 10 procs total. You can see that each individual core: Further has PU L#0 and PU L#1. These refer to the two aforementioned logacl cores created from hyperthreading. NUMA Nodes Perhaps more confusing are the smaller numbers. In package L#0 we see P#0, P#4, P#36, P#40, P#44, and P#76 actually drawn. As other posts have mentioned, these correspond to the various NUMA Nodes . As mentioned in Wikipedia, modern architectures use some sort of cache coherent NUMA (ccNUMA). The subject can be quite complicated but a simple explanation is this: we want memory access to be faster. The best way to do this is to literally put the memory closer to the processor. NUMA nodes do this by directly attaching processors to some part of memory. The smaller numbers indicate the logical cores created by HyperThreading. From the output of lscpu you can see that all the logical processes shown in package L#0: are included in NUMA node 0. You may also notice at the top it says NUMANode L#0 P#0 (45GB). This refers to the fact that NUMA node 0 has direct access to 1/4 of the total memory or 45GB. We can confirm this by looking at cat /proc/meminfo : [root@r8402 ~]# cat /proc/meminfo MemTotal: 196207488 kB MemFree: 191452708 kB MemAvailable: 191700740 kB ...SNIP... In our example, if a process is running on Core L#0, logical processor P#36, then it can directly access any of the 45GBs directly attached to package L#0. An important note: modern operating systems are NUMA aware and will do their absolute best when you spawn a process to make sure that the memory allocated for that process will be on the same NUMA node. However, let's say that there is some larger process and that multiple threads are sharing access to memory. In this case you may have a scenario where a process is running on logical processor P#5 on package L#1 in which case that process will have to reach out from physical package of L#1 through what's called the QPI bus, through package L#0, and then gain access to the memory local to package L#0. The QPI bus is part of the Intel architecture and it provides interconnects between all the physical packages for exactly this purpose. AMD uses something called XGMI. This gets us pretty in the weeds on computer architecture because this actually changes over time. For example, on Intel's newer Skylake processors it is no longer the QPI bus but ULtra Path Interconnect that serves this function so when you're working at this level you have to pay attention to exactly what processor you are using. Fun fact: When looking at things like the Technical Guide for servers you have to account for this when looking at total PCIe lane availability. The AMD EPYC gen 2/3 processors both support up to 128 PCIe lanes per processor BUT this is a bit misleading. When you have servers like the Dell R7525 (or any other vendor's two socket AMD server like HP's DL385) the two procs are connected via XGMI however XGMI consumes 48 PCIe lanes per processor so when doing your calculus you actually only end up seeing 80 per processor for a total of 160. PCIe Background The last part of the diagram we haven't touched on is PCIe. Not to be confused with NUMA, PCIe devices also have locality to processors. Each processor has a certain number of PCIe lanes attached directly to it. Just like memory, if you have a process running on, let's say, package L#3 and it is writing to NVMe drive nvme3n1 that process will write more quickly than a process running on package L#0 which must write across the QPI bus, through package L#3, and then to the drive. To fully understand how to interpret the PCIe results, it helps to understand a few PCIe basics. This is lengthy but all the things mentioned will help you interpret a potential configuration you might see in lstopo. PCIe Root Complex All PCIe Express (PCIe) devices connect to the processor and memory through what is called the PCIe root complex It is important to understand that PCIe devices can write directly to memory in modern architectures without ever touching the CPU. This is generically called Direct Memory Access (DMA). Each PCIe device has some region of memory assigned to it. This gets pretty deep - see here for an overview. In modern computer architectures, there is more than one root complex in a system ( NOTE that link does a good job of explaining things but has references to older architectures with pieces that are no longer in existence [Ex: the platform controller hub (PCH) is no longer a thing]). There is a root complex for every processor on the system. Now, where this gets confusing is a mixing of terminology. In an attempt to abstract the very architecture-specific nature of PCIe people use different terms like host bridge, root complex, and system on chip, etc. In modern architectures, from a physical perspective, there is no longer a physically separate thing for the root complex. Modern architectures have what is called system on chip (SoC) where all that stuff is built into the processor die (including the PCIe root complex). PCIe Switches Another concept that can be confusing is that you generally have two different devices which attach to the root complex. Either a PCIe switch or a PCIe endpoint. PCIe works more or less just like a network does and like a network it has switches. Say for example that a manufacturer wants to cram more PCIe devices onto a server than the server actually has PCIe lanes. A real world example: when the new AMD Rome processors dropped, manufacturers rushed to get servers to market. There wasn't time to actually make new motherboards fully supporting all the features the Rome processor offered so they instead modified existing motherboards which lead to some sub-optimal designs which included oversubscribing PCIe (it was not a single vendor that did this - I've seen this across the board). Let's say you have a server that supports 24 NVMe drives on a single backplane but each side of the backplane only has two x8 cables. NVMe drives run at x4 speeds so if you have 12 drives per side of the backplane, you're looking at a total of 48 PCIe lanes required to not have oversubscription. However, you only have 16 lanes available to play with. What do you do? Put a PCIe switch in front of the drives. It works just like an oversubscribed network. So in this case the NVMe drives are oversubscribed at a rate of 48:16 or simplified, 3:1. Another common use is bifurcation. Let's say you have one x16 lane but you want to use two x8 devices. You can bifurcate the lane to do just that - break a single x16 lane into two x8 lanes. This is where you might also hear the terminology \"electrically x16\". For example, a vendor might make a riser that allows two x8 devices but in reality, the electrical traces will each independently support x16. The BIOS will let you reconfigure the PCIe switch to instead operate at x16 speeds and you can ignore one of the riser slots. There are a lot of other fancy uses for PCIe switches but those two are the most common. For example NVLink will let graphics cards talk directly to each other. PCIe Bridge This term is also overused and confusing. In older architectures there's a lot about connecting to legacy PCI which isn't really a concern in 2022. In 2022 all a bridge does is connect a PCIe slot to the microprocessor responsible for controlling it. Generically, it is just a hardware connection between two different buses. What is the PCIe bus : This term is also super confusing in 2022. Way back in the original PCI spec before PCIe was a thing there was a literal parallel bus. As in, you had a whole bunch of devices sharing a physical bus along with all the problems that brings (of which there are many). In PCIe devices aren't attached to this kind of bus. In fact, I find it a bit obnoxious that we even use the word bus (even though it is technically correct). When I see people use the word \"bus\" in terms of PCIe what they usually mean is the serial connection consisting of multiple bidirectional PCIe lanes connecting the PCIe device (endpoint) to the root complex or PCIe switch. Interpreting PCIe As you might be able to guess, this is extremely device specific. The numbers shown next to the wires (3.9, 1.2, etc) are the unrounded transfer speeds in gigabytes per second . The various NVMe devices are fairly self explanatory. However, when it comes to networking, this is where it gets interesting. The network device shown is actually a Dell network daughter card (NDC) and all four interfaces shown in Package L#0 are actually the same network card. I confirmed this by cross referencing the xml output of lstopo with ip a s . See below picture It would seem that under the hood, for the NDC, Dell actually ran a x4 lane to the two SFP interfaces and a x2 lane for the copper interfaces. This makes sense because those ethernet interfaces are only 1Gb/s. The rest of the devices are as follows: Bus 25:00:0 is a BOSS card Bus 00:11.5 is the platform controller hub (PCH) If you're curious why you don't see the iDRAC's ethernet interface, this is because all communication with the iDRAC, assuming you aren't using RMI, goes through the PCH. Bus 03:00.0 is the VGA port For the eagle eyed you may notice the PERC is absent - I have it disconnected right now. It would normally have shown up on package L#0. Understanding PCIe Switches vs Functions The last thing I thought was a bit hard to descipher without some cross reference Research Translation Lookaside Buffer See: https://en.wikipedia.org/wiki/Translation_lookaside_buffer A translation lookaside buffer (TLB) is a memory cache that is used to reduce the time taken to access a user memory location.[1] It is a part of the chip's memory-management unit (MMU). The TLB stores the recent translations of virtual memory to physical memory and can be called an address-translation cache. A TLB may reside between the CPU and the CPU cache, between CPU cache and the main memory or between the different levels of the multi-level cache. The majority of desktop, laptop, and server processors include one or more TLBs in the memory-management hardware, and it is nearly always present in any processor that utilizes paged or segmented virtual memory. TLB Misses See 18-notes.pdf Data and Instruction Caches See 18-notes.pdf StackExchange Explanaton of lstopo https://unix.stackexchange.com/a/113549/240147","title":"How to Read lstopo and a PCIe Overview"},{"location":"How%20to%20Read%20lstopo%20and%20a%20PCIe%20Overview/#how-to-read-lstopo-and-a-pcie-overview","text":"The first time I looked at lstopo , I found the output rather overwhelming so I wrote this guide to break down what I was looking at.","title":"How to Read lstopo and a PCIe Overview"},{"location":"How%20to%20Read%20lstopo%20and%20a%20PCIe%20Overview/#subject-platform","text":"Dell R840","title":"Subject Platform"},{"location":"How%20to%20Read%20lstopo%20and%20a%20PCIe%20Overview/#package","text":"The word package is synonymous with the word socket. In the R840's case there are four separate, physical, sockets.","title":"Package"},{"location":"How%20to%20Read%20lstopo%20and%20a%20PCIe%20Overview/#caches-lxi-lxd-and-l3","text":"This in particular confused me at first as the nomenclature is specific to your architecture (ex: Intel or AMD). Here you see three caches LXi, LXd, and L3. L i refers to an instruction cache , L d refers to a data cache , and L3 is a mixed cache including data and instructions. In the Intel architecture the first couple of levels may be dedicated to data or instruction caches but higher levels are mixed.","title":"Caches (LXi, LXd, and L3)"},{"location":"How%20to%20Read%20lstopo%20and%20a%20PCIe%20Overview/#cores","text":"It helps here to look at the output of lscpu [root@r8402 ~]# lscpu Architecture: x86_64 CPU op-mode(s): 32-bit, 64-bit Byte Order: Little Endian CPU(s): 80 On-line CPU(s) list: 0-79 Thread(s) per core: 2 Core(s) per socket: 10 Socket(s): 4 NUMA node(s): 4 Vendor ID: GenuineIntel BIOS Vendor ID: Intel CPU family: 6 Model: 85 Model name: Intel(R) Xeon(R) Gold 5215 CPU @ 2.50GHz BIOS Model name: Intel(R) Xeon(R) Gold 5215 CPU @ 2.50GHz Stepping: 7 CPU MHz: 2643.471 CPU max MHz: 3400.0000 CPU min MHz: 1000.0000 BogoMIPS: 5000.00 Virtualization: VT-x L1d cache: 32K L1i cache: 32K L2 cache: 1024K L3 cache: 14080K NUMA node0 CPU(s): 0,4,8,12,16,20,24,28,32,36,40,44,48,52,56,60,64,68,72,76 NUMA node1 CPU(s): 1,5,9,13,17,21,25,29,33,37,41,45,49,53,57,61,65,69,73,77 NUMA node2 CPU(s): 2,6,10,14,18,22,26,30,34,38,42,46,50,54,58,62,66,70,74,78 NUMA node3 CPU(s): 3,7,11,15,19,23,27,31,35,39,43,47,51,55,59,63,67,71,75,79 First we need to understand a bit about computer architecture. In modern servers, you have the physical processors, cores within those processors, and finally logical cores (due to HyperThreading in Intel architectures ar Simultaneous Multi-Threading [SMT] in AMD architectures). AS you can see from the output of lscpu , in my server, each processor has 10 cores and each of those cores, due to hyperthreading, has 20 logical cores (as indicated by the NUMA node field). What you are seeing in this part: are the 10 cores in each proc. Notice that the numbers are in ascending order starting with package L#0 which has cores 0-9, then L#1 has 10-19, on up to 39 in package L#3. Also notice that the output omits some of the procs for space reasons and simply says \"10x total\" to indicate that there are actually 10 procs total. You can see that each individual core: Further has PU L#0 and PU L#1. These refer to the two aforementioned logacl cores created from hyperthreading.","title":"Cores"},{"location":"How%20to%20Read%20lstopo%20and%20a%20PCIe%20Overview/#numa-nodes","text":"Perhaps more confusing are the smaller numbers. In package L#0 we see P#0, P#4, P#36, P#40, P#44, and P#76 actually drawn. As other posts have mentioned, these correspond to the various NUMA Nodes . As mentioned in Wikipedia, modern architectures use some sort of cache coherent NUMA (ccNUMA). The subject can be quite complicated but a simple explanation is this: we want memory access to be faster. The best way to do this is to literally put the memory closer to the processor. NUMA nodes do this by directly attaching processors to some part of memory. The smaller numbers indicate the logical cores created by HyperThreading. From the output of lscpu you can see that all the logical processes shown in package L#0: are included in NUMA node 0. You may also notice at the top it says NUMANode L#0 P#0 (45GB). This refers to the fact that NUMA node 0 has direct access to 1/4 of the total memory or 45GB. We can confirm this by looking at cat /proc/meminfo : [root@r8402 ~]# cat /proc/meminfo MemTotal: 196207488 kB MemFree: 191452708 kB MemAvailable: 191700740 kB ...SNIP... In our example, if a process is running on Core L#0, logical processor P#36, then it can directly access any of the 45GBs directly attached to package L#0. An important note: modern operating systems are NUMA aware and will do their absolute best when you spawn a process to make sure that the memory allocated for that process will be on the same NUMA node. However, let's say that there is some larger process and that multiple threads are sharing access to memory. In this case you may have a scenario where a process is running on logical processor P#5 on package L#1 in which case that process will have to reach out from physical package of L#1 through what's called the QPI bus, through package L#0, and then gain access to the memory local to package L#0. The QPI bus is part of the Intel architecture and it provides interconnects between all the physical packages for exactly this purpose. AMD uses something called XGMI. This gets us pretty in the weeds on computer architecture because this actually changes over time. For example, on Intel's newer Skylake processors it is no longer the QPI bus but ULtra Path Interconnect that serves this function so when you're working at this level you have to pay attention to exactly what processor you are using. Fun fact: When looking at things like the Technical Guide for servers you have to account for this when looking at total PCIe lane availability. The AMD EPYC gen 2/3 processors both support up to 128 PCIe lanes per processor BUT this is a bit misleading. When you have servers like the Dell R7525 (or any other vendor's two socket AMD server like HP's DL385) the two procs are connected via XGMI however XGMI consumes 48 PCIe lanes per processor so when doing your calculus you actually only end up seeing 80 per processor for a total of 160.","title":"NUMA Nodes"},{"location":"How%20to%20Read%20lstopo%20and%20a%20PCIe%20Overview/#pcie-background","text":"The last part of the diagram we haven't touched on is PCIe. Not to be confused with NUMA, PCIe devices also have locality to processors. Each processor has a certain number of PCIe lanes attached directly to it. Just like memory, if you have a process running on, let's say, package L#3 and it is writing to NVMe drive nvme3n1 that process will write more quickly than a process running on package L#0 which must write across the QPI bus, through package L#3, and then to the drive. To fully understand how to interpret the PCIe results, it helps to understand a few PCIe basics. This is lengthy but all the things mentioned will help you interpret a potential configuration you might see in lstopo.","title":"PCIe Background"},{"location":"How%20to%20Read%20lstopo%20and%20a%20PCIe%20Overview/#pcie-root-complex","text":"All PCIe Express (PCIe) devices connect to the processor and memory through what is called the PCIe root complex It is important to understand that PCIe devices can write directly to memory in modern architectures without ever touching the CPU. This is generically called Direct Memory Access (DMA). Each PCIe device has some region of memory assigned to it. This gets pretty deep - see here for an overview. In modern computer architectures, there is more than one root complex in a system ( NOTE that link does a good job of explaining things but has references to older architectures with pieces that are no longer in existence [Ex: the platform controller hub (PCH) is no longer a thing]). There is a root complex for every processor on the system. Now, where this gets confusing is a mixing of terminology. In an attempt to abstract the very architecture-specific nature of PCIe people use different terms like host bridge, root complex, and system on chip, etc. In modern architectures, from a physical perspective, there is no longer a physically separate thing for the root complex. Modern architectures have what is called system on chip (SoC) where all that stuff is built into the processor die (including the PCIe root complex).","title":"PCIe Root Complex"},{"location":"How%20to%20Read%20lstopo%20and%20a%20PCIe%20Overview/#pcie-switches","text":"Another concept that can be confusing is that you generally have two different devices which attach to the root complex. Either a PCIe switch or a PCIe endpoint. PCIe works more or less just like a network does and like a network it has switches. Say for example that a manufacturer wants to cram more PCIe devices onto a server than the server actually has PCIe lanes. A real world example: when the new AMD Rome processors dropped, manufacturers rushed to get servers to market. There wasn't time to actually make new motherboards fully supporting all the features the Rome processor offered so they instead modified existing motherboards which lead to some sub-optimal designs which included oversubscribing PCIe (it was not a single vendor that did this - I've seen this across the board). Let's say you have a server that supports 24 NVMe drives on a single backplane but each side of the backplane only has two x8 cables. NVMe drives run at x4 speeds so if you have 12 drives per side of the backplane, you're looking at a total of 48 PCIe lanes required to not have oversubscription. However, you only have 16 lanes available to play with. What do you do? Put a PCIe switch in front of the drives. It works just like an oversubscribed network. So in this case the NVMe drives are oversubscribed at a rate of 48:16 or simplified, 3:1. Another common use is bifurcation. Let's say you have one x16 lane but you want to use two x8 devices. You can bifurcate the lane to do just that - break a single x16 lane into two x8 lanes. This is where you might also hear the terminology \"electrically x16\". For example, a vendor might make a riser that allows two x8 devices but in reality, the electrical traces will each independently support x16. The BIOS will let you reconfigure the PCIe switch to instead operate at x16 speeds and you can ignore one of the riser slots. There are a lot of other fancy uses for PCIe switches but those two are the most common. For example NVLink will let graphics cards talk directly to each other.","title":"PCIe Switches"},{"location":"How%20to%20Read%20lstopo%20and%20a%20PCIe%20Overview/#pcie-bridge","text":"This term is also overused and confusing. In older architectures there's a lot about connecting to legacy PCI which isn't really a concern in 2022. In 2022 all a bridge does is connect a PCIe slot to the microprocessor responsible for controlling it. Generically, it is just a hardware connection between two different buses. What is the PCIe bus : This term is also super confusing in 2022. Way back in the original PCI spec before PCIe was a thing there was a literal parallel bus. As in, you had a whole bunch of devices sharing a physical bus along with all the problems that brings (of which there are many). In PCIe devices aren't attached to this kind of bus. In fact, I find it a bit obnoxious that we even use the word bus (even though it is technically correct). When I see people use the word \"bus\" in terms of PCIe what they usually mean is the serial connection consisting of multiple bidirectional PCIe lanes connecting the PCIe device (endpoint) to the root complex or PCIe switch.","title":"PCIe Bridge"},{"location":"How%20to%20Read%20lstopo%20and%20a%20PCIe%20Overview/#interpreting-pcie","text":"As you might be able to guess, this is extremely device specific. The numbers shown next to the wires (3.9, 1.2, etc) are the unrounded transfer speeds in gigabytes per second . The various NVMe devices are fairly self explanatory. However, when it comes to networking, this is where it gets interesting. The network device shown is actually a Dell network daughter card (NDC) and all four interfaces shown in Package L#0 are actually the same network card. I confirmed this by cross referencing the xml output of lstopo with ip a s . See below picture It would seem that under the hood, for the NDC, Dell actually ran a x4 lane to the two SFP interfaces and a x2 lane for the copper interfaces. This makes sense because those ethernet interfaces are only 1Gb/s. The rest of the devices are as follows: Bus 25:00:0 is a BOSS card Bus 00:11.5 is the platform controller hub (PCH) If you're curious why you don't see the iDRAC's ethernet interface, this is because all communication with the iDRAC, assuming you aren't using RMI, goes through the PCH. Bus 03:00.0 is the VGA port For the eagle eyed you may notice the PERC is absent - I have it disconnected right now. It would normally have shown up on package L#0.","title":"Interpreting PCIe"},{"location":"How%20to%20Read%20lstopo%20and%20a%20PCIe%20Overview/#understanding-pcie-switches-vs-functions","text":"The last thing I thought was a bit hard to descipher without some cross reference","title":"Understanding PCIe Switches vs Functions"},{"location":"How%20to%20Read%20lstopo%20and%20a%20PCIe%20Overview/#research","text":"","title":"Research"},{"location":"How%20to%20Read%20lstopo%20and%20a%20PCIe%20Overview/#translation-lookaside-buffer","text":"See: https://en.wikipedia.org/wiki/Translation_lookaside_buffer A translation lookaside buffer (TLB) is a memory cache that is used to reduce the time taken to access a user memory location.[1] It is a part of the chip's memory-management unit (MMU). The TLB stores the recent translations of virtual memory to physical memory and can be called an address-translation cache. A TLB may reside between the CPU and the CPU cache, between CPU cache and the main memory or between the different levels of the multi-level cache. The majority of desktop, laptop, and server processors include one or more TLBs in the memory-management hardware, and it is nearly always present in any processor that utilizes paged or segmented virtual memory.","title":"Translation Lookaside Buffer"},{"location":"How%20to%20Read%20lstopo%20and%20a%20PCIe%20Overview/#tlb-misses","text":"See 18-notes.pdf","title":"TLB Misses"},{"location":"How%20to%20Read%20lstopo%20and%20a%20PCIe%20Overview/#data-and-instruction-caches","text":"See 18-notes.pdf","title":"Data and Instruction Caches"},{"location":"How%20to%20Read%20lstopo%20and%20a%20PCIe%20Overview/#stackexchange-explanaton-of-lstopo","text":"https://unix.stackexchange.com/a/113549/240147","title":"StackExchange Explanaton of lstopo"},{"location":"IO%20Identities%20with%20LifeCycle%20Controller/","text":"IO Identities with LifeCycle Controller Sources IO Identity Setup Using Lifecycle Controller Dell Simple NIC Profile Dell SystemInfo Profile Explanation On a per card basis there is a way to set virtual attributes for all of the following:\u200b Virtual MAC Address Virtual iSCSI MAC Address Virtual FIP MAC Address Virtual WWN Virtual WWPN When you create an identity pool in OME, it leverages this capability under the hood to make it happen. The way it works is Dell seems to provide an upper API capability described here: https://downloads.dell.com/manuals/common/dell-simple_nic_profile.pdf\u200b via the idrac with a thing called CIM (Common Information Model - see https://downloads.dell.com/solutions/general-solution-resources/White%20Papers/Dell_SystemInfo_Profile.pdf). The common information model is basically our standardized way of representing resources on the idrac. One of those resources is \"Simple NIC profile\": \u200b You can see from the spec sheet that the following functions have to be implemented for any given NIC: The DCIM_NICService class is the one that implements the SetAttributes method mentioned in the white paper. The white paper explores how this is done for each card, but the TLDR is that we really do have a card-specific API implementation for all our major vendors and that is how we are changing this stuff under the hood.","title":"IO Identities with LifeCycle Controller"},{"location":"IO%20Identities%20with%20LifeCycle%20Controller/#io-identities-with-lifecycle-controller","text":"","title":"IO Identities with LifeCycle Controller"},{"location":"IO%20Identities%20with%20LifeCycle%20Controller/#sources","text":"IO Identity Setup Using Lifecycle Controller Dell Simple NIC Profile Dell SystemInfo Profile","title":"Sources"},{"location":"IO%20Identities%20with%20LifeCycle%20Controller/#explanation","text":"On a per card basis there is a way to set virtual attributes for all of the following:\u200b Virtual MAC Address Virtual iSCSI MAC Address Virtual FIP MAC Address Virtual WWN Virtual WWPN When you create an identity pool in OME, it leverages this capability under the hood to make it happen. The way it works is Dell seems to provide an upper API capability described here: https://downloads.dell.com/manuals/common/dell-simple_nic_profile.pdf\u200b via the idrac with a thing called CIM (Common Information Model - see https://downloads.dell.com/solutions/general-solution-resources/White%20Papers/Dell_SystemInfo_Profile.pdf). The common information model is basically our standardized way of representing resources on the idrac. One of those resources is \"Simple NIC profile\": \u200b You can see from the spec sheet that the following functions have to be implemented for any given NIC: The DCIM_NICService class is the one that implements the SetAttributes method mentioned in the white paper. The white paper explores how this is done for each card, but the TLDR is that we really do have a card-specific API implementation for all our major vendors and that is how we are changing this stuff under the hood.","title":"Explanation"},{"location":"Importing%20Elasticsearch%20Data/","text":"Importing Elasticsearch Data WARNING These instructions are rough. Move your Elasticsearch snapshot folder to a known folder. For our example, I used /opt/snapshots . Next you will need to tell Elasticsearch about this path by adding the path.repo directive in the configuration file. Run: vim /etc/elasticsearch/elasticsearch.yml # Add `path.repo: [\"/opt/snapshots\"]` Now you need to import the data itself into Elasticsearch. Import with: curl -X PUT \"localhost:9200/_snapshot/esdata?pretty\" -H 'Content-Type: application/json' -d' { \"type\": \"fs\", \"settings\": { \"location\": \"/opt/snapshots\", \"compress\": true } } ' Note: It took about 5 minutes for the snapshot to load Next you will need to restore the snapshot by doing the following: Go to management, saved objects, import, select file on your local computer Import dashboards into Elasticsearch curl -O http://192.168.122.1:8000/kibana-export.json Helpful Commands View a list of the snapshots: curl localhost:9200/_cat/snapshots/esdata curl -X GET \"localhost:9200/_snapshot/_status?pretty\" Restore the snapshot with: curl -X POST \"localhost:9200/_snapshot/esdata/snapshot_1/_restore?pretty\" Install Moloch Run yum install -y https://files.molo.ch/builds/centos-7/moloch-1.1.1-1.x86_64.rpm Configure Moloch with /data/moloch/bin/Configure Run /data/moloch/db/db.pl http://ESHOST:9200 init to init the cluster Add user /data/moloch/bin/moloch_add_user.sh admin \"Admin User\" password --admin Import your data with ./bin/moloch-capture -c etc/config.ini -R <YOUR_DIRECTORY> . You can import individual files with lowercase -r.","title":"Importing Elasticsearch Data"},{"location":"Importing%20Elasticsearch%20Data/#importing-elasticsearch-data","text":"WARNING These instructions are rough. Move your Elasticsearch snapshot folder to a known folder. For our example, I used /opt/snapshots . Next you will need to tell Elasticsearch about this path by adding the path.repo directive in the configuration file. Run: vim /etc/elasticsearch/elasticsearch.yml # Add `path.repo: [\"/opt/snapshots\"]` Now you need to import the data itself into Elasticsearch. Import with: curl -X PUT \"localhost:9200/_snapshot/esdata?pretty\" -H 'Content-Type: application/json' -d' { \"type\": \"fs\", \"settings\": { \"location\": \"/opt/snapshots\", \"compress\": true } } ' Note: It took about 5 minutes for the snapshot to load Next you will need to restore the snapshot by doing the following: Go to management, saved objects, import, select file on your local computer Import dashboards into Elasticsearch curl -O http://192.168.122.1:8000/kibana-export.json","title":"Importing Elasticsearch Data"},{"location":"Importing%20Elasticsearch%20Data/#helpful-commands","text":"View a list of the snapshots: curl localhost:9200/_cat/snapshots/esdata curl -X GET \"localhost:9200/_snapshot/_status?pretty\" Restore the snapshot with: curl -X POST \"localhost:9200/_snapshot/esdata/snapshot_1/_restore?pretty\"","title":"Helpful Commands"},{"location":"Importing%20Elasticsearch%20Data/#install-moloch","text":"Run yum install -y https://files.molo.ch/builds/centos-7/moloch-1.1.1-1.x86_64.rpm Configure Moloch with /data/moloch/bin/Configure Run /data/moloch/db/db.pl http://ESHOST:9200 init to init the cluster Add user /data/moloch/bin/moloch_add_user.sh admin \"Admin User\" password --admin Import your data with ./bin/moloch-capture -c etc/config.ini -R <YOUR_DIRECTORY> . You can import individual files with lowercase -r.","title":"Install Moloch"},{"location":"Installing%20DPDK%20with%20NapaTech%20Card/","text":"Installing DPDK with NapaTech Card System Info I wrote this on CentOS 7 Release CentOS Linux release 7.7.1908 (Core) NAME=\"CentOS Linux\" VERSION=\"7 (Core)\" ID=\"centos\" ID_LIKE=\"rhel fedora\" VERSION_ID=\"7\" PRETTY_NAME=\"CentOS Linux 7 (Core)\" ANSI_COLOR=\"0;31\" CPE_NAME=\"cpe:/o:centos:centos:7\" HOME_URL=\"https://www.centos.org/\" BUG_REPORT_URL=\"https://bugs.centos.org/\" CENTOS_MANTISBT_PROJECT=\"CentOS-7\" CENTOS_MANTISBT_PROJECT_VERSION=\"7\" REDHAT_SUPPORT_PRODUCT=\"centos\" REDHAT_SUPPORT_PRODUCT_VERSION=\"7\" CentOS Linux release 7.7.1908 (Core) CentOS Linux release 7.7.1908 (Core) Kernel Linux r840-1.lan 3.10.0-1062.9.1.el7.x86_64 #1 SMP Fri Dec 6 15:49:49 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux Helpful Documents NUMA Nodes and NapaTech Optimizing the NapaTech Card Settings Install NapaTech Driver Download the Napatech software from here Run yum groupinstall \"Development Tools\" && yum install -y kernel-devel gettext-devel openssl-devel perl-CPAN perl-devel zlib-devel pciutils && yum install -y https://centos7.iuscommunity.org/ius-release.rpm && yum remove -y git && yum install -y git2u-all Unzip and run package_install_3gd.sh Run /opt/napatech3/bin/ntstop.sh && /opt/napatech3/bin/ntstart.sh to restart the NapaTech driver which will create a default configuration file. Update Host Buffers The /opt/napatech3/config/ntservice.ini contains the following settings that control the number and size of hostbuffers. To increase the number of CPUs/Cores which we can leverage with the SmartNIC, we need to increase the number of hostbuffers. Host buffers in Napatech Software Suite are the buffers used for moving (DMA) data packets between the accelerator and the applications. In the config file it should look like this: HostBuffersRx = [64,16,-1] # [x1, x2, x3], ... HostBuffersTx = [64,16,-1] # [x1, x2, x3], ... First number is the number of host buffers Second number is the size of the host buffers in MegaBytes Third number is the NUMA node You have to have one set of numbers for each NUMA node. -1 means read the NUMA node setting in the NumaNode directive in the configuration file. HostBuffersRx = [16,2048,0],[16,2048,1],[16,2048,2],[16,2048,3] HostBuffersTx = [16,2048,0],[16,2048,1],[16,2048,2],[16,2048,3] You can check what NUMA node your card is on with: cat \"/sys/bus/pci/devices/0000:5b:00.0/numa_node\" . You'll just have to replace the bus ID with your card's bus ID. You can retrieve that with the /opt/napatech3/bin/adapterinfo command. Install DPDK export NAPATECH3_PATH=/opt/napatech3 Download with git clone https://github.com/napatech/dpdk.git 1.I put this in /opt and will assume you have done the same in this guide. Edit the security limits with vim /etc/security/limits.conf . After making the below edits you will need to log out and log back in for them to take effect. 1.Add the following lines at the end of the file. This assumes you are running as root: root hard memlock unlimited root soft memlock unlimited Run yum install -y gcc numactl-devel kernel-devel pciutils elfutils-libelf-devel make libpcap python3 tar vim wget tmux vim mlocate hwloc libpcap-devel python36-devel set the environment variable RTE_SDK. It is the directory in which you extracted all the DPDK files. export RTE_SDK=/opt/dpdk (or your directory) Run make config T=x86_64-native-linuxapp-gcc install CONFIG_RTE_LIBRTE_PMD_PCAP=y CONFIG_RTE_LIBRTE_PDUMP=y DESTDIR=install CONFIG_RTE_LIBRTE_PMD_NTACC=y to build dpdk. Ensure your install directory exists. make -j NOTE : The option CONFIG_RTE_LIBRTE_PMD_PCAP=y enabled libpcap support in DPDK. This is required for pdump to work. Once an DPDK target environment directory has been created (such as x86_64-native-linux-gcc), it contains all libraries and header files required to build an application. When compiling an application in the Linux* environment on the DPDK, the following variables must be exported: RTE_TARGET - Points to the DPDK target environment directory. export RTE_TARGET=/opt/dpdk-19.08/x86_64-native-linux-gcc You may want to add this variable and RTE_SDK to ~/.bash_profile Update Your Bash Profile Add: export RTE_SDK=/opt/dpdk export NAPATECH3_PATH=/opt/napatech3 export RTE_TARGET=/opt/dpdk/x86_64-native-linuxapp-gcc to ~/.bash_profile Install ELF Tools Run the following: pip3 install numpy pip3 install elftools pip3 install pyelftools Configuration Configure Ports Move to your dpdk dir and run ./install/share/dpdk/usertools/dpdk-setup.sh . This should give you a menu with all available DPDK options. The menu is setup in such a way that you must perform each step listed in the menu. If things have gone correctly to this point your Step 1 should look like the following: ---------------------------------------------------------- Step 1: Select the DPDK environment to build ---------------------------------------------------------- [1] * My menu looks like this: ---------------------------------------------------------- Step 1: Select the DPDK environment to build ---------------------------------------------------------- [1] * ---------------------------------------------------------- Step 2: Setup linux environment ---------------------------------------------------------- [2] Insert IGB UIO module [3] Insert VFIO module [4] Insert KNI module [5] Setup hugepage mappings for non-NUMA systems [6] Setup hugepage mappings for NUMA systems [7] Display current Ethernet/Baseband/Crypto device settings [8] Bind Ethernet/Baseband/Crypto device to IGB UIO module [9] Bind Ethernet/Baseband/Crypto device to VFIO module [10] Setup VFIO permissions ---------------------------------------------------------- Step 3: Run test application for linux environment ---------------------------------------------------------- [11] Run test application ($RTE_TARGET/app/test) [12] Run testpmd application in interactive mode ($RTE_TARGET/app/testpmd) ---------------------------------------------------------- Step 4: Other tools ---------------------------------------------------------- [13] List hugepage info from /proc/meminfo ---------------------------------------------------------- Step 5: Uninstall and system cleanup ---------------------------------------------------------- [14] Unbind devices from IGB UIO or VFIO driver [15] Remove IGB UIO module [16] Remove VFIO module [17] Remove KNI module [18] Remove hugepage mappings [19] Exit Script Next run option 6 to instert huge pages for NUMA systems. Notice you will be prompted to select an amount of memory on a per processor basis. This is because there are pages associated with each individual processor to increase performance via locality. I used a value of 4096 for each NUMA node. For 1GB per NUMA node insert 512.","title":"Installing DPDK with NapaTech Card"},{"location":"Installing%20DPDK%20with%20NapaTech%20Card/#installing-dpdk-with-napatech-card","text":"","title":"Installing DPDK with NapaTech Card"},{"location":"Installing%20DPDK%20with%20NapaTech%20Card/#system-info","text":"I wrote this on CentOS 7","title":"System Info"},{"location":"Installing%20DPDK%20with%20NapaTech%20Card/#release","text":"CentOS Linux release 7.7.1908 (Core) NAME=\"CentOS Linux\" VERSION=\"7 (Core)\" ID=\"centos\" ID_LIKE=\"rhel fedora\" VERSION_ID=\"7\" PRETTY_NAME=\"CentOS Linux 7 (Core)\" ANSI_COLOR=\"0;31\" CPE_NAME=\"cpe:/o:centos:centos:7\" HOME_URL=\"https://www.centos.org/\" BUG_REPORT_URL=\"https://bugs.centos.org/\" CENTOS_MANTISBT_PROJECT=\"CentOS-7\" CENTOS_MANTISBT_PROJECT_VERSION=\"7\" REDHAT_SUPPORT_PRODUCT=\"centos\" REDHAT_SUPPORT_PRODUCT_VERSION=\"7\" CentOS Linux release 7.7.1908 (Core) CentOS Linux release 7.7.1908 (Core)","title":"Release"},{"location":"Installing%20DPDK%20with%20NapaTech%20Card/#kernel","text":"Linux r840-1.lan 3.10.0-1062.9.1.el7.x86_64 #1 SMP Fri Dec 6 15:49:49 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux","title":"Kernel"},{"location":"Installing%20DPDK%20with%20NapaTech%20Card/#helpful-documents","text":"NUMA Nodes and NapaTech Optimizing the NapaTech Card Settings","title":"Helpful Documents"},{"location":"Installing%20DPDK%20with%20NapaTech%20Card/#install-napatech-driver","text":"Download the Napatech software from here Run yum groupinstall \"Development Tools\" && yum install -y kernel-devel gettext-devel openssl-devel perl-CPAN perl-devel zlib-devel pciutils && yum install -y https://centos7.iuscommunity.org/ius-release.rpm && yum remove -y git && yum install -y git2u-all Unzip and run package_install_3gd.sh Run /opt/napatech3/bin/ntstop.sh && /opt/napatech3/bin/ntstart.sh to restart the NapaTech driver which will create a default configuration file.","title":"Install NapaTech Driver"},{"location":"Installing%20DPDK%20with%20NapaTech%20Card/#update-host-buffers","text":"The /opt/napatech3/config/ntservice.ini contains the following settings that control the number and size of hostbuffers. To increase the number of CPUs/Cores which we can leverage with the SmartNIC, we need to increase the number of hostbuffers. Host buffers in Napatech Software Suite are the buffers used for moving (DMA) data packets between the accelerator and the applications. In the config file it should look like this: HostBuffersRx = [64,16,-1] # [x1, x2, x3], ... HostBuffersTx = [64,16,-1] # [x1, x2, x3], ... First number is the number of host buffers Second number is the size of the host buffers in MegaBytes Third number is the NUMA node You have to have one set of numbers for each NUMA node. -1 means read the NUMA node setting in the NumaNode directive in the configuration file. HostBuffersRx = [16,2048,0],[16,2048,1],[16,2048,2],[16,2048,3] HostBuffersTx = [16,2048,0],[16,2048,1],[16,2048,2],[16,2048,3] You can check what NUMA node your card is on with: cat \"/sys/bus/pci/devices/0000:5b:00.0/numa_node\" . You'll just have to replace the bus ID with your card's bus ID. You can retrieve that with the /opt/napatech3/bin/adapterinfo command.","title":"Update Host Buffers"},{"location":"Installing%20DPDK%20with%20NapaTech%20Card/#install-dpdk","text":"export NAPATECH3_PATH=/opt/napatech3 Download with git clone https://github.com/napatech/dpdk.git 1.I put this in /opt and will assume you have done the same in this guide. Edit the security limits with vim /etc/security/limits.conf . After making the below edits you will need to log out and log back in for them to take effect. 1.Add the following lines at the end of the file. This assumes you are running as root: root hard memlock unlimited root soft memlock unlimited Run yum install -y gcc numactl-devel kernel-devel pciutils elfutils-libelf-devel make libpcap python3 tar vim wget tmux vim mlocate hwloc libpcap-devel python36-devel set the environment variable RTE_SDK. It is the directory in which you extracted all the DPDK files. export RTE_SDK=/opt/dpdk (or your directory) Run make config T=x86_64-native-linuxapp-gcc install CONFIG_RTE_LIBRTE_PMD_PCAP=y CONFIG_RTE_LIBRTE_PDUMP=y DESTDIR=install CONFIG_RTE_LIBRTE_PMD_NTACC=y to build dpdk. Ensure your install directory exists. make -j NOTE : The option CONFIG_RTE_LIBRTE_PMD_PCAP=y enabled libpcap support in DPDK. This is required for pdump to work. Once an DPDK target environment directory has been created (such as x86_64-native-linux-gcc), it contains all libraries and header files required to build an application. When compiling an application in the Linux* environment on the DPDK, the following variables must be exported: RTE_TARGET - Points to the DPDK target environment directory. export RTE_TARGET=/opt/dpdk-19.08/x86_64-native-linux-gcc You may want to add this variable and RTE_SDK to ~/.bash_profile","title":"Install DPDK"},{"location":"Installing%20DPDK%20with%20NapaTech%20Card/#update-your-bash-profile","text":"Add: export RTE_SDK=/opt/dpdk export NAPATECH3_PATH=/opt/napatech3 export RTE_TARGET=/opt/dpdk/x86_64-native-linuxapp-gcc to ~/.bash_profile","title":"Update Your Bash Profile"},{"location":"Installing%20DPDK%20with%20NapaTech%20Card/#install-elf-tools","text":"Run the following: pip3 install numpy pip3 install elftools pip3 install pyelftools","title":"Install ELF Tools"},{"location":"Installing%20DPDK%20with%20NapaTech%20Card/#configuration","text":"","title":"Configuration"},{"location":"Installing%20DPDK%20with%20NapaTech%20Card/#configure-ports","text":"Move to your dpdk dir and run ./install/share/dpdk/usertools/dpdk-setup.sh . This should give you a menu with all available DPDK options. The menu is setup in such a way that you must perform each step listed in the menu. If things have gone correctly to this point your Step 1 should look like the following: ---------------------------------------------------------- Step 1: Select the DPDK environment to build ---------------------------------------------------------- [1] * My menu looks like this: ---------------------------------------------------------- Step 1: Select the DPDK environment to build ---------------------------------------------------------- [1] * ---------------------------------------------------------- Step 2: Setup linux environment ---------------------------------------------------------- [2] Insert IGB UIO module [3] Insert VFIO module [4] Insert KNI module [5] Setup hugepage mappings for non-NUMA systems [6] Setup hugepage mappings for NUMA systems [7] Display current Ethernet/Baseband/Crypto device settings [8] Bind Ethernet/Baseband/Crypto device to IGB UIO module [9] Bind Ethernet/Baseband/Crypto device to VFIO module [10] Setup VFIO permissions ---------------------------------------------------------- Step 3: Run test application for linux environment ---------------------------------------------------------- [11] Run test application ($RTE_TARGET/app/test) [12] Run testpmd application in interactive mode ($RTE_TARGET/app/testpmd) ---------------------------------------------------------- Step 4: Other tools ---------------------------------------------------------- [13] List hugepage info from /proc/meminfo ---------------------------------------------------------- Step 5: Uninstall and system cleanup ---------------------------------------------------------- [14] Unbind devices from IGB UIO or VFIO driver [15] Remove IGB UIO module [16] Remove VFIO module [17] Remove KNI module [18] Remove hugepage mappings [19] Exit Script Next run option 6 to instert huge pages for NUMA systems. Notice you will be prompted to select an amount of memory on a per processor basis. This is because there are pages associated with each individual processor to increase performance via locality. I used a value of 4096 for each NUMA node. For 1GB per NUMA node insert 512.","title":"Configure Ports"},{"location":"LDAP%20with%20OpenManage/","text":"LDAP with OpenManage My Environment RHEL Version NAME=\"Red Hat Enterprise Linux\" VERSION=\"8.2 (Ootpa)\" ID=\"rhel\" ID_LIKE=\"fedora\" VERSION_ID=\"8.2\" PLATFORM_ID=\"platform:el8\" PRETTY_NAME=\"Red Hat Enterprise Linux 8.2 (Ootpa)\" ANSI_COLOR=\"0;31\" CPE_NAME=\"cpe:/o:redhat:enterprise_linux:8.2:GA\" HOME_URL=\"https://www.redhat.com/\" BUG_REPORT_URL=\"https://bugzilla.redhat.com/\" REDHAT_BUGZILLA_PRODUCT=\"Red Hat Enterprise Linux 8\" REDHAT_BUGZILLA_PRODUCT_VERSION=8.2 REDHAT_SUPPORT_PRODUCT=\"Red Hat Enterprise Linux\" REDHAT_SUPPORT_PRODUCT_VERSION=\"8.2\" Red Hat Enterprise Linux release 8.2 (Ootpa) Red Hat Enterprise Linux release 8.2 (Ootpa) FreeIPA Version [root@centos ~]# ipa --version VERSION: 4.8.4, API_VERSION: 2.235 OpenManage Version Version 3.4.1 (Build 24) Helpful Resources Dell Tutorial Logs Explained LDAP Result Codes Helpful Post on Bind DN Install Instructions Install RHEL Change hostname 1. hostname freeipa.grant.lan && hostnamectl set-hostname freeipa.grant.lan 2.Change in /etc/hostname 3.Configure DNS to return for this hostname. Double check with dig +short freeipa.grant.lan A && dig +short -x 192.168.1.95 Follow RHEL's instructions 1.I used Chapter 5 for primary installation 2.Make sure you add the requested DNS entries at the end Run firewall-cmd --permanent --add-port={80/tcp,443/tcp,389/tcp,636/tcp,88/tcp,464/tcp,88/udp,464/udp,123/udp} && firewall-cmd --reload to allow the appropriate ports Run kinit admin - this allows you to use the command line tools otherwise they'll complain about kerberos. Log into FreeIPA server at https://<your_hostname> . In my case, Windows popped up a username and password prompt. That prompt didn't work - I had to exit it and then log into the webGUI. Add a user other than administrator. Go to Users and then directory services in OpenManage. I used the following: Note: You can get the Bind DN by running ldapsearch from the command line. Create a new user and new group in the UI and assign the new user to the new group. Install OpenManage Go to Application Settings -> Directory Services Substitute with your values and then click test. I wasn't able to get this to work with the generic admin user. In the test screen I used that new user to connect to directory services Helpful Commands To start the IPA service use ipactl start|stop|restart . You can check the status with ipactl status . Test LDAP credentials: ldapwhoami -vvv -h 192.168.1.95 -p 389 -D 'uid=grant,cn=users,cn=accounts,dc=grant,dc=lan' -x -w <PASSWORD> Dump the structure of FreeIPA: ldapsearch -x -H ldap://localhost -b \"cn=accounts,dc=grant,dc=lan\" -D \"uid=grant,cn=users,cn=accounts,dc=grant,dc=lan\" -w <YOUR_USERS_PASSWORD> | less You can also use this to check to see if a user is a member of a group by changing the base path to search (-b). Change it to what you think the FQDN of the group is. The user in question should show up. Update FreeIPA Schema Request received: [23/Oct/2020:07:12:13.558111497 -0400] conn=22 fd=101 slot=101 SSL connection from 192.168.1.93 to 192.168.1.95 [23/Oct/2020:07:12:13.605452505 -0400] conn=22 TLS1.2 128-bit AES-GCM [23/Oct/2020:07:12:13.607423551 -0400] conn=22 op=0 BIND dn=\"uid=grant,cn=users,cn=accounts,dc=grant,dc=lan\" method=128 version=3 [23/Oct/2020:07:12:13.633734712 -0400] conn=22 op=0 RESULT err=0 tag=97 nentries=0 etime=0.075252172 dn=\"uid=grant,cn=users,cn=accounts,dc=grant,dc=lan\" [23/Oct/2020:07:12:13.634444660 -0400] conn=22 op=1 SRCH base=\"cn=accounts,dc=grant,dc=lan\" scope=2 filter=\"(uid=grant)\" attrs=ALL [23/Oct/2020:07:12:13.636377426 -0400] conn=22 op=1 RESULT err=0 tag=101 nentries=1 etime=0.001994528 [23/Oct/2020:07:12:13.637728783 -0400] conn=22 op=2 SRCH base=\"cn=accounts,dc=grant,dc=lan\" scope=2 filter=\"(|(member=uid=grant,cn=users,cn=accounts,dc=grant,dc=lan)(member=grant))\" attrs=ALL [23/Oct/2020:07:12:13.638251981 -0400] conn=22 op=2 RESULT err=0 tag=101 nentries=2 etime=0.000588138 [23/Oct/2020:07:12:13.640124457 -0400] conn=23 fd=103 slot=103 SSL connection from 192.168.1.93 to 192.168.1.95 [23/Oct/2020:07:12:13.684231843 -0400] conn=23 TLS1.2 128-bit AES-GCM [23/Oct/2020:07:12:13.685087831 -0400] conn=23 op=0 BIND dn=\"uid=grant,cn=users,cn=accounts,dc=grant,dc=lan\" method=128 version=3 [23/Oct/2020:07:12:13.685478906 -0400] conn=23 op=0 RESULT err=0 tag=97 nentries=0 etime=0.044995726 dn=\"uid=grant,cn=users,cn=accounts,dc=grant,dc=lan\" [23/Oct/2020:07:12:13.686411703 -0400] conn=23 op=1 SRCH base=\"cn=accounts,dc=grant,dc=lan\" scope=2 filter=\"(|(member=uid=grant,cn=users,cn=accounts,dc=grant,dc=lan)(member=grant))\" attrs=\"entryuuid cn\" [23/Oct/2020:07:12:13.686675968 -0400] conn=23 op=1 RESULT err=0 tag=101 nentries=2 etime=0.000324070 [23/Oct/2020:07:12:13.687691917 -0400] conn=23 op=2 UNBIND [23/Oct/2020:07:12:13.687704782 -0400] conn=23 op=2 fd=103 closed - U1 sed -i -e \"s/NAME 'ipaUniqueID'/NAME ('ipaUniqueID' 'entryUUID')/\" /etc/dirsrv/slapd-*/schema/60basev2.ldif ipa-ldap-updater -u --schema-file=$(ls /etc/dirsrv/slapd-*/schema/60basev2.ldif) After making the above modification I now get: [23/Oct/2020:08:16:51.891366602 -0400] conn=30 fd=101 slot=101 SSL connection from 192.168.1.93 to 192.168.1.95 [23/Oct/2020:08:16:51.938289338 -0400] conn=30 TLS1.2 128-bit AES-GCM [23/Oct/2020:08:16:51.939209205 -0400] conn=30 op=0 BIND dn=\"uid=grant,cn=users,cn=accounts,dc=grant,dc=lan\" method=128 version=3 [23/Oct/2020:08:16:51.965507560 -0400] conn=30 op=0 RESULT err=49 tag=97 nentries=0 etime=0.073775627 - Invalid credentials [23/Oct/2020:08:16:51.966077916 -0400] conn=30 op=-1 fd=101 closed - B1 [23/Oct/2020:08:16:51.970300516 -0400] conn=31 fd=101 slot=101 SSL connection from 192.168.1.93 to 192.168.1.95 [23/Oct/2020:08:16:52.015244484 -0400] conn=31 TLS1.2 128-bit AES-GCM [23/Oct/2020:08:16:52.015949382 -0400] conn=31 op=0 BIND dn=\"\" method=128 version=3 [23/Oct/2020:08:16:52.016032928 -0400] conn=31 op=0 RESULT err=0 tag=97 nentries=0 etime=0.045312604 dn=\"\" [23/Oct/2020:08:16:52.017337373 -0400] conn=31 op=1 SRCH base=\"cn=accounts,dc=grant,dc=lan\" scope=2 filter=\"(|(member=null)(member=grant))\" attrs=\"ipaUniqueID cn\" [23/Oct/2020:08:16:52.017436879 -0400] conn=31 op=1 RESULT err=0 tag=101 nentries=0 etime=0.000150394 [23/Oct/2020:08:16:52.018338463 -0400] conn=31 op=2 UNBIND [23/Oct/2020:08:16:52.018350155 -0400] conn=31 op=2 fd=101 closed - U1 [23/Oct/2020:08:20:26.725516663 -0400] conn=21 op=2 SRCH base=\"ou=sessions,ou=Security Domain,o=ipaca\" scope=2 filter=\"(objectClass=securityDomainSessionEntry)\" attrs=\"cn\" [23/Oct/2020:08:20:26.725732986 -0400] conn=21 op=2 RESULT err=32 tag=101 nentries=0 etime=0.000316769","title":"LDAP with OpenManage"},{"location":"LDAP%20with%20OpenManage/#ldap-with-openmanage","text":"","title":"LDAP with OpenManage"},{"location":"LDAP%20with%20OpenManage/#my-environment","text":"","title":"My Environment"},{"location":"LDAP%20with%20OpenManage/#rhel-version","text":"NAME=\"Red Hat Enterprise Linux\" VERSION=\"8.2 (Ootpa)\" ID=\"rhel\" ID_LIKE=\"fedora\" VERSION_ID=\"8.2\" PLATFORM_ID=\"platform:el8\" PRETTY_NAME=\"Red Hat Enterprise Linux 8.2 (Ootpa)\" ANSI_COLOR=\"0;31\" CPE_NAME=\"cpe:/o:redhat:enterprise_linux:8.2:GA\" HOME_URL=\"https://www.redhat.com/\" BUG_REPORT_URL=\"https://bugzilla.redhat.com/\" REDHAT_BUGZILLA_PRODUCT=\"Red Hat Enterprise Linux 8\" REDHAT_BUGZILLA_PRODUCT_VERSION=8.2 REDHAT_SUPPORT_PRODUCT=\"Red Hat Enterprise Linux\" REDHAT_SUPPORT_PRODUCT_VERSION=\"8.2\" Red Hat Enterprise Linux release 8.2 (Ootpa) Red Hat Enterprise Linux release 8.2 (Ootpa)","title":"RHEL Version"},{"location":"LDAP%20with%20OpenManage/#freeipa-version","text":"[root@centos ~]# ipa --version VERSION: 4.8.4, API_VERSION: 2.235","title":"FreeIPA Version"},{"location":"LDAP%20with%20OpenManage/#openmanage-version","text":"Version 3.4.1 (Build 24)","title":"OpenManage Version"},{"location":"LDAP%20with%20OpenManage/#helpful-resources","text":"Dell Tutorial Logs Explained LDAP Result Codes Helpful Post on Bind DN","title":"Helpful Resources"},{"location":"LDAP%20with%20OpenManage/#install-instructions","text":"Install RHEL Change hostname 1. hostname freeipa.grant.lan && hostnamectl set-hostname freeipa.grant.lan 2.Change in /etc/hostname 3.Configure DNS to return for this hostname. Double check with dig +short freeipa.grant.lan A && dig +short -x 192.168.1.95 Follow RHEL's instructions 1.I used Chapter 5 for primary installation 2.Make sure you add the requested DNS entries at the end Run firewall-cmd --permanent --add-port={80/tcp,443/tcp,389/tcp,636/tcp,88/tcp,464/tcp,88/udp,464/udp,123/udp} && firewall-cmd --reload to allow the appropriate ports Run kinit admin - this allows you to use the command line tools otherwise they'll complain about kerberos. Log into FreeIPA server at https://<your_hostname> . In my case, Windows popped up a username and password prompt. That prompt didn't work - I had to exit it and then log into the webGUI. Add a user other than administrator. Go to Users and then directory services in OpenManage. I used the following: Note: You can get the Bind DN by running ldapsearch from the command line. Create a new user and new group in the UI and assign the new user to the new group. Install OpenManage Go to Application Settings -> Directory Services Substitute with your values and then click test. I wasn't able to get this to work with the generic admin user. In the test screen I used that new user to connect to directory services","title":"Install Instructions"},{"location":"LDAP%20with%20OpenManage/#helpful-commands","text":"To start the IPA service use ipactl start|stop|restart . You can check the status with ipactl status . Test LDAP credentials: ldapwhoami -vvv -h 192.168.1.95 -p 389 -D 'uid=grant,cn=users,cn=accounts,dc=grant,dc=lan' -x -w <PASSWORD> Dump the structure of FreeIPA: ldapsearch -x -H ldap://localhost -b \"cn=accounts,dc=grant,dc=lan\" -D \"uid=grant,cn=users,cn=accounts,dc=grant,dc=lan\" -w <YOUR_USERS_PASSWORD> | less You can also use this to check to see if a user is a member of a group by changing the base path to search (-b). Change it to what you think the FQDN of the group is. The user in question should show up.","title":"Helpful Commands"},{"location":"LDAP%20with%20OpenManage/#update-freeipa-schema","text":"Request received: [23/Oct/2020:07:12:13.558111497 -0400] conn=22 fd=101 slot=101 SSL connection from 192.168.1.93 to 192.168.1.95 [23/Oct/2020:07:12:13.605452505 -0400] conn=22 TLS1.2 128-bit AES-GCM [23/Oct/2020:07:12:13.607423551 -0400] conn=22 op=0 BIND dn=\"uid=grant,cn=users,cn=accounts,dc=grant,dc=lan\" method=128 version=3 [23/Oct/2020:07:12:13.633734712 -0400] conn=22 op=0 RESULT err=0 tag=97 nentries=0 etime=0.075252172 dn=\"uid=grant,cn=users,cn=accounts,dc=grant,dc=lan\" [23/Oct/2020:07:12:13.634444660 -0400] conn=22 op=1 SRCH base=\"cn=accounts,dc=grant,dc=lan\" scope=2 filter=\"(uid=grant)\" attrs=ALL [23/Oct/2020:07:12:13.636377426 -0400] conn=22 op=1 RESULT err=0 tag=101 nentries=1 etime=0.001994528 [23/Oct/2020:07:12:13.637728783 -0400] conn=22 op=2 SRCH base=\"cn=accounts,dc=grant,dc=lan\" scope=2 filter=\"(|(member=uid=grant,cn=users,cn=accounts,dc=grant,dc=lan)(member=grant))\" attrs=ALL [23/Oct/2020:07:12:13.638251981 -0400] conn=22 op=2 RESULT err=0 tag=101 nentries=2 etime=0.000588138 [23/Oct/2020:07:12:13.640124457 -0400] conn=23 fd=103 slot=103 SSL connection from 192.168.1.93 to 192.168.1.95 [23/Oct/2020:07:12:13.684231843 -0400] conn=23 TLS1.2 128-bit AES-GCM [23/Oct/2020:07:12:13.685087831 -0400] conn=23 op=0 BIND dn=\"uid=grant,cn=users,cn=accounts,dc=grant,dc=lan\" method=128 version=3 [23/Oct/2020:07:12:13.685478906 -0400] conn=23 op=0 RESULT err=0 tag=97 nentries=0 etime=0.044995726 dn=\"uid=grant,cn=users,cn=accounts,dc=grant,dc=lan\" [23/Oct/2020:07:12:13.686411703 -0400] conn=23 op=1 SRCH base=\"cn=accounts,dc=grant,dc=lan\" scope=2 filter=\"(|(member=uid=grant,cn=users,cn=accounts,dc=grant,dc=lan)(member=grant))\" attrs=\"entryuuid cn\" [23/Oct/2020:07:12:13.686675968 -0400] conn=23 op=1 RESULT err=0 tag=101 nentries=2 etime=0.000324070 [23/Oct/2020:07:12:13.687691917 -0400] conn=23 op=2 UNBIND [23/Oct/2020:07:12:13.687704782 -0400] conn=23 op=2 fd=103 closed - U1 sed -i -e \"s/NAME 'ipaUniqueID'/NAME ('ipaUniqueID' 'entryUUID')/\" /etc/dirsrv/slapd-*/schema/60basev2.ldif ipa-ldap-updater -u --schema-file=$(ls /etc/dirsrv/slapd-*/schema/60basev2.ldif) After making the above modification I now get: [23/Oct/2020:08:16:51.891366602 -0400] conn=30 fd=101 slot=101 SSL connection from 192.168.1.93 to 192.168.1.95 [23/Oct/2020:08:16:51.938289338 -0400] conn=30 TLS1.2 128-bit AES-GCM [23/Oct/2020:08:16:51.939209205 -0400] conn=30 op=0 BIND dn=\"uid=grant,cn=users,cn=accounts,dc=grant,dc=lan\" method=128 version=3 [23/Oct/2020:08:16:51.965507560 -0400] conn=30 op=0 RESULT err=49 tag=97 nentries=0 etime=0.073775627 - Invalid credentials [23/Oct/2020:08:16:51.966077916 -0400] conn=30 op=-1 fd=101 closed - B1 [23/Oct/2020:08:16:51.970300516 -0400] conn=31 fd=101 slot=101 SSL connection from 192.168.1.93 to 192.168.1.95 [23/Oct/2020:08:16:52.015244484 -0400] conn=31 TLS1.2 128-bit AES-GCM [23/Oct/2020:08:16:52.015949382 -0400] conn=31 op=0 BIND dn=\"\" method=128 version=3 [23/Oct/2020:08:16:52.016032928 -0400] conn=31 op=0 RESULT err=0 tag=97 nentries=0 etime=0.045312604 dn=\"\" [23/Oct/2020:08:16:52.017337373 -0400] conn=31 op=1 SRCH base=\"cn=accounts,dc=grant,dc=lan\" scope=2 filter=\"(|(member=null)(member=grant))\" attrs=\"ipaUniqueID cn\" [23/Oct/2020:08:16:52.017436879 -0400] conn=31 op=1 RESULT err=0 tag=101 nentries=0 etime=0.000150394 [23/Oct/2020:08:16:52.018338463 -0400] conn=31 op=2 UNBIND [23/Oct/2020:08:16:52.018350155 -0400] conn=31 op=2 fd=101 closed - U1 [23/Oct/2020:08:20:26.725516663 -0400] conn=21 op=2 SRCH base=\"ou=sessions,ou=Security Domain,o=ipaca\" scope=2 filter=\"(objectClass=securityDomainSessionEntry)\" attrs=\"cn\" [23/Oct/2020:08:20:26.725732986 -0400] conn=21 op=2 RESULT err=32 tag=101 nentries=0 etime=0.000316769","title":"Update FreeIPA Schema"},{"location":"LDAP%20with%20OpenManage/README-CentOS-OpenLDAP/","text":"Setting Up OpenLDAP with OpenManage My Environment CentOS Version CentOS Linux release 8.2.2004 (Core) NAME=\"CentOS Linux\" VERSION=\"8 (Core)\" ID=\"centos\" ID_LIKE=\"rhel fedora\" VERSION_ID=\"8\" PLATFORM_ID=\"platform:el8\" PRETTY_NAME=\"CentOS Linux 8 (Core)\" ANSI_COLOR=\"0;31\" CPE_NAME=\"cpe:/o:centos:centos:8\" HOME_URL=\"https://www.centos.org/\" BUG_REPORT_URL=\"https://bugs.centos.org/\" CENTOS_MANTISBT_PROJECT=\"CentOS-8\" CENTOS_MANTISBT_PROJECT_VERSION=\"8\" REDHAT_SUPPORT_PRODUCT=\"centos\" REDHAT_SUPPORT_PRODUCT_VERSION=\"8\" CentOS Linux release 8.2.2004 (Core) CentOS Linux release 8.2.2004 (Core) IPA Version [root@centos ~]# ipa --version VERSION: 4.8.4, API_VERSION: 2.235 OpenManage Version Version 3.5.0 (Build 60) Helpful Resources Dell Tutorial Logs Explained LDAP Result Codes Helpful Post on Bind DN OpenManage User's Guide Install Instructions Install CentOS8 1.I installed CentOS minimal 2.Make sure NTP is working correctly Install OpenManage For the OpenLDAP setup I followed Install and Setup OpenLDAP on CentOS 8 by koromicha I want the current version of OpenLDAP so I'll be building it from source. Install dependencies with dnf install -y cyrus-sasl-devel make libtool autoconf libtool-ltdl-devel openssl-devel libdb-devel tar gcc perl perl-devel wget vim Create non privileged system user: useradd -r -M -d /var/lib/openldap -u 55 -s /usr/sbin/nologin ldap Pull tarball wget https://www.openldap.org/software/download/OpenLDAP/openldap-release/openldap-2.4.54.tgz && tar xzf openldap-2.4.54.tgz && cd openldap-2.4.54 Build and install OpenLDAP ./configure --prefix=/usr --sysconfdir=/etc --disable-static --enable-debug --with-tls=openssl --with-cyrus-sasl --enable-dynamic --enable-crypt --enable-spasswd --enable-slapd --enable-modules --enable-rlookups --enable-backends=mod --disable-ndb --disable-sql --disable-shell --disable-bdb --disable-hdb --enable-overlays=mod && make depend && make -j2 && make install Configure OpenLDAP mkdir /var/lib/openldap /etc/openldap/slapd.d && chown -R ldap:ldap /var/lib/openldap && chown root:ldap /etc/openldap/slapd.conf Add an OpenLDAP systemd service vim /etc/systemd/system/slapd.service [Unit] Description=OpenLDAP Server Daemon After=syslog.target network-online.target Documentation=man:slapd Documentation=man:slapd-mdb [Service] Type=forking PIDFile=/var/lib/openldap/slapd.pid Environment=\"SLAPD_URLS=ldap:/// ldapi:/// ldaps:///\" Environment=\"SLAPD_OPTIONS=-F /etc/openldap/slapd.d\" ExecStart=/usr/libexec/slapd -u ldap -g ldap -h ${SLAPD_URLS} $SLAPD_OPTIONS [Install] WantedBy=multi-user.target Check if your version of sudo supports lday with sudo -V | grep -i \"ldap\" and confirm the below lines are present: ldap.conf path: /etc/sudo-ldap.conf ldap.secret path: /etc/ldap.secret Make sure LDAP sudo schema is available with rpm -ql sudo | grep -i schema.openldap . You should see /usr/share/doc/sudo/schema.OpenLDAP Run: cp /usr/share/doc/sudo/schema.OpenLDAP /etc/openldap/schema/sudo.schema cat << 'EOL' > /etc/openldap/schema/sudo.ldif dn: cn=sudo,cn=schema,cn=config objectClass: olcSchemaConfig cn: sudo olcAttributeTypes: ( 1.3.6.1.4.1.15953.9.1.1 NAME 'sudoUser' DESC 'User(s) who may run sudo' EQUALITY caseExactIA5Match SUBSTR caseExactIA5SubstringsMatch SYNTAX 1.3.6.1.4.1.1466.115.121.1.26 ) olcAttributeTypes: ( 1.3.6.1.4.1.15953.9.1.2 NAME 'sudoHost' DESC 'Host(s) who may run sudo' EQUALITY caseExactIA5Match SUBSTR caseExactIA5SubstringsMatch SYNTAX 1.3.6.1.4.1.1466.115.121.1.26 ) olcAttributeTypes: ( 1.3.6.1.4.1.15953.9.1.3 NAME 'sudoCommand' DESC 'Command(s) to be executed by sudo' EQUALITY caseExactIA5Match SYNTAX 1.3.6.1.4.1.1466.115.121.1.26 ) olcAttributeTypes: ( 1.3.6.1.4.1.15953.9.1.4 NAME 'sudoRunAs' DESC 'User(s) impersonated by sudo (deprecated)' EQUALITY caseExactIA5Match SYNTAX 1.3.6.1.4.1.1466.115.121.1.26 ) olcAttributeTypes: ( 1.3.6.1.4.1.15953.9.1.5 NAME 'sudoOption' DESC 'Options(s) followed by sudo' EQUALITY caseExactIA5Match SYNTAX 1.3.6.1.4.1.1466.115.121.1.26 ) olcAttributeTypes: ( 1.3.6.1.4.1.15953.9.1.6 NAME 'sudoRunAsUser' DESC 'User(s) impersonated by sudo' EQUALITY caseExactIA5Match SYNTAX 1.3.6.1.4.1.1466.115.121.1.26 ) olcAttributeTypes: ( 1.3.6.1.4.1.15953.9.1.7 NAME 'sudoRunAsGroup' DESC 'Group(s) impersonated by sudo' EQUALITY caseExactIA5Match SYNTAX 1.3.6.1.4.1.1466.115.121.1.26 ) olcObjectClasses: ( 1.3.6.1.4.1.15953.9.2.1 NAME 'sudoRole' SUP top STRUCTURAL DESC 'Sudoer Entries' MUST ( cn ) MAY ( sudoUser $ sudoHost $ sudoCommand $ sudoRunAs $ sudoRunAsUser $ sudoRunAsGroup $ sudoOption $ description ) ) EOL mv /etc/openldap/slapd.ldif /etc/openldap/slapd.ldif.bak vim /etc/openldap/slapd.ldif dn: cn=config objectClass: olcGlobal cn: config olcArgsFile: /var/lib/openldap/slapd.args olcPidFile: /var/lib/openldap/slapd.pid dn: cn=schema,cn=config objectClass: olcSchemaConfig cn: schema dn: cn=module,cn=config objectClass: olcModuleList cn: module olcModulepath: /usr/libexec/openldap olcModuleload: back_mdb.la include: file:///etc/openldap/schema/core.ldif include: file:///etc/openldap/schema/cosine.ldif include: file:///etc/openldap/schema/nis.ldif include: file:///etc/openldap/schema/inetorgperson.ldif include: file:///etc/openldap/schema/ppolicy.ldif include: file:///etc/openldap/schema/sudo.ldif dn: olcDatabase=frontend,cn=config objectClass: olcDatabaseConfig objectClass: olcFrontendConfig olcDatabase: frontend olcAccess: to dn.base=\"cn=Subschema\" by * read olcAccess: to * by dn.base=\"gidNumber=0+uidNumber=0,cn=peercred,cn=external,cn=auth\" manage by * none dn: olcDatabase=config,cn=config objectClass: olcDatabaseConfig olcDatabase: config olcRootDN: cn=config olcAccess: to * by dn.base=\"gidNumber=0+uidNumber=0,cn=peercred,cn=external,cn=auth\" manage by * none Make sure slapadd -n 0 -F /etc/openldap/slapd.d -l /etc/openldap/slapd.ldif -u runs without error Run slapadd -n 0 -F /etc/openldap/slapd.d -l /etc/openldap/slapd.ldif && chown -R ldap:ldap /etc/openldap/slapd.d/* && systemctl daemon-reload && systemctl enable --now slapd && systemctl status slapd SLAPD LOGGING NOT WORKING Run slappasswd and note the hash output. Helpful Commands To start the IPA service use ipactl start|stop|restart . You can check the status with ipactl status .","title":"Setting Up OpenLDAP with OpenManage"},{"location":"LDAP%20with%20OpenManage/README-CentOS-OpenLDAP/#setting-up-openldap-with-openmanage","text":"","title":"Setting Up OpenLDAP with OpenManage"},{"location":"LDAP%20with%20OpenManage/README-CentOS-OpenLDAP/#my-environment","text":"","title":"My Environment"},{"location":"LDAP%20with%20OpenManage/README-CentOS-OpenLDAP/#centos-version","text":"CentOS Linux release 8.2.2004 (Core) NAME=\"CentOS Linux\" VERSION=\"8 (Core)\" ID=\"centos\" ID_LIKE=\"rhel fedora\" VERSION_ID=\"8\" PLATFORM_ID=\"platform:el8\" PRETTY_NAME=\"CentOS Linux 8 (Core)\" ANSI_COLOR=\"0;31\" CPE_NAME=\"cpe:/o:centos:centos:8\" HOME_URL=\"https://www.centos.org/\" BUG_REPORT_URL=\"https://bugs.centos.org/\" CENTOS_MANTISBT_PROJECT=\"CentOS-8\" CENTOS_MANTISBT_PROJECT_VERSION=\"8\" REDHAT_SUPPORT_PRODUCT=\"centos\" REDHAT_SUPPORT_PRODUCT_VERSION=\"8\" CentOS Linux release 8.2.2004 (Core) CentOS Linux release 8.2.2004 (Core)","title":"CentOS Version"},{"location":"LDAP%20with%20OpenManage/README-CentOS-OpenLDAP/#ipa-version","text":"[root@centos ~]# ipa --version VERSION: 4.8.4, API_VERSION: 2.235","title":"IPA Version"},{"location":"LDAP%20with%20OpenManage/README-CentOS-OpenLDAP/#openmanage-version","text":"Version 3.5.0 (Build 60)","title":"OpenManage Version"},{"location":"LDAP%20with%20OpenManage/README-CentOS-OpenLDAP/#helpful-resources","text":"Dell Tutorial Logs Explained LDAP Result Codes Helpful Post on Bind DN OpenManage User's Guide","title":"Helpful Resources"},{"location":"LDAP%20with%20OpenManage/README-CentOS-OpenLDAP/#install-instructions","text":"Install CentOS8 1.I installed CentOS minimal 2.Make sure NTP is working correctly Install OpenManage For the OpenLDAP setup I followed Install and Setup OpenLDAP on CentOS 8 by koromicha I want the current version of OpenLDAP so I'll be building it from source. Install dependencies with dnf install -y cyrus-sasl-devel make libtool autoconf libtool-ltdl-devel openssl-devel libdb-devel tar gcc perl perl-devel wget vim Create non privileged system user: useradd -r -M -d /var/lib/openldap -u 55 -s /usr/sbin/nologin ldap Pull tarball wget https://www.openldap.org/software/download/OpenLDAP/openldap-release/openldap-2.4.54.tgz && tar xzf openldap-2.4.54.tgz && cd openldap-2.4.54 Build and install OpenLDAP ./configure --prefix=/usr --sysconfdir=/etc --disable-static --enable-debug --with-tls=openssl --with-cyrus-sasl --enable-dynamic --enable-crypt --enable-spasswd --enable-slapd --enable-modules --enable-rlookups --enable-backends=mod --disable-ndb --disable-sql --disable-shell --disable-bdb --disable-hdb --enable-overlays=mod && make depend && make -j2 && make install Configure OpenLDAP mkdir /var/lib/openldap /etc/openldap/slapd.d && chown -R ldap:ldap /var/lib/openldap && chown root:ldap /etc/openldap/slapd.conf Add an OpenLDAP systemd service vim /etc/systemd/system/slapd.service [Unit] Description=OpenLDAP Server Daemon After=syslog.target network-online.target Documentation=man:slapd Documentation=man:slapd-mdb [Service] Type=forking PIDFile=/var/lib/openldap/slapd.pid Environment=\"SLAPD_URLS=ldap:/// ldapi:/// ldaps:///\" Environment=\"SLAPD_OPTIONS=-F /etc/openldap/slapd.d\" ExecStart=/usr/libexec/slapd -u ldap -g ldap -h ${SLAPD_URLS} $SLAPD_OPTIONS [Install] WantedBy=multi-user.target Check if your version of sudo supports lday with sudo -V | grep -i \"ldap\" and confirm the below lines are present: ldap.conf path: /etc/sudo-ldap.conf ldap.secret path: /etc/ldap.secret Make sure LDAP sudo schema is available with rpm -ql sudo | grep -i schema.openldap . You should see /usr/share/doc/sudo/schema.OpenLDAP Run: cp /usr/share/doc/sudo/schema.OpenLDAP /etc/openldap/schema/sudo.schema cat << 'EOL' > /etc/openldap/schema/sudo.ldif dn: cn=sudo,cn=schema,cn=config objectClass: olcSchemaConfig cn: sudo olcAttributeTypes: ( 1.3.6.1.4.1.15953.9.1.1 NAME 'sudoUser' DESC 'User(s) who may run sudo' EQUALITY caseExactIA5Match SUBSTR caseExactIA5SubstringsMatch SYNTAX 1.3.6.1.4.1.1466.115.121.1.26 ) olcAttributeTypes: ( 1.3.6.1.4.1.15953.9.1.2 NAME 'sudoHost' DESC 'Host(s) who may run sudo' EQUALITY caseExactIA5Match SUBSTR caseExactIA5SubstringsMatch SYNTAX 1.3.6.1.4.1.1466.115.121.1.26 ) olcAttributeTypes: ( 1.3.6.1.4.1.15953.9.1.3 NAME 'sudoCommand' DESC 'Command(s) to be executed by sudo' EQUALITY caseExactIA5Match SYNTAX 1.3.6.1.4.1.1466.115.121.1.26 ) olcAttributeTypes: ( 1.3.6.1.4.1.15953.9.1.4 NAME 'sudoRunAs' DESC 'User(s) impersonated by sudo (deprecated)' EQUALITY caseExactIA5Match SYNTAX 1.3.6.1.4.1.1466.115.121.1.26 ) olcAttributeTypes: ( 1.3.6.1.4.1.15953.9.1.5 NAME 'sudoOption' DESC 'Options(s) followed by sudo' EQUALITY caseExactIA5Match SYNTAX 1.3.6.1.4.1.1466.115.121.1.26 ) olcAttributeTypes: ( 1.3.6.1.4.1.15953.9.1.6 NAME 'sudoRunAsUser' DESC 'User(s) impersonated by sudo' EQUALITY caseExactIA5Match SYNTAX 1.3.6.1.4.1.1466.115.121.1.26 ) olcAttributeTypes: ( 1.3.6.1.4.1.15953.9.1.7 NAME 'sudoRunAsGroup' DESC 'Group(s) impersonated by sudo' EQUALITY caseExactIA5Match SYNTAX 1.3.6.1.4.1.1466.115.121.1.26 ) olcObjectClasses: ( 1.3.6.1.4.1.15953.9.2.1 NAME 'sudoRole' SUP top STRUCTURAL DESC 'Sudoer Entries' MUST ( cn ) MAY ( sudoUser $ sudoHost $ sudoCommand $ sudoRunAs $ sudoRunAsUser $ sudoRunAsGroup $ sudoOption $ description ) ) EOL mv /etc/openldap/slapd.ldif /etc/openldap/slapd.ldif.bak vim /etc/openldap/slapd.ldif dn: cn=config objectClass: olcGlobal cn: config olcArgsFile: /var/lib/openldap/slapd.args olcPidFile: /var/lib/openldap/slapd.pid dn: cn=schema,cn=config objectClass: olcSchemaConfig cn: schema dn: cn=module,cn=config objectClass: olcModuleList cn: module olcModulepath: /usr/libexec/openldap olcModuleload: back_mdb.la include: file:///etc/openldap/schema/core.ldif include: file:///etc/openldap/schema/cosine.ldif include: file:///etc/openldap/schema/nis.ldif include: file:///etc/openldap/schema/inetorgperson.ldif include: file:///etc/openldap/schema/ppolicy.ldif include: file:///etc/openldap/schema/sudo.ldif dn: olcDatabase=frontend,cn=config objectClass: olcDatabaseConfig objectClass: olcFrontendConfig olcDatabase: frontend olcAccess: to dn.base=\"cn=Subschema\" by * read olcAccess: to * by dn.base=\"gidNumber=0+uidNumber=0,cn=peercred,cn=external,cn=auth\" manage by * none dn: olcDatabase=config,cn=config objectClass: olcDatabaseConfig olcDatabase: config olcRootDN: cn=config olcAccess: to * by dn.base=\"gidNumber=0+uidNumber=0,cn=peercred,cn=external,cn=auth\" manage by * none Make sure slapadd -n 0 -F /etc/openldap/slapd.d -l /etc/openldap/slapd.ldif -u runs without error Run slapadd -n 0 -F /etc/openldap/slapd.d -l /etc/openldap/slapd.ldif && chown -R ldap:ldap /etc/openldap/slapd.d/* && systemctl daemon-reload && systemctl enable --now slapd && systemctl status slapd SLAPD LOGGING NOT WORKING Run slappasswd and note the hash output.","title":"Install Instructions"},{"location":"LDAP%20with%20OpenManage/README-CentOS-OpenLDAP/#helpful-commands","text":"To start the IPA service use ipactl start|stop|restart . You can check the status with ipactl status .","title":"Helpful Commands"},{"location":"LDAP%20with%20OpenManage/README-FreeIPA-CENTOS/","text":"Setting Up FreeIPA with OpenManage Conclusion: Currently FreeIPA isn't supported or tested against OpenManage. See the User's Guide page 137. I'm going to try it with OpenLDAP My Environment CentOS Version CentOS Linux release 8.2.2004 (Core) NAME=\"CentOS Linux\" VERSION=\"8 (Core)\" ID=\"centos\" ID_LIKE=\"rhel fedora\" VERSION_ID=\"8\" PLATFORM_ID=\"platform:el8\" PRETTY_NAME=\"CentOS Linux 8 (Core)\" ANSI_COLOR=\"0;31\" CPE_NAME=\"cpe:/o:centos:centos:8\" HOME_URL=\"https://www.centos.org/\" BUG_REPORT_URL=\"https://bugs.centos.org/\" CENTOS_MANTISBT_PROJECT=\"CentOS-8\" CENTOS_MANTISBT_PROJECT_VERSION=\"8\" REDHAT_SUPPORT_PRODUCT=\"centos\" REDHAT_SUPPORT_PRODUCT_VERSION=\"8\" CentOS Linux release 8.2.2004 (Core) CentOS Linux release 8.2.2004 (Core) FreeIPA Version [root@centos ~]# ipa --version VERSION: 4.8.4, API_VERSION: 2.235 OpenManage Version Version 3.4.1 (Build 24) Helpful Resources Dell Tutorial Logs Explained LDAP Result Codes Helpful Post on Bind DN Install Instructions Install CentOS8 1.I installed CentOS minimal 2.Make sure NTP is working correctly Install OpenManage Install the idm system module with dnf install -y @idm:DL1 freeipa-server Configure your DNS server ( /etc/hosts did not work for me) with a record for the hostname of your FreeIPA server. I added a record for centos.grant.lan . Run ipa-server-install 1.If you have any DNS failures edit the file /tmp/ipa.system.records.tu5qyl09.db (you may have to change the name) and add the record centos.grant.lan 86400 IN A 192.168.1.92 (adjust accordingly). Afterwards run ipa dns-update-system-records Run kinit admin Open firewall ports firewall-cmd --add-port=80/tcp --permanent --zone=public firewall-cmd --add-port=443/tcp --permanent --zone=public firewall-cmd --add-port=389/tcp --permanent --zone=public firewall-cmd --add-port=636/tcp --permanent --zone=public firewall-cmd --add-port=88/tcp --permanent --zone=public firewall-cmd --add-port=464/tcp --permanent --zone=public firewall-cmd --add-port=88/udp --permanent --zone=public firewall-cmd --add-port=464/udp --permanent --zone=public firewall-cmd --add-port=123/udp --permanent --zone=public firewall-cmd --reload Log into FreeIPA server at https://centos.grant.lan . In my case, Windows popped up a username and password prompt. That prompt didn't work - I had to exit it and then log into the webGUI. Under Active users I added an admin user. Go to Users and then directory services in OpenManage. I used the following: Note: You can get the Bind DN by running ldapsearch from the command line. Helpful Commands To start the IPA service use ipactl start|stop|restart . You can check the status with ipactl status . Testing Scenario 1 This got me a working test connection: Output [15/Oct/2020:14:09:18.440103345 -0400] conn=107 fd=74 slot=74 SSL connection from 192.168.1.93 to 192.168.1.92 [15/Oct/2020:14:09:18.484254084 -0400] conn=107 TLS1.2 128-bit AES-GCM [15/Oct/2020:14:09:18.484862509 -0400] conn=107 op=0 BIND dn=\"\" method=128 version=3 [15/Oct/2020:14:09:18.485048511 -0400] conn=107 op=0 RESULT err=0 tag=97 nentries=0 etime=0.044677059 dn=\"\" [15/Oct/2020:14:09:18.485743204 -0400] conn=107 op=1 SRCH base=\"dc=grant,dc=lan\" scope=2 filter=\"(uid=grant)\" attrs=ALL [15/Oct/2020:14:09:18.487440884 -0400] conn=107 op=1 RESULT err=0 tag=101 nentries=1 etime=0.001795760 [15/Oct/2020:14:09:18.488143502 -0400] conn=107 op=2 UNBIND [15/Oct/2020:14:09:18.488159848 -0400] conn=107 op=2 fd=74 closed - U1 [15/Oct/2020:14:09:18.491313979 -0400] conn=108 fd=74 slot=74 SSL connection from 192.168.1.93 to 192.168.1.92 [15/Oct/2020:14:09:18.536590087 -0400] conn=108 TLS1.2 128-bit AES-GCM [15/Oct/2020:14:09:18.537372005 -0400] conn=108 op=0 BIND dn=\"uid=grant,cn=users,cn=compat,dc=grant,dc=lan\" method=128 version=3 [15/Oct/2020:14:09:18.538144502 -0400] conn=108 op=0 RESULT err=0 tag=97 nentries=0 etime=0.046223517 dn=\"uid=grant,cn=users,cn=accounts,dc=grant,dc=lan\" [15/Oct/2020:14:09:18.538536207 -0400] conn=108 op=1 UNBIND [15/Oct/2020:14:09:18.538566004 -0400] conn=108 op=1 fd=74 closed - U1 [15/Oct/2020:14:09:28.961238173 -0400] conn=109 fd=74 slot=74 SSL connection from 192.168.1.93 to 192.168.1.92 [15/Oct/2020:14:09:29.005228025 -0400] conn=109 TLS1.2 128-bit AES-GCM [15/Oct/2020:14:09:29.005755286 -0400] conn=109 op=0 BIND dn=\"\" method=128 version=3 [15/Oct/2020:14:09:29.005931161 -0400] conn=109 op=0 RESULT err=0 tag=97 nentries=0 etime=0.044397507 dn=\"\" [15/Oct/2020:14:09:29.006898618 -0400] conn=109 op=1 SRCH base=\"dc=grant,dc=lan\" scope=2 filter=\"(uid=grant)\" attrs=ALL [15/Oct/2020:14:09:29.008536186 -0400] conn=109 op=1 RESULT err=0 tag=101 nentries=1 etime=0.001740822 [15/Oct/2020:14:09:29.009182689 -0400] conn=109 op=2 UNBIND [15/Oct/2020:14:09:29.009196697 -0400] conn=109 op=2 fd=74 closed - U1 [15/Oct/2020:14:09:29.012428320 -0400] conn=110 fd=74 slot=74 SSL connection from 192.168.1.93 to 192.168.1.92 [15/Oct/2020:14:09:29.057469132 -0400] conn=110 TLS1.2 128-bit AES-GCM [15/Oct/2020:14:09:29.058084024 -0400] conn=110 op=0 BIND dn=\"uid=grant,cn=users,cn=compat,dc=grant,dc=lan\" method=128 version=3 [15/Oct/2020:14:09:29.058825635 -0400] conn=110 op=0 RESULT err=0 tag=97 nentries=0 etime=0.046016675 dn=\"uid=grant,cn=users,cn=accounts,dc=grant,dc=lan\" [15/Oct/2020:14:09:29.059166870 -0400] conn=110 op=1 UNBIND [15/Oct/2020:14:09:29.059195118 -0400] conn=110 op=1 fd=74 closed - U1 Scenario 2 When I added a Bind DN as shown in this video I get a failure. Error Message Log Output [15/Oct/2020:14:12:07.504942397 -0400] conn=112 fd=74 slot=74 SSL connection from 192.168.1.93 to 192.168.1.92 [15/Oct/2020:14:12:07.550456498 -0400] conn=112 TLS1.2 128-bit AES-GCM [15/Oct/2020:14:12:07.551077266 -0400] conn=112 op=0 BIND dn=\"uid=grant,cn=users,cn=accounts,dc=grant,dc=lan\" method=128 version=3 [15/Oct/2020:14:12:07.551767359 -0400] conn=112 op=0 RESULT err=0 tag=97 nentries=0 etime=0.046428458 dn=\"uid=grant,cn=users,cn=accounts,dc=grant,dc=lan\" [15/Oct/2020:14:12:07.552283396 -0400] conn=112 op=1 SRCH base=\"dc=grant,dc=lan\" scope=2 filter=\"(uid=grant)\" attrs=ALL [15/Oct/2020:14:12:07.553922212 -0400] conn=112 op=1 RESULT err=0 tag=101 nentries=1 etime=0.001752227 [15/Oct/2020:14:12:07.555518270 -0400] conn=112 op=2 UNBIND [15/Oct/2020:14:12:07.555534158 -0400] conn=112 op=2 fd=74 closed - U1 Scenario 3 - Current Sticking Point Using the settings from scenario 1, I continued. When trying to add a group though, no groups are displayed in available groups. Output [15/Oct/2020:15:19:49.806462707 -0400] conn=169 fd=103 slot=103 SSL connection from 192.168.1.18 to 192.168.1.92 [15/Oct/2020:15:19:49.856773316 -0400] conn=169 TLS1.2 128-bit AES-GCM [15/Oct/2020:15:19:49.858681446 -0400] conn=169 op=0 BIND dn=\"\" method=128 version=3 [15/Oct/2020:15:19:49.858906917 -0400] conn=169 op=0 RESULT err=0 tag=97 nentries=0 etime=0.051584941 dn=\"\" [15/Oct/2020:15:19:49.864335125 -0400] conn=169 op=1 SRCH base=\"dc=grant,dc=lan\" scope=2 filter=\"(uid=grant)\" attrs=ALL [15/Oct/2020:15:19:49.866001831 -0400] conn=169 op=1 RESULT err=0 tag=101 nentries=1 etime=0.001790286 [15/Oct/2020:15:19:49.867368889 -0400] conn=169 op=2 UNBIND [15/Oct/2020:15:19:49.867386461 -0400] conn=169 op=2 fd=103 closed - U1 [15/Oct/2020:15:19:49.873375865 -0400] conn=170 fd=103 slot=103 SSL connection from 192.168.1.18 to 192.168.1.92 [15/Oct/2020:15:19:49.925956643 -0400] conn=170 TLS1.2 128-bit AES-GCM [15/Oct/2020:15:19:49.928180447 -0400] conn=170 op=0 BIND dn=\"uid=grant,cn=users,cn=compat,dc=grant,dc=lan\" method=128 version=3 [15/Oct/2020:15:19:49.928993026 -0400] conn=170 op=0 RESULT err=0 tag=97 nentries=0 etime=0.053916831 dn=\"uid=grant,cn=users,cn=accounts,dc=grant,dc=lan\" [15/Oct/2020:15:19:49.934942058 -0400] conn=170 op=1 SRCH base=\"dc=grant,dc=lan\" scope=2 filter=\"(&(cn=grantgroup*)(uniqueMember=*))\" attrs=\"cn entryuuid\" [15/Oct/2020:15:19:49.935438340 -0400] conn=170 op=1 RESULT err=0 tag=101 nentries=0 etime=0.000639539 [15/Oct/2020:15:19:49.936729725 -0400] conn=170 op=2 UNBIND [15/Oct/2020:15:19:49.936744908 -0400] conn=170 op=2 fd=103 closed - U1","title":"Setting Up FreeIPA with OpenManage"},{"location":"LDAP%20with%20OpenManage/README-FreeIPA-CENTOS/#setting-up-freeipa-with-openmanage","text":"Conclusion: Currently FreeIPA isn't supported or tested against OpenManage. See the User's Guide page 137. I'm going to try it with OpenLDAP","title":"Setting Up FreeIPA with OpenManage"},{"location":"LDAP%20with%20OpenManage/README-FreeIPA-CENTOS/#my-environment","text":"","title":"My Environment"},{"location":"LDAP%20with%20OpenManage/README-FreeIPA-CENTOS/#centos-version","text":"CentOS Linux release 8.2.2004 (Core) NAME=\"CentOS Linux\" VERSION=\"8 (Core)\" ID=\"centos\" ID_LIKE=\"rhel fedora\" VERSION_ID=\"8\" PLATFORM_ID=\"platform:el8\" PRETTY_NAME=\"CentOS Linux 8 (Core)\" ANSI_COLOR=\"0;31\" CPE_NAME=\"cpe:/o:centos:centos:8\" HOME_URL=\"https://www.centos.org/\" BUG_REPORT_URL=\"https://bugs.centos.org/\" CENTOS_MANTISBT_PROJECT=\"CentOS-8\" CENTOS_MANTISBT_PROJECT_VERSION=\"8\" REDHAT_SUPPORT_PRODUCT=\"centos\" REDHAT_SUPPORT_PRODUCT_VERSION=\"8\" CentOS Linux release 8.2.2004 (Core) CentOS Linux release 8.2.2004 (Core)","title":"CentOS Version"},{"location":"LDAP%20with%20OpenManage/README-FreeIPA-CENTOS/#freeipa-version","text":"[root@centos ~]# ipa --version VERSION: 4.8.4, API_VERSION: 2.235","title":"FreeIPA Version"},{"location":"LDAP%20with%20OpenManage/README-FreeIPA-CENTOS/#openmanage-version","text":"Version 3.4.1 (Build 24)","title":"OpenManage Version"},{"location":"LDAP%20with%20OpenManage/README-FreeIPA-CENTOS/#helpful-resources","text":"Dell Tutorial Logs Explained LDAP Result Codes Helpful Post on Bind DN","title":"Helpful Resources"},{"location":"LDAP%20with%20OpenManage/README-FreeIPA-CENTOS/#install-instructions","text":"Install CentOS8 1.I installed CentOS minimal 2.Make sure NTP is working correctly Install OpenManage Install the idm system module with dnf install -y @idm:DL1 freeipa-server Configure your DNS server ( /etc/hosts did not work for me) with a record for the hostname of your FreeIPA server. I added a record for centos.grant.lan . Run ipa-server-install 1.If you have any DNS failures edit the file /tmp/ipa.system.records.tu5qyl09.db (you may have to change the name) and add the record centos.grant.lan 86400 IN A 192.168.1.92 (adjust accordingly). Afterwards run ipa dns-update-system-records Run kinit admin Open firewall ports firewall-cmd --add-port=80/tcp --permanent --zone=public firewall-cmd --add-port=443/tcp --permanent --zone=public firewall-cmd --add-port=389/tcp --permanent --zone=public firewall-cmd --add-port=636/tcp --permanent --zone=public firewall-cmd --add-port=88/tcp --permanent --zone=public firewall-cmd --add-port=464/tcp --permanent --zone=public firewall-cmd --add-port=88/udp --permanent --zone=public firewall-cmd --add-port=464/udp --permanent --zone=public firewall-cmd --add-port=123/udp --permanent --zone=public firewall-cmd --reload Log into FreeIPA server at https://centos.grant.lan . In my case, Windows popped up a username and password prompt. That prompt didn't work - I had to exit it and then log into the webGUI. Under Active users I added an admin user. Go to Users and then directory services in OpenManage. I used the following: Note: You can get the Bind DN by running ldapsearch from the command line.","title":"Install Instructions"},{"location":"LDAP%20with%20OpenManage/README-FreeIPA-CENTOS/#helpful-commands","text":"To start the IPA service use ipactl start|stop|restart . You can check the status with ipactl status .","title":"Helpful Commands"},{"location":"LDAP%20with%20OpenManage/README-FreeIPA-CENTOS/#testing","text":"","title":"Testing"},{"location":"LDAP%20with%20OpenManage/README-FreeIPA-CENTOS/#scenario-1","text":"This got me a working test connection:","title":"Scenario 1"},{"location":"LDAP%20with%20OpenManage/README-FreeIPA-CENTOS/#output","text":"[15/Oct/2020:14:09:18.440103345 -0400] conn=107 fd=74 slot=74 SSL connection from 192.168.1.93 to 192.168.1.92 [15/Oct/2020:14:09:18.484254084 -0400] conn=107 TLS1.2 128-bit AES-GCM [15/Oct/2020:14:09:18.484862509 -0400] conn=107 op=0 BIND dn=\"\" method=128 version=3 [15/Oct/2020:14:09:18.485048511 -0400] conn=107 op=0 RESULT err=0 tag=97 nentries=0 etime=0.044677059 dn=\"\" [15/Oct/2020:14:09:18.485743204 -0400] conn=107 op=1 SRCH base=\"dc=grant,dc=lan\" scope=2 filter=\"(uid=grant)\" attrs=ALL [15/Oct/2020:14:09:18.487440884 -0400] conn=107 op=1 RESULT err=0 tag=101 nentries=1 etime=0.001795760 [15/Oct/2020:14:09:18.488143502 -0400] conn=107 op=2 UNBIND [15/Oct/2020:14:09:18.488159848 -0400] conn=107 op=2 fd=74 closed - U1 [15/Oct/2020:14:09:18.491313979 -0400] conn=108 fd=74 slot=74 SSL connection from 192.168.1.93 to 192.168.1.92 [15/Oct/2020:14:09:18.536590087 -0400] conn=108 TLS1.2 128-bit AES-GCM [15/Oct/2020:14:09:18.537372005 -0400] conn=108 op=0 BIND dn=\"uid=grant,cn=users,cn=compat,dc=grant,dc=lan\" method=128 version=3 [15/Oct/2020:14:09:18.538144502 -0400] conn=108 op=0 RESULT err=0 tag=97 nentries=0 etime=0.046223517 dn=\"uid=grant,cn=users,cn=accounts,dc=grant,dc=lan\" [15/Oct/2020:14:09:18.538536207 -0400] conn=108 op=1 UNBIND [15/Oct/2020:14:09:18.538566004 -0400] conn=108 op=1 fd=74 closed - U1 [15/Oct/2020:14:09:28.961238173 -0400] conn=109 fd=74 slot=74 SSL connection from 192.168.1.93 to 192.168.1.92 [15/Oct/2020:14:09:29.005228025 -0400] conn=109 TLS1.2 128-bit AES-GCM [15/Oct/2020:14:09:29.005755286 -0400] conn=109 op=0 BIND dn=\"\" method=128 version=3 [15/Oct/2020:14:09:29.005931161 -0400] conn=109 op=0 RESULT err=0 tag=97 nentries=0 etime=0.044397507 dn=\"\" [15/Oct/2020:14:09:29.006898618 -0400] conn=109 op=1 SRCH base=\"dc=grant,dc=lan\" scope=2 filter=\"(uid=grant)\" attrs=ALL [15/Oct/2020:14:09:29.008536186 -0400] conn=109 op=1 RESULT err=0 tag=101 nentries=1 etime=0.001740822 [15/Oct/2020:14:09:29.009182689 -0400] conn=109 op=2 UNBIND [15/Oct/2020:14:09:29.009196697 -0400] conn=109 op=2 fd=74 closed - U1 [15/Oct/2020:14:09:29.012428320 -0400] conn=110 fd=74 slot=74 SSL connection from 192.168.1.93 to 192.168.1.92 [15/Oct/2020:14:09:29.057469132 -0400] conn=110 TLS1.2 128-bit AES-GCM [15/Oct/2020:14:09:29.058084024 -0400] conn=110 op=0 BIND dn=\"uid=grant,cn=users,cn=compat,dc=grant,dc=lan\" method=128 version=3 [15/Oct/2020:14:09:29.058825635 -0400] conn=110 op=0 RESULT err=0 tag=97 nentries=0 etime=0.046016675 dn=\"uid=grant,cn=users,cn=accounts,dc=grant,dc=lan\" [15/Oct/2020:14:09:29.059166870 -0400] conn=110 op=1 UNBIND [15/Oct/2020:14:09:29.059195118 -0400] conn=110 op=1 fd=74 closed - U1","title":"Output"},{"location":"LDAP%20with%20OpenManage/README-FreeIPA-CENTOS/#scenario-2","text":"When I added a Bind DN as shown in this video I get a failure.","title":"Scenario 2"},{"location":"LDAP%20with%20OpenManage/README-FreeIPA-CENTOS/#error-message","text":"","title":"Error Message"},{"location":"LDAP%20with%20OpenManage/README-FreeIPA-CENTOS/#log-output","text":"[15/Oct/2020:14:12:07.504942397 -0400] conn=112 fd=74 slot=74 SSL connection from 192.168.1.93 to 192.168.1.92 [15/Oct/2020:14:12:07.550456498 -0400] conn=112 TLS1.2 128-bit AES-GCM [15/Oct/2020:14:12:07.551077266 -0400] conn=112 op=0 BIND dn=\"uid=grant,cn=users,cn=accounts,dc=grant,dc=lan\" method=128 version=3 [15/Oct/2020:14:12:07.551767359 -0400] conn=112 op=0 RESULT err=0 tag=97 nentries=0 etime=0.046428458 dn=\"uid=grant,cn=users,cn=accounts,dc=grant,dc=lan\" [15/Oct/2020:14:12:07.552283396 -0400] conn=112 op=1 SRCH base=\"dc=grant,dc=lan\" scope=2 filter=\"(uid=grant)\" attrs=ALL [15/Oct/2020:14:12:07.553922212 -0400] conn=112 op=1 RESULT err=0 tag=101 nentries=1 etime=0.001752227 [15/Oct/2020:14:12:07.555518270 -0400] conn=112 op=2 UNBIND [15/Oct/2020:14:12:07.555534158 -0400] conn=112 op=2 fd=74 closed - U1","title":"Log Output"},{"location":"LDAP%20with%20OpenManage/README-FreeIPA-CENTOS/#scenario-3-current-sticking-point","text":"Using the settings from scenario 1, I continued. When trying to add a group though, no groups are displayed in available groups.","title":"Scenario 3 - Current Sticking Point"},{"location":"LDAP%20with%20OpenManage/README-FreeIPA-CENTOS/#output_1","text":"[15/Oct/2020:15:19:49.806462707 -0400] conn=169 fd=103 slot=103 SSL connection from 192.168.1.18 to 192.168.1.92 [15/Oct/2020:15:19:49.856773316 -0400] conn=169 TLS1.2 128-bit AES-GCM [15/Oct/2020:15:19:49.858681446 -0400] conn=169 op=0 BIND dn=\"\" method=128 version=3 [15/Oct/2020:15:19:49.858906917 -0400] conn=169 op=0 RESULT err=0 tag=97 nentries=0 etime=0.051584941 dn=\"\" [15/Oct/2020:15:19:49.864335125 -0400] conn=169 op=1 SRCH base=\"dc=grant,dc=lan\" scope=2 filter=\"(uid=grant)\" attrs=ALL [15/Oct/2020:15:19:49.866001831 -0400] conn=169 op=1 RESULT err=0 tag=101 nentries=1 etime=0.001790286 [15/Oct/2020:15:19:49.867368889 -0400] conn=169 op=2 UNBIND [15/Oct/2020:15:19:49.867386461 -0400] conn=169 op=2 fd=103 closed - U1 [15/Oct/2020:15:19:49.873375865 -0400] conn=170 fd=103 slot=103 SSL connection from 192.168.1.18 to 192.168.1.92 [15/Oct/2020:15:19:49.925956643 -0400] conn=170 TLS1.2 128-bit AES-GCM [15/Oct/2020:15:19:49.928180447 -0400] conn=170 op=0 BIND dn=\"uid=grant,cn=users,cn=compat,dc=grant,dc=lan\" method=128 version=3 [15/Oct/2020:15:19:49.928993026 -0400] conn=170 op=0 RESULT err=0 tag=97 nentries=0 etime=0.053916831 dn=\"uid=grant,cn=users,cn=accounts,dc=grant,dc=lan\" [15/Oct/2020:15:19:49.934942058 -0400] conn=170 op=1 SRCH base=\"dc=grant,dc=lan\" scope=2 filter=\"(&(cn=grantgroup*)(uniqueMember=*))\" attrs=\"cn entryuuid\" [15/Oct/2020:15:19:49.935438340 -0400] conn=170 op=1 RESULT err=0 tag=101 nentries=0 etime=0.000639539 [15/Oct/2020:15:19:49.936729725 -0400] conn=170 op=2 UNBIND [15/Oct/2020:15:19:49.936744908 -0400] conn=170 op=2 fd=103 closed - U1","title":"Output"},{"location":"LDAP%20with%20OpenManage/README-Turnkey-Linux/","text":"Setting Up OpenLDAP with OpenManage My Environment OpenManage Version Version 3.5.0 (Build 60) Helpful Resources Dell Tutorial LDAP Result Codes Helpful Post on Bind DN OpenManage User's Guide Fix This base cannot be created with PLA in phpldapadmin Turnkey OpenLDAP Instructions Download OpenLDAP appliance from here 1.Alternatively, you can build it yourself. [This tutorial](https://medium.com/@benjamin.dronen/installing-openldap-and-phpldapadmin-on-ubuntu-20-04-lts-7ef3ca40dc00 is helpful however you will have to add LDAPS for it to work with OpenManage Enterprise. Helpful Commands/Things Run slapd in Foreground sudo slapd -d 256 -d 128 View Database Configuration The database configuration for OpenLDAP is stored at /etc/ldap/slapd.d You can find a config your interested in with grep -R <THING> * . For example my user config was at cn\\=config/olcDatabase\\=\\{1\\}mdb.ldif . phpldapadmin Config Location /etc/phpldapadmin/config.php Line 300 has login stuff Use a SRV record for Discovery If you use DNS for Domain Controller Lookup when setting up LDAP what it will do is use a SRV record lookup to find your LDAP server. If you want discovery to happen this way, you just need to add the appropriate SRV record to your DNS server. I was using PFSense so I added the following in Custom options: server: local-data: \"_ldap._tcp.ubuntuldap.grant.lan 3600 IN SRV 0 100 389 ubuntuldap.grant.lan\" Potential Bug? When testing LDAP on OpenManage I noticed it would issue the message \"Unable to connect to the LDAP or AD server because the entered credentials are invalid.\" However, while watching Wireshark I noted this coincided with a failed DNS query. This error message appears to be a erroneous.","title":"Setting Up OpenLDAP with OpenManage"},{"location":"LDAP%20with%20OpenManage/README-Turnkey-Linux/#setting-up-openldap-with-openmanage","text":"","title":"Setting Up OpenLDAP with OpenManage"},{"location":"LDAP%20with%20OpenManage/README-Turnkey-Linux/#my-environment","text":"","title":"My Environment"},{"location":"LDAP%20with%20OpenManage/README-Turnkey-Linux/#openmanage-version","text":"Version 3.5.0 (Build 60)","title":"OpenManage Version"},{"location":"LDAP%20with%20OpenManage/README-Turnkey-Linux/#helpful-resources","text":"Dell Tutorial LDAP Result Codes Helpful Post on Bind DN OpenManage User's Guide Fix This base cannot be created with PLA in phpldapadmin Turnkey OpenLDAP","title":"Helpful Resources"},{"location":"LDAP%20with%20OpenManage/README-Turnkey-Linux/#instructions","text":"Download OpenLDAP appliance from here 1.Alternatively, you can build it yourself. [This tutorial](https://medium.com/@benjamin.dronen/installing-openldap-and-phpldapadmin-on-ubuntu-20-04-lts-7ef3ca40dc00 is helpful however you will have to add LDAPS for it to work with OpenManage Enterprise.","title":"Instructions"},{"location":"LDAP%20with%20OpenManage/README-Turnkey-Linux/#helpful-commandsthings","text":"","title":"Helpful Commands/Things"},{"location":"LDAP%20with%20OpenManage/README-Turnkey-Linux/#run-slapd-in-foreground","text":"sudo slapd -d 256 -d 128","title":"Run slapd in Foreground"},{"location":"LDAP%20with%20OpenManage/README-Turnkey-Linux/#view-database-configuration","text":"The database configuration for OpenLDAP is stored at /etc/ldap/slapd.d You can find a config your interested in with grep -R <THING> * . For example my user config was at cn\\=config/olcDatabase\\=\\{1\\}mdb.ldif .","title":"View Database Configuration"},{"location":"LDAP%20with%20OpenManage/README-Turnkey-Linux/#phpldapadmin-config-location","text":"/etc/phpldapadmin/config.php Line 300 has login stuff","title":"phpldapadmin Config Location"},{"location":"LDAP%20with%20OpenManage/README-Turnkey-Linux/#use-a-srv-record-for-discovery","text":"If you use DNS for Domain Controller Lookup when setting up LDAP what it will do is use a SRV record lookup to find your LDAP server. If you want discovery to happen this way, you just need to add the appropriate SRV record to your DNS server. I was using PFSense so I added the following in Custom options: server: local-data: \"_ldap._tcp.ubuntuldap.grant.lan 3600 IN SRV 0 100 389 ubuntuldap.grant.lan\"","title":"Use a SRV record for Discovery"},{"location":"LDAP%20with%20OpenManage/README-Turnkey-Linux/#potential-bug","text":"When testing LDAP on OpenManage I noticed it would issue the message \"Unable to connect to the LDAP or AD server because the entered credentials are invalid.\" However, while watching Wireshark I noted this coincided with a failed DNS query. This error message appears to be a erroneous.","title":"Potential Bug?"},{"location":"LDAP%20with%20OpenManage/keycloak/","text":"Some people like this: https://www.keycloak.org/","title":"Keycloak"},{"location":"LDAP%20with%20OpenManage/LDAP%20Group%20Import%20Bug/","text":"Setting Up FreeIPA with OpenManage My Environment RHEL Version NAME=\"Red Hat Enterprise Linux\" VERSION=\"8.2 (Ootpa)\" ID=\"rhel\" ID_LIKE=\"fedora\" VERSION_ID=\"8.2\" PLATFORM_ID=\"platform:el8\" PRETTY_NAME=\"Red Hat Enterprise Linux 8.2 (Ootpa)\" ANSI_COLOR=\"0;31\" CPE_NAME=\"cpe:/o:redhat:enterprise_linux:8.2:GA\" HOME_URL=\"https://www.redhat.com/\" BUG_REPORT_URL=\"https://bugzilla.redhat.com/\" REDHAT_BUGZILLA_PRODUCT=\"Red Hat Enterprise Linux 8\" REDHAT_BUGZILLA_PRODUCT_VERSION=8.2 REDHAT_SUPPORT_PRODUCT=\"Red Hat Enterprise Linux\" REDHAT_SUPPORT_PRODUCT_VERSION=\"8.2\" Red Hat Enterprise Linux release 8.2 (Ootpa) Red Hat Enterprise Linux release 8.2 (Ootpa) FreeIPA Version [root@centos ~]# ipa --version VERSION: 4.8.4, API_VERSION: 2.235 OpenManage Version Version 3.4.1 (Build 24) Helpful Resources Dell Tutorial Logs Explained LDAP Result Codes Helpful Post on Bind DN Install Instructions Install RHEL Change hostname 1. hostname freeipa.grant.lan && hostnamectl set-hostname freeipa.grant.lan 2.Change in /etc/hostname 3.Configure DNS to return for this hostname. Double check with dig +short freeipa.grant.lan A && dig +short -x 192.168.1.95 Follow RHEL's instructions 1.I used Chapter 5 for primary installation Run kinit admin - this allows you to use the command line tools otherwise they'll complain about kerberos. Log into FreeIPA server at https://<your_hostname> . In my case, Windows popped up a username and password prompt. That prompt didn't work - I had to exit it and then log into the webGUI. Go to Users and then directory services in OpenManage. I used the following: Note: You can get the Bind DN by running ldapsearch from the command line. Create a new user and new group in the UI and assign the new user to the new group. Install OpenManage Go to Application Settings -> Directory Services Substitute with your values and then click test. I wasn't able to get this to work with the generic admin user. In the test screen I used that new user to connect to directory services Helpful Commands To start the IPA service use ipactl start|stop|restart . You can check the status with ipactl status . Bug I used the settings defined here: When I went to import the users from a group I received the following: The code in question: Below was the value of u at runtime: [ { \"userTypeId\":2, \"objectGuid\":null, \"objectSid\":null, \"directoryServiceId\":13483, \"name\":\"grantgroup\", \"password\":\"\", \"userName\":\"grantgroup\", \"roleId\":\"10\", \"locked\":false, \"isBuiltin\":false, \"enabled\":true } ] Error in logs [ERROR] 2020-10-22 07:33:08.392 [ajp-bio-8009-exec-2] BaseController - com.dell.enterprise.exception.ui.ConsoleException: error.general_known_error_occurred com.dell.enterprise.exception.ui.ConsoleException: error.general_known_error_occurred at com.dell.enterprise.controller.console.ADController.importGroups(ADController.java:400) ~[UI.ADPlugin-0.0.1-SNAPSHOT.jar:?] at sun.reflect.GeneratedMethodAccessor824.invoke(Unknown Source) ~[?:?] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_262] at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_262] at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:205) ~[spring-web-4.3.28.RELEASE.jar:4.3.28.RELEASE] at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:133) ~[spring-web-4.3.28.RELEASE.jar:4.3.28.RELEASE] at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:97) ~[spring-webmvc-4.3.28.RELEASE.jar:4.3.28.RELEASE] at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:854) ~[spring-webmvc-4.3.28.RELEASE.jar:4.3.28.RELEASE] at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:765) ~[spring-webmvc-4.3.28.RELEASE.jar:4.3.28.RELEASE] at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:85) ~[spring-webmvc-4.3.28.RELEASE.jar:4.3.28.RELEASE] at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:967) [spring-webmvc-4.3.28.RELEASE.jar:4.3.28.RELEASE] at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:901) [spring-webmvc-4.3.28.RELEASE.jar:4.3.28.RELEASE] at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:970) [spring-webmvc-4.3.28.RELEASE.jar:4.3.28.RELEASE] at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:872) [spring-webmvc-4.3.28.RELEASE.jar:4.3.28.RELEASE] at javax.servlet.http.HttpServlet.service(HttpServlet.java:650) [tomcat-servlet-3.0-api.jar:?] at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:846) [spring-webmvc-4.3.28.RELEASE.jar:4.3.28.RELEASE] at javax.servlet.http.HttpServlet.service(HttpServlet.java:731) [tomcat-servlet-3.0-api.jar:?] at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:303) [catalina.jar:7.0.76] at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208) [catalina.jar:7.0.76] at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:52) [tomcat7-websocket.jar:7.0.76] at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241) [catalina.jar:7.0.76] at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208) [catalina.jar:7.0.76] at com.dell.enterprise.filter.ui.CacheControlFilter.doFilterInternal(CacheControlFilter.java:23) [classes/:?] at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-4.3.28.RELEASE.jar:4.3.28.RELEASE] at org.springframework.web.filter.DelegatingFilterProxy.invokeDelegate(DelegatingFilterProxy.java:347) [spring-web-4.3.28.RELEASE.jar:4.3.28.RELEASE] at org.springframework.web.filter.DelegatingFilterProxy.doFilter(DelegatingFilterProxy.java:263) [spring-web-4.3.28.RELEASE.jar:4.3.28.RELEASE] at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241) [catalina.jar:7.0.76] at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208) [catalina.jar:7.0.76] at org.apache.catalina.filters.SetCharacterEncodingFilter.doFilter(SetCharacterEncodingFilter.java:108) [catalina.jar:7.0.76] at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241) [catalina.jar:7.0.76] at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208) [catalina.jar:7.0.76] at org.apache.shiro.web.servlet.ProxiedFilterChain.doFilter(ProxiedFilterChain.java:61) [shiro-web-1.6.0.jar:1.6.0] at org.apache.shiro.web.servlet.AdviceFilter.executeChain(AdviceFilter.java:108) [shiro-web-1.6.0.jar:1.6.0] at org.apache.shiro.web.servlet.AdviceFilter.doFilterInternal(AdviceFilter.java:137) [shiro-web-1.6.0.jar:1.6.0] at org.apache.shiro.web.servlet.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:125) [shiro-web-1.6.0.jar:1.6.0] at org.apache.shiro.web.servlet.ProxiedFilterChain.doFilter(ProxiedFilterChain.java:66) [shiro-web-1.6.0.jar:1.6.0] at org.apache.shiro.web.servlet.AdviceFilter.executeChain(AdviceFilter.java:108) [shiro-web-1.6.0.jar:1.6.0] at org.apache.shiro.web.servlet.AdviceFilter.doFilterInternal(AdviceFilter.java:137) [shiro-web-1.6.0.jar:1.6.0] at org.apache.shiro.web.servlet.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:125) [shiro-web-1.6.0.jar:1.6.0] at org.apache.shiro.web.servlet.ProxiedFilterChain.doFilter(ProxiedFilterChain.java:66) [shiro-web-1.6.0.jar:1.6.0] at org.apache.shiro.web.servlet.AbstractShiroFilter.executeChain(AbstractShiroFilter.java:450) [shiro-web-1.6.0.jar:1.6.0] at org.apache.shiro.web.servlet.AbstractShiroFilter$1.call(AbstractShiroFilter.java:365) [shiro-web-1.6.0.jar:1.6.0] at org.apache.shiro.subject.support.SubjectCallable.doCall(SubjectCallable.java:90) [shiro-core-1.6.0.jar:1.6.0] at org.apache.shiro.subject.support.SubjectCallable.call(SubjectCallable.java:83) [shiro-core-1.6.0.jar:1.6.0] at org.apache.shiro.subject.support.DelegatingSubject.execute(DelegatingSubject.java:387) [shiro-core-1.6.0.jar:1.6.0] at org.apache.shiro.web.servlet.AbstractShiroFilter.doFilterInternal(AbstractShiroFilter.java:362) [shiro-web-1.6.0.jar:1.6.0] at org.apache.shiro.web.servlet.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:125) [shiro-web-1.6.0.jar:1.6.0] at org.springframework.web.filter.DelegatingFilterProxy.invokeDelegate(DelegatingFilterProxy.java:347) [spring-web-4.3.28.RELEASE.jar:4.3.28.RELEASE] at org.springframework.web.filter.DelegatingFilterProxy.doFilter(DelegatingFilterProxy.java:263) [spring-web-4.3.28.RELEASE.jar:4.3.28.RELEASE] at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241) [catalina.jar:7.0.76] at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208) [catalina.jar:7.0.76] at com.dell.enterprise.core.integration.lib.common.filter.RequestFilter.doFilter(RequestFilter.java:103) [common-0.0.1-SNAPSHOT.jar:?] at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241) [catalina.jar:7.0.76] at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208) [catalina.jar:7.0.76] at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:218) [catalina.jar:7.0.76] at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:110) [catalina.jar:7.0.76] at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:498) [catalina.jar:7.0.76] at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:169) [catalina.jar:7.0.76] at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:103) [catalina.jar:7.0.76] at org.apache.catalina.valves.AccessLogValve.invoke(AccessLogValve.java:962) [catalina.jar:7.0.76] at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:116) [catalina.jar:7.0.76] at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:445) [catalina.jar:7.0.76] at org.apache.coyote.ajp.AjpProcessor.process(AjpProcessor.java:190) [tomcat-coyote.jar:7.0.76] at org.apache.coyote.AbstractProtocol$AbstractConnectionHandler.process(AbstractProtocol.java:637) [tomcat-coyote.jar:7.0.76] at org.apache.tomcat.util.net.JIoEndpoint$SocketProcessor.run(JIoEndpoint.java:316) [tomcat-coyote.jar:7.0.76] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_262] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_262] at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61) [tomcat-coyote.jar:7.0.76] at java.lang.Thread.run(Thread.java:748) [?:1.8.0_262] Further Explanation The URI endpoint is: https://192.168.1.93/core/api/Console/import-groups This is the JSON returned from the call. {severity: \"IGNORE\", message: \"error.general_known_error_occurred\",\u2026} debug: false details: {error: {code: \"Base.1.0.GeneralError\",\u2026}} error: {code: \"Base.1.0.GeneralError\",\u2026} @Message.ExtendedInfo: [{MessageId: \"CGEN1004\", RelatedProperties: [],\u2026}] code: \"Base.1.0.GeneralError\" message: \"A general error has occurred. See ExtendedInfo for more information.\" message: \"error.general_known_error_occurred\" severity: \"IGNORE\" timestamp: \"2020-10-21T14:43:44.385-0500\" The errors occurs at: M.send(y(u) ? null : u) See here for a description of Javascript's ternary operator. In this case it is saying if y(u) is true then the value is set to null, otherwise it is set to u . The y function is: function y(e) { return \"undefined\" == typeof e } It is just a basic check to see if u is defined or not. M is an instance of XMLHttpRequest . We can see M being called with open M.open(i, s, !0) where i is \"POST\" and s is \"/core/api/Console/import-groups\". The problem occurs because objectGuid and objectSid are set to null. Resolution See duplicate_bug.py for a replication of the problem. Replace the payload: { \"userTypeId\":2, \"objectGuid\":null, \"objectSid\":null, \"directoryServiceId\":13483, \"name\":\"grantgroup\", \"password\":\"\", \"userName\":\"grantgroup\", \"roleId\":\"10\", \"locked\":false, \"isBuiltin\":false, \"enabled\":true } with the data from your instance. I grabbed this out of the javascript debugger. To fix the problem, you have to lookup the uid/gid (which correspond to objectSid and objectGuid respectively) on your LDAP server and replace the null values. I used ldapsearch to find mine: # grant, users, compat, grant.lan dn: uid=grant,cn=users,cn=compat,dc=grant,dc=lan objectClass: posixAccount objectClass: ipaOverrideTarget objectClass: top gecos: Grant Curell cn: Grant Curell uidNumber: 1314600001 gidNumber: 1314600001 loginShell: /bin/sh homeDirectory: /home/grant ipaAnchorUUID:: OklQQTpncmFudC5sYW46OWIzOTYwNDQtMTNhZS0xMWViLTllNzctMDA1MDU2Ym U4NGIw uid: grant You can see the uidNumber and gidNumber fields. Change the payload out in duplicate_bug.py and it will correctly import the group. test_payload = [ { \"userTypeId\": 2, \"objectGuid\": 1314600001, \"objectSid\": 1314600001, \"directoryServiceId\": 13483, \"name\": \"grantgroup\", \"password\": \"\", \"userName\": \"grant\", \"roleId\": \"10\", \"locked\": False, \"isBuiltin\": False, \"enabled\": True } ]","title":"Setting Up FreeIPA with OpenManage"},{"location":"LDAP%20with%20OpenManage/LDAP%20Group%20Import%20Bug/#setting-up-freeipa-with-openmanage","text":"","title":"Setting Up FreeIPA with OpenManage"},{"location":"LDAP%20with%20OpenManage/LDAP%20Group%20Import%20Bug/#my-environment","text":"","title":"My Environment"},{"location":"LDAP%20with%20OpenManage/LDAP%20Group%20Import%20Bug/#rhel-version","text":"NAME=\"Red Hat Enterprise Linux\" VERSION=\"8.2 (Ootpa)\" ID=\"rhel\" ID_LIKE=\"fedora\" VERSION_ID=\"8.2\" PLATFORM_ID=\"platform:el8\" PRETTY_NAME=\"Red Hat Enterprise Linux 8.2 (Ootpa)\" ANSI_COLOR=\"0;31\" CPE_NAME=\"cpe:/o:redhat:enterprise_linux:8.2:GA\" HOME_URL=\"https://www.redhat.com/\" BUG_REPORT_URL=\"https://bugzilla.redhat.com/\" REDHAT_BUGZILLA_PRODUCT=\"Red Hat Enterprise Linux 8\" REDHAT_BUGZILLA_PRODUCT_VERSION=8.2 REDHAT_SUPPORT_PRODUCT=\"Red Hat Enterprise Linux\" REDHAT_SUPPORT_PRODUCT_VERSION=\"8.2\" Red Hat Enterprise Linux release 8.2 (Ootpa) Red Hat Enterprise Linux release 8.2 (Ootpa)","title":"RHEL Version"},{"location":"LDAP%20with%20OpenManage/LDAP%20Group%20Import%20Bug/#freeipa-version","text":"[root@centos ~]# ipa --version VERSION: 4.8.4, API_VERSION: 2.235","title":"FreeIPA Version"},{"location":"LDAP%20with%20OpenManage/LDAP%20Group%20Import%20Bug/#openmanage-version","text":"Version 3.4.1 (Build 24)","title":"OpenManage Version"},{"location":"LDAP%20with%20OpenManage/LDAP%20Group%20Import%20Bug/#helpful-resources","text":"Dell Tutorial Logs Explained LDAP Result Codes Helpful Post on Bind DN","title":"Helpful Resources"},{"location":"LDAP%20with%20OpenManage/LDAP%20Group%20Import%20Bug/#install-instructions","text":"Install RHEL Change hostname 1. hostname freeipa.grant.lan && hostnamectl set-hostname freeipa.grant.lan 2.Change in /etc/hostname 3.Configure DNS to return for this hostname. Double check with dig +short freeipa.grant.lan A && dig +short -x 192.168.1.95 Follow RHEL's instructions 1.I used Chapter 5 for primary installation Run kinit admin - this allows you to use the command line tools otherwise they'll complain about kerberos. Log into FreeIPA server at https://<your_hostname> . In my case, Windows popped up a username and password prompt. That prompt didn't work - I had to exit it and then log into the webGUI. Go to Users and then directory services in OpenManage. I used the following: Note: You can get the Bind DN by running ldapsearch from the command line. Create a new user and new group in the UI and assign the new user to the new group. Install OpenManage Go to Application Settings -> Directory Services Substitute with your values and then click test. I wasn't able to get this to work with the generic admin user. In the test screen I used that new user to connect to directory services","title":"Install Instructions"},{"location":"LDAP%20with%20OpenManage/LDAP%20Group%20Import%20Bug/#helpful-commands","text":"To start the IPA service use ipactl start|stop|restart . You can check the status with ipactl status .","title":"Helpful Commands"},{"location":"LDAP%20with%20OpenManage/LDAP%20Group%20Import%20Bug/#bug","text":"I used the settings defined here: When I went to import the users from a group I received the following: The code in question: Below was the value of u at runtime: [ { \"userTypeId\":2, \"objectGuid\":null, \"objectSid\":null, \"directoryServiceId\":13483, \"name\":\"grantgroup\", \"password\":\"\", \"userName\":\"grantgroup\", \"roleId\":\"10\", \"locked\":false, \"isBuiltin\":false, \"enabled\":true } ] Error in logs [ERROR] 2020-10-22 07:33:08.392 [ajp-bio-8009-exec-2] BaseController - com.dell.enterprise.exception.ui.ConsoleException: error.general_known_error_occurred com.dell.enterprise.exception.ui.ConsoleException: error.general_known_error_occurred at com.dell.enterprise.controller.console.ADController.importGroups(ADController.java:400) ~[UI.ADPlugin-0.0.1-SNAPSHOT.jar:?] at sun.reflect.GeneratedMethodAccessor824.invoke(Unknown Source) ~[?:?] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_262] at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_262] at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:205) ~[spring-web-4.3.28.RELEASE.jar:4.3.28.RELEASE] at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:133) ~[spring-web-4.3.28.RELEASE.jar:4.3.28.RELEASE] at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:97) ~[spring-webmvc-4.3.28.RELEASE.jar:4.3.28.RELEASE] at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:854) ~[spring-webmvc-4.3.28.RELEASE.jar:4.3.28.RELEASE] at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:765) ~[spring-webmvc-4.3.28.RELEASE.jar:4.3.28.RELEASE] at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:85) ~[spring-webmvc-4.3.28.RELEASE.jar:4.3.28.RELEASE] at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:967) [spring-webmvc-4.3.28.RELEASE.jar:4.3.28.RELEASE] at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:901) [spring-webmvc-4.3.28.RELEASE.jar:4.3.28.RELEASE] at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:970) [spring-webmvc-4.3.28.RELEASE.jar:4.3.28.RELEASE] at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:872) [spring-webmvc-4.3.28.RELEASE.jar:4.3.28.RELEASE] at javax.servlet.http.HttpServlet.service(HttpServlet.java:650) [tomcat-servlet-3.0-api.jar:?] at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:846) [spring-webmvc-4.3.28.RELEASE.jar:4.3.28.RELEASE] at javax.servlet.http.HttpServlet.service(HttpServlet.java:731) [tomcat-servlet-3.0-api.jar:?] at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:303) [catalina.jar:7.0.76] at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208) [catalina.jar:7.0.76] at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:52) [tomcat7-websocket.jar:7.0.76] at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241) [catalina.jar:7.0.76] at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208) [catalina.jar:7.0.76] at com.dell.enterprise.filter.ui.CacheControlFilter.doFilterInternal(CacheControlFilter.java:23) [classes/:?] at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-4.3.28.RELEASE.jar:4.3.28.RELEASE] at org.springframework.web.filter.DelegatingFilterProxy.invokeDelegate(DelegatingFilterProxy.java:347) [spring-web-4.3.28.RELEASE.jar:4.3.28.RELEASE] at org.springframework.web.filter.DelegatingFilterProxy.doFilter(DelegatingFilterProxy.java:263) [spring-web-4.3.28.RELEASE.jar:4.3.28.RELEASE] at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241) [catalina.jar:7.0.76] at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208) [catalina.jar:7.0.76] at org.apache.catalina.filters.SetCharacterEncodingFilter.doFilter(SetCharacterEncodingFilter.java:108) [catalina.jar:7.0.76] at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241) [catalina.jar:7.0.76] at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208) [catalina.jar:7.0.76] at org.apache.shiro.web.servlet.ProxiedFilterChain.doFilter(ProxiedFilterChain.java:61) [shiro-web-1.6.0.jar:1.6.0] at org.apache.shiro.web.servlet.AdviceFilter.executeChain(AdviceFilter.java:108) [shiro-web-1.6.0.jar:1.6.0] at org.apache.shiro.web.servlet.AdviceFilter.doFilterInternal(AdviceFilter.java:137) [shiro-web-1.6.0.jar:1.6.0] at org.apache.shiro.web.servlet.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:125) [shiro-web-1.6.0.jar:1.6.0] at org.apache.shiro.web.servlet.ProxiedFilterChain.doFilter(ProxiedFilterChain.java:66) [shiro-web-1.6.0.jar:1.6.0] at org.apache.shiro.web.servlet.AdviceFilter.executeChain(AdviceFilter.java:108) [shiro-web-1.6.0.jar:1.6.0] at org.apache.shiro.web.servlet.AdviceFilter.doFilterInternal(AdviceFilter.java:137) [shiro-web-1.6.0.jar:1.6.0] at org.apache.shiro.web.servlet.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:125) [shiro-web-1.6.0.jar:1.6.0] at org.apache.shiro.web.servlet.ProxiedFilterChain.doFilter(ProxiedFilterChain.java:66) [shiro-web-1.6.0.jar:1.6.0] at org.apache.shiro.web.servlet.AbstractShiroFilter.executeChain(AbstractShiroFilter.java:450) [shiro-web-1.6.0.jar:1.6.0] at org.apache.shiro.web.servlet.AbstractShiroFilter$1.call(AbstractShiroFilter.java:365) [shiro-web-1.6.0.jar:1.6.0] at org.apache.shiro.subject.support.SubjectCallable.doCall(SubjectCallable.java:90) [shiro-core-1.6.0.jar:1.6.0] at org.apache.shiro.subject.support.SubjectCallable.call(SubjectCallable.java:83) [shiro-core-1.6.0.jar:1.6.0] at org.apache.shiro.subject.support.DelegatingSubject.execute(DelegatingSubject.java:387) [shiro-core-1.6.0.jar:1.6.0] at org.apache.shiro.web.servlet.AbstractShiroFilter.doFilterInternal(AbstractShiroFilter.java:362) [shiro-web-1.6.0.jar:1.6.0] at org.apache.shiro.web.servlet.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:125) [shiro-web-1.6.0.jar:1.6.0] at org.springframework.web.filter.DelegatingFilterProxy.invokeDelegate(DelegatingFilterProxy.java:347) [spring-web-4.3.28.RELEASE.jar:4.3.28.RELEASE] at org.springframework.web.filter.DelegatingFilterProxy.doFilter(DelegatingFilterProxy.java:263) [spring-web-4.3.28.RELEASE.jar:4.3.28.RELEASE] at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241) [catalina.jar:7.0.76] at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208) [catalina.jar:7.0.76] at com.dell.enterprise.core.integration.lib.common.filter.RequestFilter.doFilter(RequestFilter.java:103) [common-0.0.1-SNAPSHOT.jar:?] at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241) [catalina.jar:7.0.76] at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208) [catalina.jar:7.0.76] at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:218) [catalina.jar:7.0.76] at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:110) [catalina.jar:7.0.76] at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:498) [catalina.jar:7.0.76] at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:169) [catalina.jar:7.0.76] at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:103) [catalina.jar:7.0.76] at org.apache.catalina.valves.AccessLogValve.invoke(AccessLogValve.java:962) [catalina.jar:7.0.76] at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:116) [catalina.jar:7.0.76] at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:445) [catalina.jar:7.0.76] at org.apache.coyote.ajp.AjpProcessor.process(AjpProcessor.java:190) [tomcat-coyote.jar:7.0.76] at org.apache.coyote.AbstractProtocol$AbstractConnectionHandler.process(AbstractProtocol.java:637) [tomcat-coyote.jar:7.0.76] at org.apache.tomcat.util.net.JIoEndpoint$SocketProcessor.run(JIoEndpoint.java:316) [tomcat-coyote.jar:7.0.76] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_262] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_262] at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61) [tomcat-coyote.jar:7.0.76] at java.lang.Thread.run(Thread.java:748) [?:1.8.0_262]","title":"Bug"},{"location":"LDAP%20with%20OpenManage/LDAP%20Group%20Import%20Bug/#further-explanation","text":"The URI endpoint is: https://192.168.1.93/core/api/Console/import-groups This is the JSON returned from the call. {severity: \"IGNORE\", message: \"error.general_known_error_occurred\",\u2026} debug: false details: {error: {code: \"Base.1.0.GeneralError\",\u2026}} error: {code: \"Base.1.0.GeneralError\",\u2026} @Message.ExtendedInfo: [{MessageId: \"CGEN1004\", RelatedProperties: [],\u2026}] code: \"Base.1.0.GeneralError\" message: \"A general error has occurred. See ExtendedInfo for more information.\" message: \"error.general_known_error_occurred\" severity: \"IGNORE\" timestamp: \"2020-10-21T14:43:44.385-0500\" The errors occurs at: M.send(y(u) ? null : u) See here for a description of Javascript's ternary operator. In this case it is saying if y(u) is true then the value is set to null, otherwise it is set to u . The y function is: function y(e) { return \"undefined\" == typeof e } It is just a basic check to see if u is defined or not. M is an instance of XMLHttpRequest . We can see M being called with open M.open(i, s, !0) where i is \"POST\" and s is \"/core/api/Console/import-groups\". The problem occurs because objectGuid and objectSid are set to null.","title":"Further Explanation"},{"location":"LDAP%20with%20OpenManage/LDAP%20Group%20Import%20Bug/#resolution","text":"See duplicate_bug.py for a replication of the problem. Replace the payload: { \"userTypeId\":2, \"objectGuid\":null, \"objectSid\":null, \"directoryServiceId\":13483, \"name\":\"grantgroup\", \"password\":\"\", \"userName\":\"grantgroup\", \"roleId\":\"10\", \"locked\":false, \"isBuiltin\":false, \"enabled\":true } with the data from your instance. I grabbed this out of the javascript debugger. To fix the problem, you have to lookup the uid/gid (which correspond to objectSid and objectGuid respectively) on your LDAP server and replace the null values. I used ldapsearch to find mine: # grant, users, compat, grant.lan dn: uid=grant,cn=users,cn=compat,dc=grant,dc=lan objectClass: posixAccount objectClass: ipaOverrideTarget objectClass: top gecos: Grant Curell cn: Grant Curell uidNumber: 1314600001 gidNumber: 1314600001 loginShell: /bin/sh homeDirectory: /home/grant ipaAnchorUUID:: OklQQTpncmFudC5sYW46OWIzOTYwNDQtMTNhZS0xMWViLTllNzctMDA1MDU2Ym U4NGIw uid: grant You can see the uidNumber and gidNumber fields. Change the payload out in duplicate_bug.py and it will correctly import the group. test_payload = [ { \"userTypeId\": 2, \"objectGuid\": 1314600001, \"objectSid\": 1314600001, \"directoryServiceId\": 13483, \"name\": \"grantgroup\", \"password\": \"\", \"userName\": \"grant\", \"roleId\": \"10\", \"locked\": False, \"isBuiltin\": False, \"enabled\": True } ]","title":"Resolution"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/","text":"Load Balance Testing on 4112F-ON w/OS10 See: Test Case 1 Test Case 2 Test Case 3 Test Case 4","title":"Load Balance Testing on 4112F-ON w/OS10"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/#load-balance-testing-on-4112f-on-wos10","text":"See: Test Case 1 Test Case 2 Test Case 3 Test Case 4","title":"Load Balance Testing on 4112F-ON w/OS10"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%201/","text":"Dell OS10 Load Balancing with LAG Config In this test case the goal is to create a simple load balancer using a reverse LAG port. The idea is to have one input port which is then mirrored to a logical LAG port and at the other end of the LAG port is a number of security sensors. Helpful Links ONIE Network Install Process Overview Dell OS10 Manual My Configuration General Configuration ONIE host is running RHEL 8 I am using a Dell S4112F-ON for testing OS10 10.5.0.2 PFSense running DNS and DHCP as services RHEL Release Info NAME=\"Red Hat Enterprise Linux\" VERSION=\"8.0 (Ootpa)\" ID=\"rhel\" ID_LIKE=\"fedora\" VERSION_ID=\"8.0\" PLATFORM_ID=\"platform:el8\" PRETTY_NAME=\"Red Hat Enterprise Linux 8.0 (Ootpa)\" ANSI_COLOR=\"0;31\" CPE_NAME=\"cpe:/o:redhat:enterprise_linux:8.0:GA\" HOME_URL=\"https://www.redhat.com/\" BUG_REPORT_URL=\"https://bugzilla.redhat.com/\" REDHAT_BUGZILLA_PRODUCT=\"Red Hat Enterprise Linux 8\" REDHAT_BUGZILLA_PRODUCT_VERSION=8.0 REDHAT_SUPPORT_PRODUCT=\"Red Hat Enterprise Linux\" REDHAT_SUPPORT_PRODUCT_VERSION=\"8.0\" Red Hat Enterprise Linux release 8.0 (Ootpa) Red Hat Enterprise Linux release 8.0 (Ootpa) OS 10 Version OS10# show version Dell EMC Networking OS10 Enterprise Copyright (c) 1999-2019 by Dell Inc. All Rights Reserved. OS Version: 10.5.0.2 Build Version: 10.5.0.2.468 Build Time: 2019-10-19T00:29:00+0000 System Type: S4112F-ON Architecture: x86_64 Up Time: 00:03:39 Setup ONIE Prerequisites See ONIE Install Setup for instructions. Configure Management Interface See Configure Management Interface on Dell OS10 Configure Device for Reverse LAG Physical Configuration I used the following SFPs 1, 1Gb/s copper SFP (Ethernet 1/1/1) for input 2, 1Gb/s copper SFPs (Ethernet 1/1/5/Ethernet 1/1/9) and 1, 1Gb/s, fiber SFP (Ethernet 1/1/12) for output I used the following optics: Input Port Output Ports LAG Configuration Enable LAG Ports and Input Port Verify All Interfaces are Running at the Same Speed All interfaces must be the same speed in a LAG. In my case, the fiber interface was running at 10Gb/s so I brought that down to 1Gb/s by doing the following: OS10(config)# interface ethernet 1/1/12 OS10(conf-if-eth1/1/12)# speed 1000 OS10(conf-if-eth1/1/12)# <165>1 2019-10-28T19:10:22.616888+00:00 OS10 dn_alm 669 - - Node.1-Unit.1:PRI [event], Dell EMC (OS10) %IFM_OSTATE_DN: Interface operational state is down :ethernet1/1/12 OS10(conf-if-eth1/1/12)# OS10(conf-if-eth1/1/12)# <165>1 2019-10-28T19:10:29.591467+00:00 OS10 dn_alm 669 - - Node.1-Unit.1:PRI [event], Dell EMC (OS10) %IFM_OSTATE_UP: Interface operational state is up :ethernet1/1/12 Add Interfaces to the Port Channel Group OS10(config)# interface port-channel 1 OS10(conf-if-po-1)# exit OS10(config)# interface ethernet 1/1/5 OS10(conf-if-eth1/1/5)# channel-group 1 mode on OS10(conf-if-eth1/1/5)# <165>1 2019-10-28T19:17:33.746593+00:00 OS10 dn_alm 669 - - Node.1-Unit.1:PRI [event], Dell EMC (OS10) %IFM_OSTATE_UP: Interface operational state is up :port-channel1 OS10(conf-if-eth1/1/5)# exit OS10(config)# interface ethernet 1/1/9 OS10(conf-if-eth1/1/9)# channel-group 1 mode on OS10(conf-if-eth1/1/9)# exit OS10(config)# interface ethernet 1/1/12 OS10(conf-if-eth1/1/12)# channel-group 1 mode on Configure the Port Channel Hash Algorithm We want to load balance on the standard network 5 tuple. You can configure this with OS10(config)# load-balancing ip-selection destination-ip source-ip protocol l4-destination-port l4-source-port Configure Mirror Port Session from Source to LAG Interface Next we need to send all the traffic from our \"TAP\" input interface to our port channel to be load balanced out to all of our listening devices. OS10(config)# monitor session 1 OS10(conf-mon-local-1)# source interface ethernet 1/1/1 OS10(conf-mon-local-1)# destination interface port-channel 1 OS10(conf-mon-local-1)# no shut Final Configuration OS10# show running-configuration ! Version 10.5.0.2 ! Last configuration change at Oct 29 14:53:37 2019 ! ip vrf default ! interface breakout 1/1/13 map 100g-1x interface breakout 1/1/14 map 100g-1x interface breakout 1/1/15 map 100g-1x iscsi enable iscsi target port 860 iscsi target port 3260 system-user linuxadmin password XXXXX username admin password XXXXX role sysadmin priv-lvl 15 aaa authentication login default local aaa authentication login console local ! class-map type application class-iscsi ! policy-map type application policy-iscsi ! interface vlan1 no shutdown ! interface port-channel1 no shutdown switchport access vlan 1 ! interface mgmt1/1/1 no shutdown no ip address dhcp ip address 192.168.1.20/24 ipv6 address autoconfig ! interface ethernet1/1/1 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/2 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/3 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/4 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/5 no shutdown channel-group 1 no switchport flowcontrol receive on ! interface ethernet1/1/6 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/7 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/8 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/9 no shutdown channel-group 1 no switchport flowcontrol receive on ! interface ethernet1/1/10 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/11 no shutdown no switchport flowcontrol receive on ! interface ethernet1/1/12 no shutdown channel-group 1 no switchport speed 1000 flowcontrol receive on ! interface ethernet1/1/13 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/14 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/15 no shutdown switchport access vlan 1 flowcontrol receive on ! monitor session 1 destination interface port-channel1 source interface ethernet1/1/1 no shut ! snmp-server contact \"Contact Support\" ! telemetry Findings The reverse LAG strategy will load balance traffic, but there is a critical problem. The hash algorithm is sensitive to the order of the fields. This means that in a standard TCP conversation as the IP/TCP/UDP source and destinations reverse for inbound and outbound traffic they will always go to different hosts on a five tuple hash. For example, see the below: Host 1 Host 2 Host 3 If you look at host 1 and host 3 you can see that both sides of the traffic consistently landed on different sessions. Without modifying the guts of how the algorithm itself is implemented, there isn't a way to fix this. IE: The idea isn't going to work. The reason for this is that security sensors like Bro and Suricata require the complete conversation be sent to a single instance. That is to say, a single instance of Bro or Suricata must see the entire conversation. The configuration above will cause an instance to see only one side of any given conversation. Other Notes The default VLAN on our OS10 switch is VLAN 1 and is untagged. The default configuration of a port is Switchport access vlan 1 on all ports (factory default) All ports will show in vlan 1, and vlan 1 will be labeled as the default vlan using command \u201csho vlan\u201d If you change the default vlan using the command \u201cdefault vlan-id\u201d it will change the switchport access vlan on all interfaces that were in the default vlan to the new specified default vlan. default vlan-id 3 all vlan 1 ports get changed to vlan 3 ports automatically (vlan 3 is the new default vlan), and the interfaces will sho Switchport access vlan 3 If you want any port to be in a different untagged vlan other the default vlan, you must change it via the command \u201cswitchport access vlan \u201d On a trunk port, the default vlan is the native vlan. If you want to change the native vlan on trunk port, then you use the command \u201cswitchport access vlan \u201d So in my example I sent earlier The default vlan is vlan 1 on all ports except the trunk port. sho run will sho Switchport access vlan 1 on all interfaces except the trunk port because I changed it. I specified vlan 2 as the native vlan for the trunk port only. Untagged VLAN ==> switchport access vlan 2 Tagged VLAN ==> switchport trunk allowed vlan 1612-1615,3939 Example: interface ethernet1/1/17 description Node1_Port1 switchport mode trunk switchport access vlan 2 switchport trunk allowed vlan 1612-1615,3939 spanning-tree port type edge no shutdown","title":"Dell OS10 Load Balancing with LAG Config"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%201/#dell-os10-load-balancing-with-lag-config","text":"In this test case the goal is to create a simple load balancer using a reverse LAG port. The idea is to have one input port which is then mirrored to a logical LAG port and at the other end of the LAG port is a number of security sensors.","title":"Dell OS10 Load Balancing with LAG Config"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%201/#helpful-links","text":"ONIE Network Install Process Overview Dell OS10 Manual","title":"Helpful Links"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%201/#my-configuration","text":"","title":"My Configuration"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%201/#general-configuration","text":"ONIE host is running RHEL 8 I am using a Dell S4112F-ON for testing OS10 10.5.0.2 PFSense running DNS and DHCP as services","title":"General Configuration"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%201/#rhel-release-info","text":"NAME=\"Red Hat Enterprise Linux\" VERSION=\"8.0 (Ootpa)\" ID=\"rhel\" ID_LIKE=\"fedora\" VERSION_ID=\"8.0\" PLATFORM_ID=\"platform:el8\" PRETTY_NAME=\"Red Hat Enterprise Linux 8.0 (Ootpa)\" ANSI_COLOR=\"0;31\" CPE_NAME=\"cpe:/o:redhat:enterprise_linux:8.0:GA\" HOME_URL=\"https://www.redhat.com/\" BUG_REPORT_URL=\"https://bugzilla.redhat.com/\" REDHAT_BUGZILLA_PRODUCT=\"Red Hat Enterprise Linux 8\" REDHAT_BUGZILLA_PRODUCT_VERSION=8.0 REDHAT_SUPPORT_PRODUCT=\"Red Hat Enterprise Linux\" REDHAT_SUPPORT_PRODUCT_VERSION=\"8.0\" Red Hat Enterprise Linux release 8.0 (Ootpa) Red Hat Enterprise Linux release 8.0 (Ootpa)","title":"RHEL Release Info"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%201/#os-10-version","text":"OS10# show version Dell EMC Networking OS10 Enterprise Copyright (c) 1999-2019 by Dell Inc. All Rights Reserved. OS Version: 10.5.0.2 Build Version: 10.5.0.2.468 Build Time: 2019-10-19T00:29:00+0000 System Type: S4112F-ON Architecture: x86_64 Up Time: 00:03:39","title":"OS 10 Version"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%201/#setup-onie-prerequisites","text":"See ONIE Install Setup for instructions.","title":"Setup ONIE Prerequisites"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%201/#configure-management-interface","text":"See Configure Management Interface on Dell OS10","title":"Configure Management Interface"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%201/#configure-device-for-reverse-lag","text":"","title":"Configure Device for Reverse LAG"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%201/#physical-configuration","text":"I used the following SFPs 1, 1Gb/s copper SFP (Ethernet 1/1/1) for input 2, 1Gb/s copper SFPs (Ethernet 1/1/5/Ethernet 1/1/9) and 1, 1Gb/s, fiber SFP (Ethernet 1/1/12) for output I used the following optics:","title":"Physical Configuration"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%201/#input-port","text":"","title":"Input Port"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%201/#output-ports","text":"","title":"Output Ports"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%201/#lag-configuration","text":"","title":"LAG Configuration"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%201/#enable-lag-ports-and-input-port","text":"","title":"Enable LAG Ports and Input Port"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%201/#verify-all-interfaces-are-running-at-the-same-speed","text":"All interfaces must be the same speed in a LAG. In my case, the fiber interface was running at 10Gb/s so I brought that down to 1Gb/s by doing the following: OS10(config)# interface ethernet 1/1/12 OS10(conf-if-eth1/1/12)# speed 1000 OS10(conf-if-eth1/1/12)# <165>1 2019-10-28T19:10:22.616888+00:00 OS10 dn_alm 669 - - Node.1-Unit.1:PRI [event], Dell EMC (OS10) %IFM_OSTATE_DN: Interface operational state is down :ethernet1/1/12 OS10(conf-if-eth1/1/12)# OS10(conf-if-eth1/1/12)# <165>1 2019-10-28T19:10:29.591467+00:00 OS10 dn_alm 669 - - Node.1-Unit.1:PRI [event], Dell EMC (OS10) %IFM_OSTATE_UP: Interface operational state is up :ethernet1/1/12","title":"Verify All Interfaces are Running at the Same Speed"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%201/#add-interfaces-to-the-port-channel-group","text":"OS10(config)# interface port-channel 1 OS10(conf-if-po-1)# exit OS10(config)# interface ethernet 1/1/5 OS10(conf-if-eth1/1/5)# channel-group 1 mode on OS10(conf-if-eth1/1/5)# <165>1 2019-10-28T19:17:33.746593+00:00 OS10 dn_alm 669 - - Node.1-Unit.1:PRI [event], Dell EMC (OS10) %IFM_OSTATE_UP: Interface operational state is up :port-channel1 OS10(conf-if-eth1/1/5)# exit OS10(config)# interface ethernet 1/1/9 OS10(conf-if-eth1/1/9)# channel-group 1 mode on OS10(conf-if-eth1/1/9)# exit OS10(config)# interface ethernet 1/1/12 OS10(conf-if-eth1/1/12)# channel-group 1 mode on","title":"Add Interfaces to the Port Channel Group"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%201/#configure-the-port-channel-hash-algorithm","text":"We want to load balance on the standard network 5 tuple. You can configure this with OS10(config)# load-balancing ip-selection destination-ip source-ip protocol l4-destination-port l4-source-port","title":"Configure the Port Channel Hash Algorithm"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%201/#configure-mirror-port-session-from-source-to-lag-interface","text":"Next we need to send all the traffic from our \"TAP\" input interface to our port channel to be load balanced out to all of our listening devices. OS10(config)# monitor session 1 OS10(conf-mon-local-1)# source interface ethernet 1/1/1 OS10(conf-mon-local-1)# destination interface port-channel 1 OS10(conf-mon-local-1)# no shut","title":"Configure Mirror Port Session from Source to LAG Interface"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%201/#final-configuration","text":"OS10# show running-configuration ! Version 10.5.0.2 ! Last configuration change at Oct 29 14:53:37 2019 ! ip vrf default ! interface breakout 1/1/13 map 100g-1x interface breakout 1/1/14 map 100g-1x interface breakout 1/1/15 map 100g-1x iscsi enable iscsi target port 860 iscsi target port 3260 system-user linuxadmin password XXXXX username admin password XXXXX role sysadmin priv-lvl 15 aaa authentication login default local aaa authentication login console local ! class-map type application class-iscsi ! policy-map type application policy-iscsi ! interface vlan1 no shutdown ! interface port-channel1 no shutdown switchport access vlan 1 ! interface mgmt1/1/1 no shutdown no ip address dhcp ip address 192.168.1.20/24 ipv6 address autoconfig ! interface ethernet1/1/1 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/2 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/3 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/4 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/5 no shutdown channel-group 1 no switchport flowcontrol receive on ! interface ethernet1/1/6 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/7 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/8 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/9 no shutdown channel-group 1 no switchport flowcontrol receive on ! interface ethernet1/1/10 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/11 no shutdown no switchport flowcontrol receive on ! interface ethernet1/1/12 no shutdown channel-group 1 no switchport speed 1000 flowcontrol receive on ! interface ethernet1/1/13 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/14 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/15 no shutdown switchport access vlan 1 flowcontrol receive on ! monitor session 1 destination interface port-channel1 source interface ethernet1/1/1 no shut ! snmp-server contact \"Contact Support\" ! telemetry","title":"Final Configuration"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%201/#findings","text":"The reverse LAG strategy will load balance traffic, but there is a critical problem. The hash algorithm is sensitive to the order of the fields. This means that in a standard TCP conversation as the IP/TCP/UDP source and destinations reverse for inbound and outbound traffic they will always go to different hosts on a five tuple hash. For example, see the below:","title":"Findings"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%201/#host-1","text":"","title":"Host 1"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%201/#host-2","text":"","title":"Host 2"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%201/#host-3","text":"If you look at host 1 and host 3 you can see that both sides of the traffic consistently landed on different sessions. Without modifying the guts of how the algorithm itself is implemented, there isn't a way to fix this. IE: The idea isn't going to work. The reason for this is that security sensors like Bro and Suricata require the complete conversation be sent to a single instance. That is to say, a single instance of Bro or Suricata must see the entire conversation. The configuration above will cause an instance to see only one side of any given conversation.","title":"Host 3"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%201/#other-notes","text":"The default VLAN on our OS10 switch is VLAN 1 and is untagged. The default configuration of a port is Switchport access vlan 1 on all ports (factory default) All ports will show in vlan 1, and vlan 1 will be labeled as the default vlan using command \u201csho vlan\u201d If you change the default vlan using the command \u201cdefault vlan-id\u201d it will change the switchport access vlan on all interfaces that were in the default vlan to the new specified default vlan. default vlan-id 3 all vlan 1 ports get changed to vlan 3 ports automatically (vlan 3 is the new default vlan), and the interfaces will sho Switchport access vlan 3 If you want any port to be in a different untagged vlan other the default vlan, you must change it via the command \u201cswitchport access vlan \u201d On a trunk port, the default vlan is the native vlan. If you want to change the native vlan on trunk port, then you use the command \u201cswitchport access vlan \u201d So in my example I sent earlier The default vlan is vlan 1 on all ports except the trunk port. sho run will sho Switchport access vlan 1 on all interfaces except the trunk port because I changed it. I specified vlan 2 as the native vlan for the trunk port only. Untagged VLAN ==> switchport access vlan 2 Tagged VLAN ==> switchport trunk allowed vlan 1612-1615,3939 Example: interface ethernet1/1/17 description Node1_Port1 switchport mode trunk switchport access vlan 2 switchport trunk allowed vlan 1612-1615,3939 spanning-tree port type edge no shutdown","title":"Other Notes"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%202/","text":"Dell OS10 Load Balancing with LAG Config In this test case the goal is to create a simple load balancer using a reverse LAG port. The idea is to have one input port which is then mirrored to a logical LAG port and at the other end of the LAG port is a number of security sensors. This test case was a duplicate of test 1 except with 1 port unplugged to see how it affected the algorithm. Helpful Links ONIE Network Install Process Overview Dell OS10 Manual My Configuration General Configuration ONIE host is running RHEL 8 I am using a Dell S4112F-ON for testing OS10 10.5.0.2 PFSense running DNS and DHCP as services RHEL Release Info NAME=\"Red Hat Enterprise Linux\" VERSION=\"8.0 (Ootpa)\" ID=\"rhel\" ID_LIKE=\"fedora\" VERSION_ID=\"8.0\" PLATFORM_ID=\"platform:el8\" PRETTY_NAME=\"Red Hat Enterprise Linux 8.0 (Ootpa)\" ANSI_COLOR=\"0;31\" CPE_NAME=\"cpe:/o:redhat:enterprise_linux:8.0:GA\" HOME_URL=\"https://www.redhat.com/\" BUG_REPORT_URL=\"https://bugzilla.redhat.com/\" REDHAT_BUGZILLA_PRODUCT=\"Red Hat Enterprise Linux 8\" REDHAT_BUGZILLA_PRODUCT_VERSION=8.0 REDHAT_SUPPORT_PRODUCT=\"Red Hat Enterprise Linux\" REDHAT_SUPPORT_PRODUCT_VERSION=\"8.0\" Red Hat Enterprise Linux release 8.0 (Ootpa) Red Hat Enterprise Linux release 8.0 (Ootpa) OS 10 Version OS10# show version Dell EMC Networking OS10 Enterprise Copyright (c) 1999-2019 by Dell Inc. All Rights Reserved. OS Version: 10.5.0.2 Build Version: 10.5.0.2.468 Build Time: 2019-10-19T00:29:00+0000 System Type: S4112F-ON Architecture: x86_64 Up Time: 00:03:39 Setup ONIE Prerequisites See ONIE Install Setup for instructions. Configure Management Interface See Configure Management Interface on Dell OS10 Configure Device for LAG Physical Configuration 1, 1Gb/s copper SFP (Ethernet 1/1/1) for input 1, 1Gb/s copper SFPs (Ethernet 1/1/5) and 1, 1Gb/s, fiber SFP (Ethernet 1/1/12) for output MAJOR DIFFERENCE WITH TEST 1 : In this test Ethernet 1/1/9 was disconnected. I actually did this by accident originally. I discovered VMWare autonegotiates to 10Gb/s and if you leave the interface at 1Gb/s the interface will not come up. Configuration - Same as Test 1 Except with Ethernet 1/1/9 Unplugged OS10(config)# do show ip interface brief Interface Name IP-Address OK Method Status Protocol ========================================================================================= Ethernet 1/1/1 unassigned YES unset up up Ethernet 1/1/2 unassigned NO unset up down Ethernet 1/1/3 unassigned NO unset up down Ethernet 1/1/4 unassigned NO unset up down Ethernet 1/1/5 unassigned YES unset up up Ethernet 1/1/6 unassigned NO unset up down Ethernet 1/1/7 unassigned NO unset up down Ethernet 1/1/8 unassigned NO unset up down Ethernet 1/1/9 unassigned NO unset up down Ethernet 1/1/10 unassigned NO unset up down Ethernet 1/1/11 unassigned NO unset up down Ethernet 1/1/12 unassigned YES unset up up Ethernet 1/1/13 unassigned NO unset up down Ethernet 1/1/14 unassigned NO unset up down Ethernet 1/1/15 unassigned NO unset up down Management 1/1/1 192.168.1.20/24 YES manual up up Vlan 1 unassigned YES unset up up Port-channel 1 unassigned YES unset up up OS10(config)# do show running-configuration ! Version 10.5.0.2 ! Last configuration change at Nov 01 01:09:30 2019 ! ip vrf default ! interface breakout 1/1/13 map 100g-1x interface breakout 1/1/14 map 100g-1x interface breakout 1/1/15 map 100g-1x iscsi enable iscsi target port 860 iscsi target port 3260 system-user linuxadmin password $6$5DdOHYg5$JCE1vMSmkQOrbh31U74PIPv7lyOgRmba1IxhkYibppMXs1KM4Y.gbTPcxyMP/PHUkMc5rdk/ZLv9Sfv3ALtB61 username admin password $6$q9QBeYjZ$jfxzVqGhkxX3smxJSH9DDz7/3OJc6m5wjF8nnLD7/VKx8SloIhp4NoGZs0I/UNwh8WVuxwfd9q4pWIgNs5BKH. role sysadmin priv-lvl 15 aaa authentication login default local aaa authentication login console local ! class-map type application class-iscsi ! policy-map type application policy-iscsi ! interface vlan1 no shutdown ! interface port-channel1 no shutdown switchport access vlan 1 ! interface mgmt1/1/1 no shutdown no ip address dhcp ip address 192.168.1.20/24 ipv6 address autoconfig ! interface ethernet1/1/1 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/2 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/3 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/4 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/5 no shutdown channel-group 1 no switchport speed 1000 flowcontrol receive on ! interface ethernet1/1/6 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/7 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/8 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/9 no shutdown channel-group 1 no switchport speed 1000 flowcontrol receive on ! interface ethernet1/1/10 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/11 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/12 no shutdown channel-group 1 no switchport speed 1000 flowcontrol receive on ! interface ethernet1/1/13 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/14 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/15 no shutdown switchport access vlan 1 flowcontrol receive on ! monitor session 1 destination interface port-channel1 source interface ethernet1/1/1 no shut ! snmp-server contact \"Contact Support\" ! telemetry Findings ~~I noticed in this configuration traffic appears to balance correctly. Will need to sit down and think on the math.~~ I am no longer convinced these results are valid. See test case 3. After further examination it looks like on the surface it is working when in reality it may not be. Interface 1/1/9 was down because ESXi was set to 10Gb/s and I had set the speed on 9 manually to 1Gb/s causing it to go down. Host 1 Host 2","title":"Dell OS10 Load Balancing with LAG Config"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%202/#dell-os10-load-balancing-with-lag-config","text":"In this test case the goal is to create a simple load balancer using a reverse LAG port. The idea is to have one input port which is then mirrored to a logical LAG port and at the other end of the LAG port is a number of security sensors. This test case was a duplicate of test 1 except with 1 port unplugged to see how it affected the algorithm.","title":"Dell OS10 Load Balancing with LAG Config"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%202/#helpful-links","text":"ONIE Network Install Process Overview Dell OS10 Manual","title":"Helpful Links"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%202/#my-configuration","text":"","title":"My Configuration"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%202/#general-configuration","text":"ONIE host is running RHEL 8 I am using a Dell S4112F-ON for testing OS10 10.5.0.2 PFSense running DNS and DHCP as services","title":"General Configuration"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%202/#rhel-release-info","text":"NAME=\"Red Hat Enterprise Linux\" VERSION=\"8.0 (Ootpa)\" ID=\"rhel\" ID_LIKE=\"fedora\" VERSION_ID=\"8.0\" PLATFORM_ID=\"platform:el8\" PRETTY_NAME=\"Red Hat Enterprise Linux 8.0 (Ootpa)\" ANSI_COLOR=\"0;31\" CPE_NAME=\"cpe:/o:redhat:enterprise_linux:8.0:GA\" HOME_URL=\"https://www.redhat.com/\" BUG_REPORT_URL=\"https://bugzilla.redhat.com/\" REDHAT_BUGZILLA_PRODUCT=\"Red Hat Enterprise Linux 8\" REDHAT_BUGZILLA_PRODUCT_VERSION=8.0 REDHAT_SUPPORT_PRODUCT=\"Red Hat Enterprise Linux\" REDHAT_SUPPORT_PRODUCT_VERSION=\"8.0\" Red Hat Enterprise Linux release 8.0 (Ootpa) Red Hat Enterprise Linux release 8.0 (Ootpa)","title":"RHEL Release Info"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%202/#os-10-version","text":"OS10# show version Dell EMC Networking OS10 Enterprise Copyright (c) 1999-2019 by Dell Inc. All Rights Reserved. OS Version: 10.5.0.2 Build Version: 10.5.0.2.468 Build Time: 2019-10-19T00:29:00+0000 System Type: S4112F-ON Architecture: x86_64 Up Time: 00:03:39","title":"OS 10 Version"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%202/#setup-onie-prerequisites","text":"See ONIE Install Setup for instructions.","title":"Setup ONIE Prerequisites"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%202/#configure-management-interface","text":"See Configure Management Interface on Dell OS10","title":"Configure Management Interface"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%202/#configure-device-for-lag","text":"","title":"Configure Device for LAG"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%202/#physical-configuration","text":"1, 1Gb/s copper SFP (Ethernet 1/1/1) for input 1, 1Gb/s copper SFPs (Ethernet 1/1/5) and 1, 1Gb/s, fiber SFP (Ethernet 1/1/12) for output MAJOR DIFFERENCE WITH TEST 1 : In this test Ethernet 1/1/9 was disconnected. I actually did this by accident originally. I discovered VMWare autonegotiates to 10Gb/s and if you leave the interface at 1Gb/s the interface will not come up.","title":"Physical Configuration"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%202/#configuration-same-as-test-1-except-with-ethernet-119-unplugged","text":"OS10(config)# do show ip interface brief Interface Name IP-Address OK Method Status Protocol ========================================================================================= Ethernet 1/1/1 unassigned YES unset up up Ethernet 1/1/2 unassigned NO unset up down Ethernet 1/1/3 unassigned NO unset up down Ethernet 1/1/4 unassigned NO unset up down Ethernet 1/1/5 unassigned YES unset up up Ethernet 1/1/6 unassigned NO unset up down Ethernet 1/1/7 unassigned NO unset up down Ethernet 1/1/8 unassigned NO unset up down Ethernet 1/1/9 unassigned NO unset up down Ethernet 1/1/10 unassigned NO unset up down Ethernet 1/1/11 unassigned NO unset up down Ethernet 1/1/12 unassigned YES unset up up Ethernet 1/1/13 unassigned NO unset up down Ethernet 1/1/14 unassigned NO unset up down Ethernet 1/1/15 unassigned NO unset up down Management 1/1/1 192.168.1.20/24 YES manual up up Vlan 1 unassigned YES unset up up Port-channel 1 unassigned YES unset up up OS10(config)# do show running-configuration ! Version 10.5.0.2 ! Last configuration change at Nov 01 01:09:30 2019 ! ip vrf default ! interface breakout 1/1/13 map 100g-1x interface breakout 1/1/14 map 100g-1x interface breakout 1/1/15 map 100g-1x iscsi enable iscsi target port 860 iscsi target port 3260 system-user linuxadmin password $6$5DdOHYg5$JCE1vMSmkQOrbh31U74PIPv7lyOgRmba1IxhkYibppMXs1KM4Y.gbTPcxyMP/PHUkMc5rdk/ZLv9Sfv3ALtB61 username admin password $6$q9QBeYjZ$jfxzVqGhkxX3smxJSH9DDz7/3OJc6m5wjF8nnLD7/VKx8SloIhp4NoGZs0I/UNwh8WVuxwfd9q4pWIgNs5BKH. role sysadmin priv-lvl 15 aaa authentication login default local aaa authentication login console local ! class-map type application class-iscsi ! policy-map type application policy-iscsi ! interface vlan1 no shutdown ! interface port-channel1 no shutdown switchport access vlan 1 ! interface mgmt1/1/1 no shutdown no ip address dhcp ip address 192.168.1.20/24 ipv6 address autoconfig ! interface ethernet1/1/1 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/2 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/3 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/4 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/5 no shutdown channel-group 1 no switchport speed 1000 flowcontrol receive on ! interface ethernet1/1/6 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/7 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/8 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/9 no shutdown channel-group 1 no switchport speed 1000 flowcontrol receive on ! interface ethernet1/1/10 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/11 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/12 no shutdown channel-group 1 no switchport speed 1000 flowcontrol receive on ! interface ethernet1/1/13 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/14 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/15 no shutdown switchport access vlan 1 flowcontrol receive on ! monitor session 1 destination interface port-channel1 source interface ethernet1/1/1 no shut ! snmp-server contact \"Contact Support\" ! telemetry","title":"Configuration - Same as Test 1 Except with Ethernet 1/1/9 Unplugged"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%202/#findings","text":"~~I noticed in this configuration traffic appears to balance correctly. Will need to sit down and think on the math.~~ I am no longer convinced these results are valid. See test case 3. After further examination it looks like on the surface it is working when in reality it may not be. Interface 1/1/9 was down because ESXi was set to 10Gb/s and I had set the speed on 9 manually to 1Gb/s causing it to go down.","title":"Findings"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%202/#host-1","text":"","title":"Host 1"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%202/#host-2","text":"","title":"Host 2"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%203/","text":"Dell OS10 Load Balancing with LAG Config In this test case the goal is to create a simple load balancer using a reverse LAG port. The idea is to have one input port which is then mirrored to a logical LAG port and at the other end of the LAG port is a number of security sensors. This is a duplicate of test 1 to verify my results. I began questioning myself after looking more closely at the data. Helpful Links ONIE Network Install Process Overview Dell OS10 Manual My Configuration General Configuration ONIE host is running RHEL 8 I am using a Dell S4112F-ON for testing OS10 10.5.0.2 PFSense running DNS and DHCP as services RHEL Release Info NAME=\"Red Hat Enterprise Linux\" VERSION=\"8.0 (Ootpa)\" ID=\"rhel\" ID_LIKE=\"fedora\" VERSION_ID=\"8.0\" PLATFORM_ID=\"platform:el8\" PRETTY_NAME=\"Red Hat Enterprise Linux 8.0 (Ootpa)\" ANSI_COLOR=\"0;31\" CPE_NAME=\"cpe:/o:redhat:enterprise_linux:8.0:GA\" HOME_URL=\"https://www.redhat.com/\" BUG_REPORT_URL=\"https://bugzilla.redhat.com/\" REDHAT_BUGZILLA_PRODUCT=\"Red Hat Enterprise Linux 8\" REDHAT_BUGZILLA_PRODUCT_VERSION=8.0 REDHAT_SUPPORT_PRODUCT=\"Red Hat Enterprise Linux\" REDHAT_SUPPORT_PRODUCT_VERSION=\"8.0\" Red Hat Enterprise Linux release 8.0 (Ootpa) Red Hat Enterprise Linux release 8.0 (Ootpa) OS 10 Version OS10# show version Dell EMC Networking OS10 Enterprise Copyright (c) 1999-2019 by Dell Inc. All Rights Reserved. OS Version: 10.5.0.2 Build Version: 10.5.0.2.468 Build Time: 2019-10-19T00:29:00+0000 System Type: S4112F-ON Architecture: x86_64 Up Time: 00:03:39 Setup ONIE Prerequisites See ONIE Install Setup for instructions. Configure Management Interface See Configure Management Interface on Dell OS10 Configure Device for LAG Physical Configuration 1, 1Gb/s copper SFP (Ethernet 1/1/1) for input 2, 1Gb/s copper SFPs (Ethernet 1/1/5/Ethernet 1/1/10) and 1, 1Gb/s, fiber SFP (Ethernet 1/1/12) for output Configuration - Same as Test 1 The only change was I moved from port 9 to port 10. ! Version 10.5.0.2 ! Last configuration change at Nov 01 01:52:29 2019 ! ip vrf default ! interface breakout 1/1/13 map 100g-1x interface breakout 1/1/14 map 100g-1x interface breakout 1/1/15 map 100g-1x iscsi enable iscsi target port 860 iscsi target port 3260 system-user linuxadmin password $6$5DdOHYg5$JCE1vMSmkQOrbh31U74PIPv7lyOgRmba1IxhkYibppMXs1KM4Y.gbTPcxyMP/PHUkMc5rdk/ZLv9Sfv3ALtB61 username admin password $6$q9QBeYjZ$jfxzVqGhkxX3smxJSH9DDz7/3OJc6m5wjF8nnLD7/VKx8SloIhp4NoGZs0I/UNwh8WVuxwfd9q4pWIgNs5BKH. role sysadmin priv-lvl 15 aaa authentication login default local aaa authentication login console local ! class-map type application class-iscsi ! policy-map type application policy-iscsi ! interface vlan1 no shutdown ! interface port-channel1 no shutdown switchport access vlan 1 ! interface mgmt1/1/1 no shutdown no ip address dhcp ip address 192.168.1.20/24 ipv6 address autoconfig ! interface ethernet1/1/1 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/2 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/3 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/4 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/5 no shutdown channel-group 1 no switchport speed 1000 flowcontrol receive on ! interface ethernet1/1/6 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/7 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/8 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/9 no shutdown switchport access vlan 1 speed 1000 flowcontrol receive on ! interface ethernet1/1/10 no shutdown channel-group 1 no switchport speed 1000 flowcontrol receive on ! interface ethernet1/1/11 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/12 no shutdown channel-group 1 no switchport speed 1000 flowcontrol receive on ! interface ethernet1/1/13 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/14 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/15 no shutdown switchport access vlan 1 flowcontrol receive on ! monitor session 1 destination interface port-channel1 source interface ethernet1/1/1 no shut ! snmp-server contact \"Contact Support\" ! telemetry Findings I confirmed that the traffic did indeed go to different simulated sensors. This time I stopped the traffic generator, reset all the Wireshark sessions and then restarted the traffic generator. I set the filter on Wireshark to 13.107.42.12 ahead of time on all three before examining a specific stream more closely as seen on host 3. On host 3 you can see the synchronize packet go out with sequence number 3195700332 and you notice that the SYN, ACK response is missing. Look at host 2 and you can see the SYN, ACK with the expected response of 3195700333. Host 1 Host 2 Host 3","title":"Dell OS10 Load Balancing with LAG Config"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%203/#dell-os10-load-balancing-with-lag-config","text":"In this test case the goal is to create a simple load balancer using a reverse LAG port. The idea is to have one input port which is then mirrored to a logical LAG port and at the other end of the LAG port is a number of security sensors. This is a duplicate of test 1 to verify my results. I began questioning myself after looking more closely at the data.","title":"Dell OS10 Load Balancing with LAG Config"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%203/#helpful-links","text":"ONIE Network Install Process Overview Dell OS10 Manual","title":"Helpful Links"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%203/#my-configuration","text":"","title":"My Configuration"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%203/#general-configuration","text":"ONIE host is running RHEL 8 I am using a Dell S4112F-ON for testing OS10 10.5.0.2 PFSense running DNS and DHCP as services","title":"General Configuration"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%203/#rhel-release-info","text":"NAME=\"Red Hat Enterprise Linux\" VERSION=\"8.0 (Ootpa)\" ID=\"rhel\" ID_LIKE=\"fedora\" VERSION_ID=\"8.0\" PLATFORM_ID=\"platform:el8\" PRETTY_NAME=\"Red Hat Enterprise Linux 8.0 (Ootpa)\" ANSI_COLOR=\"0;31\" CPE_NAME=\"cpe:/o:redhat:enterprise_linux:8.0:GA\" HOME_URL=\"https://www.redhat.com/\" BUG_REPORT_URL=\"https://bugzilla.redhat.com/\" REDHAT_BUGZILLA_PRODUCT=\"Red Hat Enterprise Linux 8\" REDHAT_BUGZILLA_PRODUCT_VERSION=8.0 REDHAT_SUPPORT_PRODUCT=\"Red Hat Enterprise Linux\" REDHAT_SUPPORT_PRODUCT_VERSION=\"8.0\" Red Hat Enterprise Linux release 8.0 (Ootpa) Red Hat Enterprise Linux release 8.0 (Ootpa)","title":"RHEL Release Info"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%203/#os-10-version","text":"OS10# show version Dell EMC Networking OS10 Enterprise Copyright (c) 1999-2019 by Dell Inc. All Rights Reserved. OS Version: 10.5.0.2 Build Version: 10.5.0.2.468 Build Time: 2019-10-19T00:29:00+0000 System Type: S4112F-ON Architecture: x86_64 Up Time: 00:03:39","title":"OS 10 Version"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%203/#setup-onie-prerequisites","text":"See ONIE Install Setup for instructions.","title":"Setup ONIE Prerequisites"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%203/#configure-management-interface","text":"See Configure Management Interface on Dell OS10","title":"Configure Management Interface"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%203/#configure-device-for-lag","text":"","title":"Configure Device for LAG"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%203/#physical-configuration","text":"1, 1Gb/s copper SFP (Ethernet 1/1/1) for input 2, 1Gb/s copper SFPs (Ethernet 1/1/5/Ethernet 1/1/10) and 1, 1Gb/s, fiber SFP (Ethernet 1/1/12) for output","title":"Physical Configuration"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%203/#configuration-same-as-test-1","text":"The only change was I moved from port 9 to port 10. ! Version 10.5.0.2 ! Last configuration change at Nov 01 01:52:29 2019 ! ip vrf default ! interface breakout 1/1/13 map 100g-1x interface breakout 1/1/14 map 100g-1x interface breakout 1/1/15 map 100g-1x iscsi enable iscsi target port 860 iscsi target port 3260 system-user linuxadmin password $6$5DdOHYg5$JCE1vMSmkQOrbh31U74PIPv7lyOgRmba1IxhkYibppMXs1KM4Y.gbTPcxyMP/PHUkMc5rdk/ZLv9Sfv3ALtB61 username admin password $6$q9QBeYjZ$jfxzVqGhkxX3smxJSH9DDz7/3OJc6m5wjF8nnLD7/VKx8SloIhp4NoGZs0I/UNwh8WVuxwfd9q4pWIgNs5BKH. role sysadmin priv-lvl 15 aaa authentication login default local aaa authentication login console local ! class-map type application class-iscsi ! policy-map type application policy-iscsi ! interface vlan1 no shutdown ! interface port-channel1 no shutdown switchport access vlan 1 ! interface mgmt1/1/1 no shutdown no ip address dhcp ip address 192.168.1.20/24 ipv6 address autoconfig ! interface ethernet1/1/1 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/2 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/3 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/4 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/5 no shutdown channel-group 1 no switchport speed 1000 flowcontrol receive on ! interface ethernet1/1/6 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/7 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/8 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/9 no shutdown switchport access vlan 1 speed 1000 flowcontrol receive on ! interface ethernet1/1/10 no shutdown channel-group 1 no switchport speed 1000 flowcontrol receive on ! interface ethernet1/1/11 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/12 no shutdown channel-group 1 no switchport speed 1000 flowcontrol receive on ! interface ethernet1/1/13 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/14 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/15 no shutdown switchport access vlan 1 flowcontrol receive on ! monitor session 1 destination interface port-channel1 source interface ethernet1/1/1 no shut ! snmp-server contact \"Contact Support\" ! telemetry","title":"Configuration - Same as Test 1"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%203/#findings","text":"I confirmed that the traffic did indeed go to different simulated sensors. This time I stopped the traffic generator, reset all the Wireshark sessions and then restarted the traffic generator. I set the filter on Wireshark to 13.107.42.12 ahead of time on all three before examining a specific stream more closely as seen on host 3. On host 3 you can see the synchronize packet go out with sequence number 3195700332 and you notice that the SYN, ACK response is missing. Look at host 2 and you can see the SYN, ACK with the expected response of 3195700333.","title":"Findings"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%203/#host-1","text":"","title":"Host 1"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%203/#host-2","text":"","title":"Host 2"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%203/#host-3","text":"","title":"Host 3"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%204/","text":"Dell OS10 Load Balancing with LAG Config In this test case the goal is to create a simple load balancer using a reverse LAG port. The idea is to have one input port which is then mirrored to a logical LAG port and at the other end of the LAG port is a number of security sensors. After test 3 I added the command: OS10(config)# enhanced-hashing resilient-hashing lag Helpful Links ONIE Network Install Process Overview Dell OS10 Manual My Configuration General Configuration ONIE host is running RHEL 8 I am using a Dell S4112F-ON for testing OS10 10.5.0.2 PFSense running DNS and DHCP as services RHEL Release Info NAME=\"Red Hat Enterprise Linux\" VERSION=\"8.0 (Ootpa)\" ID=\"rhel\" ID_LIKE=\"fedora\" VERSION_ID=\"8.0\" PLATFORM_ID=\"platform:el8\" PRETTY_NAME=\"Red Hat Enterprise Linux 8.0 (Ootpa)\" ANSI_COLOR=\"0;31\" CPE_NAME=\"cpe:/o:redhat:enterprise_linux:8.0:GA\" HOME_URL=\"https://www.redhat.com/\" BUG_REPORT_URL=\"https://bugzilla.redhat.com/\" REDHAT_BUGZILLA_PRODUCT=\"Red Hat Enterprise Linux 8\" REDHAT_BUGZILLA_PRODUCT_VERSION=8.0 REDHAT_SUPPORT_PRODUCT=\"Red Hat Enterprise Linux\" REDHAT_SUPPORT_PRODUCT_VERSION=\"8.0\" Red Hat Enterprise Linux release 8.0 (Ootpa) Red Hat Enterprise Linux release 8.0 (Ootpa) OS 10 Version OS10# show version Dell EMC Networking OS10 Enterprise Copyright (c) 1999-2019 by Dell Inc. All Rights Reserved. OS Version: 10.5.0.2 Build Version: 10.5.0.2.468 Build Time: 2019-10-19T00:29:00+0000 System Type: S4112F-ON Architecture: x86_64 Up Time: 00:03:39 Setup ONIE Prerequisites See ONIE Install Setup for instructions. Configure Management Interface See Configure Management Interface on Dell OS10 Configure Device for LAG Physical Configuration 1, 1Gb/s copper SFP (Ethernet 1/1/1) for input 2, 1Gb/s copper SFPs (Ethernet 1/1/5/Ethernet 1/1/10) and 1, 1Gb/s, fiber SFP (Ethernet 1/1/12) for output Configuration ! Version 10.5.0.2 ! Last configuration change at Nov 01 02:25:00 2019 ! ip vrf default ! interface breakout 1/1/13 map 100g-1x interface breakout 1/1/14 map 100g-1x interface breakout 1/1/15 map 100g-1x iscsi enable iscsi target port 860 iscsi target port 3260 system-user linuxadmin password $6$5DdOHYg5$JCE1vMSmkQOrbh31U74PIPv7lyOgRmba1IxhkYibppMXs1KM4Y.gbTPcxyMP/PHUkMc5rdk/ZLv9Sfv3ALtB61 enhanced-hashing resilient-hashing lag username admin password $6$q9QBeYjZ$jfxzVqGhkxX3smxJSH9DDz7/3OJc6m5wjF8nnLD7/VKx8SloIhp4NoGZs0I/UNwh8WVuxwfd9q4pWIgNs5BKH. role sysadmin priv-lvl 15 aaa authentication login default local aaa authentication login console local ! class-map type application class-iscsi ! policy-map type application policy-iscsi ! interface vlan1 no shutdown ! interface port-channel1 no shutdown switchport access vlan 1 ! interface mgmt1/1/1 no shutdown no ip address dhcp ip address 192.168.1.20/24 ipv6 address autoconfig ! interface ethernet1/1/1 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/2 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/3 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/4 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/5 no shutdown channel-group 1 no switchport speed 1000 flowcontrol receive on ! interface ethernet1/1/6 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/7 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/8 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/9 no shutdown switchport access vlan 1 speed 1000 flowcontrol receive on ! interface ethernet1/1/10 no shutdown channel-group 1 no switchport speed 1000 flowcontrol receive on ! interface ethernet1/1/11 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/12 no shutdown channel-group 1 no switchport speed 1000 flowcontrol receive on ! interface ethernet1/1/13 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/14 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/15 no shutdown switchport access vlan 1 flowcontrol receive on ! monitor session 1 destination interface port-channel1 source interface ethernet1/1/1 no shut ! snmp-server contact \"Contact Support\" ! telemetry Findings This time traffic still went to different Wireshark sessions as you can see in the below. On host 3 you can see the synchronize packet go out with sequence number 3195700332 and you notice that the SYN, ACK response is missing. Look at host 2 and you can see the SYN, ACK with the expected response of 3195700333. Traffic to Different Wireshark Sessions However a session on host 1 seems to work correctly. Host 1 Session with Correct Output A More Definitive Test I wanted to be sure of my findings so I crafted a new PCAP. This time, I started a capture on my desktop and opened a new connection to vCenter knowing this should generate several new streams. I then closed the browser entirely to ensure those same sessions would close. I saved the capture off and sent it to my traffic replay system. I then played it back with tcpreplay . I then grabbed a random stream from the sequence to confirm whether I could see the entire three way hand shake on one host or not. As suspected the initial syn hit one Wireshark session and the response went to a separate Wireshark session. Host 1 Host 3","title":"Dell OS10 Load Balancing with LAG Config"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%204/#dell-os10-load-balancing-with-lag-config","text":"In this test case the goal is to create a simple load balancer using a reverse LAG port. The idea is to have one input port which is then mirrored to a logical LAG port and at the other end of the LAG port is a number of security sensors. After test 3 I added the command: OS10(config)# enhanced-hashing resilient-hashing lag","title":"Dell OS10 Load Balancing with LAG Config"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%204/#helpful-links","text":"ONIE Network Install Process Overview Dell OS10 Manual","title":"Helpful Links"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%204/#my-configuration","text":"","title":"My Configuration"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%204/#general-configuration","text":"ONIE host is running RHEL 8 I am using a Dell S4112F-ON for testing OS10 10.5.0.2 PFSense running DNS and DHCP as services","title":"General Configuration"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%204/#rhel-release-info","text":"NAME=\"Red Hat Enterprise Linux\" VERSION=\"8.0 (Ootpa)\" ID=\"rhel\" ID_LIKE=\"fedora\" VERSION_ID=\"8.0\" PLATFORM_ID=\"platform:el8\" PRETTY_NAME=\"Red Hat Enterprise Linux 8.0 (Ootpa)\" ANSI_COLOR=\"0;31\" CPE_NAME=\"cpe:/o:redhat:enterprise_linux:8.0:GA\" HOME_URL=\"https://www.redhat.com/\" BUG_REPORT_URL=\"https://bugzilla.redhat.com/\" REDHAT_BUGZILLA_PRODUCT=\"Red Hat Enterprise Linux 8\" REDHAT_BUGZILLA_PRODUCT_VERSION=8.0 REDHAT_SUPPORT_PRODUCT=\"Red Hat Enterprise Linux\" REDHAT_SUPPORT_PRODUCT_VERSION=\"8.0\" Red Hat Enterprise Linux release 8.0 (Ootpa) Red Hat Enterprise Linux release 8.0 (Ootpa)","title":"RHEL Release Info"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%204/#os-10-version","text":"OS10# show version Dell EMC Networking OS10 Enterprise Copyright (c) 1999-2019 by Dell Inc. All Rights Reserved. OS Version: 10.5.0.2 Build Version: 10.5.0.2.468 Build Time: 2019-10-19T00:29:00+0000 System Type: S4112F-ON Architecture: x86_64 Up Time: 00:03:39","title":"OS 10 Version"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%204/#setup-onie-prerequisites","text":"See ONIE Install Setup for instructions.","title":"Setup ONIE Prerequisites"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%204/#configure-management-interface","text":"See Configure Management Interface on Dell OS10","title":"Configure Management Interface"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%204/#configure-device-for-lag","text":"","title":"Configure Device for LAG"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%204/#physical-configuration","text":"1, 1Gb/s copper SFP (Ethernet 1/1/1) for input 2, 1Gb/s copper SFPs (Ethernet 1/1/5/Ethernet 1/1/10) and 1, 1Gb/s, fiber SFP (Ethernet 1/1/12) for output","title":"Physical Configuration"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%204/#configuration","text":"! Version 10.5.0.2 ! Last configuration change at Nov 01 02:25:00 2019 ! ip vrf default ! interface breakout 1/1/13 map 100g-1x interface breakout 1/1/14 map 100g-1x interface breakout 1/1/15 map 100g-1x iscsi enable iscsi target port 860 iscsi target port 3260 system-user linuxadmin password $6$5DdOHYg5$JCE1vMSmkQOrbh31U74PIPv7lyOgRmba1IxhkYibppMXs1KM4Y.gbTPcxyMP/PHUkMc5rdk/ZLv9Sfv3ALtB61 enhanced-hashing resilient-hashing lag username admin password $6$q9QBeYjZ$jfxzVqGhkxX3smxJSH9DDz7/3OJc6m5wjF8nnLD7/VKx8SloIhp4NoGZs0I/UNwh8WVuxwfd9q4pWIgNs5BKH. role sysadmin priv-lvl 15 aaa authentication login default local aaa authentication login console local ! class-map type application class-iscsi ! policy-map type application policy-iscsi ! interface vlan1 no shutdown ! interface port-channel1 no shutdown switchport access vlan 1 ! interface mgmt1/1/1 no shutdown no ip address dhcp ip address 192.168.1.20/24 ipv6 address autoconfig ! interface ethernet1/1/1 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/2 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/3 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/4 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/5 no shutdown channel-group 1 no switchport speed 1000 flowcontrol receive on ! interface ethernet1/1/6 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/7 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/8 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/9 no shutdown switchport access vlan 1 speed 1000 flowcontrol receive on ! interface ethernet1/1/10 no shutdown channel-group 1 no switchport speed 1000 flowcontrol receive on ! interface ethernet1/1/11 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/12 no shutdown channel-group 1 no switchport speed 1000 flowcontrol receive on ! interface ethernet1/1/13 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/14 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/15 no shutdown switchport access vlan 1 flowcontrol receive on ! monitor session 1 destination interface port-channel1 source interface ethernet1/1/1 no shut ! snmp-server contact \"Contact Support\" ! telemetry","title":"Configuration"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%204/#findings","text":"This time traffic still went to different Wireshark sessions as you can see in the below. On host 3 you can see the synchronize packet go out with sequence number 3195700332 and you notice that the SYN, ACK response is missing. Look at host 2 and you can see the SYN, ACK with the expected response of 3195700333.","title":"Findings"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%204/#traffic-to-different-wireshark-sessions","text":"However a session on host 1 seems to work correctly.","title":"Traffic to Different Wireshark Sessions"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%204/#host-1-session-with-correct-output","text":"","title":"Host 1 Session with Correct Output"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%204/#a-more-definitive-test","text":"I wanted to be sure of my findings so I crafted a new PCAP. This time, I started a capture on my desktop and opened a new connection to vCenter knowing this should generate several new streams. I then closed the browser entirely to ensure those same sessions would close. I saved the capture off and sent it to my traffic replay system. I then played it back with tcpreplay . I then grabbed a random stream from the sequence to confirm whether I could see the entire three way hand shake on one host or not. As suspected the initial syn hit one Wireshark session and the response went to a separate Wireshark session.","title":"A More Definitive Test"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%204/#host-1","text":"","title":"Host 1"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%204/#host-3","text":"","title":"Host 3"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OpenSwitch%20%28OPX%29/","text":"Load Balancing with LAG OPX In this test case the goal is to create a simple packet broker using a reverse LAG port. Helpful Links ONIE Network Install Process Overview OPX Install Instructions for Dell EMC Equipment OPX Tools Source Code OPX Command Reference OPX LAG Command Documenattion OPX Docs Home List of Supported Hardware Helpful Debug Commands cps_get_oid.py -qua target base-switch/switching-entities/switching-entity cps_model_info base-switch/switching-entities/switching-entity cps_set_oid.py -qua target base-switch/switching-entities/switching-entity name=lag-hash-fields attr=src-ip,dest-ip,l4-dest-port,l4-src-port,ip-protocol My Configuration General Configuration ONIE host is running RHEL 8 I am using a Dell S4112F-ON for testing OpenSwitch version PKGS_OPX-3.2.0-installer-x86_64 PFSense running DNS and DHCP as services RHEL Release Info NAME=\"Red Hat Enterprise Linux\" VERSION=\"8.0 (Ootpa)\" ID=\"rhel\" ID_LIKE=\"fedora\" VERSION_ID=\"8.0\" PLATFORM_ID=\"platform:el8\" PRETTY_NAME=\"Red Hat Enterprise Linux 8.0 (Ootpa)\" ANSI_COLOR=\"0;31\" CPE_NAME=\"cpe:/o:redhat:enterprise_linux:8.0:GA\" HOME_URL=\"https://www.redhat.com/\" BUG_REPORT_URL=\"https://bugzilla.redhat.com/\" REDHAT_BUGZILLA_PRODUCT=\"Red Hat Enterprise Linux 8\" REDHAT_BUGZILLA_PRODUCT_VERSION=8.0 REDHAT_SUPPORT_PRODUCT=\"Red Hat Enterprise Linux\" REDHAT_SUPPORT_PRODUCT_VERSION=\"8.0\" Red Hat Enterprise Linux release 8.0 (Ootpa) Red Hat Enterprise Linux release 8.0 (Ootpa) OPX Version OS_NAME=\"OPX\" OS_VERSION=\"unstable\" PLATFORM=\"S4112F-ON\" ARCHITECTURE=\"x86_64\" INTERNAL_BUILD_ID=\"OpenSwitch blueprint for Dell 1.0.0\" BUILD_VERSION=\"unstable.0-stretch\" BUILD_DATE=\"2019-06-21T19:04:22+0000\" INSTALL_DATE=\"2019-10-23T23:16:10+00:00\" SYSTEM_UPTIME= 1 day, 5 minutes SYSTEM_STATE= running UPGRADED_PACKAGES=no ALTERED_PACKAGES=yes Wasn't sure why I got the unstable version after installation. It didn't cause any problems for testing so I just left it as is. Setup ONIE Prerequisites See ONIE Install Setup for instructions. Configure Device for Reverse LAG Physical Configuration I used the following SFPs 1, 1Gb/s copper SFP (e101-001-0) for input 2, 1Gb/s copper SFPs (e101-005-0/e101-009-0) and 1, 10Gb/s, fiber SFP (e101-012-0) for output Input Port Output Ports LAG Configuration Enable LAG Ports and Input Port root@OPX:~# ip link set e101-001-0 up root@OPX:~# ip link set e101-005-0 up root@OPX:~# ip link set e101-009-0 up root@OPX:~# ip link set e101-012-0 up root@OPX:~# opx-show-interface --summary Port | Enabled | Operational status | Supported speed ----------------------------------------------------------- e101-001-0 | yes | up | 1G 10G e101-002-0 | no | down | 1G 10G e101-003-0 | no | down | 1G 10G e101-004-0 | no | down | 1G 10G e101-005-0 | yes | up | 1G 10G e101-006-0 | no | down | 1G 10G e101-007-0 | no | down | 1G 10G e101-008-0 | no | down | 1G 10G e101-009-0 | yes | up | 1G 10G e101-010-0 | no | down | 1G 10G e101-011-0 | no | down | 1G 10G e101-012-0 | yes | up | 1G 10G e101-013-0 | no | down | 100G e101-014-0 | no | down | 100G e101-015-0 | no | down | 100G eth0 | yes | UNKNOWN | UNKNOWN Configure LAG Configure LAG Algorithm You can see the switch's global paramters with the opx-show-global-switch command: root@OPX:~# opx-show-global-switch Switch id 0 ACL entry max priority: 2147483647 ACL entry min priority: 0 ACL table max priority: 11 ACL table min priority: 0 Bridge table size: 147456 BST enable: off BST tracking mode: current Counter refresh interval: 5 s Default mac address: 88:6f:d4:98:b7:80 ECMP group size: 256 ECMP hash algorithm: crc ECMP hash seed value: 0 Egress buffer pool num: 4 Ingress buffer pool num: 4 IPv6 extended prefix routes: 0 IPv6 extended prefix routes lpm block size: 1024 L3 nexthop table size: 32768 LAG hash algorithm: crc LAG hash seed value: 0 MAC address aging timer: 1800 s Max ECMP entries per group: 0 Max IPv6 extended prefix routes: 3072 Max MTU: 9216 Max VXLAN overlay nexthops: 4096 Max VXLAN overlay rifs: 2048 Number of multicast queues per port: 10 Number of queues cpu port: 43 Number of queues per port: 20 Number of unicast queues per port: 10 QoS rate adjust: 0 RIF table size: 12288 Switch mode: store and forward Temperature: 49 deg. C Total buffer size: 12188 UFT mode: default UFT host table size: 135168 UFT L2 mac table size: 147456 UFT L3 route table size: 16384 VXLAN riot enable: on If you want to change the hash algorithm you can do so with opx-config-global-switch --lag-hash-alg <crc | random | xor> I went ahead and left mine are CRC. This article from Dell can be helpful in deciding. Configure LAG Fields More imporantly you will probably want to configure what fields are used to determine the hash. The options are: src-mac: The source MAC address of the frame dest-mac: The destination MAC of the frame vlan-id: The VLAN ID listed in the frame ethertype: The ethertype of the frame ip-protocol: The IP protocol field in the IPv4 header src-ip: The packet source IP dest-ip: The destination IP of the packet l4-dest-port: The destination port of the segment l4-src-port: The source port of the segment in-port: The port from which the packet entered. It is unlikely you would want to use this for a reverse LAG I will use a standard 5-tuple configuration (src/dst IP, src/dest port, protocol #) Create LAG opx-config-lag create --name reverse_lag --unblockedports e101-005-0,e101-009-0,e101-012-0 --enable Results I wasn't able to complete the config. There is a bug in OPX preventing you from being able to set the fields on which the LAG will hash. I spent about a day working my way through the problem. See current status on this bug ticket","title":"Load Balancing with LAG OPX"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OpenSwitch%20%28OPX%29/#load-balancing-with-lag-opx","text":"In this test case the goal is to create a simple packet broker using a reverse LAG port.","title":"Load Balancing with LAG OPX"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OpenSwitch%20%28OPX%29/#helpful-links","text":"ONIE Network Install Process Overview OPX Install Instructions for Dell EMC Equipment OPX Tools Source Code OPX Command Reference OPX LAG Command Documenattion OPX Docs Home List of Supported Hardware","title":"Helpful Links"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OpenSwitch%20%28OPX%29/#helpful-debug-commands","text":"cps_get_oid.py -qua target base-switch/switching-entities/switching-entity cps_model_info base-switch/switching-entities/switching-entity cps_set_oid.py -qua target base-switch/switching-entities/switching-entity name=lag-hash-fields attr=src-ip,dest-ip,l4-dest-port,l4-src-port,ip-protocol","title":"Helpful Debug Commands"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OpenSwitch%20%28OPX%29/#my-configuration","text":"","title":"My Configuration"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OpenSwitch%20%28OPX%29/#general-configuration","text":"ONIE host is running RHEL 8 I am using a Dell S4112F-ON for testing OpenSwitch version PKGS_OPX-3.2.0-installer-x86_64 PFSense running DNS and DHCP as services","title":"General Configuration"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OpenSwitch%20%28OPX%29/#rhel-release-info","text":"NAME=\"Red Hat Enterprise Linux\" VERSION=\"8.0 (Ootpa)\" ID=\"rhel\" ID_LIKE=\"fedora\" VERSION_ID=\"8.0\" PLATFORM_ID=\"platform:el8\" PRETTY_NAME=\"Red Hat Enterprise Linux 8.0 (Ootpa)\" ANSI_COLOR=\"0;31\" CPE_NAME=\"cpe:/o:redhat:enterprise_linux:8.0:GA\" HOME_URL=\"https://www.redhat.com/\" BUG_REPORT_URL=\"https://bugzilla.redhat.com/\" REDHAT_BUGZILLA_PRODUCT=\"Red Hat Enterprise Linux 8\" REDHAT_BUGZILLA_PRODUCT_VERSION=8.0 REDHAT_SUPPORT_PRODUCT=\"Red Hat Enterprise Linux\" REDHAT_SUPPORT_PRODUCT_VERSION=\"8.0\" Red Hat Enterprise Linux release 8.0 (Ootpa) Red Hat Enterprise Linux release 8.0 (Ootpa)","title":"RHEL Release Info"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OpenSwitch%20%28OPX%29/#opx-version","text":"OS_NAME=\"OPX\" OS_VERSION=\"unstable\" PLATFORM=\"S4112F-ON\" ARCHITECTURE=\"x86_64\" INTERNAL_BUILD_ID=\"OpenSwitch blueprint for Dell 1.0.0\" BUILD_VERSION=\"unstable.0-stretch\" BUILD_DATE=\"2019-06-21T19:04:22+0000\" INSTALL_DATE=\"2019-10-23T23:16:10+00:00\" SYSTEM_UPTIME= 1 day, 5 minutes SYSTEM_STATE= running UPGRADED_PACKAGES=no ALTERED_PACKAGES=yes Wasn't sure why I got the unstable version after installation. It didn't cause any problems for testing so I just left it as is.","title":"OPX Version"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OpenSwitch%20%28OPX%29/#setup-onie-prerequisites","text":"See ONIE Install Setup for instructions.","title":"Setup ONIE Prerequisites"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OpenSwitch%20%28OPX%29/#configure-device-for-reverse-lag","text":"","title":"Configure Device for Reverse LAG"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OpenSwitch%20%28OPX%29/#physical-configuration","text":"I used the following SFPs 1, 1Gb/s copper SFP (e101-001-0) for input 2, 1Gb/s copper SFPs (e101-005-0/e101-009-0) and 1, 10Gb/s, fiber SFP (e101-012-0) for output","title":"Physical Configuration"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OpenSwitch%20%28OPX%29/#input-port","text":"","title":"Input Port"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OpenSwitch%20%28OPX%29/#output-ports","text":"","title":"Output Ports"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OpenSwitch%20%28OPX%29/#lag-configuration","text":"","title":"LAG Configuration"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OpenSwitch%20%28OPX%29/#enable-lag-ports-and-input-port","text":"root@OPX:~# ip link set e101-001-0 up root@OPX:~# ip link set e101-005-0 up root@OPX:~# ip link set e101-009-0 up root@OPX:~# ip link set e101-012-0 up root@OPX:~# opx-show-interface --summary Port | Enabled | Operational status | Supported speed ----------------------------------------------------------- e101-001-0 | yes | up | 1G 10G e101-002-0 | no | down | 1G 10G e101-003-0 | no | down | 1G 10G e101-004-0 | no | down | 1G 10G e101-005-0 | yes | up | 1G 10G e101-006-0 | no | down | 1G 10G e101-007-0 | no | down | 1G 10G e101-008-0 | no | down | 1G 10G e101-009-0 | yes | up | 1G 10G e101-010-0 | no | down | 1G 10G e101-011-0 | no | down | 1G 10G e101-012-0 | yes | up | 1G 10G e101-013-0 | no | down | 100G e101-014-0 | no | down | 100G e101-015-0 | no | down | 100G eth0 | yes | UNKNOWN | UNKNOWN","title":"Enable LAG Ports and Input Port"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OpenSwitch%20%28OPX%29/#configure-lag","text":"","title":"Configure LAG"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OpenSwitch%20%28OPX%29/#configure-lag-algorithm","text":"You can see the switch's global paramters with the opx-show-global-switch command: root@OPX:~# opx-show-global-switch Switch id 0 ACL entry max priority: 2147483647 ACL entry min priority: 0 ACL table max priority: 11 ACL table min priority: 0 Bridge table size: 147456 BST enable: off BST tracking mode: current Counter refresh interval: 5 s Default mac address: 88:6f:d4:98:b7:80 ECMP group size: 256 ECMP hash algorithm: crc ECMP hash seed value: 0 Egress buffer pool num: 4 Ingress buffer pool num: 4 IPv6 extended prefix routes: 0 IPv6 extended prefix routes lpm block size: 1024 L3 nexthop table size: 32768 LAG hash algorithm: crc LAG hash seed value: 0 MAC address aging timer: 1800 s Max ECMP entries per group: 0 Max IPv6 extended prefix routes: 3072 Max MTU: 9216 Max VXLAN overlay nexthops: 4096 Max VXLAN overlay rifs: 2048 Number of multicast queues per port: 10 Number of queues cpu port: 43 Number of queues per port: 20 Number of unicast queues per port: 10 QoS rate adjust: 0 RIF table size: 12288 Switch mode: store and forward Temperature: 49 deg. C Total buffer size: 12188 UFT mode: default UFT host table size: 135168 UFT L2 mac table size: 147456 UFT L3 route table size: 16384 VXLAN riot enable: on If you want to change the hash algorithm you can do so with opx-config-global-switch --lag-hash-alg <crc | random | xor> I went ahead and left mine are CRC. This article from Dell can be helpful in deciding.","title":"Configure LAG Algorithm"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OpenSwitch%20%28OPX%29/#configure-lag-fields","text":"More imporantly you will probably want to configure what fields are used to determine the hash. The options are: src-mac: The source MAC address of the frame dest-mac: The destination MAC of the frame vlan-id: The VLAN ID listed in the frame ethertype: The ethertype of the frame ip-protocol: The IP protocol field in the IPv4 header src-ip: The packet source IP dest-ip: The destination IP of the packet l4-dest-port: The destination port of the segment l4-src-port: The source port of the segment in-port: The port from which the packet entered. It is unlikely you would want to use this for a reverse LAG I will use a standard 5-tuple configuration (src/dst IP, src/dest port, protocol #)","title":"Configure LAG Fields"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OpenSwitch%20%28OPX%29/#create-lag","text":"opx-config-lag create --name reverse_lag --unblockedports e101-005-0,e101-009-0,e101-012-0 --enable","title":"Create LAG"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OpenSwitch%20%28OPX%29/#results","text":"I wasn't able to complete the config. There is a bug in OPX preventing you from being able to set the fields on which the LAG will hash. I spent about a day working my way through the problem. See current status on this bug ticket","title":"Results"},{"location":"Load%20Balance%20Testing%20with%20OpenVSwitch/","text":"Load Balance Testing with OpenVSwitch From tutorial ovs-conntrack Testing performed on OS10 v Setup ip netns add left ip netns add right ip link add veth_l0 type veth peer name veth_l1 ip link set veth_l1 netns left ip link add veth_r0 type veth peer name veth_r1 ip link set veth_r1 netns right ovs-vsctl add-br br0 ip a s | less ovs-vsctl add-port br0 veth_l0 ovs-vsctl add-port br0 veth_r0 ip netns exec left sudo ip link set lo up ip netns exec right sudo ip link set lo up Generate TCP segments ip netns exec left sudo `which scapy` ip netns exec right sudo `which scapy` Matching TCP packets Simple flows for port to port ovs-ofctl add-flow br0 \"table=0, priority=10, in_port=veth_l0, actions=veth_r0\" ovs-ofctl add-flow br0 \"table=0, priority=10, in_port=veth_r0, actions=veth_l0\" Flow matching ovs-ofctl add-flow br0 \"table=0, priority=50, ct_state=-trk, tcp, in_port=veth_l0, actions=ct(table=0)\" ovs-ofctl add-flow br0 \"table=0, priority=50, ct_state=+trk+new, tcp, in_port=veth_l0, actions=ct(commit),veth_r0\" ovs-ofctl add-flow br0 \"table=0, priority=50, ct_state=-trk, tcp, in_port=veth_r0, actions=ct(table=0)\" ovs-ofctl add-flow br0 \"table=0, priority=50, ct_state=+trk+est, tcp, in_port=veth_r0, actions=veth_l0\" ovs-ofctl add-flow br0 \"table=0, priority=50, ct_state=+trk+est, tcp, in_port=veth_l0, actions=veth_r0\" End result You can do cool stuff, but it won't work/wouldn't be a great way to do this.","title":"Load Balance Testing with OpenVSwitch"},{"location":"Load%20Balance%20Testing%20with%20OpenVSwitch/#load-balance-testing-with-openvswitch","text":"From tutorial ovs-conntrack Testing performed on OS10 v","title":"Load Balance Testing with OpenVSwitch"},{"location":"Load%20Balance%20Testing%20with%20OpenVSwitch/#setup","text":"ip netns add left ip netns add right ip link add veth_l0 type veth peer name veth_l1 ip link set veth_l1 netns left ip link add veth_r0 type veth peer name veth_r1 ip link set veth_r1 netns right ovs-vsctl add-br br0 ip a s | less ovs-vsctl add-port br0 veth_l0 ovs-vsctl add-port br0 veth_r0 ip netns exec left sudo ip link set lo up ip netns exec right sudo ip link set lo up","title":"Setup"},{"location":"Load%20Balance%20Testing%20with%20OpenVSwitch/#generate-tcp-segments","text":"ip netns exec left sudo `which scapy` ip netns exec right sudo `which scapy`","title":"Generate TCP segments"},{"location":"Load%20Balance%20Testing%20with%20OpenVSwitch/#matching-tcp-packets","text":"","title":"Matching TCP packets"},{"location":"Load%20Balance%20Testing%20with%20OpenVSwitch/#simple-flows-for-port-to-port","text":"ovs-ofctl add-flow br0 \"table=0, priority=10, in_port=veth_l0, actions=veth_r0\" ovs-ofctl add-flow br0 \"table=0, priority=10, in_port=veth_r0, actions=veth_l0\"","title":"Simple flows for port to port"},{"location":"Load%20Balance%20Testing%20with%20OpenVSwitch/#flow-matching","text":"ovs-ofctl add-flow br0 \"table=0, priority=50, ct_state=-trk, tcp, in_port=veth_l0, actions=ct(table=0)\" ovs-ofctl add-flow br0 \"table=0, priority=50, ct_state=+trk+new, tcp, in_port=veth_l0, actions=ct(commit),veth_r0\" ovs-ofctl add-flow br0 \"table=0, priority=50, ct_state=-trk, tcp, in_port=veth_r0, actions=ct(table=0)\" ovs-ofctl add-flow br0 \"table=0, priority=50, ct_state=+trk+est, tcp, in_port=veth_r0, actions=veth_l0\" ovs-ofctl add-flow br0 \"table=0, priority=50, ct_state=+trk+est, tcp, in_port=veth_l0, actions=veth_r0\"","title":"Flow matching"},{"location":"Load%20Balance%20Testing%20with%20OpenVSwitch/#end-result","text":"You can do cool stuff, but it won't work/wouldn't be a great way to do this.","title":"End result"},{"location":"Load%20Balancing%20on%20Mellanox%20Switches/","text":"Load Balancing on Mellanox Switches In this test case the goal is to create a simple load balancer using a reverse LAG port. The idea is to have one input port which is then mirrored to a logical LAG port and at the other end of the LAG port is a number of security sensors. Version Info mellanox.lan [standalone: master] # show version Product name: Onyx Product release: 3.8.2004 Build ID: #1-dev Build date: 2019-09-23 14:19:47 Target arch: x86_64 Target hw: x86_64 Built by: jenkins@7ae5fd122b61 Version summary: X86_64 3.8.2004 2019-09-23 14:19:47 x86_64 Product model: x86onie Host ID: B8599FD560BE System serial num: MT1940T00588 System UUID: 5f4c5ed2-e60b-11e9-8000-b8599f7f6f40 Uptime: 17h 27m 36.480s CPU load averages: 3.17 / 3.17 / 3.11 Number of CPUs: 4 System memory: 2738 MB used / 5065 MB free / 7803 MB total Swap: 0 MB used / 0 MB free / 0 MB total Connect to the Console Port and Management Ethernet Port Plug in both the management Ethernet cable and the serial cable. The console port is the bottom port and the ethernet management port is the top port. I had to plug the console cable into a specific USB slot on the server. It didn't work in the first one I tried. See picture below. This likely has nothing to do with the Mellanox switch itself, but as a note for those that come after you may want to try different USB ports if you find you aren't getting output on the first one you try and are confident you have the correct settings. I used the following console configuration: Baud Rate: 115200 Data Bits: 8 Stop Bits: 1 Parity: None Flow Control: None Update to Latest Version of MLNX-OS I pulled updates here The system uses a web server target for updates. I had Apache running on a RHEL 8 box. Download the update file and then upload it to your web server's root directory. On the switch itself (over a console port) do the following: switch > enable switch # configure terminal switch (config) # show images # Delete the old image if it exists. It will be under # \"Images available to be installed\" switch (config) # image delete <old_image> Download the new image from your web server with mellanox.lan [standalone: master] # image fetch http://rhel8.lan/onyx-X86_64-3.8.2004.img 100.0% [################################################################################################################################################################################################################################################################] Next install the updated OS with: mellanox.lan [standalone: master] # image install onyx-X86_64-3.8.2004.img location 2 progress track verify check-sig Step 1 of 4: Verify Image 100.0% [#################################################################] Step 2 of 4: Uncompress Image 100.0% [#################################################################] Step 3 of 4: Create Filesystems 100.0% [#################################################################] Step 4 of 4: Extract Image 98.6% [################################################################ ] 100.0% [#################################################################] Now set the switch to load from the new operating system and reload: mellanox.lan [standalone: master] # image boot next mellanox.lan [standalone: master] # reload Physical Configuration I used the following port configuration: 1, 1Gb/s copper SFP (Eth1/1) for input 2, 1Gb/s copper SFPs (Ethernet 1/1/5/Ethernet 1/1/9) and 1, 1Gb/s, fiber SFP (Ethernet 1/1/12) for output I used the following optics: Connect the input port to port 1. I connected my output ports in the following way: Bringing the Interfaces Up Mellanox does not perform testing with 3rd party NICs. During testing we found that autonegotiation of speed will not work on standard Dell SFPs. Use the below command on each interface to set the speed manually: mellanox.lan [standalone: master] (config interface ethernet 1/1) # speed 1G Configure the LAG Initial Configuration mellanox.lan [standalone: master] (config) # port-channel load-balance ethernet source-destination-mac source-destination-ip source-destination-port symmetric mellanox.lan [standalone: master] (config) # interface port-channel 1 mellanox.lan [standalone: master] (config interface port-channel 1) # switchport mode hybrid mellanox.lan [standalone: master] (config interface port-channel 1) # description load balance group mellanox.lan [standalone: master] (config interface port-channel 1) # no shut mellanox.lan [standalone: master] (config interface port-channel 1) # mtu 9000 force mellanox.lan [standalone: master] (config interface port-channel 1) # exit mellanox.lan [standalone: master] (config) # interface ethernet 1/5 switchport mode hybrid mellanox.lan [standalone: master] (config) # interface ethernet 1/9 switchport mode hybrid mellanox.lan [standalone: master] (config) # interface ethernet 1/12 switchport mode hybrid mellanox.lan [standalone: master] (config) # interface ethernet 1/1 switchport mode hybrid mellanox.lan [standalone: master] (config) # interface ethernet 1/1 mtu 9000 force mellanox.lan [standalone: master] (config) # interface ethernet 1/5 mtu 9000 force mellanox.lan [standalone: master] (config) # interface ethernet 1/9 mtu 9000 force mellanox.lan [standalone: master] (config) # interface ethernet 1/12 mtu 9000 force mellanox.lan [standalone: master] (config) # interface ethernet 1/5 channel-group 1 mode on mellanox.lan [standalone: master] (config) # interface ethernet 1/9 channel-group 1 mode on mellanox.lan [standalone: master] (config) # interface ethernet 1/12 channel-group 1 mode on Problem Using Port Mirroring Originally my plan was to use a mirror port to send all the traffic from port 1 to our LAG interface. In contrast to OS10, MLNX-OS will not allow you to do this. The problem is that MLNX-OS will not allow you to create a mirror session from interface 1 to the LAG port which prevents this configuration from working. Moreover you cannot access the Linux command line to use a utility like tc to perform the config either. Configure OpenFlow Instead of using port mirroring to send the traffic from interface one we can instead use a static OpenFlow configuration to redirect the traffic. mellanox.lan [standalone: master] (config) # interface ethernet 1/1 openflow mode hybrid mellanox.lan [standalone: master] (config) # interface port-channel 1 openflow mode hybrid mellanox.lan [standalone: master] (config) # openflow add-flows 1000 priority=50,in_port=Eth1/1,actions=output:Po1 Findings The Mellanox SN2010 works correctly and will appropriately load balance full sessions across each member of the LAG. See below for screenshots. Host 1 Host 2 Host 3","title":"Load Balancing on Mellanox Switches"},{"location":"Load%20Balancing%20on%20Mellanox%20Switches/#load-balancing-on-mellanox-switches","text":"In this test case the goal is to create a simple load balancer using a reverse LAG port. The idea is to have one input port which is then mirrored to a logical LAG port and at the other end of the LAG port is a number of security sensors.","title":"Load Balancing on Mellanox Switches"},{"location":"Load%20Balancing%20on%20Mellanox%20Switches/#version-info","text":"mellanox.lan [standalone: master] # show version Product name: Onyx Product release: 3.8.2004 Build ID: #1-dev Build date: 2019-09-23 14:19:47 Target arch: x86_64 Target hw: x86_64 Built by: jenkins@7ae5fd122b61 Version summary: X86_64 3.8.2004 2019-09-23 14:19:47 x86_64 Product model: x86onie Host ID: B8599FD560BE System serial num: MT1940T00588 System UUID: 5f4c5ed2-e60b-11e9-8000-b8599f7f6f40 Uptime: 17h 27m 36.480s CPU load averages: 3.17 / 3.17 / 3.11 Number of CPUs: 4 System memory: 2738 MB used / 5065 MB free / 7803 MB total Swap: 0 MB used / 0 MB free / 0 MB total","title":"Version Info"},{"location":"Load%20Balancing%20on%20Mellanox%20Switches/#connect-to-the-console-port-and-management-ethernet-port","text":"Plug in both the management Ethernet cable and the serial cable. The console port is the bottom port and the ethernet management port is the top port. I had to plug the console cable into a specific USB slot on the server. It didn't work in the first one I tried. See picture below. This likely has nothing to do with the Mellanox switch itself, but as a note for those that come after you may want to try different USB ports if you find you aren't getting output on the first one you try and are confident you have the correct settings. I used the following console configuration: Baud Rate: 115200 Data Bits: 8 Stop Bits: 1 Parity: None Flow Control: None","title":"Connect to the Console Port and Management Ethernet Port"},{"location":"Load%20Balancing%20on%20Mellanox%20Switches/#update-to-latest-version-of-mlnx-os","text":"I pulled updates here The system uses a web server target for updates. I had Apache running on a RHEL 8 box. Download the update file and then upload it to your web server's root directory. On the switch itself (over a console port) do the following: switch > enable switch # configure terminal switch (config) # show images # Delete the old image if it exists. It will be under # \"Images available to be installed\" switch (config) # image delete <old_image> Download the new image from your web server with mellanox.lan [standalone: master] # image fetch http://rhel8.lan/onyx-X86_64-3.8.2004.img 100.0% [################################################################################################################################################################################################################################################################] Next install the updated OS with: mellanox.lan [standalone: master] # image install onyx-X86_64-3.8.2004.img location 2 progress track verify check-sig Step 1 of 4: Verify Image 100.0% [#################################################################] Step 2 of 4: Uncompress Image 100.0% [#################################################################] Step 3 of 4: Create Filesystems 100.0% [#################################################################] Step 4 of 4: Extract Image 98.6% [################################################################ ] 100.0% [#################################################################] Now set the switch to load from the new operating system and reload: mellanox.lan [standalone: master] # image boot next mellanox.lan [standalone: master] # reload","title":"Update to Latest Version of MLNX-OS"},{"location":"Load%20Balancing%20on%20Mellanox%20Switches/#physical-configuration","text":"I used the following port configuration: 1, 1Gb/s copper SFP (Eth1/1) for input 2, 1Gb/s copper SFPs (Ethernet 1/1/5/Ethernet 1/1/9) and 1, 1Gb/s, fiber SFP (Ethernet 1/1/12) for output I used the following optics: Connect the input port to port 1. I connected my output ports in the following way:","title":"Physical Configuration"},{"location":"Load%20Balancing%20on%20Mellanox%20Switches/#bringing-the-interfaces-up","text":"Mellanox does not perform testing with 3rd party NICs. During testing we found that autonegotiation of speed will not work on standard Dell SFPs. Use the below command on each interface to set the speed manually: mellanox.lan [standalone: master] (config interface ethernet 1/1) # speed 1G","title":"Bringing the Interfaces Up"},{"location":"Load%20Balancing%20on%20Mellanox%20Switches/#configure-the-lag","text":"","title":"Configure the LAG"},{"location":"Load%20Balancing%20on%20Mellanox%20Switches/#initial-configuration","text":"mellanox.lan [standalone: master] (config) # port-channel load-balance ethernet source-destination-mac source-destination-ip source-destination-port symmetric mellanox.lan [standalone: master] (config) # interface port-channel 1 mellanox.lan [standalone: master] (config interface port-channel 1) # switchport mode hybrid mellanox.lan [standalone: master] (config interface port-channel 1) # description load balance group mellanox.lan [standalone: master] (config interface port-channel 1) # no shut mellanox.lan [standalone: master] (config interface port-channel 1) # mtu 9000 force mellanox.lan [standalone: master] (config interface port-channel 1) # exit mellanox.lan [standalone: master] (config) # interface ethernet 1/5 switchport mode hybrid mellanox.lan [standalone: master] (config) # interface ethernet 1/9 switchport mode hybrid mellanox.lan [standalone: master] (config) # interface ethernet 1/12 switchport mode hybrid mellanox.lan [standalone: master] (config) # interface ethernet 1/1 switchport mode hybrid mellanox.lan [standalone: master] (config) # interface ethernet 1/1 mtu 9000 force mellanox.lan [standalone: master] (config) # interface ethernet 1/5 mtu 9000 force mellanox.lan [standalone: master] (config) # interface ethernet 1/9 mtu 9000 force mellanox.lan [standalone: master] (config) # interface ethernet 1/12 mtu 9000 force mellanox.lan [standalone: master] (config) # interface ethernet 1/5 channel-group 1 mode on mellanox.lan [standalone: master] (config) # interface ethernet 1/9 channel-group 1 mode on mellanox.lan [standalone: master] (config) # interface ethernet 1/12 channel-group 1 mode on","title":"Initial Configuration"},{"location":"Load%20Balancing%20on%20Mellanox%20Switches/#problem-using-port-mirroring","text":"Originally my plan was to use a mirror port to send all the traffic from port 1 to our LAG interface. In contrast to OS10, MLNX-OS will not allow you to do this. The problem is that MLNX-OS will not allow you to create a mirror session from interface 1 to the LAG port which prevents this configuration from working. Moreover you cannot access the Linux command line to use a utility like tc to perform the config either.","title":"Problem Using Port Mirroring"},{"location":"Load%20Balancing%20on%20Mellanox%20Switches/#configure-openflow","text":"Instead of using port mirroring to send the traffic from interface one we can instead use a static OpenFlow configuration to redirect the traffic. mellanox.lan [standalone: master] (config) # interface ethernet 1/1 openflow mode hybrid mellanox.lan [standalone: master] (config) # interface port-channel 1 openflow mode hybrid mellanox.lan [standalone: master] (config) # openflow add-flows 1000 priority=50,in_port=Eth1/1,actions=output:Po1","title":"Configure OpenFlow"},{"location":"Load%20Balancing%20on%20Mellanox%20Switches/#findings","text":"The Mellanox SN2010 works correctly and will appropriately load balance full sessions across each member of the LAG. See below for screenshots.","title":"Findings"},{"location":"Load%20Balancing%20on%20Mellanox%20Switches/#host-1","text":"","title":"Host 1"},{"location":"Load%20Balancing%20on%20Mellanox%20Switches/#host-2","text":"","title":"Host 2"},{"location":"Load%20Balancing%20on%20Mellanox%20Switches/#host-3","text":"","title":"Host 3"},{"location":"Load%20Balancing%20with%20LAG%20on%205112F-ON/","text":"Load Balancing with LAG on 5112F-ON Load Balancing with LAG on 5112F-ON Version/Hardware Information The Test Environment Configure Devices Configure 5212F-ON Configure RHEL 8 Test Results Version/Hardware Information Note This requires at least 10.5.3.0 and is only supported on switches with the Trident proc OS10(config)# do show version Dell EMC Networking OS10 Enterprise Copyright (c) 1999-2021 by Dell Inc. All Rights Reserved. OS Version: 10.5.3.0 Build Version: 10.5.3.0.44 Build Time: 2021-10-06T23:03:55+0000 System Type: S5212F-ON Architecture: x86_64 Up Time: 00:03:27 The Test Environment I used a RHEL box to generate traffic inbound to the switch on port ethernet 1/1/5:1 which I then expected to load balance across ports ethernet 1/1/1:1 and 1/1/4:1. I used source and dest IP and source and dest port as the criteria for load balancing. Traffic was captured with a single laptop which was directly connected to the two independent receiving interfaces. A single Wireshark monitor session was established for each of the individual interfaces to ensure that they received independent, mutually-exclusive, session load balancing. The traffic was a session I generated by using a laptop that was serving as a virtual web gateway along with being used as an end user device. Configure Devices Configure 5212F-ON configure terminal load-balancing ip-selection source-ip destination-ip l4-destination-port l4-source-port port-group 1/1/1 mode eth 10g-4x interface port-channel 1 no shutdown no switchport exit interface ethernet 1/1/1:1 no shut channel-group 1 speed 1000 exit interface ethernet 1/1/4:1 no shut channel-group 1 speed 1000 exit monitor session 1 destination interface port-channel 1 source interface ethernet 1/1/5:1 no shut exit Your listening source captures traffic be that a tap, span, Linux host, etc Traffic is pushed into a physical interface (ethernet 1/1/5:1) on the 5212 Traffic is mirrored from the physical interface (Ethernet 1/1/5:1) to the virtual port group interface The port group interface is tied to two physical interfaces and set to perform load balancing The virtual port group interface will load balance based on a hash of the aforementioned attributes (ip-selection source-ip destination-ip l4-destination-port l4-source-port) Configure RHEL 8 sudo dnf install -y https://dl.fedoraproject.org/pub/epel/epel-release-latest-8.noarch.rpm sudo subscription-manager repos --enable codeready-builder-for-rhel-8-x86_64-rpms dnf group install \"Development Tools\" dnf install -y libpcap-devel make make install /usr/local/bin/tcpreplay -i ens32 test.pcap Test Results The test succeeded. This was verified by checking that the sessions seen by two separate streams were mutually exclusive. At no point did we see effects as described in 4112F-ON Test Case 4 where a single session was sent down two separate lanes. Moreover, by checking the individual sessions you can see the bidirectional flow:","title":"Load Balancing with LAG on 5112F-ON"},{"location":"Load%20Balancing%20with%20LAG%20on%205112F-ON/#load-balancing-with-lag-on-5112f-on","text":"Load Balancing with LAG on 5112F-ON Version/Hardware Information The Test Environment Configure Devices Configure 5212F-ON Configure RHEL 8 Test Results","title":"Load Balancing with LAG on 5112F-ON"},{"location":"Load%20Balancing%20with%20LAG%20on%205112F-ON/#versionhardware-information","text":"Note This requires at least 10.5.3.0 and is only supported on switches with the Trident proc OS10(config)# do show version Dell EMC Networking OS10 Enterprise Copyright (c) 1999-2021 by Dell Inc. All Rights Reserved. OS Version: 10.5.3.0 Build Version: 10.5.3.0.44 Build Time: 2021-10-06T23:03:55+0000 System Type: S5212F-ON Architecture: x86_64 Up Time: 00:03:27","title":"Version/Hardware Information"},{"location":"Load%20Balancing%20with%20LAG%20on%205112F-ON/#the-test-environment","text":"I used a RHEL box to generate traffic inbound to the switch on port ethernet 1/1/5:1 which I then expected to load balance across ports ethernet 1/1/1:1 and 1/1/4:1. I used source and dest IP and source and dest port as the criteria for load balancing. Traffic was captured with a single laptop which was directly connected to the two independent receiving interfaces. A single Wireshark monitor session was established for each of the individual interfaces to ensure that they received independent, mutually-exclusive, session load balancing. The traffic was a session I generated by using a laptop that was serving as a virtual web gateway along with being used as an end user device.","title":"The Test Environment"},{"location":"Load%20Balancing%20with%20LAG%20on%205112F-ON/#configure-devices","text":"","title":"Configure Devices"},{"location":"Load%20Balancing%20with%20LAG%20on%205112F-ON/#configure-5212f-on","text":"configure terminal load-balancing ip-selection source-ip destination-ip l4-destination-port l4-source-port port-group 1/1/1 mode eth 10g-4x interface port-channel 1 no shutdown no switchport exit interface ethernet 1/1/1:1 no shut channel-group 1 speed 1000 exit interface ethernet 1/1/4:1 no shut channel-group 1 speed 1000 exit monitor session 1 destination interface port-channel 1 source interface ethernet 1/1/5:1 no shut exit Your listening source captures traffic be that a tap, span, Linux host, etc Traffic is pushed into a physical interface (ethernet 1/1/5:1) on the 5212 Traffic is mirrored from the physical interface (Ethernet 1/1/5:1) to the virtual port group interface The port group interface is tied to two physical interfaces and set to perform load balancing The virtual port group interface will load balance based on a hash of the aforementioned attributes (ip-selection source-ip destination-ip l4-destination-port l4-source-port)","title":"Configure 5212F-ON"},{"location":"Load%20Balancing%20with%20LAG%20on%205112F-ON/#configure-rhel-8","text":"sudo dnf install -y https://dl.fedoraproject.org/pub/epel/epel-release-latest-8.noarch.rpm sudo subscription-manager repos --enable codeready-builder-for-rhel-8-x86_64-rpms dnf group install \"Development Tools\" dnf install -y libpcap-devel make make install /usr/local/bin/tcpreplay -i ens32 test.pcap","title":"Configure RHEL 8"},{"location":"Load%20Balancing%20with%20LAG%20on%205112F-ON/#test-results","text":"The test succeeded. This was verified by checking that the sessions seen by two separate streams were mutually exclusive. At no point did we see effects as described in 4112F-ON Test Case 4 where a single session was sent down two separate lanes. Moreover, by checking the individual sessions you can see the bidirectional flow:","title":"Test Results"},{"location":"Make%20USB%20Read%20Only/","text":"Make USB Read Only Note : These instructions are for a UEFI bootable device with an ext4 filesystem Boot up a separate Linux machine and plug in the USB device you would like to make readonly Run sudo fdisk -l to list the available partitions 1.Confirm you have not built the device with a swap partition grant@telemetrytest:/media$ sudo fdisk -l ...SNIP.. Disk /dev/sdb: 57.29 GiB, 61505273856 bytes, 120127488 sectors Disk model: Ultra Units: sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disklabel type: gpt Disk identifier: C02A7A30-1742-47CE-9DFB-5B3AB96C958C Device Start End Sectors Size Type /dev/sdb1 2048 1050623 1048576 512M EFI System /dev/sdb2 1050624 118126591 117075968 55.8G Linux filesystem The first thing we need to do is get the uuid for the EFI partition. Run sudo blkid /dev/<YOUR_EFI_PARTITION> , in my case /dev/sdb1 grant@telemetrytest:/media$ sudo blkid /dev/sdb1 /dev/sdb1: UUID=\"CD68-8FEA\" TYPE=\"vfat\" PARTUUID=\"d27eda17-c6df-4115-80f3-bd86b56882ac\" Next, we need to edit the base filesystem's fstab to make sure that when this filesystem loads, it will load as readonly. Mount your ext4 filesystem with sudo mount /dev/<YOUR_PARTITION> <YOUR_MOUNT_POINT> . Ex: sudo mount /dev/sdb2 /media . Next, we need to edit fstab. sudo vim /media/etc/fstab # /etc/fstab: static file system information. # # Use 'blkid' to print the universally unique identifier for a # device; this may be used with UUID= as a more robust way to name devices # that works even if disks are added and removed. See fstab(5). # # systemd generates mount units based on this file, see systemd.mount(5). # Please run 'systemctl daemon-reload' after making changes here. # # <file system> <mount point> <type> <options> <dump> <pass> # / was on /dev/sdb2 during installation UUID=6634741f-250a-47b7-96b9-379e517d4591 / ext4 errors=remount-ro 0 1 # /boot/efi was on /dev/sdb1 during installation UUID=CD68-8FEA /boot/efi vfat umask=0077 0 1 # swap was on /dev/sdb3 during installation UUID=4391d5d0-08cb-4fbe-81a8-0a67fe00758c none swap sw 0 0 /dev/sr0 /media/cdrom0 udf,iso9660 user,noauto 0 0 Look for the mount point /boot/efi and confirm that its UUID matches what you saw earlier. Change the options from UUID=CD68-8FEA /boot/efi vfat umask=0077 0 1 to UUID=CD68-8FEA /boot/efi vfat umask=0077,ro 0 1 Next we need to set the primary ext4 partition as read-only. Run sudo umount <YOUR_MOUNT> . Ex: sudo umount /media Set the ext4 filesystem as read only with sudo tune2fs -O read-only /dev/<YOUR_PARTITION> . Ex: sudo tune2fs -O read-only /dev/sdb2 Disconnect the USB drive, boot from it, and confirm read-only behavior.","title":"Make USB Read Only"},{"location":"Make%20USB%20Read%20Only/#make-usb-read-only","text":"Note : These instructions are for a UEFI bootable device with an ext4 filesystem Boot up a separate Linux machine and plug in the USB device you would like to make readonly Run sudo fdisk -l to list the available partitions 1.Confirm you have not built the device with a swap partition grant@telemetrytest:/media$ sudo fdisk -l ...SNIP.. Disk /dev/sdb: 57.29 GiB, 61505273856 bytes, 120127488 sectors Disk model: Ultra Units: sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disklabel type: gpt Disk identifier: C02A7A30-1742-47CE-9DFB-5B3AB96C958C Device Start End Sectors Size Type /dev/sdb1 2048 1050623 1048576 512M EFI System /dev/sdb2 1050624 118126591 117075968 55.8G Linux filesystem The first thing we need to do is get the uuid for the EFI partition. Run sudo blkid /dev/<YOUR_EFI_PARTITION> , in my case /dev/sdb1 grant@telemetrytest:/media$ sudo blkid /dev/sdb1 /dev/sdb1: UUID=\"CD68-8FEA\" TYPE=\"vfat\" PARTUUID=\"d27eda17-c6df-4115-80f3-bd86b56882ac\" Next, we need to edit the base filesystem's fstab to make sure that when this filesystem loads, it will load as readonly. Mount your ext4 filesystem with sudo mount /dev/<YOUR_PARTITION> <YOUR_MOUNT_POINT> . Ex: sudo mount /dev/sdb2 /media . Next, we need to edit fstab. sudo vim /media/etc/fstab # /etc/fstab: static file system information. # # Use 'blkid' to print the universally unique identifier for a # device; this may be used with UUID= as a more robust way to name devices # that works even if disks are added and removed. See fstab(5). # # systemd generates mount units based on this file, see systemd.mount(5). # Please run 'systemctl daemon-reload' after making changes here. # # <file system> <mount point> <type> <options> <dump> <pass> # / was on /dev/sdb2 during installation UUID=6634741f-250a-47b7-96b9-379e517d4591 / ext4 errors=remount-ro 0 1 # /boot/efi was on /dev/sdb1 during installation UUID=CD68-8FEA /boot/efi vfat umask=0077 0 1 # swap was on /dev/sdb3 during installation UUID=4391d5d0-08cb-4fbe-81a8-0a67fe00758c none swap sw 0 0 /dev/sr0 /media/cdrom0 udf,iso9660 user,noauto 0 0 Look for the mount point /boot/efi and confirm that its UUID matches what you saw earlier. Change the options from UUID=CD68-8FEA /boot/efi vfat umask=0077 0 1 to UUID=CD68-8FEA /boot/efi vfat umask=0077,ro 0 1 Next we need to set the primary ext4 partition as read-only. Run sudo umount <YOUR_MOUNT> . Ex: sudo umount /media Set the ext4 filesystem as read only with sudo tune2fs -O read-only /dev/<YOUR_PARTITION> . Ex: sudo tune2fs -O read-only /dev/sdb2 Disconnect the USB drive, boot from it, and confirm read-only behavior.","title":"Make USB Read Only"},{"location":"Migrating%20Storage%20Volumes%20to%20PowerStore/","text":"Migrating Storage Volumes to PowerStore In my scenario I wanted to migrate storage from a Compellent attached with FC to a PowerStore attached to an MX7000 with an M9116n. Note: This is only relevant for older devices in a FC or iSCSI configuration. For NAS you would use any NAS migration technique (rsync, DobiMigrate, etc) Migrating Storage Volumes to PowerStore MX7000 FC Topology Migrating from an Old Device Other Useful Resources PowerStore Educational Videos Operating Systems Compatible with Multipath Drivers Requirements for Non-Disruptive Migration M9116n Compatibility Matrix MX7000 FC Topology Migrating from an Old Device This video describes how the migration from an old device (like a Compellent) to a PowerStore works. In general, on all effected devices, you must install a host plugin which comes with a multipath driver. Before the migration is complete, the host driver will direct all reads/writes to the old device and post migration you will use a cutover option which causes the reads/writes to be redirected to the PowerStore. There is an iSCSI connection between the PowerStore and the compellent which has a synchronization feature that will keep any updates made against the Compellent (or other older device) synced to the in progress copy to the PowerStore Other Useful Resources PowerStore Educational Videos https://www.dell.com/support/kbdoc/en-us/000130110/powerstore-info-hub-product-documentation-videos Operating Systems Compatible with Multipath Drivers https://www.dell.com/support/kbdoc/en-us/000105896/powerstore-supported-host-os-for-non-disruptive-migration-of-storage-resources?lang=en Requirements for Non-Disruptive Migration https://www.dell.com/support/manuals/en-us/powerstore-1000t/pwrstr-import/additional-resources?guid=guid-f5b0a9d3-2eae-447c-b4c3-40e0927ac5f4&lang=en-us M9116n Compatibility Matrix","title":"Migrating Storage Volumes to PowerStore"},{"location":"Migrating%20Storage%20Volumes%20to%20PowerStore/#migrating-storage-volumes-to-powerstore","text":"In my scenario I wanted to migrate storage from a Compellent attached with FC to a PowerStore attached to an MX7000 with an M9116n. Note: This is only relevant for older devices in a FC or iSCSI configuration. For NAS you would use any NAS migration technique (rsync, DobiMigrate, etc) Migrating Storage Volumes to PowerStore MX7000 FC Topology Migrating from an Old Device Other Useful Resources PowerStore Educational Videos Operating Systems Compatible with Multipath Drivers Requirements for Non-Disruptive Migration M9116n Compatibility Matrix","title":"Migrating Storage Volumes to PowerStore"},{"location":"Migrating%20Storage%20Volumes%20to%20PowerStore/#mx7000-fc-topology","text":"","title":"MX7000 FC Topology"},{"location":"Migrating%20Storage%20Volumes%20to%20PowerStore/#migrating-from-an-old-device","text":"This video describes how the migration from an old device (like a Compellent) to a PowerStore works. In general, on all effected devices, you must install a host plugin which comes with a multipath driver. Before the migration is complete, the host driver will direct all reads/writes to the old device and post migration you will use a cutover option which causes the reads/writes to be redirected to the PowerStore. There is an iSCSI connection between the PowerStore and the compellent which has a synchronization feature that will keep any updates made against the Compellent (or other older device) synced to the in progress copy to the PowerStore","title":"Migrating from an Old Device"},{"location":"Migrating%20Storage%20Volumes%20to%20PowerStore/#other-useful-resources","text":"","title":"Other Useful Resources"},{"location":"Migrating%20Storage%20Volumes%20to%20PowerStore/#powerstore-educational-videos","text":"https://www.dell.com/support/kbdoc/en-us/000130110/powerstore-info-hub-product-documentation-videos","title":"PowerStore Educational Videos"},{"location":"Migrating%20Storage%20Volumes%20to%20PowerStore/#operating-systems-compatible-with-multipath-drivers","text":"https://www.dell.com/support/kbdoc/en-us/000105896/powerstore-supported-host-os-for-non-disruptive-migration-of-storage-resources?lang=en","title":"Operating Systems Compatible with Multipath Drivers"},{"location":"Migrating%20Storage%20Volumes%20to%20PowerStore/#requirements-for-non-disruptive-migration","text":"https://www.dell.com/support/manuals/en-us/powerstore-1000t/pwrstr-import/additional-resources?guid=guid-f5b0a9d3-2eae-447c-b4c3-40e0927ac5f4&lang=en-us","title":"Requirements for Non-Disruptive Migration"},{"location":"Migrating%20Storage%20Volumes%20to%20PowerStore/#m9116n-compatibility-matrix","text":"","title":"M9116n Compatibility Matrix"},{"location":"Mulitple%20Span%20on%204112F-ON%20with%20OpenSwitch/","text":"Mulitple Span on 4112F-ON with OpenSwitch In this test case I am testing to see if we can configure a Dell 4112F-ON with OpenSwitch to create a one to many port configuration using SPAN. Helpful Links ONIE Network Install Process Overview OPX Install Instructions for Dell EMC Equipment OPX Tools Source Code OPX Command Reference OPX Docs Home List of Supported Hardware My Configuration General Configuration ONIE host is running RHEL 8 I am using a Dell S4112F-ON for testing OpenSwitch version PKGS_OPX-3.2.0-installer-x86_64 PFSense running DNS and DHCP as services RHEL Release Info NAME=\"Red Hat Enterprise Linux\" VERSION=\"8.0 (Ootpa)\" ID=\"rhel\" ID_LIKE=\"fedora\" VERSION_ID=\"8.0\" PLATFORM_ID=\"platform:el8\" PRETTY_NAME=\"Red Hat Enterprise Linux 8.0 (Ootpa)\" ANSI_COLOR=\"0;31\" CPE_NAME=\"cpe:/o:redhat:enterprise_linux:8.0:GA\" HOME_URL=\"https://www.redhat.com/\" BUG_REPORT_URL=\"https://bugzilla.redhat.com/\" REDHAT_BUGZILLA_PRODUCT=\"Red Hat Enterprise Linux 8\" REDHAT_BUGZILLA_PRODUCT_VERSION=8.0 REDHAT_SUPPORT_PRODUCT=\"Red Hat Enterprise Linux\" REDHAT_SUPPORT_PRODUCT_VERSION=\"8.0\" Red Hat Enterprise Linux release 8.0 (Ootpa) Red Hat Enterprise Linux release 8.0 (Ootpa) OPX Version OS_NAME=\"OPX\" OS_VERSION=\"unstable\" PLATFORM=\"S4112F-ON\" ARCHITECTURE=\"x86_64\" INTERNAL_BUILD_ID=\"OpenSwitch blueprint for Dell 1.0.0\" BUILD_VERSION=\"unstable.0-stretch\" BUILD_DATE=\"2019-06-21T19:04:22+0000\" INSTALL_DATE=\"2019-10-23T23:16:10+00:00\" SYSTEM_UPTIME= 1 day, 5 minutes SYSTEM_STATE= running UPGRADED_PACKAGES=no ALTERED_PACKAGES=yes Wasn't sure why I got the unstable version after installation. It didn't cause any problems for testing so I just left it as is. Setup ONIE Prerequisites See ONIE Install Setup for instructions. Configure Device as TAP Physical Configuration For this configuration to work, we will use the management interface as the input interface for the tap. See image below. You will need to move your network cable over from your usual network to your traffic generator. Configure Management Interface (Optional) Update: After I got it working I ended up using this as an ingress interface so this step is more or less optional. You won't be able to SSH into this interface in the end config (at least not easily). Start by running sudo -i to move to privileged mode. Warning: I noticed the OPX command line tools won't behave correctly unless you are privileged. Ex: opx-show-interface won't list any interfaces. I added vim to my box before continuing with sudo apt-get install -y vim The management interface is configured like a typicaly Debian interface with vim /etc/network/interface.d/eth0 Use the following configuration modified to your needs: auto eth0 allow-hotplug eth0 iface eth0 inet static address 192.168.1.20 netmask 255.255.255.0 gateway 192.168.1.1 When you are finished with your configuration run systemctl restart networking to apply the changes. Confirm the changes were applied with ip address show dev eth0 . If you see two IP addresses because you picked one up from DHCP you can delete the other with ip address del [IP ADDRESS] dev eth0 and then run systemctl restart networking At this juncture your management interface should be up and running and you should be able to SSH to it. I went ahead and swapped to SSH so as not to deal with the oddities that come with running in the console port. Bridge/tc Configuration After attempt 3 I started thinking about other ways to connect things. Realized I could just pump everything to a bridge and let that do the replication. That worked! Do the following to get it up and running: tc is a feature of modern Linux kernels designed for mirroring traffic. I found this article helpful. This Mellanox article was also useful. WARNING: You must use your management interface for the ingress port or this solution will not work! I noticed the other ports do not behave like normal Linux ports. More investigation required to figure out the difference. Create a bridge interface with brctl addbr br0 Attach all interfaces you want as part of the port mirroring to the bridge with brctl addif br0 < INTERFACE > 1.Make sure all interfaces in use are enabled with ip link set < INTERFACE > up Disable MAC address learning on the bridge with brctl setageing br0 0 Set the device's management interface to promiscuous mode with ip link set < MGMT_INTERFACE > promisc on The first thing I did was create an ingress queue on my input interface with tc qdisc add dev < MGMT_INTERFACE > handle ffff: ingress 1.If you need to delete a qdisc you can do it with tc qdisc del dev < MGMT_INTERFACE > [ root | ingress ] Double check your queue with handle ffff was created with tc -s qdisc ls dev < MGMT_INTERFACE > Next we want to mirror all traffic from the ingress port to an output port with tc filter add dev < MGMT_INTERFACE > parent ffff: protocol all u32 match u32 0 0 action mirred egress mirror dev br0 Check that your port mirror appeared in the config with tc -s -p filter ls dev < MGMT_INTERFACE > parent ffff: 1.If you need to delete the filters you can do so with tc filter del dev < MGMT_INTERFACE > parent ffff: Set queue to not shape traffic with tc qdisc add dev < MGMT_INTERFACE > handle 1: root prio Things I Tried I added 7 interfaces to my bridge to make sure there weren't any strange limitations I moved the SFPs around to multiple different ports to make sure the traffic was mirroing on all of them I double checked the traffic I was capturing belonged to the PCAP in question. Easy enough to see because it has IP addresses the hosts in question wouldn't ever otherwise see. Screenshots for confirmation below. Host 1 Host 2 - I checked that pure L3 traffic was passed correctly using ICMP. Noted Problem The only major issue I noticed is that pure layer 2 traffic didn't get passed. Haven't figured out how to fix that yet. Failed Ideas Attempt 1 - Mirror Ports My first go is to try using OpenSwitch's built in mirroring capability. Physical Configuration I didn't have enough target hosts to try outputting from one port to all ports so I simulated it. The purple cable in the image is the input port from the traffic generator (tcpreplay) and the white and yellow cables go out to the hosts listed as host 1 and host 2 in the test results section. The ports with the white and yellow cables were configured as the mirror's target ports. Mirror Configuration For each port you want to mirror to run opx-config-mirror create --src_intf e101-001-0 --dest_intf e101-005-0 --direction ingress --type span . Substitute your source and destination ports appropriately. Results Mirror Port Failure After 4 I was only able to get this to work on up to 4 ports. After that I received errors. See output below: root@OPX:~# opx-config-mirror create --src_intf e101-001-0 --dest_intf e101-005-0 --direction ingress --type span root@OPX:~# ip link set e101-009-0 up root@OPX:~# opx-config-mirror create --src_intf e101-001-0 --dest_intf e101-009-0 --direction ingress --type span root@OPX:~# opx-config-mirror create --src_intf e101-001-0 --dest_intf e101-002-0 --direction ingress --type span root@OPX:~# opx-config-mirror create --src_intf e101-001-0 --dest_intf e101-003-0 --direction ingress --type span root@OPX:~# opx-config-mirror create --src_intf e101-001-0 --dest_intf e101-004-0 --direction ingress --type span {'data': {'base-mirror/entry/dst-intf': bytearray(b'\\x0f\\x00\\x00\\x00'), 'base-mirror/entry/type': bytearray(b'\\x01\\x00\\x00\\x00'), 'base-mirror/entry/intf': {'0': {'base-mirror/entry/intf/src': bytearray(b'\\x0c\\x00\\x00\\x00'), 'base-mirror/entry/intf/direction': bytearray(b'\\x01\\x00\\x00\\x00')}}}, 'key': '1.27.1769488.1769473.'} opx-config-mirror: Commit failed root@OPX:~# opx-config-mirror create --src_intf e101-001-0 --dest_intf e101-006-0 --direction ingress --type span {'data': {'base-mirror/entry/dst-intf': bytearray(b'\\x11\\x00\\x00\\x00'), 'base-mirror/entry/type': bytearray(b'\\x01\\x00\\x00\\x00'), 'base-mirror/entry/intf': {'0': {'base-mirror/entry/intf/src': bytearray(b'\\x0c\\x00\\x00\\x00'), 'base-mirror/entry/intf/direction': bytearray(b'\\x01\\x00\\x00\\x00')}}}, 'key': '1.27.1769488.1769473.'} opx-config-mirror: Commit failed root@OPX:~# opx-config-mirror create --src_intf e101-001-0 --dest_intf e101-007-0 --direction ingress --type span {'data': {'base-mirror/entry/dst-intf': bytearray(b'\\x12\\x00\\x00\\x00'), 'base-mirror/entry/type': bytearray(b'\\x01\\x00\\x00\\x00'), 'base-mirror/entry/intf': {'0': {'base-mirror/entry/intf/src': bytearray(b'\\x0c\\x00\\x00\\x00'), 'base-mirror/entry/intf/direction': bytearray(b'\\x01\\x00\\x00\\x00')}}}, 'key': '1.27.1769488.1769473.'} opx-config-mirror: Commit failed Pretty printed version for ease of reading: { 'data': { 'base-mirror/entry/dst-intf': bytearray(b '\\x12\\x00\\x00\\x00'), 'base-mirror/entry/type': bytearray(b '\\x01\\x00\\x00\\x00'), 'base-mirror/entry/intf': { '0': { 'base-mirror/entry/intf/src': bytearray(b '\\x0c\\x00\\x00\\x00'), 'base-mirror/entry/intf/direction': bytearray(b '\\x01\\x00\\x00\\x00') } } }, 'key': '1.27.1769488.1769473.' } Functioning Mirror Ports Before I caught the error, I did test the first two mirror ports I made and they worked as expected. See the below. I used tcpreplay with some traffic I captured on my desktop to test the idea. I just uploaded the PCAP and replayed it with tcpreplay -i ens224 ./test_pcap.pcap --loop 500 I then confirmed that all target ports received traffic. See screenshots below: Host 1 Host 2 The host I collected the traffic on was 192.168.1.6 and as you can see from the images both hosts were able to see traffic from the tcpreplay session. Attempt 2 - tc Configuration tc is a feature of modern Linux kernels designed for mirroring traffic. I found this article helpful. This Mellanox article was also useful. The first thing I did was create an ingress queue on my input interface with tc qdisc add dev e101-001-0 handle ffff: ingress 1.If you need to delete a qdisc you can do it with tc qdisc del dev e101-001-0 [ root | ingress ] Double check your queue with handle ffff was created with tc -s qdisc ls dev e101-001-0 Next we want to mirror all traffic from the ingress port to an output port with tc filter add dev e101-001-0 parent ffff: protocol all u32 match u32 0 0 action mirred egress mirror dev e101-005-0 Check that your port mirror appeared in the config with tc -s -p filter ls dev e101-001-0 parent ffff: 1.If you need to delete the filters you can do so with tc filter del dev e101-001-0 parent ffff: Set queue to not shape traffic with tc qdisc add dev e101-001-0 handle 1: root prio Alternate Configuration I tried instead running: tc qdisc add dev e101-001-0 clsact tc filter add dev e101-001-0 ingress matchall skip_sw action mirred egress mirror dev e101-005-0 Other Things Tried Haven't been able to figure out why just yet, but only Layer 2 traffic is making it through the port mirror. Everything above gets dropped. I thought maybe it was MAC address learning, but the problem persisted when I ran opx-config-global-switch --mac-age-time 0 I also thought that it was the port not being set to promiscuous mode so I gave it ifconfig e101-001-0 promisc . That didn't work either. Conclusions I'm pretty confident that because this is a network OS for switching something funky is going on. Ex: When you run a port mirror, all the traffic passes correctly, but you won't see any of that traffic on a tcpdump session. Need to study up on the architecture. I'm pretty sure there's a way to make this particular tactic work, but for time's sake I'm going to try something else. Attempt 3 - tc on Management Interface I realized something is going on with the forwarding tables on the switch at a low level that was intercepting our traffic in attempt 2. That said, I noticed that the management interface for the switch effectively works like a standard Linux interface. I did the same thing I did in attempt 2 except I used the managament interface instead of one of the other interfaces. Results The switch accepts the config. However, the traffic only goes out to one port at a time.","title":"Mulitple Span on 4112F-ON with OpenSwitch"},{"location":"Mulitple%20Span%20on%204112F-ON%20with%20OpenSwitch/#mulitple-span-on-4112f-on-with-openswitch","text":"In this test case I am testing to see if we can configure a Dell 4112F-ON with OpenSwitch to create a one to many port configuration using SPAN.","title":"Mulitple Span on 4112F-ON with OpenSwitch"},{"location":"Mulitple%20Span%20on%204112F-ON%20with%20OpenSwitch/#helpful-links","text":"ONIE Network Install Process Overview OPX Install Instructions for Dell EMC Equipment OPX Tools Source Code OPX Command Reference OPX Docs Home List of Supported Hardware","title":"Helpful Links"},{"location":"Mulitple%20Span%20on%204112F-ON%20with%20OpenSwitch/#my-configuration","text":"","title":"My Configuration"},{"location":"Mulitple%20Span%20on%204112F-ON%20with%20OpenSwitch/#general-configuration","text":"ONIE host is running RHEL 8 I am using a Dell S4112F-ON for testing OpenSwitch version PKGS_OPX-3.2.0-installer-x86_64 PFSense running DNS and DHCP as services","title":"General Configuration"},{"location":"Mulitple%20Span%20on%204112F-ON%20with%20OpenSwitch/#rhel-release-info","text":"NAME=\"Red Hat Enterprise Linux\" VERSION=\"8.0 (Ootpa)\" ID=\"rhel\" ID_LIKE=\"fedora\" VERSION_ID=\"8.0\" PLATFORM_ID=\"platform:el8\" PRETTY_NAME=\"Red Hat Enterprise Linux 8.0 (Ootpa)\" ANSI_COLOR=\"0;31\" CPE_NAME=\"cpe:/o:redhat:enterprise_linux:8.0:GA\" HOME_URL=\"https://www.redhat.com/\" BUG_REPORT_URL=\"https://bugzilla.redhat.com/\" REDHAT_BUGZILLA_PRODUCT=\"Red Hat Enterprise Linux 8\" REDHAT_BUGZILLA_PRODUCT_VERSION=8.0 REDHAT_SUPPORT_PRODUCT=\"Red Hat Enterprise Linux\" REDHAT_SUPPORT_PRODUCT_VERSION=\"8.0\" Red Hat Enterprise Linux release 8.0 (Ootpa) Red Hat Enterprise Linux release 8.0 (Ootpa)","title":"RHEL Release Info"},{"location":"Mulitple%20Span%20on%204112F-ON%20with%20OpenSwitch/#opx-version","text":"OS_NAME=\"OPX\" OS_VERSION=\"unstable\" PLATFORM=\"S4112F-ON\" ARCHITECTURE=\"x86_64\" INTERNAL_BUILD_ID=\"OpenSwitch blueprint for Dell 1.0.0\" BUILD_VERSION=\"unstable.0-stretch\" BUILD_DATE=\"2019-06-21T19:04:22+0000\" INSTALL_DATE=\"2019-10-23T23:16:10+00:00\" SYSTEM_UPTIME= 1 day, 5 minutes SYSTEM_STATE= running UPGRADED_PACKAGES=no ALTERED_PACKAGES=yes Wasn't sure why I got the unstable version after installation. It didn't cause any problems for testing so I just left it as is.","title":"OPX Version"},{"location":"Mulitple%20Span%20on%204112F-ON%20with%20OpenSwitch/#setup-onie-prerequisites","text":"See ONIE Install Setup for instructions.","title":"Setup ONIE Prerequisites"},{"location":"Mulitple%20Span%20on%204112F-ON%20with%20OpenSwitch/#configure-device-as-tap","text":"","title":"Configure Device as TAP"},{"location":"Mulitple%20Span%20on%204112F-ON%20with%20OpenSwitch/#physical-configuration","text":"For this configuration to work, we will use the management interface as the input interface for the tap. See image below. You will need to move your network cable over from your usual network to your traffic generator.","title":"Physical Configuration"},{"location":"Mulitple%20Span%20on%204112F-ON%20with%20OpenSwitch/#configure-management-interface-optional","text":"Update: After I got it working I ended up using this as an ingress interface so this step is more or less optional. You won't be able to SSH into this interface in the end config (at least not easily). Start by running sudo -i to move to privileged mode. Warning: I noticed the OPX command line tools won't behave correctly unless you are privileged. Ex: opx-show-interface won't list any interfaces. I added vim to my box before continuing with sudo apt-get install -y vim The management interface is configured like a typicaly Debian interface with vim /etc/network/interface.d/eth0 Use the following configuration modified to your needs: auto eth0 allow-hotplug eth0 iface eth0 inet static address 192.168.1.20 netmask 255.255.255.0 gateway 192.168.1.1 When you are finished with your configuration run systemctl restart networking to apply the changes. Confirm the changes were applied with ip address show dev eth0 . If you see two IP addresses because you picked one up from DHCP you can delete the other with ip address del [IP ADDRESS] dev eth0 and then run systemctl restart networking At this juncture your management interface should be up and running and you should be able to SSH to it. I went ahead and swapped to SSH so as not to deal with the oddities that come with running in the console port.","title":"Configure Management Interface (Optional)"},{"location":"Mulitple%20Span%20on%204112F-ON%20with%20OpenSwitch/#bridgetc-configuration","text":"After attempt 3 I started thinking about other ways to connect things. Realized I could just pump everything to a bridge and let that do the replication. That worked! Do the following to get it up and running: tc is a feature of modern Linux kernels designed for mirroring traffic. I found this article helpful. This Mellanox article was also useful. WARNING: You must use your management interface for the ingress port or this solution will not work! I noticed the other ports do not behave like normal Linux ports. More investigation required to figure out the difference. Create a bridge interface with brctl addbr br0 Attach all interfaces you want as part of the port mirroring to the bridge with brctl addif br0 < INTERFACE > 1.Make sure all interfaces in use are enabled with ip link set < INTERFACE > up Disable MAC address learning on the bridge with brctl setageing br0 0 Set the device's management interface to promiscuous mode with ip link set < MGMT_INTERFACE > promisc on The first thing I did was create an ingress queue on my input interface with tc qdisc add dev < MGMT_INTERFACE > handle ffff: ingress 1.If you need to delete a qdisc you can do it with tc qdisc del dev < MGMT_INTERFACE > [ root | ingress ] Double check your queue with handle ffff was created with tc -s qdisc ls dev < MGMT_INTERFACE > Next we want to mirror all traffic from the ingress port to an output port with tc filter add dev < MGMT_INTERFACE > parent ffff: protocol all u32 match u32 0 0 action mirred egress mirror dev br0 Check that your port mirror appeared in the config with tc -s -p filter ls dev < MGMT_INTERFACE > parent ffff: 1.If you need to delete the filters you can do so with tc filter del dev < MGMT_INTERFACE > parent ffff: Set queue to not shape traffic with tc qdisc add dev < MGMT_INTERFACE > handle 1: root prio","title":"Bridge/tc Configuration"},{"location":"Mulitple%20Span%20on%204112F-ON%20with%20OpenSwitch/#things-i-tried","text":"I added 7 interfaces to my bridge to make sure there weren't any strange limitations I moved the SFPs around to multiple different ports to make sure the traffic was mirroing on all of them I double checked the traffic I was capturing belonged to the PCAP in question. Easy enough to see because it has IP addresses the hosts in question wouldn't ever otherwise see. Screenshots for confirmation below.","title":"Things I Tried"},{"location":"Mulitple%20Span%20on%204112F-ON%20with%20OpenSwitch/#host-1","text":"","title":"Host 1"},{"location":"Mulitple%20Span%20on%204112F-ON%20with%20OpenSwitch/#host-2","text":"- I checked that pure L3 traffic was passed correctly using ICMP.","title":"Host 2"},{"location":"Mulitple%20Span%20on%204112F-ON%20with%20OpenSwitch/#noted-problem","text":"The only major issue I noticed is that pure layer 2 traffic didn't get passed. Haven't figured out how to fix that yet.","title":"Noted Problem"},{"location":"Mulitple%20Span%20on%204112F-ON%20with%20OpenSwitch/#failed-ideas","text":"","title":"Failed Ideas"},{"location":"Mulitple%20Span%20on%204112F-ON%20with%20OpenSwitch/#attempt-1-mirror-ports","text":"My first go is to try using OpenSwitch's built in mirroring capability.","title":"Attempt 1 - Mirror Ports"},{"location":"Mulitple%20Span%20on%204112F-ON%20with%20OpenSwitch/#physical-configuration_1","text":"I didn't have enough target hosts to try outputting from one port to all ports so I simulated it. The purple cable in the image is the input port from the traffic generator (tcpreplay) and the white and yellow cables go out to the hosts listed as host 1 and host 2 in the test results section. The ports with the white and yellow cables were configured as the mirror's target ports.","title":"Physical Configuration"},{"location":"Mulitple%20Span%20on%204112F-ON%20with%20OpenSwitch/#mirror-configuration","text":"For each port you want to mirror to run opx-config-mirror create --src_intf e101-001-0 --dest_intf e101-005-0 --direction ingress --type span . Substitute your source and destination ports appropriately.","title":"Mirror Configuration"},{"location":"Mulitple%20Span%20on%204112F-ON%20with%20OpenSwitch/#results","text":"","title":"Results"},{"location":"Mulitple%20Span%20on%204112F-ON%20with%20OpenSwitch/#mirror-port-failure-after-4","text":"I was only able to get this to work on up to 4 ports. After that I received errors. See output below: root@OPX:~# opx-config-mirror create --src_intf e101-001-0 --dest_intf e101-005-0 --direction ingress --type span root@OPX:~# ip link set e101-009-0 up root@OPX:~# opx-config-mirror create --src_intf e101-001-0 --dest_intf e101-009-0 --direction ingress --type span root@OPX:~# opx-config-mirror create --src_intf e101-001-0 --dest_intf e101-002-0 --direction ingress --type span root@OPX:~# opx-config-mirror create --src_intf e101-001-0 --dest_intf e101-003-0 --direction ingress --type span root@OPX:~# opx-config-mirror create --src_intf e101-001-0 --dest_intf e101-004-0 --direction ingress --type span {'data': {'base-mirror/entry/dst-intf': bytearray(b'\\x0f\\x00\\x00\\x00'), 'base-mirror/entry/type': bytearray(b'\\x01\\x00\\x00\\x00'), 'base-mirror/entry/intf': {'0': {'base-mirror/entry/intf/src': bytearray(b'\\x0c\\x00\\x00\\x00'), 'base-mirror/entry/intf/direction': bytearray(b'\\x01\\x00\\x00\\x00')}}}, 'key': '1.27.1769488.1769473.'} opx-config-mirror: Commit failed root@OPX:~# opx-config-mirror create --src_intf e101-001-0 --dest_intf e101-006-0 --direction ingress --type span {'data': {'base-mirror/entry/dst-intf': bytearray(b'\\x11\\x00\\x00\\x00'), 'base-mirror/entry/type': bytearray(b'\\x01\\x00\\x00\\x00'), 'base-mirror/entry/intf': {'0': {'base-mirror/entry/intf/src': bytearray(b'\\x0c\\x00\\x00\\x00'), 'base-mirror/entry/intf/direction': bytearray(b'\\x01\\x00\\x00\\x00')}}}, 'key': '1.27.1769488.1769473.'} opx-config-mirror: Commit failed root@OPX:~# opx-config-mirror create --src_intf e101-001-0 --dest_intf e101-007-0 --direction ingress --type span {'data': {'base-mirror/entry/dst-intf': bytearray(b'\\x12\\x00\\x00\\x00'), 'base-mirror/entry/type': bytearray(b'\\x01\\x00\\x00\\x00'), 'base-mirror/entry/intf': {'0': {'base-mirror/entry/intf/src': bytearray(b'\\x0c\\x00\\x00\\x00'), 'base-mirror/entry/intf/direction': bytearray(b'\\x01\\x00\\x00\\x00')}}}, 'key': '1.27.1769488.1769473.'} opx-config-mirror: Commit failed Pretty printed version for ease of reading: { 'data': { 'base-mirror/entry/dst-intf': bytearray(b '\\x12\\x00\\x00\\x00'), 'base-mirror/entry/type': bytearray(b '\\x01\\x00\\x00\\x00'), 'base-mirror/entry/intf': { '0': { 'base-mirror/entry/intf/src': bytearray(b '\\x0c\\x00\\x00\\x00'), 'base-mirror/entry/intf/direction': bytearray(b '\\x01\\x00\\x00\\x00') } } }, 'key': '1.27.1769488.1769473.' }","title":"Mirror Port Failure After 4"},{"location":"Mulitple%20Span%20on%204112F-ON%20with%20OpenSwitch/#functioning-mirror-ports","text":"Before I caught the error, I did test the first two mirror ports I made and they worked as expected. See the below. I used tcpreplay with some traffic I captured on my desktop to test the idea. I just uploaded the PCAP and replayed it with tcpreplay -i ens224 ./test_pcap.pcap --loop 500 I then confirmed that all target ports received traffic. See screenshots below:","title":"Functioning Mirror Ports"},{"location":"Mulitple%20Span%20on%204112F-ON%20with%20OpenSwitch/#host-1_1","text":"","title":"Host 1"},{"location":"Mulitple%20Span%20on%204112F-ON%20with%20OpenSwitch/#host-2_1","text":"The host I collected the traffic on was 192.168.1.6 and as you can see from the images both hosts were able to see traffic from the tcpreplay session.","title":"Host 2"},{"location":"Mulitple%20Span%20on%204112F-ON%20with%20OpenSwitch/#attempt-2-tc","text":"","title":"Attempt 2 - tc"},{"location":"Mulitple%20Span%20on%204112F-ON%20with%20OpenSwitch/#configuration","text":"tc is a feature of modern Linux kernels designed for mirroring traffic. I found this article helpful. This Mellanox article was also useful. The first thing I did was create an ingress queue on my input interface with tc qdisc add dev e101-001-0 handle ffff: ingress 1.If you need to delete a qdisc you can do it with tc qdisc del dev e101-001-0 [ root | ingress ] Double check your queue with handle ffff was created with tc -s qdisc ls dev e101-001-0 Next we want to mirror all traffic from the ingress port to an output port with tc filter add dev e101-001-0 parent ffff: protocol all u32 match u32 0 0 action mirred egress mirror dev e101-005-0 Check that your port mirror appeared in the config with tc -s -p filter ls dev e101-001-0 parent ffff: 1.If you need to delete the filters you can do so with tc filter del dev e101-001-0 parent ffff: Set queue to not shape traffic with tc qdisc add dev e101-001-0 handle 1: root prio","title":"Configuration"},{"location":"Mulitple%20Span%20on%204112F-ON%20with%20OpenSwitch/#alternate-configuration","text":"I tried instead running: tc qdisc add dev e101-001-0 clsact tc filter add dev e101-001-0 ingress matchall skip_sw action mirred egress mirror dev e101-005-0","title":"Alternate Configuration"},{"location":"Mulitple%20Span%20on%204112F-ON%20with%20OpenSwitch/#other-things-tried","text":"Haven't been able to figure out why just yet, but only Layer 2 traffic is making it through the port mirror. Everything above gets dropped. I thought maybe it was MAC address learning, but the problem persisted when I ran opx-config-global-switch --mac-age-time 0 I also thought that it was the port not being set to promiscuous mode so I gave it ifconfig e101-001-0 promisc . That didn't work either.","title":"Other Things Tried"},{"location":"Mulitple%20Span%20on%204112F-ON%20with%20OpenSwitch/#conclusions","text":"I'm pretty confident that because this is a network OS for switching something funky is going on. Ex: When you run a port mirror, all the traffic passes correctly, but you won't see any of that traffic on a tcpdump session. Need to study up on the architecture. I'm pretty sure there's a way to make this particular tactic work, but for time's sake I'm going to try something else.","title":"Conclusions"},{"location":"Mulitple%20Span%20on%204112F-ON%20with%20OpenSwitch/#attempt-3-tc-on-management-interface","text":"I realized something is going on with the forwarding tables on the switch at a low level that was intercepting our traffic in attempt 2. That said, I noticed that the management interface for the switch effectively works like a standard Linux interface. I did the same thing I did in attempt 2 except I used the managament interface instead of one of the other interfaces.","title":"Attempt 3  - tc on Management Interface"},{"location":"Mulitple%20Span%20on%204112F-ON%20with%20OpenSwitch/#results_1","text":"The switch accepts the config. However, the traffic only goes out to one port at a time.","title":"Results"},{"location":"Multiple%20Span%20on%204112F-ON%20with%20OS10/","text":"Multiple Span on 4112F-ON with OS10 In this test case I am testing to see if we can configure a Dell 4112F-ON with OS10 to create a one to many port configuration using SPAN. Helpful Links ONIE Network Install Process Overview My Configuration General Configuration ONIE host is running RHEL 8 I am using a Dell S4112F-ON for testing OS10 version 10.5.0.2 PFSense running DNS and DHCP as services RHEL Release Info NAME=\"Red Hat Enterprise Linux\" VERSION=\"8.0 (Ootpa)\" ID=\"rhel\" ID_LIKE=\"fedora\" VERSION_ID=\"8.0\" PLATFORM_ID=\"platform:el8\" PRETTY_NAME=\"Red Hat Enterprise Linux 8.0 (Ootpa)\" ANSI_COLOR=\"0;31\" CPE_NAME=\"cpe:/o:redhat:enterprise_linux:8.0:GA\" HOME_URL=\"https://www.redhat.com/\" BUG_REPORT_URL=\"https://bugzilla.redhat.com/\" REDHAT_BUGZILLA_PRODUCT=\"Red Hat Enterprise Linux 8\" REDHAT_BUGZILLA_PRODUCT_VERSION=8.0 REDHAT_SUPPORT_PRODUCT=\"Red Hat Enterprise Linux\" REDHAT_SUPPORT_PRODUCT_VERSION=\"8.0\" Red Hat Enterprise Linux release 8.0 (Ootpa) Red Hat Enterprise Linux release 8.0 (Ootpa) OS10 Version Dell EMC Networking OS10 Enterprise Copyright (c) 1999-2019 by Dell Inc. All Rights Reserved. OS Version: 10.5.0.2 Build Version: 10.5.0.2.468 Build Time: 2019-10-19T00:29:00+0000 System Type: S4112F-ON Architecture: x86_64 Up Time: 00:13:11 Setup ONIE Prerequisites See ONIE Install Setup for instructions. Configure Device as TAP Physical Configuration For this configuration to work, we will use the management interface as the input interface for the tap. See image below. You will need to move your network cable over from your usual network to your traffic generator. Problem The way this worked on OPX was to use the Linux kernel module called TC. The net_sched module which supports ingress packet manipulation is not available on OS10. It could be reinstalled, but I didn't explore this option. Currently I don't have a working config on OS10. Test with Mirror Ports It looks like OS10 only supports one destination port on a port mirror. See the below. OS10(conf-mon-local-1)# do show monitor session 1 S.Id Source Destination Dir Mode Source IP Dest IP DSCP TTL Gre-Protocol State Reason --------------------------------------------------------------------------------------------------------------------------------------- 1 ethernet1/1/3 both port N/A N/A N/A N/A N/A false Destination is not configured OS10(conf-mon-local-1)# destination interface ethernet 1/1/7 OS10(conf-mon-local-1)# destination interface ethernet 1/1/8 % Error: Configuration mismatch. OS10(conf-mon-local-1)# no destination interface ethernet 1/1/7 OS10(conf-mon-local-1)# destination interface ethernet 1/1/8 OS10(conf-mon-local-1)#","title":"Multiple Span on 4112F-ON with OS10"},{"location":"Multiple%20Span%20on%204112F-ON%20with%20OS10/#multiple-span-on-4112f-on-with-os10","text":"In this test case I am testing to see if we can configure a Dell 4112F-ON with OS10 to create a one to many port configuration using SPAN.","title":"Multiple Span on 4112F-ON with OS10"},{"location":"Multiple%20Span%20on%204112F-ON%20with%20OS10/#helpful-links","text":"ONIE Network Install Process Overview","title":"Helpful Links"},{"location":"Multiple%20Span%20on%204112F-ON%20with%20OS10/#my-configuration","text":"","title":"My Configuration"},{"location":"Multiple%20Span%20on%204112F-ON%20with%20OS10/#general-configuration","text":"ONIE host is running RHEL 8 I am using a Dell S4112F-ON for testing OS10 version 10.5.0.2 PFSense running DNS and DHCP as services","title":"General Configuration"},{"location":"Multiple%20Span%20on%204112F-ON%20with%20OS10/#rhel-release-info","text":"NAME=\"Red Hat Enterprise Linux\" VERSION=\"8.0 (Ootpa)\" ID=\"rhel\" ID_LIKE=\"fedora\" VERSION_ID=\"8.0\" PLATFORM_ID=\"platform:el8\" PRETTY_NAME=\"Red Hat Enterprise Linux 8.0 (Ootpa)\" ANSI_COLOR=\"0;31\" CPE_NAME=\"cpe:/o:redhat:enterprise_linux:8.0:GA\" HOME_URL=\"https://www.redhat.com/\" BUG_REPORT_URL=\"https://bugzilla.redhat.com/\" REDHAT_BUGZILLA_PRODUCT=\"Red Hat Enterprise Linux 8\" REDHAT_BUGZILLA_PRODUCT_VERSION=8.0 REDHAT_SUPPORT_PRODUCT=\"Red Hat Enterprise Linux\" REDHAT_SUPPORT_PRODUCT_VERSION=\"8.0\" Red Hat Enterprise Linux release 8.0 (Ootpa) Red Hat Enterprise Linux release 8.0 (Ootpa)","title":"RHEL Release Info"},{"location":"Multiple%20Span%20on%204112F-ON%20with%20OS10/#os10-version","text":"Dell EMC Networking OS10 Enterprise Copyright (c) 1999-2019 by Dell Inc. All Rights Reserved. OS Version: 10.5.0.2 Build Version: 10.5.0.2.468 Build Time: 2019-10-19T00:29:00+0000 System Type: S4112F-ON Architecture: x86_64 Up Time: 00:13:11","title":"OS10 Version"},{"location":"Multiple%20Span%20on%204112F-ON%20with%20OS10/#setup-onie-prerequisites","text":"See ONIE Install Setup for instructions.","title":"Setup ONIE Prerequisites"},{"location":"Multiple%20Span%20on%204112F-ON%20with%20OS10/#configure-device-as-tap","text":"","title":"Configure Device as TAP"},{"location":"Multiple%20Span%20on%204112F-ON%20with%20OS10/#physical-configuration","text":"For this configuration to work, we will use the management interface as the input interface for the tap. See image below. You will need to move your network cable over from your usual network to your traffic generator.","title":"Physical Configuration"},{"location":"Multiple%20Span%20on%204112F-ON%20with%20OS10/#problem","text":"The way this worked on OPX was to use the Linux kernel module called TC. The net_sched module which supports ingress packet manipulation is not available on OS10. It could be reinstalled, but I didn't explore this option. Currently I don't have a working config on OS10.","title":"Problem"},{"location":"Multiple%20Span%20on%204112F-ON%20with%20OS10/#test-with-mirror-ports","text":"It looks like OS10 only supports one destination port on a port mirror. See the below. OS10(conf-mon-local-1)# do show monitor session 1 S.Id Source Destination Dir Mode Source IP Dest IP DSCP TTL Gre-Protocol State Reason --------------------------------------------------------------------------------------------------------------------------------------- 1 ethernet1/1/3 both port N/A N/A N/A N/A N/A false Destination is not configured OS10(conf-mon-local-1)# destination interface ethernet 1/1/7 OS10(conf-mon-local-1)# destination interface ethernet 1/1/8 % Error: Configuration mismatch. OS10(conf-mon-local-1)# no destination interface ethernet 1/1/7 OS10(conf-mon-local-1)# destination interface ethernet 1/1/8 OS10(conf-mon-local-1)#","title":"Test with Mirror Ports"},{"location":"NVMe%20Performance%20Testing/","text":"NVMe Performance Testing Helpful Resources How fast are your disks? Find out the open source way, with fio (arstechnica) Configuration Drives OS [root@r8402 ~]# cat /etc/*-release NAME=\"Red Hat Enterprise Linux\" VERSION=\"8.2 (Ootpa)\" ID=\"rhel\" ID_LIKE=\"fedora\" VERSION_ID=\"8.2\" PLATFORM_ID=\"platform:el8\" PRETTY_NAME=\"Red Hat Enterprise Linux 8.2 (Ootpa)\" ANSI_COLOR=\"0;31\" CPE_NAME=\"cpe:/o:redhat:enterprise_linux:8.2:GA\" HOME_URL=\"https://www.redhat.com/\" BUG_REPORT_URL=\"https://bugzilla.redhat.com/\" REDHAT_BUGZILLA_PRODUCT=\"Red Hat Enterprise Linux 8\" REDHAT_BUGZILLA_PRODUCT_VERSION=8.2 REDHAT_SUPPORT_PRODUCT=\"Red Hat Enterprise Linux\" REDHAT_SUPPORT_PRODUCT_VERSION=\"8.2\" Red Hat Enterprise Linux release 8.2 (Ootpa) Red Hat Enterprise Linux release 8.2 (Ootpa) Tests Test 1 Config pvcreate /dev/nvme2n1 pvcreate /dev/nvme1n1 pvcreate /dev/nvme0n1 pvcreate /dev/nvme3n1 vgcreate data /dev/nvme3n1 /dev/nvme2n1 /dev/nvme1n1 /dev/nvme0n1 lvcreate -l 100%FREE -i4 -I128 -n data data mkfs.ext4 -F -b 4096 -E discard,stride=16,stripe-width=256 /dev/mapper/data-data mkdir /data mount -o rw,auto,discard /dev/mapper/data-data /data fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test --filename=test --bs=64k --iodepth=32 --size=50G --readwrite=randrw --rwmixread=60 Results Test: (g=0): rw=randrw, bs=(R) 64.0KiB-64.0KiB, (W) 64.0KiB-64.0KiB, (T) 64.0KiB-64.0KiB, ioengine=libaio, iodepth=32 fio-3.7 Starting 1 process Test: Laying out IO file (1 file / 51200MiB) Jobs: 1 (f=1): [m(1)][100.0%][r=217MiB/s,w=148MiB/s][r=3476,w=2371 IOPS][eta 00m:00s] Test: (groupid=0, jobs=1): err= 0: pid=3290: Mon Sep 14 11:17:39 2020 read: IOPS=3639, BW=227MiB/s (239MB/s)(29.0GiB/134968msec) bw ( KiB/s): min=211328, max=261504, per=99.99%, avg=232911.32, stdev=8921.48, samples=269 iops : min= 3302, max= 4086, avg=3639.22, stdev=139.41, samples=269 write: IOPS=2429, BW=152MiB/s (159MB/s)(20.0GiB/134968msec) bw ( KiB/s): min=140800, max=169856, per=100.00%, avg=155506.13, stdev=5852.42, samples=269 iops : min= 2200, max= 2654, avg=2429.77, stdev=91.43, samples=269 cpu : usr=2.19%, sys=8.61%, ctx=98853, majf=0, minf=13 IO depths : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=100.0%, >=64=0.0% submit : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0% complete : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.1%, 64=0.0%, >=64=0.0% issued rwts: total=491242,327958,0,0 short=0,0,0,0 dropped=0,0,0,0 latency : target=0, window=0, percentile=100.00%, depth=32 Run status group 0 (all jobs): READ: bw=227MiB/s (239MB/s), 227MiB/s-227MiB/s (239MB/s-239MB/s), io=29.0GiB (32.2GB), run=134968-134968msec WRITE: bw=152MiB/s (159MB/s), 152MiB/s-152MiB/s (159MB/s-159MB/s), io=20.0GiB (21.5GB), run=134968-134968msec Disk stats (read/write): dm-0: ios=490775/327707, merge=0/0, ticks=3019139/1204501, in_queue=4223640, util=68.01%, aggrios=491243/328000, aggrmerge=1/11, aggrticks=3031345/1210547, aggrin_queue=3827355, aggrutil=67.98% sda: ios=491243/328000, merge=1/11, ticks=3031345/1210547, in_queue=3827355, util=67.98%","title":"NVMe Performance Testing"},{"location":"NVMe%20Performance%20Testing/#nvme-performance-testing","text":"","title":"NVMe Performance Testing"},{"location":"NVMe%20Performance%20Testing/#helpful-resources","text":"How fast are your disks? Find out the open source way, with fio (arstechnica)","title":"Helpful Resources"},{"location":"NVMe%20Performance%20Testing/#configuration","text":"","title":"Configuration"},{"location":"NVMe%20Performance%20Testing/#drives","text":"","title":"Drives"},{"location":"NVMe%20Performance%20Testing/#os","text":"[root@r8402 ~]# cat /etc/*-release NAME=\"Red Hat Enterprise Linux\" VERSION=\"8.2 (Ootpa)\" ID=\"rhel\" ID_LIKE=\"fedora\" VERSION_ID=\"8.2\" PLATFORM_ID=\"platform:el8\" PRETTY_NAME=\"Red Hat Enterprise Linux 8.2 (Ootpa)\" ANSI_COLOR=\"0;31\" CPE_NAME=\"cpe:/o:redhat:enterprise_linux:8.2:GA\" HOME_URL=\"https://www.redhat.com/\" BUG_REPORT_URL=\"https://bugzilla.redhat.com/\" REDHAT_BUGZILLA_PRODUCT=\"Red Hat Enterprise Linux 8\" REDHAT_BUGZILLA_PRODUCT_VERSION=8.2 REDHAT_SUPPORT_PRODUCT=\"Red Hat Enterprise Linux\" REDHAT_SUPPORT_PRODUCT_VERSION=\"8.2\" Red Hat Enterprise Linux release 8.2 (Ootpa) Red Hat Enterprise Linux release 8.2 (Ootpa)","title":"OS"},{"location":"NVMe%20Performance%20Testing/#tests","text":"","title":"Tests"},{"location":"NVMe%20Performance%20Testing/#test-1","text":"","title":"Test 1"},{"location":"NVMe%20Performance%20Testing/#config","text":"pvcreate /dev/nvme2n1 pvcreate /dev/nvme1n1 pvcreate /dev/nvme0n1 pvcreate /dev/nvme3n1 vgcreate data /dev/nvme3n1 /dev/nvme2n1 /dev/nvme1n1 /dev/nvme0n1 lvcreate -l 100%FREE -i4 -I128 -n data data mkfs.ext4 -F -b 4096 -E discard,stride=16,stripe-width=256 /dev/mapper/data-data mkdir /data mount -o rw,auto,discard /dev/mapper/data-data /data fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test --filename=test --bs=64k --iodepth=32 --size=50G --readwrite=randrw --rwmixread=60","title":"Config"},{"location":"NVMe%20Performance%20Testing/#results","text":"Test: (g=0): rw=randrw, bs=(R) 64.0KiB-64.0KiB, (W) 64.0KiB-64.0KiB, (T) 64.0KiB-64.0KiB, ioengine=libaio, iodepth=32 fio-3.7 Starting 1 process Test: Laying out IO file (1 file / 51200MiB) Jobs: 1 (f=1): [m(1)][100.0%][r=217MiB/s,w=148MiB/s][r=3476,w=2371 IOPS][eta 00m:00s] Test: (groupid=0, jobs=1): err= 0: pid=3290: Mon Sep 14 11:17:39 2020 read: IOPS=3639, BW=227MiB/s (239MB/s)(29.0GiB/134968msec) bw ( KiB/s): min=211328, max=261504, per=99.99%, avg=232911.32, stdev=8921.48, samples=269 iops : min= 3302, max= 4086, avg=3639.22, stdev=139.41, samples=269 write: IOPS=2429, BW=152MiB/s (159MB/s)(20.0GiB/134968msec) bw ( KiB/s): min=140800, max=169856, per=100.00%, avg=155506.13, stdev=5852.42, samples=269 iops : min= 2200, max= 2654, avg=2429.77, stdev=91.43, samples=269 cpu : usr=2.19%, sys=8.61%, ctx=98853, majf=0, minf=13 IO depths : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=100.0%, >=64=0.0% submit : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0% complete : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.1%, 64=0.0%, >=64=0.0% issued rwts: total=491242,327958,0,0 short=0,0,0,0 dropped=0,0,0,0 latency : target=0, window=0, percentile=100.00%, depth=32 Run status group 0 (all jobs): READ: bw=227MiB/s (239MB/s), 227MiB/s-227MiB/s (239MB/s-239MB/s), io=29.0GiB (32.2GB), run=134968-134968msec WRITE: bw=152MiB/s (159MB/s), 152MiB/s-152MiB/s (159MB/s-159MB/s), io=20.0GiB (21.5GB), run=134968-134968msec Disk stats (read/write): dm-0: ios=490775/327707, merge=0/0, ticks=3019139/1204501, in_queue=4223640, util=68.01%, aggrios=491243/328000, aggrmerge=1/11, aggrticks=3031345/1210547, aggrin_queue=3827355, aggrutil=67.98% sda: ios=491243/328000, merge=1/11, ticks=3031345/1210547, in_queue=3827355, util=67.98%","title":"Results"},{"location":"Notes%20on%20AMD%20Processor/","text":"Notes on AMD Processor Notes on AMD Processor Pictures of Physical Package Notes from Processor Programming Reference I/O Diagram One Socket I/O Two Socket I/O with 4 XGMI Links Two Socket I/O with 3 XGMI Links Core Complex (CCX) Diagram How is the iDRAC connected? Measuring Performance - Effective Frequency System Management Unit How to Change Processor Settings Run Commands on the Processor Pictures of Physical Package Link Notes from Processor Programming Reference See here for source. I/O Diagram One Socket I/O See page 32 Two Socket I/O with 4 XGMI Links See page 34 Two Socket I/O with 3 XGMI Links See page 35 Core Complex (CCX) Diagram See page 36 How is the iDRAC connected? See page 30 Each IOD (I/O die) has: Four instances of NorthBridge IO (NBIO), each of which includes: Two 8x16 PCIe\u00ae Gen4 controllers. One instance includes a 2x2 PCIe\u00ae Gen 2 controller, which can be used to attach a Baseband Management Controller (BMC) Measuring Performance - Effective Frequency When using something like iDRAC telemetry it can poll the effective frequency via the effective frequency interface. See page 47 Review on C vs P States The effective frequency interface allows software to discern the average, or effective, frequency of a given core over a configurable window of time. This provides software a measure of actual performance rather than forcing software to assume the current frequency of the core is the frequency of the last P-state requested System Management Unit There is a system management unit for each proc integrated onto the I/O die . How to Change Processor Settings There are three ways to change settings on the processor: AMD Host System Management Port (HSMP) Epyc System Management Interface (E-SMI) Library Technically, this rides on top of the HSMP but it is a separate userspace library Run Commands Directly on the Processor Run Commands on the Processor Let's say you want to disable the L2 Stream HW Prefetcher. This can be accomplished with the write MSR (Model Specific Register) Command . Model Specific Registers (MSRs) are: any of various control registers in the x86 instruction set used for debugging, program execution tracing, computer performance monitoring, and toggling certain CPU features. There is a good lecture available on the subject available here . If you want to control prefetch you would select the appropriate register and then bitmask 1 to bit 3 as described here:","title":"Notes on AMD Processor"},{"location":"Notes%20on%20AMD%20Processor/#notes-on-amd-processor","text":"Notes on AMD Processor Pictures of Physical Package Notes from Processor Programming Reference I/O Diagram One Socket I/O Two Socket I/O with 4 XGMI Links Two Socket I/O with 3 XGMI Links Core Complex (CCX) Diagram How is the iDRAC connected? Measuring Performance - Effective Frequency System Management Unit How to Change Processor Settings Run Commands on the Processor","title":"Notes on AMD Processor"},{"location":"Notes%20on%20AMD%20Processor/#pictures-of-physical-package","text":"Link","title":"Pictures of Physical Package"},{"location":"Notes%20on%20AMD%20Processor/#notes-from-processor-programming-reference","text":"See here for source.","title":"Notes from Processor Programming Reference"},{"location":"Notes%20on%20AMD%20Processor/#io-diagram","text":"","title":"I/O Diagram"},{"location":"Notes%20on%20AMD%20Processor/#one-socket-io","text":"See page 32","title":"One Socket I/O"},{"location":"Notes%20on%20AMD%20Processor/#two-socket-io-with-4-xgmi-links","text":"See page 34","title":"Two Socket I/O with 4 XGMI Links"},{"location":"Notes%20on%20AMD%20Processor/#two-socket-io-with-3-xgmi-links","text":"See page 35","title":"Two Socket I/O with 3 XGMI Links"},{"location":"Notes%20on%20AMD%20Processor/#core-complex-ccx-diagram","text":"See page 36","title":"Core Complex (CCX) Diagram"},{"location":"Notes%20on%20AMD%20Processor/#how-is-the-idrac-connected","text":"See page 30 Each IOD (I/O die) has: Four instances of NorthBridge IO (NBIO), each of which includes: Two 8x16 PCIe\u00ae Gen4 controllers. One instance includes a 2x2 PCIe\u00ae Gen 2 controller, which can be used to attach a Baseband Management Controller (BMC)","title":"How is the iDRAC connected?"},{"location":"Notes%20on%20AMD%20Processor/#measuring-performance-effective-frequency","text":"When using something like iDRAC telemetry it can poll the effective frequency via the effective frequency interface. See page 47 Review on C vs P States The effective frequency interface allows software to discern the average, or effective, frequency of a given core over a configurable window of time. This provides software a measure of actual performance rather than forcing software to assume the current frequency of the core is the frequency of the last P-state requested","title":"Measuring Performance - Effective Frequency"},{"location":"Notes%20on%20AMD%20Processor/#system-management-unit","text":"There is a system management unit for each proc integrated onto the I/O die .","title":"System Management Unit"},{"location":"Notes%20on%20AMD%20Processor/#how-to-change-processor-settings","text":"There are three ways to change settings on the processor: AMD Host System Management Port (HSMP) Epyc System Management Interface (E-SMI) Library Technically, this rides on top of the HSMP but it is a separate userspace library Run Commands Directly on the Processor","title":"How to Change Processor Settings"},{"location":"Notes%20on%20AMD%20Processor/#run-commands-on-the-processor","text":"Let's say you want to disable the L2 Stream HW Prefetcher. This can be accomplished with the write MSR (Model Specific Register) Command . Model Specific Registers (MSRs) are: any of various control registers in the x86 instruction set used for debugging, program execution tracing, computer performance monitoring, and toggling certain CPU features. There is a good lecture available on the subject available here . If you want to control prefetch you would select the appropriate register and then bitmask 1 to bit 3 as described here:","title":"Run Commands on the Processor"},{"location":"Notes%20on%20Building%20a%20Datacenter%20from%20Scratch/","text":"Notes on Building a Datacenter from Scratch Notes on Building a Datacenter from Scratch Power Rough Flow of Power Details on UPS Offline UPS Line Interactive Online Double conversion UPS** Online Delta conversion UPS** Rotary UPS Systems Cooling Cooling Types Room Cooling Row Cooling Rack Cooling Notes on Physics Heat Removal Techniques Humidity Cooling Strategies Aisle Arrangement Sealed Rooms Airflow Management CRAC vs CRAH Power Typically, the total power supplied to the Data Center should be two times or more than the total power required by the IT equipment (including future Loads). The other half will be consumed by the cooling and other facilities. Power is usually measured in Watts (W). Each piece of IT equipment has a specific Wattage, or Power Rating, specifying the amount of Power it consumes. The total sum of Power Ratings of all IT equipment running in the Data Center gives the total IT Load of the Data Center. IT Load, or Critical Load, is the total Power consumed by all critical IT equipment in the Data Center. Most manufacturer specified power requirements for IT equipment are usually over-stated. Actual consumption typically does not exceed 70% of stated value. A good practice is to de-rate stated values by 60 \u2013 70% before computing total IT Load Industry best practice is to allocate 4 -5 kW per rack. (TODO - still current in 2022?) Rough Flow of Power Utility Supply Generators Transfer Switches Distribution Panels Uninterruptible Power Supply (UPS) PDU Utility Supply is the power supply to the Data Center sourced from the public distribution grid. It is controlled by the government or public power distribution companies and is not considered a reliable source for powering the Data Center. They are however utilized to minimize the costs of providing power to the Data Center. Generators are machines used to generate electrical Power. They convert mechanical energy, usually from motors, to the electrical energy used to power the Data Center. They are the primary source of Power to the Data Center, since they are completely in the control of the Data Center Operators. Transfer Switches are electrical switches used to transfer electric Load from one power source to the other. The transfer could be from one utility line to another, from engine-generators to utility and vice versa, or between two generators. The transfer could be manually activated. It could also be automatic when Automatic Transfer Switches (ATS) or Static Transfer Switches (STS) are used. Distribution Panel as the name implies is an enclosure wherein a single electrical power feed is divided into separate subsidiary circuits for feeding multiple distinct Loads. The circuits may all be of equal or differing capacities. Each circuit or power feed is protected by a circuit breaker or an electrical fuse to prevent the end electrical Loads from over-drawing power beyond specified limits UPS means Uninterruptible Power Supply . The UPS is an electrical device that provides continuous power to a Load even when the mains power source is unavailable. It works by storing electrical energy in backup devices, such as batteries, from input power. The UPS then supplies the Load with the stored energy almost instantaneously when the input power is cut off. Another important function of the UPS is to clean out and stabilize the power from the mains supply. The power from the mains supply can be subject to fluctuations in form, voltage, and frequency owing to interference or generation conditions. The UPS has the ability to correct some of these anomalies. PDU means Power Distribution Unit. The PDU distributes power to the individual pieces of equipment. The PDUs come in various sizes and forms, with some being rack-mountable while others occupy Data Center whitespace. Details on UPS There are two types of UPS Systems: Static UPS and Rotary UPS Static UPS Systems are so named because they have no moving parts throughout the power flow. They typically store electricity in the form of chemical energy in batteries. This UPS system has three main components: The rectifier, which converts AC from the mains into DC The storage medium, which stores the converted DC. The most common form for storing electrical energy is through batteries[8] The inverter, which converts stored DC to AC for supply to the electrical load Offline UPS Here, the Load is powered directly from the mains when the mains input is present. The UPS switches the Load over to the battery when the mains input goes off. There is a noticeable time lag during the switching process. More so, the irregularities (if any) in the mains power are carried over to the Load. These make this configuration not conducive for sensitive critical Load. Line Interactive This configuration is similar to the Offline system. However, a voltage regulator is introduced after the mains just before the Load. The Voltage Regulator corrects some of the irregularities, but cannot correct frequency. There is still a noticeable time lag during the switching process. Critical Load is not to be powered with this system. Online Double conversion UPS** This configuration completely isolates the Load from the mains input. The Load is always fed from the DC Power. The AC power from the mains is converted to DC to keep the batteries charged. The inverter then converts the DC back to AC to supply the Load. This ensures that the supply to the Load is always clean and continuous, making it suitable to critical Data Center Loads. A bypass is included so that the Load can be switched manually switched over to the mains supply temporarily during maintenance operations. One drawback however is that this configuration is not very efficient due to losses accrued during the conversion processes. Online Delta conversion UPS** This is a variation of the Line Interactive system. It uses a Delta Converter in place of the Voltage Regulator. The Delta Conversion UPS allows a portion of the Load to be fed from the main, while the rest is fed from the Inverter. This allows the stabilization of the output voltage. It also ensures that there is no switching time lag if the main\u2019s input is cut, as the Inverter can seamlessly assume the rest of the Load Rotary UPS Systems Rotary UPS Systems are so named because they store electrical energy in the form of kinetic energy. The incoming main\u2019s supply drives a motor which in turn spins an electro-mechanical flywheel at a very high rate. The flywheel drives an electrical generator to provide power to the Load, while at the same time storing Kinetic Energy. Once power failure occurs, the Kinetic Energy in the flywheel is released to drive the generator, so that it continues to power the Load, providing a ride-through period within which the backup generator can be started. Some variations add batteries to the flywheel for energy storage. Another variation incorporates a Diesel Generator into the Rotary UPS System. Once the power outage exceeds a few seconds, the Diesel Generator is started to provide the input power. This precludes the need for an external backup generator. This system is known as a Diesel Rotary UPS (DRUPS) system. There has been an ongoing argument about which UPS system is best suited for Data Center functions. While Static UPS systems dominate the existing Data Center deployments, there is still much benefit a Rotary UPS system can offer. Manufacturers of both systems continue to advance arguments supporting their chosen lines. Rotary UPS systems are usually manufactured for higher power ranges (200kW and above). DRUPS systems are generally not found in capacities lower than 500kW. Rotary systems provide little ride-through time (about 15 seconds) compared to Static systems, which can backup power for up to 30 minutes. Also, Rotary systems require vigorous maintenance regimes, unlike Static systems, which most times just need routine cleaning. On the other hand, Rotary systems allow for massive savings in expensive Data Center space. They also have a considerable lifespan compared to Static UPS system components. In all cases, the choice of which UPS system to utilize will depend on what system suits the Business objective better. Cooling Cooling Types Room Cooling In this approach, cooling is provided for the room as a whole. This method can be suitable for small data centers, but decidedly become more cumbersome as the data center density increases. This is because the air conditioners have to constantly stir and mix the air in the room to prevent hot-spots and bring it to a common regular temperature. Row Cooling In this approach, cooling is provided on a row by row basis. This allows each row to run different load densities, so that differing cooling intensities can be applied as required. Hot-spot and cooling irregularities can be easily managed by proper layout and equipment placement. Rack Cooling In this approach, cooling is provided on a rack by rack basis. Specific air-conditioning units are dedicated to specific racks. This approach allows for maximum densities to be deployed per rack. However, this advantage can only be realized in data centers with fully loaded racks, otherwise, there would be too much cooling capacity, and air-conditioning losses alone can exceed total IT load. Notes on Physics Heat Removal Techniques From Physics, we learn that heat can only flow in one direction \u2013 from hot to cold. We also learn heat can be transferred either by Conduction, Convection, or Radiation. Conduction is the transfer of heat through a solid material, known as a conductor. Convection is the transfer of heat through the movement of a liquid or gas. Radiation is the transfer of heat by means of electromagnetic waves, emitted due to the temperature difference between two objects. Convection is the method used to transfer heat away from the data center. This transfer is done through a process known as the Refrigeration Cycle. The Refrigeration Cycle is a cycle of Evaporation, Compression, Condensation, and Expansion of a fluid or gas. This fluid or gas is known as the Refrigerant. The Refrigeration Cycle effectively transfers heat away from the data center into the external environment. Through the different stages of the refrigeration cycle, the refrigerant\u2019s physical state oscillates between liquid and gas. Evaporation will absorb heat from the data center environment and turn the refrigerant into a gas. This heat in the gaseous refrigerant is channeled to the compressor Compression will apply pressure to the gaseous refrigerant, making it absorb much more heat thereby causing its internal temperature to rise. This hot gaseous refrigerant is channeled to the condenser in the next phase of eliminating data center heat. Condensation will pass heat from the high-temperature high-pressure gas to the outside air. As heat flows from the hot region to the cold region, outside air is directed to the condensation coil. The refrigerant flowing through the coil then transfers heat to the outside air, which is then channeled out to the outdoor environment. The refrigerant then becomes a hot, high-pressure liquid. Expansion will reduce the pressure in the refrigerant thereby reducing the temperature. This ends the cycle with the refrigerant returning to a cold liquid. The cycle is then restarted. Another method of removing heat from the data center is via Chilled Water Systems. This method, while more efficient and cost-effective than direct expansion systems using the refrigeration cycle, is much more complex. It utilizes fans and cooling coils to remove heat from the data center via chilled water. Humidity Like temperature, humidity[12] is also an important environmental factor in the data center. Regulating humidity is critical. Too low humidity levels affect the incidence of static electricity, which is an electric charge at rest. This electric charge can lead to an Electrostatic Discharge (ESD), which could cause significant damage to IT equipment. Too high humidity can cause water condensation on IT equipment, which could lead to water dropping on the chips in equipment, resulting in a current short-circuit. In measuring humidity, the terms Relative Humidity, Dew Point, and Saturation are used. Relative humidity is the amount of water vapor in the air as a percentage of the maximum amount of water vapor the air can hold at a given temperature. It follows that relative humidity can vary as the air temperature changes. e.g. at a higher temperature, air will expand causing it to be able to hold more water, thus relative humidity becomes lower. The reverse is the case at lower temperatures. ASHRAE recommends maximum relative humidity levels of 60%. Dew point is the exact temperature where relative humidity becomes 100%. At this point, the water vapor leaves the air and appears as liquid water droplets on any object in the data center. ASHRAE recommends a maximum of 15.50C dew point. The air is said to be \u201csaturated\u201d at this temperature. Humidity in the data center is regulated using Precision Cooling units, which regulate temperature and water vapor levels in the environment. Humidification/Dehumidification systems are used. These produce/reduce water vapor in the atmosphere to the desired quantities. In addition to humidity regulation equipment, the following practices should be followed to restrict fluctuation in humidity levels: - Reduce the frequency of entry/exit into/from the data center. Constant opening of data center entrances can lead to infiltration of warmer air from outside, which could destabilize the environment - Seal perimeter infiltrations and entrance points that lead to uncontrolled environments - Seal doorways to guard against air and vapor leaks - Paint perimeter walls to prevent the penetration of moisture - Avoid unnecessary openings Cooling Strategies Aisle Arrangement As discussed in an earlier section, it is advantageous to provide cooling on a row-by-row basis. Taking this further, many standard bodies, including TIA and ASHRAE, recommend arranging racks and cabinets in a hot-aisle/cold-aisle alignment. In this arrangement, the racks in each row are arranged such that the front and backs of adjacent rows face each other. This leads to a repeatable sequence after every 7 tiles, known as the 7 pitch tile rule. When cold air is channeled to the front of the racks in this arrangement, the aisle between racks\u2019 fronts facing each other is distinctly cold (cold aisle), while that at racks\u2019 backs is distinctly hot (hot aisle). The cooling unit is placed at the hot aisle. Proper distance should be maintained between the air cooling unit and the equipment, typically between 2.5m (8ft) and 10m (32ft). Sealed Rooms The data center should be completely sealed to prevent the escape of cold air to the external environment. This could progressively worsen the energy situation as more and more energy is consumed to adequately cool the server room space only for the cold air to escape to other areas Airflow Management Open spaces in the cabinets should be minimized as much as possible. This can be done by placing blanking plates in unused rack unit spaces, blocking of cabinet sides, and making use of perforated front and back doors. In-Row Units - Can be used to cool by sucking in the hot air in the hot aisle. CRAC vs CRAH CRAH is an acronym for Computer Room Air Handler, while CRAC is an acronym for Computer Room Air Conditioner. CRAHs are indoor cooling units that do not have their own compressors. They are typically components of a Chilled Water-based cooling system. CRACs on the other hand have their own self-contained compressor. They are a staple of Direct Expansion based cooling systems.","title":"Notes on Building a Datacenter from Scratch"},{"location":"Notes%20on%20Building%20a%20Datacenter%20from%20Scratch/#notes-on-building-a-datacenter-from-scratch","text":"Notes on Building a Datacenter from Scratch Power Rough Flow of Power Details on UPS Offline UPS Line Interactive Online Double conversion UPS** Online Delta conversion UPS** Rotary UPS Systems Cooling Cooling Types Room Cooling Row Cooling Rack Cooling Notes on Physics Heat Removal Techniques Humidity Cooling Strategies Aisle Arrangement Sealed Rooms Airflow Management CRAC vs CRAH","title":"Notes on Building a Datacenter from Scratch"},{"location":"Notes%20on%20Building%20a%20Datacenter%20from%20Scratch/#power","text":"Typically, the total power supplied to the Data Center should be two times or more than the total power required by the IT equipment (including future Loads). The other half will be consumed by the cooling and other facilities. Power is usually measured in Watts (W). Each piece of IT equipment has a specific Wattage, or Power Rating, specifying the amount of Power it consumes. The total sum of Power Ratings of all IT equipment running in the Data Center gives the total IT Load of the Data Center. IT Load, or Critical Load, is the total Power consumed by all critical IT equipment in the Data Center. Most manufacturer specified power requirements for IT equipment are usually over-stated. Actual consumption typically does not exceed 70% of stated value. A good practice is to de-rate stated values by 60 \u2013 70% before computing total IT Load Industry best practice is to allocate 4 -5 kW per rack. (TODO - still current in 2022?)","title":"Power"},{"location":"Notes%20on%20Building%20a%20Datacenter%20from%20Scratch/#rough-flow-of-power","text":"Utility Supply Generators Transfer Switches Distribution Panels Uninterruptible Power Supply (UPS) PDU Utility Supply is the power supply to the Data Center sourced from the public distribution grid. It is controlled by the government or public power distribution companies and is not considered a reliable source for powering the Data Center. They are however utilized to minimize the costs of providing power to the Data Center. Generators are machines used to generate electrical Power. They convert mechanical energy, usually from motors, to the electrical energy used to power the Data Center. They are the primary source of Power to the Data Center, since they are completely in the control of the Data Center Operators. Transfer Switches are electrical switches used to transfer electric Load from one power source to the other. The transfer could be from one utility line to another, from engine-generators to utility and vice versa, or between two generators. The transfer could be manually activated. It could also be automatic when Automatic Transfer Switches (ATS) or Static Transfer Switches (STS) are used. Distribution Panel as the name implies is an enclosure wherein a single electrical power feed is divided into separate subsidiary circuits for feeding multiple distinct Loads. The circuits may all be of equal or differing capacities. Each circuit or power feed is protected by a circuit breaker or an electrical fuse to prevent the end electrical Loads from over-drawing power beyond specified limits UPS means Uninterruptible Power Supply . The UPS is an electrical device that provides continuous power to a Load even when the mains power source is unavailable. It works by storing electrical energy in backup devices, such as batteries, from input power. The UPS then supplies the Load with the stored energy almost instantaneously when the input power is cut off. Another important function of the UPS is to clean out and stabilize the power from the mains supply. The power from the mains supply can be subject to fluctuations in form, voltage, and frequency owing to interference or generation conditions. The UPS has the ability to correct some of these anomalies. PDU means Power Distribution Unit. The PDU distributes power to the individual pieces of equipment. The PDUs come in various sizes and forms, with some being rack-mountable while others occupy Data Center whitespace.","title":"Rough Flow of Power"},{"location":"Notes%20on%20Building%20a%20Datacenter%20from%20Scratch/#details-on-ups","text":"There are two types of UPS Systems: Static UPS and Rotary UPS Static UPS Systems are so named because they have no moving parts throughout the power flow. They typically store electricity in the form of chemical energy in batteries. This UPS system has three main components: The rectifier, which converts AC from the mains into DC The storage medium, which stores the converted DC. The most common form for storing electrical energy is through batteries[8] The inverter, which converts stored DC to AC for supply to the electrical load","title":"Details on UPS"},{"location":"Notes%20on%20Building%20a%20Datacenter%20from%20Scratch/#offline-ups","text":"Here, the Load is powered directly from the mains when the mains input is present. The UPS switches the Load over to the battery when the mains input goes off. There is a noticeable time lag during the switching process. More so, the irregularities (if any) in the mains power are carried over to the Load. These make this configuration not conducive for sensitive critical Load.","title":"Offline UPS"},{"location":"Notes%20on%20Building%20a%20Datacenter%20from%20Scratch/#line-interactive","text":"This configuration is similar to the Offline system. However, a voltage regulator is introduced after the mains just before the Load. The Voltage Regulator corrects some of the irregularities, but cannot correct frequency. There is still a noticeable time lag during the switching process. Critical Load is not to be powered with this system.","title":"Line Interactive"},{"location":"Notes%20on%20Building%20a%20Datacenter%20from%20Scratch/#online-double-conversion-ups","text":"This configuration completely isolates the Load from the mains input. The Load is always fed from the DC Power. The AC power from the mains is converted to DC to keep the batteries charged. The inverter then converts the DC back to AC to supply the Load. This ensures that the supply to the Load is always clean and continuous, making it suitable to critical Data Center Loads. A bypass is included so that the Load can be switched manually switched over to the mains supply temporarily during maintenance operations. One drawback however is that this configuration is not very efficient due to losses accrued during the conversion processes.","title":"Online Double conversion UPS**"},{"location":"Notes%20on%20Building%20a%20Datacenter%20from%20Scratch/#online-delta-conversion-ups","text":"This is a variation of the Line Interactive system. It uses a Delta Converter in place of the Voltage Regulator. The Delta Conversion UPS allows a portion of the Load to be fed from the main, while the rest is fed from the Inverter. This allows the stabilization of the output voltage. It also ensures that there is no switching time lag if the main\u2019s input is cut, as the Inverter can seamlessly assume the rest of the Load","title":"Online Delta conversion UPS**"},{"location":"Notes%20on%20Building%20a%20Datacenter%20from%20Scratch/#rotary-ups-systems","text":"Rotary UPS Systems are so named because they store electrical energy in the form of kinetic energy. The incoming main\u2019s supply drives a motor which in turn spins an electro-mechanical flywheel at a very high rate. The flywheel drives an electrical generator to provide power to the Load, while at the same time storing Kinetic Energy. Once power failure occurs, the Kinetic Energy in the flywheel is released to drive the generator, so that it continues to power the Load, providing a ride-through period within which the backup generator can be started. Some variations add batteries to the flywheel for energy storage. Another variation incorporates a Diesel Generator into the Rotary UPS System. Once the power outage exceeds a few seconds, the Diesel Generator is started to provide the input power. This precludes the need for an external backup generator. This system is known as a Diesel Rotary UPS (DRUPS) system. There has been an ongoing argument about which UPS system is best suited for Data Center functions. While Static UPS systems dominate the existing Data Center deployments, there is still much benefit a Rotary UPS system can offer. Manufacturers of both systems continue to advance arguments supporting their chosen lines. Rotary UPS systems are usually manufactured for higher power ranges (200kW and above). DRUPS systems are generally not found in capacities lower than 500kW. Rotary systems provide little ride-through time (about 15 seconds) compared to Static systems, which can backup power for up to 30 minutes. Also, Rotary systems require vigorous maintenance regimes, unlike Static systems, which most times just need routine cleaning. On the other hand, Rotary systems allow for massive savings in expensive Data Center space. They also have a considerable lifespan compared to Static UPS system components. In all cases, the choice of which UPS system to utilize will depend on what system suits the Business objective better.","title":"Rotary UPS Systems"},{"location":"Notes%20on%20Building%20a%20Datacenter%20from%20Scratch/#cooling","text":"","title":"Cooling"},{"location":"Notes%20on%20Building%20a%20Datacenter%20from%20Scratch/#cooling-types","text":"","title":"Cooling Types"},{"location":"Notes%20on%20Building%20a%20Datacenter%20from%20Scratch/#room-cooling","text":"In this approach, cooling is provided for the room as a whole. This method can be suitable for small data centers, but decidedly become more cumbersome as the data center density increases. This is because the air conditioners have to constantly stir and mix the air in the room to prevent hot-spots and bring it to a common regular temperature.","title":"Room Cooling"},{"location":"Notes%20on%20Building%20a%20Datacenter%20from%20Scratch/#row-cooling","text":"In this approach, cooling is provided on a row by row basis. This allows each row to run different load densities, so that differing cooling intensities can be applied as required. Hot-spot and cooling irregularities can be easily managed by proper layout and equipment placement.","title":"Row Cooling"},{"location":"Notes%20on%20Building%20a%20Datacenter%20from%20Scratch/#rack-cooling","text":"In this approach, cooling is provided on a rack by rack basis. Specific air-conditioning units are dedicated to specific racks. This approach allows for maximum densities to be deployed per rack. However, this advantage can only be realized in data centers with fully loaded racks, otherwise, there would be too much cooling capacity, and air-conditioning losses alone can exceed total IT load.","title":"Rack Cooling"},{"location":"Notes%20on%20Building%20a%20Datacenter%20from%20Scratch/#notes-on-physics","text":"","title":"Notes on Physics"},{"location":"Notes%20on%20Building%20a%20Datacenter%20from%20Scratch/#heat-removal-techniques","text":"From Physics, we learn that heat can only flow in one direction \u2013 from hot to cold. We also learn heat can be transferred either by Conduction, Convection, or Radiation. Conduction is the transfer of heat through a solid material, known as a conductor. Convection is the transfer of heat through the movement of a liquid or gas. Radiation is the transfer of heat by means of electromagnetic waves, emitted due to the temperature difference between two objects. Convection is the method used to transfer heat away from the data center. This transfer is done through a process known as the Refrigeration Cycle. The Refrigeration Cycle is a cycle of Evaporation, Compression, Condensation, and Expansion of a fluid or gas. This fluid or gas is known as the Refrigerant. The Refrigeration Cycle effectively transfers heat away from the data center into the external environment. Through the different stages of the refrigeration cycle, the refrigerant\u2019s physical state oscillates between liquid and gas. Evaporation will absorb heat from the data center environment and turn the refrigerant into a gas. This heat in the gaseous refrigerant is channeled to the compressor Compression will apply pressure to the gaseous refrigerant, making it absorb much more heat thereby causing its internal temperature to rise. This hot gaseous refrigerant is channeled to the condenser in the next phase of eliminating data center heat. Condensation will pass heat from the high-temperature high-pressure gas to the outside air. As heat flows from the hot region to the cold region, outside air is directed to the condensation coil. The refrigerant flowing through the coil then transfers heat to the outside air, which is then channeled out to the outdoor environment. The refrigerant then becomes a hot, high-pressure liquid. Expansion will reduce the pressure in the refrigerant thereby reducing the temperature. This ends the cycle with the refrigerant returning to a cold liquid. The cycle is then restarted. Another method of removing heat from the data center is via Chilled Water Systems. This method, while more efficient and cost-effective than direct expansion systems using the refrigeration cycle, is much more complex. It utilizes fans and cooling coils to remove heat from the data center via chilled water.","title":"Heat Removal Techniques"},{"location":"Notes%20on%20Building%20a%20Datacenter%20from%20Scratch/#humidity","text":"Like temperature, humidity[12] is also an important environmental factor in the data center. Regulating humidity is critical. Too low humidity levels affect the incidence of static electricity, which is an electric charge at rest. This electric charge can lead to an Electrostatic Discharge (ESD), which could cause significant damage to IT equipment. Too high humidity can cause water condensation on IT equipment, which could lead to water dropping on the chips in equipment, resulting in a current short-circuit. In measuring humidity, the terms Relative Humidity, Dew Point, and Saturation are used. Relative humidity is the amount of water vapor in the air as a percentage of the maximum amount of water vapor the air can hold at a given temperature. It follows that relative humidity can vary as the air temperature changes. e.g. at a higher temperature, air will expand causing it to be able to hold more water, thus relative humidity becomes lower. The reverse is the case at lower temperatures. ASHRAE recommends maximum relative humidity levels of 60%. Dew point is the exact temperature where relative humidity becomes 100%. At this point, the water vapor leaves the air and appears as liquid water droplets on any object in the data center. ASHRAE recommends a maximum of 15.50C dew point. The air is said to be \u201csaturated\u201d at this temperature. Humidity in the data center is regulated using Precision Cooling units, which regulate temperature and water vapor levels in the environment. Humidification/Dehumidification systems are used. These produce/reduce water vapor in the atmosphere to the desired quantities. In addition to humidity regulation equipment, the following practices should be followed to restrict fluctuation in humidity levels: - Reduce the frequency of entry/exit into/from the data center. Constant opening of data center entrances can lead to infiltration of warmer air from outside, which could destabilize the environment - Seal perimeter infiltrations and entrance points that lead to uncontrolled environments - Seal doorways to guard against air and vapor leaks - Paint perimeter walls to prevent the penetration of moisture - Avoid unnecessary openings","title":"Humidity"},{"location":"Notes%20on%20Building%20a%20Datacenter%20from%20Scratch/#cooling-strategies","text":"","title":"Cooling Strategies"},{"location":"Notes%20on%20Building%20a%20Datacenter%20from%20Scratch/#aisle-arrangement","text":"As discussed in an earlier section, it is advantageous to provide cooling on a row-by-row basis. Taking this further, many standard bodies, including TIA and ASHRAE, recommend arranging racks and cabinets in a hot-aisle/cold-aisle alignment. In this arrangement, the racks in each row are arranged such that the front and backs of adjacent rows face each other. This leads to a repeatable sequence after every 7 tiles, known as the 7 pitch tile rule. When cold air is channeled to the front of the racks in this arrangement, the aisle between racks\u2019 fronts facing each other is distinctly cold (cold aisle), while that at racks\u2019 backs is distinctly hot (hot aisle). The cooling unit is placed at the hot aisle. Proper distance should be maintained between the air cooling unit and the equipment, typically between 2.5m (8ft) and 10m (32ft).","title":"Aisle Arrangement"},{"location":"Notes%20on%20Building%20a%20Datacenter%20from%20Scratch/#sealed-rooms","text":"The data center should be completely sealed to prevent the escape of cold air to the external environment. This could progressively worsen the energy situation as more and more energy is consumed to adequately cool the server room space only for the cold air to escape to other areas","title":"Sealed Rooms"},{"location":"Notes%20on%20Building%20a%20Datacenter%20from%20Scratch/#airflow-management","text":"Open spaces in the cabinets should be minimized as much as possible. This can be done by placing blanking plates in unused rack unit spaces, blocking of cabinet sides, and making use of perforated front and back doors. In-Row Units - Can be used to cool by sucking in the hot air in the hot aisle.","title":"Airflow Management"},{"location":"Notes%20on%20Building%20a%20Datacenter%20from%20Scratch/#crac-vs-crah","text":"CRAH is an acronym for Computer Room Air Handler, while CRAC is an acronym for Computer Room Air Conditioner. CRAHs are indoor cooling units that do not have their own compressors. They are typically components of a Chilled Water-based cooling system. CRACs on the other hand have their own self-contained compressor. They are a staple of Direct Expansion based cooling systems.","title":"CRAC vs CRAH"},{"location":"Notes%20on%20Building%20a%20Datacenter%20from%20Scratch/README_temp/","text":"Notes on Building a Datacenter from Scratch","title":"Notes on Building a Datacenter from Scratch"},{"location":"Notes%20on%20Building%20a%20Datacenter%20from%20Scratch/README_temp/#notes-on-building-a-datacenter-from-scratch","text":"","title":"Notes on Building a Datacenter from Scratch"},{"location":"Notes%20on%20HPC/","text":"Notes on HPC KEY TAKEAWAY Economics will drive the pooling of main memory, and whether or not customers choose the CXL way or the Gen-Z way. Considering that memory can account for half of the cost of a server at a hyperscaler, anything that allows a machine to have a minimal amount of capacity on the node and then share the rest in the rack with all of it being transparent to the operating system and all of it looking local will be adopted. There is just no question about that. Memory area networks, in one fashion or another, are going to be common in datacenters before too long, and this will be driven by economics. Load Store Architecture A load\u2013store architecture is an instruction set architecture that divides instructions into two categories: memory access (load and store between memory and registers) and ALU operations (which only occur between registers). For instance, in a load\u2013store approach both operands and destination for an ADD operation must be in registers. This differs from a register-memory architecture (for example, a CISC instruction set architecture such as x86) in which one of the operands for the ADD operation may be in memory, while the other is in a register. https://www.sciencedirect.com/topics/computer-science/load-store-architecture https://en.wikipedia.org/wiki/Load%E2%80%93store_architecture Fabric Attached Memory See Fabric Attached Memory CXL See Compute Express Link Radix https://github.com/HewlettPackard/meadowlark https://ieeexplore.ieee.org/document/6307777 Things to investigate https://www.techpowerup.com/292256/amd-details-its-3d-v-cache-design-at-isscc","title":"Notes on HPC"},{"location":"Notes%20on%20HPC/#notes-on-hpc","text":"KEY TAKEAWAY Economics will drive the pooling of main memory, and whether or not customers choose the CXL way or the Gen-Z way. Considering that memory can account for half of the cost of a server at a hyperscaler, anything that allows a machine to have a minimal amount of capacity on the node and then share the rest in the rack with all of it being transparent to the operating system and all of it looking local will be adopted. There is just no question about that. Memory area networks, in one fashion or another, are going to be common in datacenters before too long, and this will be driven by economics.","title":"Notes on HPC"},{"location":"Notes%20on%20HPC/#load-store-architecture","text":"A load\u2013store architecture is an instruction set architecture that divides instructions into two categories: memory access (load and store between memory and registers) and ALU operations (which only occur between registers). For instance, in a load\u2013store approach both operands and destination for an ADD operation must be in registers. This differs from a register-memory architecture (for example, a CISC instruction set architecture such as x86) in which one of the operands for the ADD operation may be in memory, while the other is in a register. https://www.sciencedirect.com/topics/computer-science/load-store-architecture https://en.wikipedia.org/wiki/Load%E2%80%93store_architecture","title":"Load Store Architecture"},{"location":"Notes%20on%20HPC/#fabric-attached-memory","text":"See Fabric Attached Memory","title":"Fabric Attached Memory"},{"location":"Notes%20on%20HPC/#cxl","text":"See Compute Express Link","title":"CXL"},{"location":"Notes%20on%20HPC/#radix","text":"https://github.com/HewlettPackard/meadowlark https://ieeexplore.ieee.org/document/6307777","title":"Radix"},{"location":"Notes%20on%20HPC/#things-to-investigate","text":"https://www.techpowerup.com/292256/amd-details-its-3d-v-cache-design-at-isscc","title":"Things to investigate"},{"location":"Notes%20on%20HPC/chiplet_based_systems/","text":"Chiplet-Based Systems Return to HPC Notes README.md What is the problem Moore's Low is slowing Manufacturing costs are rising You could make larger chips to increase performance but: They are more expensive to make Verification costs are higher Manufacturing defects in densely packed logic reduce wafer yield You could create specialized chips but it is difficult to make a financial case for that What are chiplet-based systems Chiplet-based systems propose the integration of multiple discrete chips within the same package via an integration technology such as a multi-chip module or silicon interposer. Ex: From: https://www.sigarch.org/chiplet-based-systems/ Why chiplets Cost Previously, chiplets weren't considered practical because you introduce multiple parts plus having to contend with on-dye communication between them. However, now the smaller chips have sufficiently cheap manufacturing costs (compared to larger) that this is a viable alternative. Flexibility If you want to move from mobile, to desktop, to server this may be a matter of just increasing the number of chiplets. It is also possible that if we develop the proper standards that we could have a future system where we have general interconnects which would allow multiple different vendor chiplets to work together. Ex: Common Heterogeneous Integration and IP Reuse Strategies (CHIPS) Questions What is process technology? A process technology is the process of creating a single chip. In this case, you could use older process technologies to create the chiplets and then put them together.","title":"Chiplet-Based Systems"},{"location":"Notes%20on%20HPC/chiplet_based_systems/#chiplet-based-systems","text":"Return to HPC Notes README.md","title":"Chiplet-Based Systems"},{"location":"Notes%20on%20HPC/chiplet_based_systems/#what-is-the-problem","text":"Moore's Low is slowing Manufacturing costs are rising You could make larger chips to increase performance but: They are more expensive to make Verification costs are higher Manufacturing defects in densely packed logic reduce wafer yield You could create specialized chips but it is difficult to make a financial case for that","title":"What is the problem"},{"location":"Notes%20on%20HPC/chiplet_based_systems/#what-are-chiplet-based-systems","text":"Chiplet-based systems propose the integration of multiple discrete chips within the same package via an integration technology such as a multi-chip module or silicon interposer. Ex: From: https://www.sigarch.org/chiplet-based-systems/","title":"What are chiplet-based systems"},{"location":"Notes%20on%20HPC/chiplet_based_systems/#why-chiplets","text":"","title":"Why chiplets"},{"location":"Notes%20on%20HPC/chiplet_based_systems/#cost","text":"Previously, chiplets weren't considered practical because you introduce multiple parts plus having to contend with on-dye communication between them. However, now the smaller chips have sufficiently cheap manufacturing costs (compared to larger) that this is a viable alternative.","title":"Cost"},{"location":"Notes%20on%20HPC/chiplet_based_systems/#flexibility","text":"If you want to move from mobile, to desktop, to server this may be a matter of just increasing the number of chiplets. It is also possible that if we develop the proper standards that we could have a future system where we have general interconnects which would allow multiple different vendor chiplets to work together. Ex: Common Heterogeneous Integration and IP Reuse Strategies (CHIPS)","title":"Flexibility"},{"location":"Notes%20on%20HPC/chiplet_based_systems/#questions","text":"","title":"Questions"},{"location":"Notes%20on%20HPC/chiplet_based_systems/#what-is-process-technology","text":"A process technology is the process of creating a single chip. In this case, you could use older process technologies to create the chiplets and then put them together.","title":"What is process technology?"},{"location":"Notes%20on%20HPC/cxl/","text":"Compute Express Link (CXL) Return to HPC Notes README.md Compute Express Link (CXL) Useful Resources Examples of Vendor Interconnects More Specific Interconnects What is the Purpose of CXL? Protocols Devices Type 1 Device Type 2 Device Type 3 Device Memory Pooling Switching Useful Resources Interesting article on roadmap: https://www.nextplatform.com/2021/09/07/the-cxl-roadmap-opens-up-the-memory-hierarchy/ Deep Dive: https://www.nextplatform.com/2019/09/18/eating-the-interconnect-alphabet-soup-with-intels-cxl/ Examples of Vendor Interconnects Intel's Compute Express Link (CXL) IBM's Coherent Accelerator Interface (CAPI) Xilinx's Cache Coherence Interconnect for Accelerators (CCIX) AMD's Infinity Fabric More Specific Interconnects Nvidia's NVLink IBM's OpenCAPI HPE's Gen-Z: It can be used to hook anything from DRAM to flash to accelerators in meshes with any manner of CPU. What is the Purpose of CXL? The primary purpose is to disaggregate I/O and memory and to effectively virtualize the motherboard to make the following malleable across clusters of components: - Compute, memory, and I/O - Computational offload to devices such as GPU and FPGA accelerators - Memory buffers and other kinds of devices such as SmartNICs CXL is a set of sub-protocols that ride on the PCI-Express bus on a single link Protocols CXL.io: This sub-protocol is functionally equivalent to the PCIe 5.0 protocol and utilizes the broad industry adoption and familiarity of PCIe. It is effectively the PCIe transaction layer reformatted to allow two sub-protocols to co-exist side by side. CXL.io is used to discover devices in systems, manage interrupts, give access to registers, handle initialization, and deal with signaling errors. CXL.cache: This sub-protocol, which is designed for more specific applications, enables accelerators to efficiently access and cache host memory for optimized performance. It allows an accelerator to access the CPU's DRAM. CXL.memory: This sub-protocol enables a host, such as a processor, to access device-attached memory using load/store commands. It is not expected that all three protocols are used in all configurations. There are three basic usage templates which represent the three usages expected: From https://www.servethehome.com/compute-express-link-cxl-2-0-specification-released-the-big-one/cxl-1-0-and-1-1-usages/ Devices Type 1 Device Accelerators such as smart NICs typically lack local memory. However, they can leverage the CXL.io protocol and CXL.cache to communicate with the host processors DDR memory. Type 2 Device The idea here is there is memory (like HBM or DDR) on the accelerator (GPUs, ASICs, FPGAs, etc) and you want the accelerator's memory to be locally available to the CPU and the CPU's memory to be locally available to the accelerator. The CXL.io protocol is used to allow the CPU to discover the device and configure it and then you use the CXL.cache to allow the processor to touch the device\u2019s memory and CXL.memory to allow the accelerator to touch the CPU's memory. This memory should be co-located in the same cache coherent domain. Type 3 Device The CXL.io and CXL.memory protocols can be leveraged for memory expansion and pooling. For example, a buffer attached to the CXL bus could be used to enable DRAM capacity expansion, augmenting memory bandwidth, or adding persistent memory without the loss of DRAM slots. In real world terms, this means the high-speed, low-latency storage devices that would have previously displaced DRAM can instead complement it with CXL-enabled devices. These could include non-volatile technologies in various form factors such as add-in cards, U.2, and EDSFF. For type 3 devices you need the CXL.io sub-protocol to discover and configure the device and the CXL.memory sub-protocol to allow the CPU to reach into the memory attached to your memory buffer. WHERE I LEFT OFF : Left off studying symmetric cache coherency protocols vs asymmetric on this article: https://www.nextplatform.com/2019/09/18/eating-the-interconnect-alphabet-soup-with-intels-cxl/ Memory Pooling CXL 2.0 supports switching to enable memory pooling. With a CXL 2.0 switch, a host can access one or more devices from the pool. Although the hosts must be CXL 2.0-enabled to leverage this capability, the memory devices can be a mix of CXL 1.0, 1.1, and 2.0-enabled hardware. At 1.0/1.1, a device is limited to behaving as a single logical device accessible by only one host at a time. However, a 2.0 level device can be partitioned as multiple logical devices, allowing up to 16 hosts to simultaneously access different portions of the memory. As an example, a host 1 (H1) can use half the memory in device 1 (D1) and a quarter of the memory in device 2 (D2) to finely match the memory requirements of its workload to the available capacity in the memory pool. The remaining capacity in devices D1 and D2 can be used by one or more of the other hosts up to a maximum of 16. Devices D3 and D4, CXL 1.0 and 1.1-enabled respectively, can be used by only one host at a time. Switching By moving to a CXL 2.0 direct-connect architecture, data centers can achieve the performance benefits of main memory expansion\ufffdand the efficiency and total cost of ownership (TCO) benefits of pooled memory. Assuming all hosts and devices are CXL 2.0-enabled, \ufffdswitching is incorporated into the memory devices via a crossbar in the CXL memory pooling chip. This keeps latency low but requires a more powerful chip since it is now responsible for the control plane functionality performed by the switch. With low-latency direct connections, attached memory devices can employ DDR DRAM to provide expansion of host main memory. This can be done on a very flexible basis, as a host is able to access all\ufffdor portions of\ufffdthe capacity of as many devices as needed to tackle a specific workload.","title":"Compute Express Link (CXL)"},{"location":"Notes%20on%20HPC/cxl/#compute-express-link-cxl","text":"Return to HPC Notes README.md Compute Express Link (CXL) Useful Resources Examples of Vendor Interconnects More Specific Interconnects What is the Purpose of CXL? Protocols Devices Type 1 Device Type 2 Device Type 3 Device Memory Pooling Switching","title":"Compute Express Link (CXL)"},{"location":"Notes%20on%20HPC/cxl/#useful-resources","text":"Interesting article on roadmap: https://www.nextplatform.com/2021/09/07/the-cxl-roadmap-opens-up-the-memory-hierarchy/ Deep Dive: https://www.nextplatform.com/2019/09/18/eating-the-interconnect-alphabet-soup-with-intels-cxl/","title":"Useful Resources"},{"location":"Notes%20on%20HPC/cxl/#examples-of-vendor-interconnects","text":"Intel's Compute Express Link (CXL) IBM's Coherent Accelerator Interface (CAPI) Xilinx's Cache Coherence Interconnect for Accelerators (CCIX) AMD's Infinity Fabric","title":"Examples of Vendor Interconnects"},{"location":"Notes%20on%20HPC/cxl/#more-specific-interconnects","text":"Nvidia's NVLink IBM's OpenCAPI HPE's Gen-Z: It can be used to hook anything from DRAM to flash to accelerators in meshes with any manner of CPU.","title":"More Specific Interconnects"},{"location":"Notes%20on%20HPC/cxl/#what-is-the-purpose-of-cxl","text":"The primary purpose is to disaggregate I/O and memory and to effectively virtualize the motherboard to make the following malleable across clusters of components: - Compute, memory, and I/O - Computational offload to devices such as GPU and FPGA accelerators - Memory buffers and other kinds of devices such as SmartNICs CXL is a set of sub-protocols that ride on the PCI-Express bus on a single link","title":"What is the Purpose of CXL?"},{"location":"Notes%20on%20HPC/cxl/#protocols","text":"CXL.io: This sub-protocol is functionally equivalent to the PCIe 5.0 protocol and utilizes the broad industry adoption and familiarity of PCIe. It is effectively the PCIe transaction layer reformatted to allow two sub-protocols to co-exist side by side. CXL.io is used to discover devices in systems, manage interrupts, give access to registers, handle initialization, and deal with signaling errors. CXL.cache: This sub-protocol, which is designed for more specific applications, enables accelerators to efficiently access and cache host memory for optimized performance. It allows an accelerator to access the CPU's DRAM. CXL.memory: This sub-protocol enables a host, such as a processor, to access device-attached memory using load/store commands. It is not expected that all three protocols are used in all configurations. There are three basic usage templates which represent the three usages expected: From https://www.servethehome.com/compute-express-link-cxl-2-0-specification-released-the-big-one/cxl-1-0-and-1-1-usages/","title":"Protocols"},{"location":"Notes%20on%20HPC/cxl/#devices","text":"","title":"Devices"},{"location":"Notes%20on%20HPC/cxl/#type-1-device","text":"Accelerators such as smart NICs typically lack local memory. However, they can leverage the CXL.io protocol and CXL.cache to communicate with the host processors DDR memory.","title":"Type 1 Device"},{"location":"Notes%20on%20HPC/cxl/#type-2-device","text":"The idea here is there is memory (like HBM or DDR) on the accelerator (GPUs, ASICs, FPGAs, etc) and you want the accelerator's memory to be locally available to the CPU and the CPU's memory to be locally available to the accelerator. The CXL.io protocol is used to allow the CPU to discover the device and configure it and then you use the CXL.cache to allow the processor to touch the device\u2019s memory and CXL.memory to allow the accelerator to touch the CPU's memory. This memory should be co-located in the same cache coherent domain.","title":"Type 2 Device"},{"location":"Notes%20on%20HPC/cxl/#type-3-device","text":"The CXL.io and CXL.memory protocols can be leveraged for memory expansion and pooling. For example, a buffer attached to the CXL bus could be used to enable DRAM capacity expansion, augmenting memory bandwidth, or adding persistent memory without the loss of DRAM slots. In real world terms, this means the high-speed, low-latency storage devices that would have previously displaced DRAM can instead complement it with CXL-enabled devices. These could include non-volatile technologies in various form factors such as add-in cards, U.2, and EDSFF. For type 3 devices you need the CXL.io sub-protocol to discover and configure the device and the CXL.memory sub-protocol to allow the CPU to reach into the memory attached to your memory buffer. WHERE I LEFT OFF : Left off studying symmetric cache coherency protocols vs asymmetric on this article: https://www.nextplatform.com/2019/09/18/eating-the-interconnect-alphabet-soup-with-intels-cxl/","title":"Type 3 Device"},{"location":"Notes%20on%20HPC/cxl/#memory-pooling","text":"CXL 2.0 supports switching to enable memory pooling. With a CXL 2.0 switch, a host can access one or more devices from the pool. Although the hosts must be CXL 2.0-enabled to leverage this capability, the memory devices can be a mix of CXL 1.0, 1.1, and 2.0-enabled hardware. At 1.0/1.1, a device is limited to behaving as a single logical device accessible by only one host at a time. However, a 2.0 level device can be partitioned as multiple logical devices, allowing up to 16 hosts to simultaneously access different portions of the memory. As an example, a host 1 (H1) can use half the memory in device 1 (D1) and a quarter of the memory in device 2 (D2) to finely match the memory requirements of its workload to the available capacity in the memory pool. The remaining capacity in devices D1 and D2 can be used by one or more of the other hosts up to a maximum of 16. Devices D3 and D4, CXL 1.0 and 1.1-enabled respectively, can be used by only one host at a time.","title":"Memory Pooling"},{"location":"Notes%20on%20HPC/cxl/#switching","text":"By moving to a CXL 2.0 direct-connect architecture, data centers can achieve the performance benefits of main memory expansion\ufffdand the efficiency and total cost of ownership (TCO) benefits of pooled memory. Assuming all hosts and devices are CXL 2.0-enabled, \ufffdswitching is incorporated into the memory devices via a crossbar in the CXL memory pooling chip. This keeps latency low but requires a more powerful chip since it is now responsible for the control plane functionality performed by the switch. With low-latency direct connections, attached memory devices can employ DDR DRAM to provide expansion of host main memory. This can be done on a very flexible basis, as a host is able to access all\ufffdor portions of\ufffdthe capacity of as many devices as needed to tackle a specific workload.","title":"Switching"},{"location":"Notes%20on%20HPC/fabric_attached_memory/","text":"Fabric Attached Memory Return to README.md Fabric Attached Memory Resources What is it What Problem is it Solving The Future of FAM Questions: Resources Broad overview: https://itigic.com/fabric-attached-memory-is-not-ram-or-cache-in-cpu/ The Machine background information: https://github.com/FabricAttachedMemory/Emulation/wiki How the emulation for the Machine works: https://github.com/FabricAttachedMemory/Emulation/wiki/Emulation-via-Virtual-Machines What is it FAM is a type of scratchpad memory. Scratchpad memory is memory that resides inside the processor (usually). This brings with it two obvious benefits: Programs that run inside scratchpad memory run faster due to the low distance to the processor and with lower power consumption Due to its proximity to the processor, a cache system is not needed to access said memory The difference between regular scratchpad memory and FAM is that FAM uses some network interface to communicate. What Problem is it Solving The main problem with main memory is that the increased wire distance leads to a large increase in power. FAM seeks to solve this by moving it closer to the processor. It also allows processors to directly share information (or at least not have to write it to main memory and then recover it) by instead writing to FAM which is located between the last level cache and the interface to RAM for each of them. The Future of FAM To understand this part you will need to understand chiplets The best solution is a chiplet-based system where the Northbridge is disconnected from the rest of the system, as is the case in AMD\u2018s Ryzen 3000 and Ryzen 5000 CPUs. FAM, by its nature should have more capacity than the fastest cache but less than RAM. With the northbridge on a separate chip you can integrate FAM into it. However this is difficult to do on 2D chip so instead it would be preferable to use a 3D chip with the northbridge on one level and FAM on the others. Questions: Why is it difficult to integrate FAM onto a 2D chip?","title":"Fabric Attached Memory"},{"location":"Notes%20on%20HPC/fabric_attached_memory/#fabric-attached-memory","text":"Return to README.md Fabric Attached Memory Resources What is it What Problem is it Solving The Future of FAM Questions:","title":"Fabric Attached Memory"},{"location":"Notes%20on%20HPC/fabric_attached_memory/#resources","text":"Broad overview: https://itigic.com/fabric-attached-memory-is-not-ram-or-cache-in-cpu/ The Machine background information: https://github.com/FabricAttachedMemory/Emulation/wiki How the emulation for the Machine works: https://github.com/FabricAttachedMemory/Emulation/wiki/Emulation-via-Virtual-Machines","title":"Resources"},{"location":"Notes%20on%20HPC/fabric_attached_memory/#what-is-it","text":"FAM is a type of scratchpad memory. Scratchpad memory is memory that resides inside the processor (usually). This brings with it two obvious benefits: Programs that run inside scratchpad memory run faster due to the low distance to the processor and with lower power consumption Due to its proximity to the processor, a cache system is not needed to access said memory The difference between regular scratchpad memory and FAM is that FAM uses some network interface to communicate.","title":"What is it"},{"location":"Notes%20on%20HPC/fabric_attached_memory/#what-problem-is-it-solving","text":"The main problem with main memory is that the increased wire distance leads to a large increase in power. FAM seeks to solve this by moving it closer to the processor. It also allows processors to directly share information (or at least not have to write it to main memory and then recover it) by instead writing to FAM which is located between the last level cache and the interface to RAM for each of them.","title":"What Problem is it Solving"},{"location":"Notes%20on%20HPC/fabric_attached_memory/#the-future-of-fam","text":"To understand this part you will need to understand chiplets The best solution is a chiplet-based system where the Northbridge is disconnected from the rest of the system, as is the case in AMD\u2018s Ryzen 3000 and Ryzen 5000 CPUs. FAM, by its nature should have more capacity than the fastest cache but less than RAM. With the northbridge on a separate chip you can integrate FAM into it. However this is difficult to do on 2D chip so instead it would be preferable to use a 3D chip with the northbridge on one level and FAM on the others.","title":"The Future of FAM"},{"location":"Notes%20on%20HPC/fabric_attached_memory/#questions","text":"Why is it difficult to integrate FAM onto a 2D chip?","title":"Questions:"},{"location":"Notes%20on%20HSM/","text":"Notes on HSM How Does HSM for Manufacturing Work? The process of creating a trusted baseline of the server's firmware and hardware components involves measuring the components and creating a hash of the measurements. The measurements can be taken at various stages of the server's lifecycle, such as during manufacturing, configuration, or deployment. The measurements can include data such as the firmware version, BIOS settings, boot configuration, hardware components, and other system settings. These measurements are then stored securely in a trusted repository, such as an HSM. When the server is booted up, the measurements of the current firmware and hardware components are taken and hashed. These current measurements are then compared to the measurements stored in the trusted repository. If the current measurements match the trusted baseline, it indicates that the server is running in a trusted state and has not been tampered with. If the measurements do not match, it suggests that the firmware or hardware components have been modified, and the server may not be running in a trusted state. Overall, the process of creating a trusted baseline, measuring the firmware and hardware components, and comparing them to the trusted baseline is used to ensure the integrity and authenticity of the server's components, and to provide assurance that the server is running in a trusted state. This is an important part of a security strategy, particularly in environments where data security and privacy are critical.","title":"Notes on HSM"},{"location":"Notes%20on%20HSM/#notes-on-hsm","text":"","title":"Notes on HSM"},{"location":"Notes%20on%20HSM/#how-does-hsm-for-manufacturing-work","text":"The process of creating a trusted baseline of the server's firmware and hardware components involves measuring the components and creating a hash of the measurements. The measurements can be taken at various stages of the server's lifecycle, such as during manufacturing, configuration, or deployment. The measurements can include data such as the firmware version, BIOS settings, boot configuration, hardware components, and other system settings. These measurements are then stored securely in a trusted repository, such as an HSM. When the server is booted up, the measurements of the current firmware and hardware components are taken and hashed. These current measurements are then compared to the measurements stored in the trusted repository. If the current measurements match the trusted baseline, it indicates that the server is running in a trusted state and has not been tampered with. If the measurements do not match, it suggests that the firmware or hardware components have been modified, and the server may not be running in a trusted state. Overall, the process of creating a trusted baseline, measuring the firmware and hardware components, and comparing them to the trusted baseline is used to ensure the integrity and authenticity of the server's components, and to provide assurance that the server is running in a trusted state. This is an important part of a security strategy, particularly in environments where data security and privacy are critical.","title":"How Does HSM for Manufacturing Work?"},{"location":"Notes%20on%20Improving%20Drive%20Performance/","text":"Notes on Improving Drive Performance Located NUMA Nodes https://community.mellanox.com/s/article/understanding-numa-node-for-performance-benchmarks Definition of Drive Stats","title":"Notes on Improving Drive Performance"},{"location":"Notes%20on%20Improving%20Drive%20Performance/#notes-on-improving-drive-performance","text":"","title":"Notes on Improving Drive Performance"},{"location":"Notes%20on%20Improving%20Drive%20Performance/#located-numa-nodes","text":"https://community.mellanox.com/s/article/understanding-numa-node-for-performance-benchmarks","title":"Located NUMA Nodes"},{"location":"Notes%20on%20Improving%20Drive%20Performance/#definition-of-drive-stats","text":"","title":"Definition of Drive Stats"},{"location":"Notes%20on%20NVMe%20Log%20Pages/","text":"Notes on NVMe Log Pages Notes on NVMe Log Pages Quick Overview My Test Drive Get Log Page Identifiers Error Information (01h) Sample Output SMART / Health Information (02h) Sample Output Firmware Slot Information (03h) Sample Output Changed Namespace List (04h) Commands Supported and Effects (05h) Sample Output Device Self-test (06h) Telemetry Host-Initiated (07h) Notes on Telemetry Sample Output Telemetry Controller-Initiated (08h) Notes on Telemetry Sample Output Endurance Group Information (09h) Predictable Latency Per NVM Set (0Ah) Predictable Latency Event Aggregate Log Page (0Bh) Asymmetric Namespace Access (0Ch) Persistent Event Log (0Dh) Endurance Group Event Aggregate (0Fh) Media Unit Status (10h) Supported Capacity Configuration List (11h) Feature Identifiers Supported and Effects (12h) NVMe-MI Commands Supported and Effects (13h) Command and Feature Lockdown (14h) Boot Partition (15h) Rotational Media Information Log (16h) Discovery Log Page (70h) Command Syntax Reservation Notification (80h) Sanitize Status (81h) Command Syntax Other NVMe CLI Commands List All NVMe Drives Quick Overview See this excel document My Test Drive [root@r8402 ~]# nvme id-ctrl /dev/nvme0n1 NVME Identify Controller: vid : 0x8086 ssvid : 0x1028 sn : PHLN939602VB3P2BGN mn : Dell Express Flash NVMe P4610 3.2TB SFF fr : VDV1DP25 rab : 0 ieee : 5cd2e4 cmic : 0 mdts : 5 cntlid : 0 ver : 0x10200 rtd3r : 0x989680 rtd3e : 0xe4e1c0 oaes : 0x200 ctratt : 0 rrls : 0 cntrltype : 0 fguid : crdt1 : 0 crdt2 : 0 crdt3 : 0 oacs : 0x6 acl : 3 aerl : 3 frmw : 0x18 lpa : 0xe elpe : 63 npss : 0 avscc : 0 apsta : 0 wctemp : 343 cctemp : 349 mtfa : 0 hmpre : 0 hmmin : 0 tnvmcap : 3200631791616 unvmcap : 0 rpmbs : 0 edstt : 0 dsto : 0 fwug : 0 kas : 0 hctma : 0 mntmt : 0 mxtmt : 0 sanicap : 0 hmminds : 0 hmmaxd : 0 nsetidmax : 0 endgidmax : 0 anatt : 0 anacap : 0 anagrpmax : 0 nanagrpid : 0 pels : 0 sqes : 0x66 cqes : 0x44 maxcmd : 0 nn : 1 oncs : 0x6 fuses : 0 fna : 0x4 vwc : 0 awun : 0 awupf : 0 icsvscc : 0 nwpc : 0 acwu : 0 sgls : 0 mnan : 0 subnqn : ioccsz : 0 iorcsz : 0 icdoff : 0 fcatt : 0 msdbd : 0 ofcs : 0 ps 0 : mp:25.00W operational enlat:0 exlat:0 rrt:0 rrl:0 rwt:0 rwl:0 idle_power:- active_power:- Get Log Page Identifiers NVMe Express Base Specification Error Information (01h) This log page is used to describe extended error information for a command that completed with error or report an error that is not specific to a particular command. Extended error information is provided when the More (M) bit is set to \u20181\u2019 in the Status Field for the completion queue entry associated with the command that completed with error or as part of an asynchronous event with an Error status type. This log page is global to the controller. This error log may return the last n errors. If host software specifies a data transfer of the size of n error logs, then the error logs for the most recent n errors are returned. The ordering of the entries is based on the time when the error occurred, with the most recent error being returned as the first log entry. Each entry in the log page returned is defined in Figure 206. The log page is a set of 64-byte entries; the maximum number of entries supported is indicated in the ELPE field in the Identify Controller data structure (refer to Figure 275). If the log page is full when a new entry is generated, the controller should insert the new entry into the log and discard the oldest entry. The controller should clear this log page by removing all entries on power cycle and Controller Level Reset. See page 178 for a description. Sample Output [root@r8402 ~]# nvme error-log /dev/nvme0n1 Error Log Entries for device:nvme0n1 entries:64 ................. Entry[ 0] ................. error_count : 0 sqid : 0 cmdid : 0 status_field : 0(SUCCESS: The command completed successfully) phase_tag : 0 parm_err_loc : 0 lba : 0 nsid : 0 vs : 0 trtype : The transport type is not indicated or the error is not transport related. cs : 0 trtype_spec_info: 0 ................. ...SNIP... Entry[63] ................. error_count : 0 sqid : 0 cmdid : 0 status_field : 0(SUCCESS: The command completed successfully) phase_tag : 0 parm_err_loc : 0 lba : 0 nsid : 0 vs : 0 trtype : The transport type is not indicated or the error is not transport related. cs : 0 trtype_spec_info: 0 ................. SMART / Health Information (02h) This log page is used to provide SMART and general health information. The information provided is over the life of the controller and is retained across power cycles. To request the controller log page, the namespace identifier specified is FFFFFFFFh or 0h. For compatibility with implementations compliant with NVM Express Base Specification revision 1.4 and earlier, hosts should use a namespace identifier of FFFFFFFFh to request the controller log page. The controller may also support requesting the log page on a per namespace basis, as indicated by bit 0 of the LPA field in the Identify Controller data structure in Figure 275. See page 180 for a description. Sample Output [root@r8402 ~]# nvme smart-log /dev/nvme0n1 Smart Log for NVME device:nvme0n1 namespace-id:ffffffff critical_warning : 0 temperature : 26 C available_spare : 100% available_spare_threshold : 10% percentage_used : 0% endurance group critical warning summary: 0 data_units_read : 4,002,753 data_units_written : 255,875,492 host_read_commands : 45,714,473 host_write_commands : 1,620,770,593 controller_busy_time : 372 power_cycles : 150 power_on_hours : 5,219 unsafe_shutdowns : 99 media_errors : 0 num_err_log_entries : 0 Warning Temperature Time : 0 Critical Composite Temperature Time : 0 Thermal Management T1 Trans Count : 0 Thermal Management T2 Trans Count : 0 Thermal Management T1 Total Time : 0 Thermal Management T2 Total Time : 0 Firmware Slot Information (03h) This log page is used to describe the firmware revision stored in each firmware slot supported. The firmware revision is indicated as an ASCII string. The log page also indicates the active slot number. The log page returned is defined in Figure 209 Sample Output [root@r8402 ~]# nvme fw-log /dev/nvme0n1 Firmware Log for device:nvme0n1 afi : 0x1 frs1 : 0x3532504431564456 (VDV1DP25) frs2 : 0x3532504431564456 (VDV1DP25) NOTE AFI stands for active firmware version. Changed Namespace List (04h) NOTE This command is not currently supported because the drive currently only has one namespace. This log page is used to describe namespaces attached to the controller that have: changed information in their Identify Namespace data structures (refer to in Figure 146) since the last time the log page was read; been added; and been deleted. The log page contains a Namespace List with up to 1,024 entries. If more than 1,024 namespaces have changed attributes since the last time the log page was read, the first entry in the log page shall be set to FFFFFFFFh and the remainder of the list shall be zero filled. See page 184 for a description. Commands Supported and Effects (05h) This log page is used to describe the commands that the controller supports and the effects of those commands on the state of the NVM subsystem. The log page is 4,096 bytes in size. There is one Commands Supported and Effects data structure per Admin command and one Commands Supported and Effects data structure per I/O command based on: the I/O Command Set selected in CC.CSS, if CC.CSS is not set to 110b; and the Command Set Identifier field in CDW 14, if CC.CSS is set to 110b. See page 185 for a description. Sample Output [root@r8402 ~]# nvme effects-log /dev/nvme0n1 Admin Command Set ACS0 [Delete I/O Submission Queue ] 00000001 ACS1 [Create I/O Submission Queue ] 00020001 ACS2 [Get Log Page ] 00000001 ACS4 [Delete I/O Completion Queue ] 00000001 ACS5 [Create I/O Completion Queue ] 00020001 ACS6 [Identify ] 00000001 ACS8 [Abort ] 00000001 ACS9 [Set Features ] 0000001d ACS10 [Get Features ] 00000001 ACS12 [Asynchronous Event Request ] 00000001 ACS16 [Firmware Commit ] 00000011 ACS17 [Firmware Image Download ] 00000001 ACS128 [Format NVM ] 0002001f ACS200 [Unknown ] 00000001 ACS210 [Unknown ] 00000001 ACS225 [Unknown ] 0002000f ACS226 [Unknown ] 0002000f NVM Command Set IOCS0 [Flush ] 00000003 IOCS1 [Write ] 00000003 IOCS2 [Read ] 00000001 IOCS4 [Write Uncorrectable ] 00000003 IOCS9 [Dataset Management ] 00000003 Device Self-test (06h) NOTE : This command is not currently supported. TODO This log page is used to indicate: the status of any device self-test operation in progress and the percentage complete of that operation; and the results of the last 20 device self-test operations. The Self-test Result Data Structure contained in the Newest Self-test Result Data Structure field is always the result of the last completed or aborted self-test operation. The next Self-test Result Data Structure field in the Device Self-test log page contains the results of the second newest self-test operation and so on. If fewer than 20 self-test operations have completed or been aborted, then the Device Self-test Status field shall be set to Fh in the unused Self-test Result Data Structure fields and all other fields in that Self-test Result Data Structure are ignored. See page 187 for additional information. Telemetry Host-Initiated (07h) This log consists of a header describing the log and zero or more Telemetry Data Blocks (refer to section 8.24). All Telemetry Data Blocks are 512 bytes in size. The controller shall initiate a capture of the controller\u2019s internal controller state to this log if the controller processes a Get Log Page command for this log with the Create Telemetry Host-Initiated Data bit set to \u20181\u2019 in the Log Specific field. If the host specifies a Log Page Offset Lower value that is not a multiple of 512 bytes in the Get Log Page command for this log, then the controller shall return an error with a status code set to Invalid Field in Command. This log page is global to the controller or global to the NVM subsystem. See page 189 for additional information. Notes on Telemetry See page 422 for additional details. Telemetry enables manufacturers to collect internal data logs to improve the functionality and reliability of products. The telemetry data collection may be initiated by the host or by the controller. The data is returned in the Telemetry Host-Initiated log page or the Telemetry Controller-Initiated log page (refer to section 5.16.1.8 and 5.16.1.9). The data captured is vendor specific. The telemetry feature defines the mechanism to collect the vendor specific data. The controller indicates support for the telemetry log pages and for the Data Area 4 size in the Log Page Attributes (LPA) field in the Identify Controller data structure (refer to Figure 275). Sample Output nvme telemetry-log --output-file /root/test.log --host-generate=1 /dev/nvme0n1 Telemetry Controller-Initiated (08h) Telemetry enables manufacturers to collect internal data logs to improve the functionality and reliability of products. The telemetry data collection may be initiated by the host or by the controller. The data is returned in the Telemetry Host-Initiated log page or the Telemetry Controller-Initiated log page (refer to section 5.16.1.8 and 5.16.1.9). The data captured is vendor specific. The telemetry feature defines the mechanism to collect the vendor specific data. The controller indicates support for the telemetry log pages and for the Data Area 4 size in the Log Page Attributes (LPA) field in the Identify Controller data structure (refer to Figure 275). See page 191 for additional information. Notes on Telemetry See Notes on Telemetry Sample Output nvme telemetry-log --output-file /root/test.log --host-generate=0 /dev/nvme0n1 Endurance Group Information (09h) NOTE This command is not currently supported because there is only one endurance group (endgidmax=0) This log page is used to provide endurance information based on the Endurance Group (refer to section 3.2.3). An Endurance Group contains capacity that may be allocated to zero or more NVM Sets. Capacity that has not been allocated to an NVM Set is unallocated Endurance Group capacity. The information provided is over the life of the Endurance Group. The Endurance Group Identifier is specified in the Log Specific Identifier field in Command Dword 11 of the Get Log Page command. The log page is 512 bytes in size. See page 193 for additional information. Predictable Latency Per NVM Set (0Ah) NOTE This log is not supported because the drives only support one NVM Set. This log page may be used to determine the current window for the specified NVM Set when Predictable Latency Mode is enabled and any events that have occurred for the specified NVM Set. There is one log page for each NVM Set when Predictable Latency Mode is supported. Command Dword 11 (refer to Figure 198) specifies the NVM Set for which the log page is to be returned. The log page is 512 bytes in size. The log page indicates typical values and reliable estimates for attributes associated with the Deterministic Window and the Non-Deterministic Window of the specified NVM Set. The Typical, Maximum, and Minimum values are static and worst-case values over the lifetime of the NVM subsystem. After the controller successfully completes a read of this log page with Retain Asynchronous Event bit cleared to \u20180\u2019, then reported events are cleared to \u20180\u2019 for the specified NVM Set and the field corresponding to the specified NVM Set is cleared to \u20180\u2019 in the Predictable Latency Event Aggregate log page. Coordination between two or more hosts is beyond the scope of this specification. See page 195 for additional information. Predictable Latency Event Aggregate Log Page (0Bh) NOTE This log page indicates if a Predictable Latency Event (refer to section 8.16) has occurred for a particular NVM Set. If a Predictable Latency Event has occurred, the details of the particular event are included in the Predictable Latency Per NVM Set log page for that NVM Set. An asynchronous event is generated when an entry for an NVM Set is newly added to this log page. This log page shall not contain an entry (i.e., an NVM Set Identifier) that is cleared to 0h. If there is an enabled Predictable Latency Event pending for an NVM Set, then the Predictable Latency Event Aggregate log page includes an entry for that NVM Set. The log page is an ordered list by NVM Set Identifier. For example, if Predictable Latency Events are pending for NVM Set 27, 13, and 17, then the log page shall have entries in numerical order of 13, 17, and 27. A particular NVM Set is removed from this log page after the Get Log Page is completed successfully with the Retain Asynchronous Event bit cleared to \u20180\u2019 for the Predictable Latency Per NVM Set log page for that NVM Set. See page 196 for details. Asymmetric Namespace Access (0Ch) NOTE This log is not supported because there is only one namespace on the drive. This log consists of a header describing the log and descriptors containing the asymmetric namespace access information for ANA Groups (refer to section 8.1.2) that contain namespaces that are attached to the controller processing the command. If ANA Reporting (refer to section 8.1) is supported, this log page is supported. ANA Group Descriptors shall be returned in ascending ANA Group Identifier order. If the Index Offset Supported bit is cleared to \u20180\u2019 in the LID Support and Effects data structure for this log page (refer to Figure 204), then: if the RGO bit is cleared to \u20180\u2019 in Command Dword 10, then the LPOL field in Command Dword 12 and the LPOU field in Command Dword 13 of the Get Log Page command should be cleared to 0h. If the Index Offset Supported bit is set to \u20181\u2019 in the LID Supported and Effects data structure for this log page (refer to Figure 204), then: the entry data structure that is indexed is an ANA Group Descriptor (e.g., specifying an index offset of 2 returns this log page starting at the offset of ANA Group Descriptor 1). If the host performs multiple Get Log Page commands to read the ANA log page (e.g., using the LPOL field or the LPOU field), the host should re-read the header of the log page and ensure that the Change Count field in the Asymmetric Namespace Access log matches the original value read. If it does not match, then the data captured is not consistent and the ANA log page should be re-read. See page 197 for details. Persistent Event Log (0Dh) NOTE TODO The Persistent Event Log page contains information about significant events not specific to a particular command. The information in this log page shall be retained across power cycles and resets. NVM subsystems should be designed for minimal loss of event information upon power failure. This log consists of a header describing the log and zero or more Persistent Events (refer to section 5.16.1.14.1). This log page is global to the NVM subsystem. A sanitize operation may alter this log page (e.g., remove or modify events to prevent derivation of user data from log page information, refer to section 8.20). The events removed from this log page by a sanitize operation are unspecified. Persistent Event Log events specified in this section should be reported in an order such that more recent events are generally reported earlier in the log data than older events. The method by which the NVM subsystem determines the order in which events occurred is vendor specific. The number of events supported is vendor specific. The supported maximum size for the Persistent Event Log is indicated in the PELS field of the Identify Controller data structure (refer to Figure 275). The number of events supported and the supported maximum size should be large enough that the number of events or the size of the Persistent Event Log data does not reach the maximum supported size over the usable life of the NVM subsystem. The controller shall log all supported events at each event occurrence unless the controller determines that the same event is occurring at a frequency that exceeds a vendor specific threshold for the frequency of event creation. If the same event is occurring at a frequency that exceeds a vendor specific threshold then the vendor may suppress further entries for the same event. A controller may indicate if events have been suppressed in vendor specific event data. See page 199 for details. Endurance Group Event Aggregate (0Fh) NOTE This is not supported because endurance groups are not in use. This log page indicates if an Endurance Group Event (refer to section 3.2.3) has occurred for a particular Endurance Group. If an Endurance Group Event has occurred, the details of the particular event are included in the Endurance Group Information log page for that Endurance Group. An asynchronous event is generated when an entry for an Endurance Group is newly added to this log page. If there is an enabled Endurance Group Event pending for an Endurance Group, then the Endurance Group Event Aggregate log page includes an entry for that Endurance Group. The log page is an ordered list by Endurance Group Identifier. For example, if Endurance Group Events are pending for Endurance Group 2, 1, and 7, then the log page shall have entries in numerical order of 1, 2, and 7. A particular Endurance Group entry is removed from this log page after the Get Log Page is completed successfully with the Retain Asynchronous Event bit cleared to \u20180\u2019 for the Endurance Group Information log page for that Endurance Group. The log page size is limited by the Endurance Group Identifier Maximum value reported in the Identify Controller data structure (refer to Figure 275). If the host reads beyond the end of the log page, zeroes are returned. The log page is defined in Figure 247. See page 220 for details. Media Unit Status (10h) NOTE This page is not supported because it is used for NVM sets which are not in use. This log page is used to describe the configuration and wear of Media Units (refer to section 8.3). The log page contains one Media Unit Status Descriptor for each Media Unit accessible by the specified domain. Each Media Unit Status Descriptor (refer to Figure 249) indicates the configuration of the Media Unit (e.g., to which Endurance Group the Media Unit is assigned, to which NVM Set the Media Unit is assigned, to which Channels the Media Unit is attached) and indications of wear (e.g., the Available Spare field and the Percentage Used field). The indications of wear change as the Media Unit is written and read. If the NVM subsystem supports multiple domains, then the controller reports the Media Unit Status log page for the domain specified in the Log Specific Identifier field (refer to Figure 198), if accessible. If the information is not accessible, then the log page is not available (refer to section 8.1.4). If the Log Specific Identifier field is cleared to 0h, then the specified domain is the domain containing the controller that is processing the command. Media Unit Identifier values (refer to Figure 249) begin with 0h and increase sequentially. If the NVM subsystem supports multiple domains, then the Media Unit Identifier values are unique within the specified domain. If the NVM subsystem does not support multiple domains, then the Media Unit Identifier values are unique within the NVM subsystem. Media Unit Status Descriptors are listed in ascending order by Media Unit Identifier. See page 220 for details. Supported Capacity Configuration List (11h) NOTE This log page is not available because the drives do not currently support multiple endurance groups. This log page is used to provide a list of Supported Capacity Configuration Descriptors (refer to Figure 250). Each entry in the list defines a different configuration of Endurance Groups supported by the specified domain. If the NVM subsystem supports multiple domains, then the controller reports the Supported Capacity Configuration List log page for the domain specified in the Log Specific Identifier field (refer to Figure 198), if accessible. If the information is not accessible, then the log page is not available (refer to section 8.1.3). If the Log Specific Identifier field is cleared to 0h, then the specified domain is the domain containing the controller that is processing the command. If the NVM subsystem supports multiple domains, then Capacity Configuration Identifier values are unique within the specified domain. If the NVM subsystem does not support multiple domains, then Capacity Configuration Identifier values are unique within the NVM subsystem. Capacity Configuration Descriptors are listed in ascending order by Capacity Configuration Identifier, and each Capacity Configuration Identifier shall appear only once. For details see page 222 Feature Identifiers Supported and Effects (12h) NOTE TODO An NVM subsystem may support several interfaces for submitting a Get Log Page command such as an Admin Submission Queue, PCIe VDM Management Endpoint, or SMBus/I2C Management Endpoint (refer the NVM Express Management Interface Specification for details on Management Endpoints) and may have zero or more instances of each of those interfaces. The feature identifiers (FIDs) supported on each instance of each interface may be different. This log page describes the FIDs that are supported on the interface to which the Get Log Page command was submitted and the effects of those features on the state of the NVM subsystem. The log page is defined in Figure 255. Each Feature Identifier\u2019s effects are described in a FID Supported and Effects data structure defined in Figure 256. The features that the controller supports are dependent on the I/O Command Set that is based on: - the I/O Command Set selected in CC.CSS, if CC.CSS is not set to 110b; and - the Command Set Identifier (CSI) field in CDW 14, if CC.CSS is set to 110b. For details see page 225 NVMe-MI Commands Supported and Effects (13h) This log page describes the Management Interface Command Set commands (refer to the NVMe Management Interface Specification ) that the controller supports using the NVMe-MI Send and NVMe-MI Receive commands and the effects of those Management Interface Command Set commands on the state of the NVM subsystem. The log page is defined in Figure 257. See page 227 for details. Command and Feature Lockdown (14h) NOTE This command is not supported. It is used for preventing certain commands for security purposes. It is not relevant to performance testing. This log page is used to indicate which commands and Set Features Feature Identifiers are supported to be prohibited from execution using the Command and Feature Lockdown capability (refer to section 8.4) and which commands are currently prohibited if received on an NVM Express controller Admin Submission Queue or received out-of-band on a Management Endpoint (refer to the NVM Express Management Interface Specification). This log page uses the Log Specific Field field (refer to Figure 259) and may use the UUID Index field in the Get Log Page command to specify the scope and content of the list returned in the Command and Feature Identifier List field of this log page. The UUID Index field may be used if the Scope field is set to 2h, allowing returning of vendor specific Set Features Feature Identifier lockdown information. See page 228 for details. Boot Partition (15h) NOTE This command is not supported and does not have any relevance to drive performance. It allows you to see the boot partition of the drive. The Boot Partition Log page provides read only access to the Boot Partition (refer to section 8.2) accessible by this controller through the BPRSEL register (refer to section 3.1.3.14). This log consists of a header describing the Boot Partition and Boot Partition data as defined by Figure 262. The Boot Partition Identifier bit in the Log Specific Field field determines the Boot Partition. A host reading this log page has no effects on the BPINFO (refer to section 3.1.3.13), BPRSEL, and BPMBL (refer to section 3.1.3.15) registers. See page 230 for details. Rotational Media Information Log (16h) NOTE This is specific to multiple endurance groups so it is not supported (since there is only one) This log page provides rotational media information (refer to section 8.20) for Endurance Groups that store data on rotational media. The information provided is retained across power cycles and resets. The Endurance Group Identifier is specified in the Log Specific Identifier field in Command Dword 11 of the Get Log Page command. If the NVM subsystem does not contain any Endurance Groups that store data on rotational media, then the Rotational Media Information Log should not be supported. See page 231 for details. Discovery Log Page (70h) NOTE This log page is not supported. It is specific to NVMe-over-Fabrics (ex: RDMA) The Discovery Log Page shall only be supported by Discovery controllers. The Discovery Log Page shall not be supported by controllers that expose namespaces for NVMe over PCIe or NVMe over Fabrics. The Discovery Log Page provides an inventory of NVM subsystems with which a host may attempt to form an association. The Discovery Log Page may be specific to the host requesting the log. The Discovery Log Page is persistent across power cycles. The Log Page Offset may be used to retrieve specific records. The number of records is returned in the header of the log page. The format for a Discovery Log Page Entry is defined in Figure 264. The format for the Discovery Log Page is defined in Figure 265. A single Get Log Page command used to read the Discovery Log Page shall be atomic. If the host reads the Discovery Log Page using multiple Get Log Page commands the host should ensure that there has not been a change in the contents of the data. The host should read the Discovery Log Page contents in order (i.e., with increasing Log Page Offset values) and then re-read the Generation Counter after the entire log page is transferred. If the Generation Counter does not match the original value read, the host should discard the log page read as the entries may be inconsistent. If the log page contents change during this command sequence, the controller may return a status code of Discover Restart. Every record indicates via the SUBTYPE field if that record is referring to another Discovery Service or if the record indicates an NVM subsystem composed of controllers that may expose namespaces. A referral to another Discovery Service (i.e., SUBTYPE 01h) is a mechanism to find additional Discovery subsystems. An NVM subsystem entry (i.e., SUBTYPE 02h) is a mechanism to find NVM subsystems that contain controllers that may expose namespaces. Referrals shall not be deeper than eight levels. If an NVM subsystem supports the dynamic controller model, then all entries for that NVM subsystem shall have the Controller ID field set to FFFFh. For a particular NVM subsystem port and NVMe Transport address in an NVM subsystem, there shall be no more than one entry with the Controller ID field set to: - FFFFh if that NVM subsystem supports the dynamic controller model; or - FFFEh if that NVM subsystem supports the static controller model. See page 232 for details. Command Syntax This log uses the nvme-discover command. Reservation Notification (80h) NOTE TODO The Reservation Notification log page reports one log page from a time ordered queue of Reservation Notification log pages, if available. A new Reservation Notification log page is created and added to the end of the queue of reservation notifications whenever an unmasked reservation notification occurs on any namespace that is attached to the controller. The Get Log Page command: returns a data buffer containing a log page corresponding to the oldest log page in the reservation notification queue (i.e., the log page containing the lowest Log Page Count field; accounting for wrapping); and removes that Reservation Notification log page from the queue. If there are no available Reservation Notification log page entries when a Get Log Page command is issued, then an empty log page (i.e., all fields in the log page cleared to 0h) shall be returned. If the controller is unable to store a reservation notification in the Reservation Notification log page due to the size of the queue, that reservation notification is lost. If a reservation notification is lost, then the controller shall increment the Log Page Count field of the last reservation notification in the queue (i.e., the Log Page Count field in the last reservation notification in the queue shall contain the value associated with the most recent reservation notification that has been lost). See page 234 for details. Sanitize Status (81h) NOTE This log is not supported but has no bearing on drive performance. The Sanitize Status log page is used to report sanitize operation time estimates and information about the most recent sanitize operation (refer to section 8.20). The Get Log Page command returns a data buffer containing a log page formatted as defined in Figure 267. This log page shall be retained across power cycles and resets. This log page shall contain valid data whenever CSTS.RDY is set to \u20181\u2019. If the Sanitize Capabilities (SANICAP) field in the Identify Controller data structure is not cleared to 0h (i.e., the Sanitize command is supported), then this log page shall be supported. If the Sanitize Capabilities field in the Identify Controller data structure is cleared to 0h, then this log page is reserved. See page 235 for details. Command Syntax This log uses the nvme-resv-notif-log command. Other NVMe CLI Commands List All NVMe Drives nvme list Lists all the NVMe SSDs attached: name, serial number, size, LBA format, and serial","title":"Notes on NVMe Log Pages"},{"location":"Notes%20on%20NVMe%20Log%20Pages/#notes-on-nvme-log-pages","text":"Notes on NVMe Log Pages Quick Overview My Test Drive Get Log Page Identifiers Error Information (01h) Sample Output SMART / Health Information (02h) Sample Output Firmware Slot Information (03h) Sample Output Changed Namespace List (04h) Commands Supported and Effects (05h) Sample Output Device Self-test (06h) Telemetry Host-Initiated (07h) Notes on Telemetry Sample Output Telemetry Controller-Initiated (08h) Notes on Telemetry Sample Output Endurance Group Information (09h) Predictable Latency Per NVM Set (0Ah) Predictable Latency Event Aggregate Log Page (0Bh) Asymmetric Namespace Access (0Ch) Persistent Event Log (0Dh) Endurance Group Event Aggregate (0Fh) Media Unit Status (10h) Supported Capacity Configuration List (11h) Feature Identifiers Supported and Effects (12h) NVMe-MI Commands Supported and Effects (13h) Command and Feature Lockdown (14h) Boot Partition (15h) Rotational Media Information Log (16h) Discovery Log Page (70h) Command Syntax Reservation Notification (80h) Sanitize Status (81h) Command Syntax Other NVMe CLI Commands List All NVMe Drives","title":"Notes on NVMe Log Pages"},{"location":"Notes%20on%20NVMe%20Log%20Pages/#quick-overview","text":"See this excel document","title":"Quick Overview"},{"location":"Notes%20on%20NVMe%20Log%20Pages/#my-test-drive","text":"[root@r8402 ~]# nvme id-ctrl /dev/nvme0n1 NVME Identify Controller: vid : 0x8086 ssvid : 0x1028 sn : PHLN939602VB3P2BGN mn : Dell Express Flash NVMe P4610 3.2TB SFF fr : VDV1DP25 rab : 0 ieee : 5cd2e4 cmic : 0 mdts : 5 cntlid : 0 ver : 0x10200 rtd3r : 0x989680 rtd3e : 0xe4e1c0 oaes : 0x200 ctratt : 0 rrls : 0 cntrltype : 0 fguid : crdt1 : 0 crdt2 : 0 crdt3 : 0 oacs : 0x6 acl : 3 aerl : 3 frmw : 0x18 lpa : 0xe elpe : 63 npss : 0 avscc : 0 apsta : 0 wctemp : 343 cctemp : 349 mtfa : 0 hmpre : 0 hmmin : 0 tnvmcap : 3200631791616 unvmcap : 0 rpmbs : 0 edstt : 0 dsto : 0 fwug : 0 kas : 0 hctma : 0 mntmt : 0 mxtmt : 0 sanicap : 0 hmminds : 0 hmmaxd : 0 nsetidmax : 0 endgidmax : 0 anatt : 0 anacap : 0 anagrpmax : 0 nanagrpid : 0 pels : 0 sqes : 0x66 cqes : 0x44 maxcmd : 0 nn : 1 oncs : 0x6 fuses : 0 fna : 0x4 vwc : 0 awun : 0 awupf : 0 icsvscc : 0 nwpc : 0 acwu : 0 sgls : 0 mnan : 0 subnqn : ioccsz : 0 iorcsz : 0 icdoff : 0 fcatt : 0 msdbd : 0 ofcs : 0 ps 0 : mp:25.00W operational enlat:0 exlat:0 rrt:0 rrl:0 rwt:0 rwl:0 idle_power:- active_power:-","title":"My Test Drive"},{"location":"Notes%20on%20NVMe%20Log%20Pages/#get-log-page-identifiers","text":"NVMe Express Base Specification","title":"Get Log Page Identifiers"},{"location":"Notes%20on%20NVMe%20Log%20Pages/#error-information-01h","text":"This log page is used to describe extended error information for a command that completed with error or report an error that is not specific to a particular command. Extended error information is provided when the More (M) bit is set to \u20181\u2019 in the Status Field for the completion queue entry associated with the command that completed with error or as part of an asynchronous event with an Error status type. This log page is global to the controller. This error log may return the last n errors. If host software specifies a data transfer of the size of n error logs, then the error logs for the most recent n errors are returned. The ordering of the entries is based on the time when the error occurred, with the most recent error being returned as the first log entry. Each entry in the log page returned is defined in Figure 206. The log page is a set of 64-byte entries; the maximum number of entries supported is indicated in the ELPE field in the Identify Controller data structure (refer to Figure 275). If the log page is full when a new entry is generated, the controller should insert the new entry into the log and discard the oldest entry. The controller should clear this log page by removing all entries on power cycle and Controller Level Reset. See page 178 for a description.","title":"Error Information (01h)"},{"location":"Notes%20on%20NVMe%20Log%20Pages/#sample-output","text":"[root@r8402 ~]# nvme error-log /dev/nvme0n1 Error Log Entries for device:nvme0n1 entries:64 ................. Entry[ 0] ................. error_count : 0 sqid : 0 cmdid : 0 status_field : 0(SUCCESS: The command completed successfully) phase_tag : 0 parm_err_loc : 0 lba : 0 nsid : 0 vs : 0 trtype : The transport type is not indicated or the error is not transport related. cs : 0 trtype_spec_info: 0 ................. ...SNIP... Entry[63] ................. error_count : 0 sqid : 0 cmdid : 0 status_field : 0(SUCCESS: The command completed successfully) phase_tag : 0 parm_err_loc : 0 lba : 0 nsid : 0 vs : 0 trtype : The transport type is not indicated or the error is not transport related. cs : 0 trtype_spec_info: 0 .................","title":"Sample Output"},{"location":"Notes%20on%20NVMe%20Log%20Pages/#smart-health-information-02h","text":"This log page is used to provide SMART and general health information. The information provided is over the life of the controller and is retained across power cycles. To request the controller log page, the namespace identifier specified is FFFFFFFFh or 0h. For compatibility with implementations compliant with NVM Express Base Specification revision 1.4 and earlier, hosts should use a namespace identifier of FFFFFFFFh to request the controller log page. The controller may also support requesting the log page on a per namespace basis, as indicated by bit 0 of the LPA field in the Identify Controller data structure in Figure 275. See page 180 for a description.","title":"SMART / Health Information (02h)"},{"location":"Notes%20on%20NVMe%20Log%20Pages/#sample-output_1","text":"[root@r8402 ~]# nvme smart-log /dev/nvme0n1 Smart Log for NVME device:nvme0n1 namespace-id:ffffffff critical_warning : 0 temperature : 26 C available_spare : 100% available_spare_threshold : 10% percentage_used : 0% endurance group critical warning summary: 0 data_units_read : 4,002,753 data_units_written : 255,875,492 host_read_commands : 45,714,473 host_write_commands : 1,620,770,593 controller_busy_time : 372 power_cycles : 150 power_on_hours : 5,219 unsafe_shutdowns : 99 media_errors : 0 num_err_log_entries : 0 Warning Temperature Time : 0 Critical Composite Temperature Time : 0 Thermal Management T1 Trans Count : 0 Thermal Management T2 Trans Count : 0 Thermal Management T1 Total Time : 0 Thermal Management T2 Total Time : 0","title":"Sample Output"},{"location":"Notes%20on%20NVMe%20Log%20Pages/#firmware-slot-information-03h","text":"This log page is used to describe the firmware revision stored in each firmware slot supported. The firmware revision is indicated as an ASCII string. The log page also indicates the active slot number. The log page returned is defined in Figure 209","title":"Firmware Slot Information (03h)"},{"location":"Notes%20on%20NVMe%20Log%20Pages/#sample-output_2","text":"[root@r8402 ~]# nvme fw-log /dev/nvme0n1 Firmware Log for device:nvme0n1 afi : 0x1 frs1 : 0x3532504431564456 (VDV1DP25) frs2 : 0x3532504431564456 (VDV1DP25) NOTE AFI stands for active firmware version.","title":"Sample Output"},{"location":"Notes%20on%20NVMe%20Log%20Pages/#changed-namespace-list-04h","text":"NOTE This command is not currently supported because the drive currently only has one namespace. This log page is used to describe namespaces attached to the controller that have: changed information in their Identify Namespace data structures (refer to in Figure 146) since the last time the log page was read; been added; and been deleted. The log page contains a Namespace List with up to 1,024 entries. If more than 1,024 namespaces have changed attributes since the last time the log page was read, the first entry in the log page shall be set to FFFFFFFFh and the remainder of the list shall be zero filled. See page 184 for a description.","title":"Changed Namespace List (04h)"},{"location":"Notes%20on%20NVMe%20Log%20Pages/#commands-supported-and-effects-05h","text":"This log page is used to describe the commands that the controller supports and the effects of those commands on the state of the NVM subsystem. The log page is 4,096 bytes in size. There is one Commands Supported and Effects data structure per Admin command and one Commands Supported and Effects data structure per I/O command based on: the I/O Command Set selected in CC.CSS, if CC.CSS is not set to 110b; and the Command Set Identifier field in CDW 14, if CC.CSS is set to 110b. See page 185 for a description.","title":"Commands Supported and Effects (05h)"},{"location":"Notes%20on%20NVMe%20Log%20Pages/#sample-output_3","text":"[root@r8402 ~]# nvme effects-log /dev/nvme0n1 Admin Command Set ACS0 [Delete I/O Submission Queue ] 00000001 ACS1 [Create I/O Submission Queue ] 00020001 ACS2 [Get Log Page ] 00000001 ACS4 [Delete I/O Completion Queue ] 00000001 ACS5 [Create I/O Completion Queue ] 00020001 ACS6 [Identify ] 00000001 ACS8 [Abort ] 00000001 ACS9 [Set Features ] 0000001d ACS10 [Get Features ] 00000001 ACS12 [Asynchronous Event Request ] 00000001 ACS16 [Firmware Commit ] 00000011 ACS17 [Firmware Image Download ] 00000001 ACS128 [Format NVM ] 0002001f ACS200 [Unknown ] 00000001 ACS210 [Unknown ] 00000001 ACS225 [Unknown ] 0002000f ACS226 [Unknown ] 0002000f NVM Command Set IOCS0 [Flush ] 00000003 IOCS1 [Write ] 00000003 IOCS2 [Read ] 00000001 IOCS4 [Write Uncorrectable ] 00000003 IOCS9 [Dataset Management ] 00000003","title":"Sample Output"},{"location":"Notes%20on%20NVMe%20Log%20Pages/#device-self-test-06h","text":"NOTE : This command is not currently supported. TODO This log page is used to indicate: the status of any device self-test operation in progress and the percentage complete of that operation; and the results of the last 20 device self-test operations. The Self-test Result Data Structure contained in the Newest Self-test Result Data Structure field is always the result of the last completed or aborted self-test operation. The next Self-test Result Data Structure field in the Device Self-test log page contains the results of the second newest self-test operation and so on. If fewer than 20 self-test operations have completed or been aborted, then the Device Self-test Status field shall be set to Fh in the unused Self-test Result Data Structure fields and all other fields in that Self-test Result Data Structure are ignored. See page 187 for additional information.","title":"Device Self-test (06h)"},{"location":"Notes%20on%20NVMe%20Log%20Pages/#telemetry-host-initiated-07h","text":"This log consists of a header describing the log and zero or more Telemetry Data Blocks (refer to section 8.24). All Telemetry Data Blocks are 512 bytes in size. The controller shall initiate a capture of the controller\u2019s internal controller state to this log if the controller processes a Get Log Page command for this log with the Create Telemetry Host-Initiated Data bit set to \u20181\u2019 in the Log Specific field. If the host specifies a Log Page Offset Lower value that is not a multiple of 512 bytes in the Get Log Page command for this log, then the controller shall return an error with a status code set to Invalid Field in Command. This log page is global to the controller or global to the NVM subsystem. See page 189 for additional information.","title":"Telemetry Host-Initiated (07h)"},{"location":"Notes%20on%20NVMe%20Log%20Pages/#notes-on-telemetry","text":"See page 422 for additional details. Telemetry enables manufacturers to collect internal data logs to improve the functionality and reliability of products. The telemetry data collection may be initiated by the host or by the controller. The data is returned in the Telemetry Host-Initiated log page or the Telemetry Controller-Initiated log page (refer to section 5.16.1.8 and 5.16.1.9). The data captured is vendor specific. The telemetry feature defines the mechanism to collect the vendor specific data. The controller indicates support for the telemetry log pages and for the Data Area 4 size in the Log Page Attributes (LPA) field in the Identify Controller data structure (refer to Figure 275).","title":"Notes on Telemetry"},{"location":"Notes%20on%20NVMe%20Log%20Pages/#sample-output_4","text":"nvme telemetry-log --output-file /root/test.log --host-generate=1 /dev/nvme0n1","title":"Sample Output"},{"location":"Notes%20on%20NVMe%20Log%20Pages/#telemetry-controller-initiated-08h","text":"Telemetry enables manufacturers to collect internal data logs to improve the functionality and reliability of products. The telemetry data collection may be initiated by the host or by the controller. The data is returned in the Telemetry Host-Initiated log page or the Telemetry Controller-Initiated log page (refer to section 5.16.1.8 and 5.16.1.9). The data captured is vendor specific. The telemetry feature defines the mechanism to collect the vendor specific data. The controller indicates support for the telemetry log pages and for the Data Area 4 size in the Log Page Attributes (LPA) field in the Identify Controller data structure (refer to Figure 275). See page 191 for additional information.","title":"Telemetry Controller-Initiated (08h)"},{"location":"Notes%20on%20NVMe%20Log%20Pages/#notes-on-telemetry_1","text":"See Notes on Telemetry","title":"Notes on Telemetry"},{"location":"Notes%20on%20NVMe%20Log%20Pages/#sample-output_5","text":"nvme telemetry-log --output-file /root/test.log --host-generate=0 /dev/nvme0n1","title":"Sample Output"},{"location":"Notes%20on%20NVMe%20Log%20Pages/#endurance-group-information-09h","text":"NOTE This command is not currently supported because there is only one endurance group (endgidmax=0) This log page is used to provide endurance information based on the Endurance Group (refer to section 3.2.3). An Endurance Group contains capacity that may be allocated to zero or more NVM Sets. Capacity that has not been allocated to an NVM Set is unallocated Endurance Group capacity. The information provided is over the life of the Endurance Group. The Endurance Group Identifier is specified in the Log Specific Identifier field in Command Dword 11 of the Get Log Page command. The log page is 512 bytes in size. See page 193 for additional information.","title":"Endurance Group Information (09h)"},{"location":"Notes%20on%20NVMe%20Log%20Pages/#predictable-latency-per-nvm-set-0ah","text":"NOTE This log is not supported because the drives only support one NVM Set. This log page may be used to determine the current window for the specified NVM Set when Predictable Latency Mode is enabled and any events that have occurred for the specified NVM Set. There is one log page for each NVM Set when Predictable Latency Mode is supported. Command Dword 11 (refer to Figure 198) specifies the NVM Set for which the log page is to be returned. The log page is 512 bytes in size. The log page indicates typical values and reliable estimates for attributes associated with the Deterministic Window and the Non-Deterministic Window of the specified NVM Set. The Typical, Maximum, and Minimum values are static and worst-case values over the lifetime of the NVM subsystem. After the controller successfully completes a read of this log page with Retain Asynchronous Event bit cleared to \u20180\u2019, then reported events are cleared to \u20180\u2019 for the specified NVM Set and the field corresponding to the specified NVM Set is cleared to \u20180\u2019 in the Predictable Latency Event Aggregate log page. Coordination between two or more hosts is beyond the scope of this specification. See page 195 for additional information.","title":"Predictable Latency Per NVM Set (0Ah)"},{"location":"Notes%20on%20NVMe%20Log%20Pages/#predictable-latency-event-aggregate-log-page-0bh","text":"NOTE This log page indicates if a Predictable Latency Event (refer to section 8.16) has occurred for a particular NVM Set. If a Predictable Latency Event has occurred, the details of the particular event are included in the Predictable Latency Per NVM Set log page for that NVM Set. An asynchronous event is generated when an entry for an NVM Set is newly added to this log page. This log page shall not contain an entry (i.e., an NVM Set Identifier) that is cleared to 0h. If there is an enabled Predictable Latency Event pending for an NVM Set, then the Predictable Latency Event Aggregate log page includes an entry for that NVM Set. The log page is an ordered list by NVM Set Identifier. For example, if Predictable Latency Events are pending for NVM Set 27, 13, and 17, then the log page shall have entries in numerical order of 13, 17, and 27. A particular NVM Set is removed from this log page after the Get Log Page is completed successfully with the Retain Asynchronous Event bit cleared to \u20180\u2019 for the Predictable Latency Per NVM Set log page for that NVM Set. See page 196 for details.","title":"Predictable Latency Event Aggregate Log Page (0Bh)"},{"location":"Notes%20on%20NVMe%20Log%20Pages/#asymmetric-namespace-access-0ch","text":"NOTE This log is not supported because there is only one namespace on the drive. This log consists of a header describing the log and descriptors containing the asymmetric namespace access information for ANA Groups (refer to section 8.1.2) that contain namespaces that are attached to the controller processing the command. If ANA Reporting (refer to section 8.1) is supported, this log page is supported. ANA Group Descriptors shall be returned in ascending ANA Group Identifier order. If the Index Offset Supported bit is cleared to \u20180\u2019 in the LID Support and Effects data structure for this log page (refer to Figure 204), then: if the RGO bit is cleared to \u20180\u2019 in Command Dword 10, then the LPOL field in Command Dword 12 and the LPOU field in Command Dword 13 of the Get Log Page command should be cleared to 0h. If the Index Offset Supported bit is set to \u20181\u2019 in the LID Supported and Effects data structure for this log page (refer to Figure 204), then: the entry data structure that is indexed is an ANA Group Descriptor (e.g., specifying an index offset of 2 returns this log page starting at the offset of ANA Group Descriptor 1). If the host performs multiple Get Log Page commands to read the ANA log page (e.g., using the LPOL field or the LPOU field), the host should re-read the header of the log page and ensure that the Change Count field in the Asymmetric Namespace Access log matches the original value read. If it does not match, then the data captured is not consistent and the ANA log page should be re-read. See page 197 for details.","title":"Asymmetric Namespace Access (0Ch)"},{"location":"Notes%20on%20NVMe%20Log%20Pages/#persistent-event-log-0dh","text":"NOTE TODO The Persistent Event Log page contains information about significant events not specific to a particular command. The information in this log page shall be retained across power cycles and resets. NVM subsystems should be designed for minimal loss of event information upon power failure. This log consists of a header describing the log and zero or more Persistent Events (refer to section 5.16.1.14.1). This log page is global to the NVM subsystem. A sanitize operation may alter this log page (e.g., remove or modify events to prevent derivation of user data from log page information, refer to section 8.20). The events removed from this log page by a sanitize operation are unspecified. Persistent Event Log events specified in this section should be reported in an order such that more recent events are generally reported earlier in the log data than older events. The method by which the NVM subsystem determines the order in which events occurred is vendor specific. The number of events supported is vendor specific. The supported maximum size for the Persistent Event Log is indicated in the PELS field of the Identify Controller data structure (refer to Figure 275). The number of events supported and the supported maximum size should be large enough that the number of events or the size of the Persistent Event Log data does not reach the maximum supported size over the usable life of the NVM subsystem. The controller shall log all supported events at each event occurrence unless the controller determines that the same event is occurring at a frequency that exceeds a vendor specific threshold for the frequency of event creation. If the same event is occurring at a frequency that exceeds a vendor specific threshold then the vendor may suppress further entries for the same event. A controller may indicate if events have been suppressed in vendor specific event data. See page 199 for details.","title":"Persistent Event Log (0Dh)"},{"location":"Notes%20on%20NVMe%20Log%20Pages/#endurance-group-event-aggregate-0fh","text":"NOTE This is not supported because endurance groups are not in use. This log page indicates if an Endurance Group Event (refer to section 3.2.3) has occurred for a particular Endurance Group. If an Endurance Group Event has occurred, the details of the particular event are included in the Endurance Group Information log page for that Endurance Group. An asynchronous event is generated when an entry for an Endurance Group is newly added to this log page. If there is an enabled Endurance Group Event pending for an Endurance Group, then the Endurance Group Event Aggregate log page includes an entry for that Endurance Group. The log page is an ordered list by Endurance Group Identifier. For example, if Endurance Group Events are pending for Endurance Group 2, 1, and 7, then the log page shall have entries in numerical order of 1, 2, and 7. A particular Endurance Group entry is removed from this log page after the Get Log Page is completed successfully with the Retain Asynchronous Event bit cleared to \u20180\u2019 for the Endurance Group Information log page for that Endurance Group. The log page size is limited by the Endurance Group Identifier Maximum value reported in the Identify Controller data structure (refer to Figure 275). If the host reads beyond the end of the log page, zeroes are returned. The log page is defined in Figure 247. See page 220 for details.","title":"Endurance Group Event Aggregate (0Fh)"},{"location":"Notes%20on%20NVMe%20Log%20Pages/#media-unit-status-10h","text":"NOTE This page is not supported because it is used for NVM sets which are not in use. This log page is used to describe the configuration and wear of Media Units (refer to section 8.3). The log page contains one Media Unit Status Descriptor for each Media Unit accessible by the specified domain. Each Media Unit Status Descriptor (refer to Figure 249) indicates the configuration of the Media Unit (e.g., to which Endurance Group the Media Unit is assigned, to which NVM Set the Media Unit is assigned, to which Channels the Media Unit is attached) and indications of wear (e.g., the Available Spare field and the Percentage Used field). The indications of wear change as the Media Unit is written and read. If the NVM subsystem supports multiple domains, then the controller reports the Media Unit Status log page for the domain specified in the Log Specific Identifier field (refer to Figure 198), if accessible. If the information is not accessible, then the log page is not available (refer to section 8.1.4). If the Log Specific Identifier field is cleared to 0h, then the specified domain is the domain containing the controller that is processing the command. Media Unit Identifier values (refer to Figure 249) begin with 0h and increase sequentially. If the NVM subsystem supports multiple domains, then the Media Unit Identifier values are unique within the specified domain. If the NVM subsystem does not support multiple domains, then the Media Unit Identifier values are unique within the NVM subsystem. Media Unit Status Descriptors are listed in ascending order by Media Unit Identifier. See page 220 for details.","title":"Media Unit Status (10h)"},{"location":"Notes%20on%20NVMe%20Log%20Pages/#supported-capacity-configuration-list-11h","text":"NOTE This log page is not available because the drives do not currently support multiple endurance groups. This log page is used to provide a list of Supported Capacity Configuration Descriptors (refer to Figure 250). Each entry in the list defines a different configuration of Endurance Groups supported by the specified domain. If the NVM subsystem supports multiple domains, then the controller reports the Supported Capacity Configuration List log page for the domain specified in the Log Specific Identifier field (refer to Figure 198), if accessible. If the information is not accessible, then the log page is not available (refer to section 8.1.3). If the Log Specific Identifier field is cleared to 0h, then the specified domain is the domain containing the controller that is processing the command. If the NVM subsystem supports multiple domains, then Capacity Configuration Identifier values are unique within the specified domain. If the NVM subsystem does not support multiple domains, then Capacity Configuration Identifier values are unique within the NVM subsystem. Capacity Configuration Descriptors are listed in ascending order by Capacity Configuration Identifier, and each Capacity Configuration Identifier shall appear only once. For details see page 222","title":"Supported Capacity Configuration List (11h)"},{"location":"Notes%20on%20NVMe%20Log%20Pages/#feature-identifiers-supported-and-effects-12h","text":"NOTE TODO An NVM subsystem may support several interfaces for submitting a Get Log Page command such as an Admin Submission Queue, PCIe VDM Management Endpoint, or SMBus/I2C Management Endpoint (refer the NVM Express Management Interface Specification for details on Management Endpoints) and may have zero or more instances of each of those interfaces. The feature identifiers (FIDs) supported on each instance of each interface may be different. This log page describes the FIDs that are supported on the interface to which the Get Log Page command was submitted and the effects of those features on the state of the NVM subsystem. The log page is defined in Figure 255. Each Feature Identifier\u2019s effects are described in a FID Supported and Effects data structure defined in Figure 256. The features that the controller supports are dependent on the I/O Command Set that is based on: - the I/O Command Set selected in CC.CSS, if CC.CSS is not set to 110b; and - the Command Set Identifier (CSI) field in CDW 14, if CC.CSS is set to 110b. For details see page 225","title":"Feature Identifiers Supported and Effects (12h)"},{"location":"Notes%20on%20NVMe%20Log%20Pages/#nvme-mi-commands-supported-and-effects-13h","text":"This log page describes the Management Interface Command Set commands (refer to the NVMe Management Interface Specification ) that the controller supports using the NVMe-MI Send and NVMe-MI Receive commands and the effects of those Management Interface Command Set commands on the state of the NVM subsystem. The log page is defined in Figure 257. See page 227 for details.","title":"NVMe-MI Commands Supported and Effects (13h)"},{"location":"Notes%20on%20NVMe%20Log%20Pages/#command-and-feature-lockdown-14h","text":"NOTE This command is not supported. It is used for preventing certain commands for security purposes. It is not relevant to performance testing. This log page is used to indicate which commands and Set Features Feature Identifiers are supported to be prohibited from execution using the Command and Feature Lockdown capability (refer to section 8.4) and which commands are currently prohibited if received on an NVM Express controller Admin Submission Queue or received out-of-band on a Management Endpoint (refer to the NVM Express Management Interface Specification). This log page uses the Log Specific Field field (refer to Figure 259) and may use the UUID Index field in the Get Log Page command to specify the scope and content of the list returned in the Command and Feature Identifier List field of this log page. The UUID Index field may be used if the Scope field is set to 2h, allowing returning of vendor specific Set Features Feature Identifier lockdown information. See page 228 for details.","title":"Command and Feature Lockdown (14h)"},{"location":"Notes%20on%20NVMe%20Log%20Pages/#boot-partition-15h","text":"NOTE This command is not supported and does not have any relevance to drive performance. It allows you to see the boot partition of the drive. The Boot Partition Log page provides read only access to the Boot Partition (refer to section 8.2) accessible by this controller through the BPRSEL register (refer to section 3.1.3.14). This log consists of a header describing the Boot Partition and Boot Partition data as defined by Figure 262. The Boot Partition Identifier bit in the Log Specific Field field determines the Boot Partition. A host reading this log page has no effects on the BPINFO (refer to section 3.1.3.13), BPRSEL, and BPMBL (refer to section 3.1.3.15) registers. See page 230 for details.","title":"Boot Partition (15h)"},{"location":"Notes%20on%20NVMe%20Log%20Pages/#rotational-media-information-log-16h","text":"NOTE This is specific to multiple endurance groups so it is not supported (since there is only one) This log page provides rotational media information (refer to section 8.20) for Endurance Groups that store data on rotational media. The information provided is retained across power cycles and resets. The Endurance Group Identifier is specified in the Log Specific Identifier field in Command Dword 11 of the Get Log Page command. If the NVM subsystem does not contain any Endurance Groups that store data on rotational media, then the Rotational Media Information Log should not be supported. See page 231 for details.","title":"Rotational Media Information Log (16h)"},{"location":"Notes%20on%20NVMe%20Log%20Pages/#discovery-log-page-70h","text":"NOTE This log page is not supported. It is specific to NVMe-over-Fabrics (ex: RDMA) The Discovery Log Page shall only be supported by Discovery controllers. The Discovery Log Page shall not be supported by controllers that expose namespaces for NVMe over PCIe or NVMe over Fabrics. The Discovery Log Page provides an inventory of NVM subsystems with which a host may attempt to form an association. The Discovery Log Page may be specific to the host requesting the log. The Discovery Log Page is persistent across power cycles. The Log Page Offset may be used to retrieve specific records. The number of records is returned in the header of the log page. The format for a Discovery Log Page Entry is defined in Figure 264. The format for the Discovery Log Page is defined in Figure 265. A single Get Log Page command used to read the Discovery Log Page shall be atomic. If the host reads the Discovery Log Page using multiple Get Log Page commands the host should ensure that there has not been a change in the contents of the data. The host should read the Discovery Log Page contents in order (i.e., with increasing Log Page Offset values) and then re-read the Generation Counter after the entire log page is transferred. If the Generation Counter does not match the original value read, the host should discard the log page read as the entries may be inconsistent. If the log page contents change during this command sequence, the controller may return a status code of Discover Restart. Every record indicates via the SUBTYPE field if that record is referring to another Discovery Service or if the record indicates an NVM subsystem composed of controllers that may expose namespaces. A referral to another Discovery Service (i.e., SUBTYPE 01h) is a mechanism to find additional Discovery subsystems. An NVM subsystem entry (i.e., SUBTYPE 02h) is a mechanism to find NVM subsystems that contain controllers that may expose namespaces. Referrals shall not be deeper than eight levels. If an NVM subsystem supports the dynamic controller model, then all entries for that NVM subsystem shall have the Controller ID field set to FFFFh. For a particular NVM subsystem port and NVMe Transport address in an NVM subsystem, there shall be no more than one entry with the Controller ID field set to: - FFFFh if that NVM subsystem supports the dynamic controller model; or - FFFEh if that NVM subsystem supports the static controller model. See page 232 for details.","title":"Discovery Log Page (70h)"},{"location":"Notes%20on%20NVMe%20Log%20Pages/#command-syntax","text":"This log uses the nvme-discover command.","title":"Command Syntax"},{"location":"Notes%20on%20NVMe%20Log%20Pages/#reservation-notification-80h","text":"NOTE TODO The Reservation Notification log page reports one log page from a time ordered queue of Reservation Notification log pages, if available. A new Reservation Notification log page is created and added to the end of the queue of reservation notifications whenever an unmasked reservation notification occurs on any namespace that is attached to the controller. The Get Log Page command: returns a data buffer containing a log page corresponding to the oldest log page in the reservation notification queue (i.e., the log page containing the lowest Log Page Count field; accounting for wrapping); and removes that Reservation Notification log page from the queue. If there are no available Reservation Notification log page entries when a Get Log Page command is issued, then an empty log page (i.e., all fields in the log page cleared to 0h) shall be returned. If the controller is unable to store a reservation notification in the Reservation Notification log page due to the size of the queue, that reservation notification is lost. If a reservation notification is lost, then the controller shall increment the Log Page Count field of the last reservation notification in the queue (i.e., the Log Page Count field in the last reservation notification in the queue shall contain the value associated with the most recent reservation notification that has been lost). See page 234 for details.","title":"Reservation Notification (80h)"},{"location":"Notes%20on%20NVMe%20Log%20Pages/#sanitize-status-81h","text":"NOTE This log is not supported but has no bearing on drive performance. The Sanitize Status log page is used to report sanitize operation time estimates and information about the most recent sanitize operation (refer to section 8.20). The Get Log Page command returns a data buffer containing a log page formatted as defined in Figure 267. This log page shall be retained across power cycles and resets. This log page shall contain valid data whenever CSTS.RDY is set to \u20181\u2019. If the Sanitize Capabilities (SANICAP) field in the Identify Controller data structure is not cleared to 0h (i.e., the Sanitize command is supported), then this log page shall be supported. If the Sanitize Capabilities field in the Identify Controller data structure is cleared to 0h, then this log page is reserved. See page 235 for details.","title":"Sanitize Status (81h)"},{"location":"Notes%20on%20NVMe%20Log%20Pages/#command-syntax_1","text":"This log uses the nvme-resv-notif-log command.","title":"Command Syntax"},{"location":"Notes%20on%20NVMe%20Log%20Pages/#other-nvme-cli-commands","text":"","title":"Other NVMe CLI Commands"},{"location":"Notes%20on%20NVMe%20Log%20Pages/#list-all-nvme-drives","text":"nvme list Lists all the NVMe SSDs attached: name, serial number, size, LBA format, and serial","title":"List All NVMe Drives"},{"location":"Notes%20on%20PCIe/","text":"Notes on PCIe Notes on PCIe PCIe Basics and Background How multiple root complexes are handled Interpreting PCIe Device to CPU Locality Information What is the PCIe PHY Human readable overview of how PCIe works How does ID-based Ordering (IDO) Work? How does transaction ordering work? What is a PCIe Root Complex? How does PCIe Enumeration Work? NVMe over PCIe vs Other Protocols What is a PCIe Function? PCIe-Bus and NUMA Node Correlation How does the root complex work? What is PCIe P2P? What is Relaxed Ordering What is a traffic class (TC)? PCIe BAR Register How NVMe Drive Opcodes Work How does SR-IOV work? PCIe Bridge vs Switch PCIe Configuration Space PCIe Switches How to Check CPU Affinity PCIe Basics and Background https://pcisig.com/sites/default/files/files/PCI_Express_Basics_Background.pdf#page=26 How multiple root complexes are handled https://codywu2010.wordpress.com/2015/11/29/how-modern-multi-processor-multi-root-complex-system-assigns-pci-bus-number/ Interpreting PCIe Device to CPU Locality Information https://dshcherb.github.io/2019/02/02/interpreting-pcie-device-to-cpu-locality-information.html What is the PCIe PHY https://www.linkedin.com/pulse/pci-express-depth-physical-layer-luigi-c-filho-/ Human readable overview of how PCIe works http://xillybus.com/tutorials/pci-express-tlp-pcie-primer-tutorial-guide-1/ http://xillybus.com/tutorials/pci-express-tlp-pcie-primer-tutorial-guide-2 How does ID-based Ordering (IDO) Work? https://blog.csdn.net/weixin_48180416/article/details/115790068 How does transaction ordering work? https://blog.csdn.net/weixin_40357487/article/details/120162461?spm=1001.2101.3001.6661.1&utm_medium=distribute.pc_relevant_t0.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-1.no_search_link&depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-1.no_search_link&utm_relevant_index=1 What is a PCIe Root Complex? https://www.quora.com/What-is-a-PCIe-root-complex?share=1 How does PCIe Enumeration Work? https://www.quora.com/What-is-PCIE-enumeration/answer/Satish-Kumar-525?ch=15&oid=31389493&share=44585235&target_type=answer NVMe over PCIe vs Other Protocols https://www.quora.com/Is-NVMe-faster-than-PCIe/answer/Mike-Jones-169?ch=15&oid=193548046&share=a587ff45&target_type=answer What is a PCIe Function? https://www.quora.com/What-is-a-PCIe-function/answer/Udit-Khanna-2?ch=15&oid=58319695&share=c7f066e5&target_type=answer PCIe-Bus and NUMA Node Correlation https://social.msdn.microsoft.com/Forums/en-US/fabb05b7-eb3f-4a7c-91c5-1ced90af3d0c/pciebus-and-numanode-correlation How does the root complex work? https://codywu2010.wordpress.com/2015/11/29/how-modern-multi-processor-multi-root-complex-system-assigns-pci-bus-number/ What is PCIe P2P? https://xilinx.github.io/XRT/master/html/p2p.html What is Relaxed Ordering https://qr.ae/pG6SWe What is a traffic class (TC)? https://www.oreilly.com/library/view/pci-express-system/0321156307/0321156307_ch06lev1sec6.html PCIe BAR Register https://github.com/cirosantilli/linux-kernel-module-cheat/blob/366b1c1af269f56d6a7e6464f2862ba2bc368062/kernel_module/pci.c How NVMe Drive Opcodes Work https://stackoverflow.com/questions/30190050/what-is-the-base-address-register-bar-in-pcie https://stackoverflow.com/questions/19006632/how-is-a-pci-pcie-bar-size-determined BIOS/OS discovers whether PCIe device exists Places the addresses for mmio or I/O port addresses in NVMe drive\u200b's BAR registers (which it figures out from the configuration registers) It seems from the documentation I found NVMe does this through 64bit mmio Driver establishes the admin queue via BAR0. The admin queue's base addresses are in ASQ and ACQ respectively I submit commands to the admin submission queue to establish I/O queues. Send/receive data via I/O queues. How does SR-IOV work? https://docs.microsoft.com/en-us/windows-hardware/drivers/network/overview-of-single-root-i-o-virtualization--sr-iov- Architecture: https://docs.microsoft.com/en-us/windows-hardware/drivers/network/sr-iov-architecture PCIe Bridge vs Switch wke...@gmail.com wrote: I would appreciate of someone can explain the difference between a PCI bridge and a PCI switch. With good ol' PCI, a single bus can have many devices. A PCI bridge is a device that connects multiple buses together, which is something that was very seldom needed. PCI Express looks, for software, very similar to PCI, but is electrically a point-to-point connection, i.e., a PCIe bus has exactly two devices. To connect PCIe with PCI, you need a PCI/PCIe or PCIe/PCI bridge. If you have a single PCIe connector and multiple PCIe devices, you need a PCIe switch. A single PCIe connection still is between exactly two devices, so a PCIe switch consists of a (virtual) PCI bridge for the upstream PCIe connection, and one (virtual) PCI bridge for each downstream PCIe connection. Regards, Clemens PCIe Configuration Space https://docs.oracle.com/cd/E19683-01/806-5222/hwovr-22/#:~:text=The%20PCI%20host%20bridge%20provides,of%20other%20PCI%20bus%20masters. https://bitwiseanne.wordpress.com/2020/05/15/pcie-101-the-root-complex-and-the-endpoint/ PCIe Switches https://linuxhint.com/pcie-switch/#:~:text=PCIe%20switches%20are%20devices%20that,the%20CPU%20alone%20can%20handle. How to Check CPU Affinity [root@r7525 ~]# cat /sys/class/pci_bus/0000\\:00/cpulistaffinity 24-31,88-95 [root@r7525 ~]# lscpu | grep -i numa NUMA node(s): 8 NUMA node0 CPU(s): 0-7,64-71 NUMA node1 CPU(s): 8-15,72-79 NUMA node2 CPU(s): 16-23,80-87 NUMA node3 CPU(s): 24-31,88-95 NUMA node4 CPU(s): 32-39,96-103 NUMA node5 CPU(s): 40-47,104-111 NUMA node6 CPU(s): 48-55,112-119 NUMA node7 CPU(s): 56-63,120-127 You can check the CPU affinity of a PCIe bus and then see what processor it is aligned to by referencing the processor ranges.","title":"Notes on PCIe"},{"location":"Notes%20on%20PCIe/#notes-on-pcie","text":"Notes on PCIe PCIe Basics and Background How multiple root complexes are handled Interpreting PCIe Device to CPU Locality Information What is the PCIe PHY Human readable overview of how PCIe works How does ID-based Ordering (IDO) Work? How does transaction ordering work? What is a PCIe Root Complex? How does PCIe Enumeration Work? NVMe over PCIe vs Other Protocols What is a PCIe Function? PCIe-Bus and NUMA Node Correlation How does the root complex work? What is PCIe P2P? What is Relaxed Ordering What is a traffic class (TC)? PCIe BAR Register How NVMe Drive Opcodes Work How does SR-IOV work? PCIe Bridge vs Switch PCIe Configuration Space PCIe Switches How to Check CPU Affinity","title":"Notes on PCIe"},{"location":"Notes%20on%20PCIe/#pcie-basics-and-background","text":"https://pcisig.com/sites/default/files/files/PCI_Express_Basics_Background.pdf#page=26","title":"PCIe Basics and Background"},{"location":"Notes%20on%20PCIe/#how-multiple-root-complexes-are-handled","text":"https://codywu2010.wordpress.com/2015/11/29/how-modern-multi-processor-multi-root-complex-system-assigns-pci-bus-number/","title":"How multiple root complexes are handled"},{"location":"Notes%20on%20PCIe/#interpreting-pcie-device-to-cpu-locality-information","text":"https://dshcherb.github.io/2019/02/02/interpreting-pcie-device-to-cpu-locality-information.html","title":"Interpreting PCIe Device to CPU Locality Information"},{"location":"Notes%20on%20PCIe/#what-is-the-pcie-phy","text":"https://www.linkedin.com/pulse/pci-express-depth-physical-layer-luigi-c-filho-/","title":"What is the PCIe PHY"},{"location":"Notes%20on%20PCIe/#human-readable-overview-of-how-pcie-works","text":"http://xillybus.com/tutorials/pci-express-tlp-pcie-primer-tutorial-guide-1/ http://xillybus.com/tutorials/pci-express-tlp-pcie-primer-tutorial-guide-2","title":"Human readable overview of how PCIe works"},{"location":"Notes%20on%20PCIe/#how-does-id-based-ordering-ido-work","text":"https://blog.csdn.net/weixin_48180416/article/details/115790068","title":"How does ID-based Ordering (IDO) Work?"},{"location":"Notes%20on%20PCIe/#how-does-transaction-ordering-work","text":"https://blog.csdn.net/weixin_40357487/article/details/120162461?spm=1001.2101.3001.6661.1&utm_medium=distribute.pc_relevant_t0.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-1.no_search_link&depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-1.no_search_link&utm_relevant_index=1","title":"How does transaction ordering work?"},{"location":"Notes%20on%20PCIe/#what-is-a-pcie-root-complex","text":"https://www.quora.com/What-is-a-PCIe-root-complex?share=1","title":"What is a PCIe Root Complex?"},{"location":"Notes%20on%20PCIe/#how-does-pcie-enumeration-work","text":"https://www.quora.com/What-is-PCIE-enumeration/answer/Satish-Kumar-525?ch=15&oid=31389493&share=44585235&target_type=answer","title":"How does PCIe Enumeration Work?"},{"location":"Notes%20on%20PCIe/#nvme-over-pcie-vs-other-protocols","text":"https://www.quora.com/Is-NVMe-faster-than-PCIe/answer/Mike-Jones-169?ch=15&oid=193548046&share=a587ff45&target_type=answer","title":"NVMe over PCIe vs Other Protocols"},{"location":"Notes%20on%20PCIe/#what-is-a-pcie-function","text":"https://www.quora.com/What-is-a-PCIe-function/answer/Udit-Khanna-2?ch=15&oid=58319695&share=c7f066e5&target_type=answer","title":"What is a PCIe Function?"},{"location":"Notes%20on%20PCIe/#pcie-bus-and-numa-node-correlation","text":"https://social.msdn.microsoft.com/Forums/en-US/fabb05b7-eb3f-4a7c-91c5-1ced90af3d0c/pciebus-and-numanode-correlation","title":"PCIe-Bus and NUMA Node Correlation"},{"location":"Notes%20on%20PCIe/#how-does-the-root-complex-work","text":"https://codywu2010.wordpress.com/2015/11/29/how-modern-multi-processor-multi-root-complex-system-assigns-pci-bus-number/","title":"How does the root complex work?"},{"location":"Notes%20on%20PCIe/#what-is-pcie-p2p","text":"https://xilinx.github.io/XRT/master/html/p2p.html","title":"What is PCIe P2P?"},{"location":"Notes%20on%20PCIe/#what-is-relaxed-ordering","text":"https://qr.ae/pG6SWe","title":"What is Relaxed Ordering"},{"location":"Notes%20on%20PCIe/#what-is-a-traffic-class-tc","text":"https://www.oreilly.com/library/view/pci-express-system/0321156307/0321156307_ch06lev1sec6.html","title":"What is a traffic class (TC)?"},{"location":"Notes%20on%20PCIe/#pcie-bar-register","text":"https://github.com/cirosantilli/linux-kernel-module-cheat/blob/366b1c1af269f56d6a7e6464f2862ba2bc368062/kernel_module/pci.c","title":"PCIe BAR Register"},{"location":"Notes%20on%20PCIe/#how-nvme-drive-opcodes-work","text":"https://stackoverflow.com/questions/30190050/what-is-the-base-address-register-bar-in-pcie https://stackoverflow.com/questions/19006632/how-is-a-pci-pcie-bar-size-determined BIOS/OS discovers whether PCIe device exists Places the addresses for mmio or I/O port addresses in NVMe drive\u200b's BAR registers (which it figures out from the configuration registers) It seems from the documentation I found NVMe does this through 64bit mmio Driver establishes the admin queue via BAR0. The admin queue's base addresses are in ASQ and ACQ respectively I submit commands to the admin submission queue to establish I/O queues. Send/receive data via I/O queues.","title":"How NVMe Drive Opcodes Work"},{"location":"Notes%20on%20PCIe/#how-does-sr-iov-work","text":"https://docs.microsoft.com/en-us/windows-hardware/drivers/network/overview-of-single-root-i-o-virtualization--sr-iov- Architecture: https://docs.microsoft.com/en-us/windows-hardware/drivers/network/sr-iov-architecture","title":"How does SR-IOV work?"},{"location":"Notes%20on%20PCIe/#pcie-bridge-vs-switch","text":"wke...@gmail.com wrote: I would appreciate of someone can explain the difference between a PCI bridge and a PCI switch. With good ol' PCI, a single bus can have many devices. A PCI bridge is a device that connects multiple buses together, which is something that was very seldom needed. PCI Express looks, for software, very similar to PCI, but is electrically a point-to-point connection, i.e., a PCIe bus has exactly two devices. To connect PCIe with PCI, you need a PCI/PCIe or PCIe/PCI bridge. If you have a single PCIe connector and multiple PCIe devices, you need a PCIe switch. A single PCIe connection still is between exactly two devices, so a PCIe switch consists of a (virtual) PCI bridge for the upstream PCIe connection, and one (virtual) PCI bridge for each downstream PCIe connection. Regards, Clemens","title":"PCIe Bridge vs Switch"},{"location":"Notes%20on%20PCIe/#pcie-configuration-space","text":"https://docs.oracle.com/cd/E19683-01/806-5222/hwovr-22/#:~:text=The%20PCI%20host%20bridge%20provides,of%20other%20PCI%20bus%20masters. https://bitwiseanne.wordpress.com/2020/05/15/pcie-101-the-root-complex-and-the-endpoint/","title":"PCIe Configuration Space"},{"location":"Notes%20on%20PCIe/#pcie-switches","text":"https://linuxhint.com/pcie-switch/#:~:text=PCIe%20switches%20are%20devices%20that,the%20CPU%20alone%20can%20handle.","title":"PCIe Switches"},{"location":"Notes%20on%20PCIe/#how-to-check-cpu-affinity","text":"[root@r7525 ~]# cat /sys/class/pci_bus/0000\\:00/cpulistaffinity 24-31,88-95 [root@r7525 ~]# lscpu | grep -i numa NUMA node(s): 8 NUMA node0 CPU(s): 0-7,64-71 NUMA node1 CPU(s): 8-15,72-79 NUMA node2 CPU(s): 16-23,80-87 NUMA node3 CPU(s): 24-31,88-95 NUMA node4 CPU(s): 32-39,96-103 NUMA node5 CPU(s): 40-47,104-111 NUMA node6 CPU(s): 48-55,112-119 NUMA node7 CPU(s): 56-63,120-127 You can check the CPU affinity of a PCIe bus and then see what processor it is aligned to by referencing the processor ranges.","title":"How to Check CPU Affinity"},{"location":"Notes%20on%20mdraid%20Performance%20Testing/","text":"Notes on mdraid Performance Testing Notes on mdraid Performance Testing Helpful Resources Helpful Commands Check I/O Scheduler Check Available CPU Governors RAW IO vs Direct IO My Notes What are NUMAs per socket? What is rq_affinity Configuration My Hardware My Configuration FIO Command libaio I/O Depth Direct Ramp Time Time Based readwrite (rw) Name numjobs Blocksize NUMA Memory Policy NUMA CPU Nodes Raw I/O Testing Research P-states and C-States Power performance states (ACPI P states) Processor idle sleep states (ACPI C states) I/O Models Blocking I/O Nonblocking I/O I/O Multiplexing Model Signal Driven I/O Model Asynchronous I/O Model What is aqu-sz From Understanding the Linux Kernel How does VFS Work? The superblock object The inode object The file object The dentry object Block Devices Handling Block Device Sizes Sectors Blocks Segments Generic Block layer My Questions Helpful Resources https://www.amd.com/system/files/TechDocs/56163-PUB.pdf https://www.computerworld.com/article/2785965/raw-disk-i-o.html https://www.cloudbees.com/blog/linux-io-scheduler-tuning https://wiki.ubuntu.com/Kernel/Reference/IOSchedulers http://developer.amd.com/wp-content/resources/56420.pdf https://infohub.delltechnologies.com/l/cpu-best-practices-3/poweredge-numa-nodes-per-socket-1#:~:text=AMD%20servers%20provide%20the%20ability,bank%20into%20two%20equal%20parts. Helpful Commands Check I/O Scheduler # cat /sys/block/sda/queue/scheduler noop [deadline] cfq Check Available CPU Governors cpupower frequency-info --governors analyzing CPU 0: available cpufreq governors: performance powersave RAW IO vs Direct IO Raw I/O is issued directly to disk offsets, bypassing the file system altogether. It has been used by some applications, especially databases, that can manage and cache their own data better than the file system cache. A drawback is more complexity in the software. From Oracle\u2019s official website, input/output (I/O) to a raw partition offers approximately a 5% to 10% performance improvement over I/O to a partition with a file system on it. Direct I/O allows applications to use a file system but bypass the file system cache, for example, by using the O_DIRECT open(2) flag on Linux. This is similar to synchronous writes (but without the guarantees that O_SYNC offers), and it works for reads as well. It isn\u2019t as direct as raw device I/O, since mapping of file offsets to disk offsets must still be performed by file system code, and I/O may also be resized to match the size used by the file system for on-disk layout (its record size) or it may error (EINVAL). My Notes What are NUMAs per socket? What are NUMAs per socket What is rq_affinity https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/performance_tuning_guide/sect-red_hat_enterprise_linux-performance_tuning_guide-storage_and_file_systems-configuration_tools By default, I/O completions can be processed on a different processor than the processor that issued the I/O request. Set rq_affinity to 1 to disable this ability and perform completions only on the processor that issued the I/O request. This can improve the effectiveness of processor data caching. Configuration Initial driver: /lib/modules/4.18.0-305.el8.x86_64/kernel/drivers/md/raid456.ko.xz I think this has been changed? mdraid looks like it has two stripes 12 NVMe drives in one and 12 in the other all in RAID5. They are all numa aligned by some program called map_numa.sh. Must check drive My Hardware Dell R840 12 Intel P4610 NVMe drives are only attached to processors three and four in the split backplane configuartion. My Configuration I checked firmware rev with nvme list to make sure that all drives were the same. If not need to update 1.TODO still need to do Set the CPU governor to performance. You can check the governors with cpupower frequency-info --governors and then set it with cpupower frequency-set --governor performance . You can check the current governor with cat /sys/devices/system/cpu/cpu0/cpufreq/scaling_governor (substitute the CPU number accordingly) FIO Command libaio Linux native asynchronous I/O. Note that Linux may only support queued behavior with non-buffered I/O (set direct=1 or buffered=0). This engine defines engine specific options. I/O Depth Number of I/O units to keep in flight against the file. Note that increasing iodepth beyond 1 will not affect synchronous ioengines (except for small degrees when verify_async is in use). Even async engines may impose OS restrictions causing the desired depth not to be achieved. This may happen on Linux when using libaio and not setting direct=1, since buffered I/O is not async on that OS. Keep an eye on the I/O depth distribution in the fio output to verify that the achieved depth is as expected. Default: 1. Direct If value is true, use non-buffered I/O. This is usually O_DIRECT. Note that OpenBSD and ZFS on Solaris don\u2019t support direct I/O. On Windows the synchronous ioengines don\u2019t support direct I/O. Default: false. See https://stackoverflow.com/a/49462406/4427375 Ramp Time If set, fio will run the specified workload for this amount of time before logging any performance numbers. Useful for letting performance settle before logging results, thus minimizing the runtime required for stable results. Note that the ramp_time is considered lead in time for a job, thus it will increase the total runtime if a special timeout or runtime is specified. When the unit is omitted, the value is given in seconds. Time Based If set, fio will run for the duration of the runtime specified even if the file(s) are completely read or written. It will simply loop over the same workload as many times as the runtime allows. readwrite (rw) Type of I/O pattern. Fio defaults to read if the option is not specified. For the mixed I/O types, the default is to split them 50/50. For certain types of I/O the result may still be skewed a bit, since the speed may be different. It is possible to specify the number of I/Os to do before getting a new offset by appending : to the end of the string given. For a random read, it would look like rw=randread:8 for passing in an offset modifier with a value of 8. If the suffix is used with a sequential I/O pattern, then the value specified will be added to the generated offset for each I/O turning sequential I/O into sequential I/O with holes. For instance, using rw=write:4k will skip 4k for every write. Also see the rw_sequencer option. Name ASCII name of the job. This may be used to override the name printed by fio for this job. Otherwise the job name is used. On the command line this parameter has the special purpose of also signaling the start of a new job. numjobs Create the specified number of clones of this job. Each clone of job is spawned as an independent thread or process. May be used to setup a larger number of threads/processes doing the same thing. Each thread is reported separately; to see statistics for all clones as a whole, use group_reporting in conjunction with new_group. See --max-jobs. Default: 1. Blocksize The block size in bytes used for I/O units. Default: 4096. A single value applies to reads, writes, and trims. Comma-separated values may be specified for reads, writes, and trims. A value not terminated in a comma applies to subsequent types. NUMA Memory Policy Set this job\u2019s memory policy and corresponding NUMA nodes. Format of the arguments: <mode>[:<nodelist>] mode is one of the following memory policies: default, prefer, bind, interleave or local. For default and local memory policies, no node needs to be specified. For prefer, only one node is allowed. For bind and interleave the nodelist may be as follows: a comma delimited list of numbers, A-B ranges, or all. NUMA CPU Nodes Set this job running on specified NUMA nodes\u2019 CPUs. The arguments allow comma delimited list of cpu numbers, A-B ranges, or all. Note, to enable NUMA options support, fio must be built on a system with libnuma-dev(el) installed. Raw I/O Testing Research P-states and C-States These are defined in the ACPI specification. Power performance states (ACPI P states) P-states provide a way to scale the frequency and voltage at which the processor runs so as to reduce the power consumption of the CPU. The number of available P-states can be different for each model of CPU, even those from the same family. Processor idle sleep states (ACPI C states) C-states are states when the CPU has reduced or turned off selected functions. Different processors support different numbers of C-states in which various parts of the CPU are turned off. To better understand the C-states that are supported and exposed, contact the CPU vendor. Generally, higher C-states turn off more parts of the CPU, which significantly reduce power consumption. Processors may have deeper C-states that are not exposed to the operating system. I/O Models Blocking I/O In networking, there is a call to recvfrom which will then lead to a system call into the kernel which will block until data is available. Nonblocking I/O Assuming UDP a call is made to recvfrom and if ther is no data available the kernel sends back EWOULDBLOCK saying no data is available and this is repeated until a datagram is available. This is polling. I/O Multiplexing Model With I/O multiplexing you call select which will block until data is available and then when data is available it ill return that there is return readable. After this you can call recvfrom. The difference is select can read from multiple potential sockets. Signal Driven I/O Model In this model we register a signal handler using the sigaction system call. This will listen for the SIGIO signal. When data is ready our SIGIO handler will be called at which point we have two options. We can call recvfrom from the handler and then pass that data to the main thread OR we can alert the main thread that data is waiting and let it handle it. Asynchronous I/O Model This is the same as signal driven I/O except the thread notifies us when the data has been copied from kernel space to user space. We call aio_read and pass the kernel the fdescriptor, buffer pointer, buffer size (the same three arguments for read), file offset (similar to lseek), and how to notify us when the entire operation is complete. When the copy is complete our signal handler is notified. What is aqu-sz The average queue length of the requests that were issued to the device. From Understanding the Linux Kernel How does VFS Work? The idea behind the Virtual Filesystem is to put a wide range of information in the kernel to represent many different types of filesystems ; there is a field or function to support each operation provided by all real filesystems supported by Linux. For each read, write, or other function called, the kernel substitutes the actual function that supports a native Linux filesystem, the NTFS filesystem, or whatever other filesystem the file is on. Bovet, Daniel P.. Understanding the Linux Kernel (Kindle Locations 14260-14263). O'Reilly Media. Kindle Edition. $ cp /floppy/ TEST /tmp/ test where /floppy is the mount point of an MS-DOS diskette and /tmp is a normal Second Extended Filesystem (Ext2) directory. The VFS is an abstraction layer between the application program and the filesystem implementations (see Figure 12-1( a)). Therefore, the cp program is not required to know the filesystem types of /floppy/ TEST and /tmp/ test. Instead, cp interacts with the VFS by means of generic system calls known to anyone who has done Unix programming (see the section \"File-Handling System Calls\" in Chapter 1); the code executed by cp is shown in Figure 12-1( b). More essentially, the Linux kernel cannot hardcode a particular function to handle an operation such as read( ) or ioctl( ) . Instead, it must use a pointer for each operation; the pointer is made to point to the proper function for the particular filesystem being accessed. Let\u2019s illustrate this concept by showing how the read( ) shown in Figure 12-1 would be translated by the kernel into a call specific to the MS-DOS filesystem. The application\u2019s call to read( ) makes the kernel invoke the corresponding sys_read( ) service routine, like every other system call. The file is represented by a file data structure in kernel memory, as we\u2019ll see later in this chapter. This data structure contains a field called f_op that contains pointers to functions specific to MS-DOS files, including a function that reads a file. sys_read( ) finds the pointer to this function and invokes it. Thus, the application\u2019s read( ) is turned into the rather indirect call: file-> f_op-> read(...); One can think of the common file model as object-oriented, where an object is a software construct that defines both a data structure and the methods that operate on it. For reasons of efficiency, Linux is not coded in an object-oriented language such as C ++. Objects are therefore implemented as plain C data structures with some fields pointing to functions that correspond to the object\u2019s methods. The common file model consists of the following object types: The superblock object Stores information concerning a mounted filesystem. For disk-based filesystems, this object usually corresponds to a filesystem control block stored on disk. The inode object Stores general information about a specific file. For disk-based filesystems, this object usually corresponds to a file control block stored on disk. Each inode object is associated with an inode number, which uniquely identifies the file within the filesystem. The file object Stores information about the interaction between an open file and a process. This information exists only in kernel memory during the period when a process has the file open. The dentry object Stores information about the linking of a directory entry (that is, a particular name of the file) with the corresponding file. Each disk-based filesystem stores this information in its own particular way on disk. The picture below illustrates with a simple example how processes interact with files. Three different processes have opened the same file, two of them using the same hard link. In this case, each of the three processes uses its own file object, while only two dentry objects are required \u2014 one for each hard link. Both dentry objects refer to the same inode object, which identifies the superblock object and, together with the latter, the common disk file. Block Devices Handling We will follow the path of a call to read() through the kernel. The service routine of the read( ) system call activates a suitable VFS function, passing to it a file descriptor and an offset inside the file. The Virtual Filesystem is the upper layer of the block device handling architecture, and it provides a common file model adopted by all filesystems supported by Linux. The VFS function determines if the requested data is already available and, if necessary, how to perform the read operation. Sometimes there is no need to access the data on disk, because the kernel keeps in RAM the data most recently read from \u2014 or written to \u2014 a block device. TODO - investigate pacge cache in chapter 15 and how VFS talks to the cache in chapter 16. Let\u2019s assume that the kernel must read the data from the block device, thus it must determine the physical location of that data. To do this, the kernel relies on the mapping layer , which typically executes two steps: 1.It determines the block size of the filesystem including the file and computes the extent of the requested data in terms of file block numbers . Essentially, the file is seen as split in many blocks, and the kernel determines the numbers (indices relative to the beginning of file) of the blocks containing the requested data. 2.Next, the mapping layer invokes a filesystem-specific function that accesses the file\u2019s disk inode and determines the position of the requested data on disk in terms of logical block numbers. Essentially, the disk is seen as split in blocks, and the kernel determines the numbers (indices relative to the beginning of the disk or partition) corresponding to the blocks storing the requested data. Because a file may be stored in nonadjacent blocks on disk, a data structure stored in the disk inode maps each file block number to a logical block number.[*] However, if the read access was done on a raw block device file, the mapping layer does not invoke a filesystem-specific method; rather, it translates the offset in the block device file to a position inside the disk \u2014 or disk partition \u2014 corresponding to the device file. The kernel can now issue the read operation on the block device. It makes use of the generic block layer , which starts the I/ O operations that transfer the requested data. In general, each I/ O operation involves a group of blocks that are adjacent on disk. Because the requested data is not necessarily adjacent on disk, the generic block layer might start several I/ O operations. Each I/ O operation is represented by a \"block I/ O\ufffd (in short, \"bio\") structure, which collects all information needed by the lower components to satisfy the request. The generic block layer hides the peculiarities of each hardware block device, thus offering an abstract view of the block devices. Because almost all block devices are disks, the generic block layer also provides some general data structures that describe \"disks\" and \"disk partitions.\" Below the generic block layer, the \"I/ O scheduler \" sorts the pending I/ O data transfer requests according to predefined kernel policies. The purpose of the scheduler is to group requests of data that lie near each other on the physical medium. Finally, the block device drivers take care of the actual data transfer by sending suitable commands to the hardware interfaces of the disk controllers. Block Device Sizes There are many kernel components that are concerned with data stored in block devices; each of them manages the disk data using chunks of different length: The controllers of the hardware block devices transfer data in chunks of fixed length called \"sectors.\" Therefore, the I/ O scheduler and the block device drivers must manage sectors of data. The Virtual Filesystem, the mapping layer, and the filesystems group the disk data in logical units called \"blocks.\" A block corresponds to the minimal disk storage unit inside a filesystem. Block device drivers should be able to cope with \"segments\" of data: each segment is a memory page \u2014 or a portion of a memory page \u2014 including chunks of data that are physically adjacent on disk. The disk caches work on \"pages\" of disk data, each of which fits in a page frame. The generic block layer glues together all the upper and lower components, thus it knows about sectors , blocks, segments, and pages of data. Even if there are many different chunks of data, they usually share the same physical RAM cells. For instance, Figure 14-2 shows the layout of a 4,096-byte page. The upper kernel components see the page as composed of four block buffers of 1,024 bytes each. The last three blocks of the page are being transferred by the block device driver, thus they are inserted in a segment covering the last 3,072 bytes of the page. The hard disk controller considers the segment as composed of six 512-byte sectors. Sectors To achieve acceptable performance, hard disks and similar devices transfer several adjacent bytes at once. Each data transfer operation for a block device acts on a group of adjacent bytes called a sector. In the following discussion, we say that groups of bytes are adjacent when they are recorded on the disk surface in such a manner that a single seek operation can access them. Although the physical geometry of a disk is usually very complicated, the hard disk controller accepts commands that refer to the disk as a large array of sectors. In most disk devices, the size of a sector is 512 bytes, although there are devices that use larger sectors (1,024 and 2,048 bytes). Notice that the sector should be considered as the basic unit of data transfer; it is never possible to transfer less than one sector, although most disk devices are capable of transferring several adjacent sectors at once. In Linux, the size of a sector is conventionally set to 512 bytes; if a block device uses larger sectors, the corresponding low-level block device driver will do the necessary conversions. Thus, a group of data stored in a block device is identified on disk by its position \u2014 the index of the first 512-byte sector \u2014 and its length as number of 512-byte sectors. Sector indices are stored in 32- or 64-bit variables of type sector_t. Blocks While the sector is the basic unit of data transfer for the hardware devices, the block is the basic unit of data transfer for the VFS and, consequently, for the filesystems. For example, when the kernel accesses the contents of a file, it must first read from disk a block containing the disk inode of the file (see the section \"Inode Objects\" in Chapter 12). This block on disk corresponds to one or more adjacent sectors, which are looked at by the VFS as a single data unit. In Linux, the block size must be a power of 2 and cannot be larger than a page frame. Moreover, it must be a multiple of the sector size, because each block must include an integral number of sectors. Therefore, on 80 \u00d7 86 architecture, the permitted block sizes are 512, 1,024, 2,048, and 4,096 bytes. The block size is not specific to a block device. When creating a disk-based filesystem, the administrator may select the proper block size. Thus, several partitions on the same disk might make use of different block sizes. Furthermore, each read or write operation issued on a block device file is a \"raw\" access that bypasses the disk-based filesystem; the kernel executes it by using blocks of largest size (4,096 bytes). Each block requires its own block buffer, which is a RAM memory area used by the kernel to store the block\u2019s content. When the kernel reads a block from disk, it fills the corresponding block buffer with the values obtained from the hardware device; similarly, when the kernel writes a block on disk, it updates the corresponding group of adjacent bytes on the hardware device with the actual values of the associated block buffer. The size of a block buffer always matches the size of the corresponding block. Each buffer has a \"buffer head\" descriptor of type buffer_head. This descriptor contains all the information needed by the kernel to know how to handle the buffer; thus, before operating on each buffer, the kernel checks its buffer head. We will give a detailed explanation of all fields of the buffer head in Chapter 15; in the present chapter, however, we will only consider a few fields: b_page, b_data, b_blocknr, and b_bdev. The b_page field stores the page descriptor address of the page frame that includes the block buffer. If the page frame is in high memory, the b_data field stores the offset of the block buffer inside the page; otherwise, it stores the starting linear address of the block buffer itself. The b_blocknr field stores the logical block number (i.e., the index of the block inside the disk partition). Finally, the b_bdev field identifies the block device that is using the buffer head Segments We know that each disk I/ O operation consists of transferring the contents of some adjacent sectors from \u2014 or to \u2014 some RAM locations. In almost all cases, the data transfer is directly performed by the disk controller with a DMA operation (see the section \"Direct Memory Access (DMA)\" in Chapter 13). The block device driver simply triggers the data transfer by sending suitable commands to the disk controller; once the data transfer is finished, the controller raises an interrupt to notify the block device driver. The data transferred by a single DMA operation must belong to sectors that are adjacent on disk. This is a physical constraint: a disk controller that allows DMA transfers to non-adjacent sectors would have a poor transfer rate, because moving a read/ write head on the disk surface is quite a slow operation. Older disk controllers support \"simple\" DMA operations only: in each such operation, data is transferred from or to memory cells that are physically contiguous in RAM. Recent disk controllers, however, may also support the so-called scatter-gather DMA transfers : in each such operation, the data can be transferred from or to several noncontiguous memory areas. For each scatter-gather DMA transfer, the block device driver must send to the disk controller: The initial disk sector number and the total number of sectors to be transferred A list of descriptors of memory areas, each of which consists of an address and a length. The disk controller takes care of the whole data transfer; for instance, in a read operation the controller fetches the data from the adjacent disk sectors and scatters it into the various memory areas. To make use of scatter-gather DMA operations, block device drivers must handle the data in units called segments . A segment is simply a memory page \u2014 or a portion of a memory page \u2014 that includes the data of some adjacent disk sectors. Thus, a scatter-gather DMA operation may involve several segments at once. Notice that a block device driver does not need to know about blocks, block sizes, and block buffers. Thus, even if a segment is seen by the higher levels as a page composed of several block buffers, the block device driver does not care about it. As we\u2019ll see, the generic block layer can merge different segments if the corresponding page frames happen to be contiguous in RAM and the corresponding chunks of disk data are adjacent on disk. The larger memory area resulting from this merge operation is called physical segment. Yet another merge operation is allowed on architectures that handle the mapping between bus addresses and physical addresses through a dedicated bus circuitry (the IO-MMU; see the section \"Direct Memory Access (DMA)\" in Chapter 13). The memory area resulting from this kind of merge operation is called hardware segment . Because we will focus on the 80 \u00d7 86 architecture, which has no such dynamic mapping between bus addresses and physical addresses, we will assume in the rest of this chapter that hardware segments always coincide with physical segments . TODO - need to go read about how DMA works Generic Block layer My Questions - When running FIO, to what extent is disk caching engaged?","title":"Notes on mdraid Performance Testing"},{"location":"Notes%20on%20mdraid%20Performance%20Testing/#notes-on-mdraid-performance-testing","text":"Notes on mdraid Performance Testing Helpful Resources Helpful Commands Check I/O Scheduler Check Available CPU Governors RAW IO vs Direct IO My Notes What are NUMAs per socket? What is rq_affinity Configuration My Hardware My Configuration FIO Command libaio I/O Depth Direct Ramp Time Time Based readwrite (rw) Name numjobs Blocksize NUMA Memory Policy NUMA CPU Nodes Raw I/O Testing Research P-states and C-States Power performance states (ACPI P states) Processor idle sleep states (ACPI C states) I/O Models Blocking I/O Nonblocking I/O I/O Multiplexing Model Signal Driven I/O Model Asynchronous I/O Model What is aqu-sz From Understanding the Linux Kernel How does VFS Work? The superblock object The inode object The file object The dentry object Block Devices Handling Block Device Sizes Sectors Blocks Segments Generic Block layer My Questions","title":"Notes on mdraid Performance Testing"},{"location":"Notes%20on%20mdraid%20Performance%20Testing/#helpful-resources","text":"https://www.amd.com/system/files/TechDocs/56163-PUB.pdf https://www.computerworld.com/article/2785965/raw-disk-i-o.html https://www.cloudbees.com/blog/linux-io-scheduler-tuning https://wiki.ubuntu.com/Kernel/Reference/IOSchedulers http://developer.amd.com/wp-content/resources/56420.pdf https://infohub.delltechnologies.com/l/cpu-best-practices-3/poweredge-numa-nodes-per-socket-1#:~:text=AMD%20servers%20provide%20the%20ability,bank%20into%20two%20equal%20parts.","title":"Helpful Resources"},{"location":"Notes%20on%20mdraid%20Performance%20Testing/#helpful-commands","text":"","title":"Helpful Commands"},{"location":"Notes%20on%20mdraid%20Performance%20Testing/#check-io-scheduler","text":"# cat /sys/block/sda/queue/scheduler noop [deadline] cfq","title":"Check I/O Scheduler"},{"location":"Notes%20on%20mdraid%20Performance%20Testing/#check-available-cpu-governors","text":"cpupower frequency-info --governors analyzing CPU 0: available cpufreq governors: performance powersave","title":"Check Available CPU Governors"},{"location":"Notes%20on%20mdraid%20Performance%20Testing/#raw-io-vs-direct-io","text":"Raw I/O is issued directly to disk offsets, bypassing the file system altogether. It has been used by some applications, especially databases, that can manage and cache their own data better than the file system cache. A drawback is more complexity in the software. From Oracle\u2019s official website, input/output (I/O) to a raw partition offers approximately a 5% to 10% performance improvement over I/O to a partition with a file system on it. Direct I/O allows applications to use a file system but bypass the file system cache, for example, by using the O_DIRECT open(2) flag on Linux. This is similar to synchronous writes (but without the guarantees that O_SYNC offers), and it works for reads as well. It isn\u2019t as direct as raw device I/O, since mapping of file offsets to disk offsets must still be performed by file system code, and I/O may also be resized to match the size used by the file system for on-disk layout (its record size) or it may error (EINVAL).","title":"RAW IO vs Direct IO"},{"location":"Notes%20on%20mdraid%20Performance%20Testing/#my-notes","text":"","title":"My Notes"},{"location":"Notes%20on%20mdraid%20Performance%20Testing/#what-are-numas-per-socket","text":"What are NUMAs per socket","title":"What are NUMAs per socket?"},{"location":"Notes%20on%20mdraid%20Performance%20Testing/#what-is-rq_affinity","text":"https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/performance_tuning_guide/sect-red_hat_enterprise_linux-performance_tuning_guide-storage_and_file_systems-configuration_tools By default, I/O completions can be processed on a different processor than the processor that issued the I/O request. Set rq_affinity to 1 to disable this ability and perform completions only on the processor that issued the I/O request. This can improve the effectiveness of processor data caching.","title":"What is rq_affinity"},{"location":"Notes%20on%20mdraid%20Performance%20Testing/#configuration","text":"Initial driver: /lib/modules/4.18.0-305.el8.x86_64/kernel/drivers/md/raid456.ko.xz I think this has been changed? mdraid looks like it has two stripes 12 NVMe drives in one and 12 in the other all in RAID5. They are all numa aligned by some program called map_numa.sh. Must check drive","title":"Configuration"},{"location":"Notes%20on%20mdraid%20Performance%20Testing/#my-hardware","text":"Dell R840 12 Intel P4610 NVMe drives are only attached to processors three and four in the split backplane configuartion.","title":"My Hardware"},{"location":"Notes%20on%20mdraid%20Performance%20Testing/#my-configuration","text":"I checked firmware rev with nvme list to make sure that all drives were the same. If not need to update 1.TODO still need to do Set the CPU governor to performance. You can check the governors with cpupower frequency-info --governors and then set it with cpupower frequency-set --governor performance . You can check the current governor with cat /sys/devices/system/cpu/cpu0/cpufreq/scaling_governor (substitute the CPU number accordingly)","title":"My Configuration"},{"location":"Notes%20on%20mdraid%20Performance%20Testing/#fio-command","text":"","title":"FIO Command"},{"location":"Notes%20on%20mdraid%20Performance%20Testing/#libaio","text":"Linux native asynchronous I/O. Note that Linux may only support queued behavior with non-buffered I/O (set direct=1 or buffered=0). This engine defines engine specific options.","title":"libaio"},{"location":"Notes%20on%20mdraid%20Performance%20Testing/#io-depth","text":"Number of I/O units to keep in flight against the file. Note that increasing iodepth beyond 1 will not affect synchronous ioengines (except for small degrees when verify_async is in use). Even async engines may impose OS restrictions causing the desired depth not to be achieved. This may happen on Linux when using libaio and not setting direct=1, since buffered I/O is not async on that OS. Keep an eye on the I/O depth distribution in the fio output to verify that the achieved depth is as expected. Default: 1.","title":"I/O Depth"},{"location":"Notes%20on%20mdraid%20Performance%20Testing/#direct","text":"If value is true, use non-buffered I/O. This is usually O_DIRECT. Note that OpenBSD and ZFS on Solaris don\u2019t support direct I/O. On Windows the synchronous ioengines don\u2019t support direct I/O. Default: false. See https://stackoverflow.com/a/49462406/4427375","title":"Direct"},{"location":"Notes%20on%20mdraid%20Performance%20Testing/#ramp-time","text":"If set, fio will run the specified workload for this amount of time before logging any performance numbers. Useful for letting performance settle before logging results, thus minimizing the runtime required for stable results. Note that the ramp_time is considered lead in time for a job, thus it will increase the total runtime if a special timeout or runtime is specified. When the unit is omitted, the value is given in seconds.","title":"Ramp Time"},{"location":"Notes%20on%20mdraid%20Performance%20Testing/#time-based","text":"If set, fio will run for the duration of the runtime specified even if the file(s) are completely read or written. It will simply loop over the same workload as many times as the runtime allows.","title":"Time Based"},{"location":"Notes%20on%20mdraid%20Performance%20Testing/#readwrite-rw","text":"Type of I/O pattern. Fio defaults to read if the option is not specified. For the mixed I/O types, the default is to split them 50/50. For certain types of I/O the result may still be skewed a bit, since the speed may be different. It is possible to specify the number of I/Os to do before getting a new offset by appending : to the end of the string given. For a random read, it would look like rw=randread:8 for passing in an offset modifier with a value of 8. If the suffix is used with a sequential I/O pattern, then the value specified will be added to the generated offset for each I/O turning sequential I/O into sequential I/O with holes. For instance, using rw=write:4k will skip 4k for every write. Also see the rw_sequencer option.","title":"readwrite (rw)"},{"location":"Notes%20on%20mdraid%20Performance%20Testing/#name","text":"ASCII name of the job. This may be used to override the name printed by fio for this job. Otherwise the job name is used. On the command line this parameter has the special purpose of also signaling the start of a new job.","title":"Name"},{"location":"Notes%20on%20mdraid%20Performance%20Testing/#numjobs","text":"Create the specified number of clones of this job. Each clone of job is spawned as an independent thread or process. May be used to setup a larger number of threads/processes doing the same thing. Each thread is reported separately; to see statistics for all clones as a whole, use group_reporting in conjunction with new_group. See --max-jobs. Default: 1.","title":"numjobs"},{"location":"Notes%20on%20mdraid%20Performance%20Testing/#blocksize","text":"The block size in bytes used for I/O units. Default: 4096. A single value applies to reads, writes, and trims. Comma-separated values may be specified for reads, writes, and trims. A value not terminated in a comma applies to subsequent types.","title":"Blocksize"},{"location":"Notes%20on%20mdraid%20Performance%20Testing/#numa-memory-policy","text":"Set this job\u2019s memory policy and corresponding NUMA nodes. Format of the arguments: <mode>[:<nodelist>] mode is one of the following memory policies: default, prefer, bind, interleave or local. For default and local memory policies, no node needs to be specified. For prefer, only one node is allowed. For bind and interleave the nodelist may be as follows: a comma delimited list of numbers, A-B ranges, or all.","title":"NUMA Memory Policy"},{"location":"Notes%20on%20mdraid%20Performance%20Testing/#numa-cpu-nodes","text":"Set this job running on specified NUMA nodes\u2019 CPUs. The arguments allow comma delimited list of cpu numbers, A-B ranges, or all. Note, to enable NUMA options support, fio must be built on a system with libnuma-dev(el) installed.","title":"NUMA CPU Nodes"},{"location":"Notes%20on%20mdraid%20Performance%20Testing/#raw-io-testing","text":"","title":"Raw I/O Testing"},{"location":"Notes%20on%20mdraid%20Performance%20Testing/#research","text":"","title":"Research"},{"location":"Notes%20on%20mdraid%20Performance%20Testing/#p-states-and-c-states","text":"These are defined in the ACPI specification.","title":"P-states and C-States"},{"location":"Notes%20on%20mdraid%20Performance%20Testing/#power-performance-states-acpi-p-states","text":"P-states provide a way to scale the frequency and voltage at which the processor runs so as to reduce the power consumption of the CPU. The number of available P-states can be different for each model of CPU, even those from the same family.","title":"Power performance states (ACPI P states)"},{"location":"Notes%20on%20mdraid%20Performance%20Testing/#processor-idle-sleep-states-acpi-c-states","text":"C-states are states when the CPU has reduced or turned off selected functions. Different processors support different numbers of C-states in which various parts of the CPU are turned off. To better understand the C-states that are supported and exposed, contact the CPU vendor. Generally, higher C-states turn off more parts of the CPU, which significantly reduce power consumption. Processors may have deeper C-states that are not exposed to the operating system.","title":"Processor idle sleep states (ACPI C states)"},{"location":"Notes%20on%20mdraid%20Performance%20Testing/#io-models","text":"","title":"I/O Models"},{"location":"Notes%20on%20mdraid%20Performance%20Testing/#blocking-io","text":"In networking, there is a call to recvfrom which will then lead to a system call into the kernel which will block until data is available.","title":"Blocking I/O"},{"location":"Notes%20on%20mdraid%20Performance%20Testing/#nonblocking-io","text":"Assuming UDP a call is made to recvfrom and if ther is no data available the kernel sends back EWOULDBLOCK saying no data is available and this is repeated until a datagram is available. This is polling.","title":"Nonblocking I/O"},{"location":"Notes%20on%20mdraid%20Performance%20Testing/#io-multiplexing-model","text":"With I/O multiplexing you call select which will block until data is available and then when data is available it ill return that there is return readable. After this you can call recvfrom. The difference is select can read from multiple potential sockets.","title":"I/O Multiplexing Model"},{"location":"Notes%20on%20mdraid%20Performance%20Testing/#signal-driven-io-model","text":"In this model we register a signal handler using the sigaction system call. This will listen for the SIGIO signal. When data is ready our SIGIO handler will be called at which point we have two options. We can call recvfrom from the handler and then pass that data to the main thread OR we can alert the main thread that data is waiting and let it handle it.","title":"Signal Driven I/O Model"},{"location":"Notes%20on%20mdraid%20Performance%20Testing/#asynchronous-io-model","text":"This is the same as signal driven I/O except the thread notifies us when the data has been copied from kernel space to user space. We call aio_read and pass the kernel the fdescriptor, buffer pointer, buffer size (the same three arguments for read), file offset (similar to lseek), and how to notify us when the entire operation is complete. When the copy is complete our signal handler is notified.","title":"Asynchronous I/O Model"},{"location":"Notes%20on%20mdraid%20Performance%20Testing/#what-is-aqu-sz","text":"The average queue length of the requests that were issued to the device.","title":"What is aqu-sz"},{"location":"Notes%20on%20mdraid%20Performance%20Testing/#from-understanding-the-linux-kernel","text":"","title":"From Understanding the Linux Kernel"},{"location":"Notes%20on%20mdraid%20Performance%20Testing/#how-does-vfs-work","text":"The idea behind the Virtual Filesystem is to put a wide range of information in the kernel to represent many different types of filesystems ; there is a field or function to support each operation provided by all real filesystems supported by Linux. For each read, write, or other function called, the kernel substitutes the actual function that supports a native Linux filesystem, the NTFS filesystem, or whatever other filesystem the file is on. Bovet, Daniel P.. Understanding the Linux Kernel (Kindle Locations 14260-14263). O'Reilly Media. Kindle Edition. $ cp /floppy/ TEST /tmp/ test where /floppy is the mount point of an MS-DOS diskette and /tmp is a normal Second Extended Filesystem (Ext2) directory. The VFS is an abstraction layer between the application program and the filesystem implementations (see Figure 12-1( a)). Therefore, the cp program is not required to know the filesystem types of /floppy/ TEST and /tmp/ test. Instead, cp interacts with the VFS by means of generic system calls known to anyone who has done Unix programming (see the section \"File-Handling System Calls\" in Chapter 1); the code executed by cp is shown in Figure 12-1( b). More essentially, the Linux kernel cannot hardcode a particular function to handle an operation such as read( ) or ioctl( ) . Instead, it must use a pointer for each operation; the pointer is made to point to the proper function for the particular filesystem being accessed. Let\u2019s illustrate this concept by showing how the read( ) shown in Figure 12-1 would be translated by the kernel into a call specific to the MS-DOS filesystem. The application\u2019s call to read( ) makes the kernel invoke the corresponding sys_read( ) service routine, like every other system call. The file is represented by a file data structure in kernel memory, as we\u2019ll see later in this chapter. This data structure contains a field called f_op that contains pointers to functions specific to MS-DOS files, including a function that reads a file. sys_read( ) finds the pointer to this function and invokes it. Thus, the application\u2019s read( ) is turned into the rather indirect call: file-> f_op-> read(...); One can think of the common file model as object-oriented, where an object is a software construct that defines both a data structure and the methods that operate on it. For reasons of efficiency, Linux is not coded in an object-oriented language such as C ++. Objects are therefore implemented as plain C data structures with some fields pointing to functions that correspond to the object\u2019s methods. The common file model consists of the following object types:","title":"How does VFS Work?"},{"location":"Notes%20on%20mdraid%20Performance%20Testing/#the-superblock-object","text":"Stores information concerning a mounted filesystem. For disk-based filesystems, this object usually corresponds to a filesystem control block stored on disk.","title":"The superblock object"},{"location":"Notes%20on%20mdraid%20Performance%20Testing/#the-inode-object","text":"Stores general information about a specific file. For disk-based filesystems, this object usually corresponds to a file control block stored on disk. Each inode object is associated with an inode number, which uniquely identifies the file within the filesystem.","title":"The inode object"},{"location":"Notes%20on%20mdraid%20Performance%20Testing/#the-file-object","text":"Stores information about the interaction between an open file and a process. This information exists only in kernel memory during the period when a process has the file open.","title":"The file object"},{"location":"Notes%20on%20mdraid%20Performance%20Testing/#the-dentry-object","text":"Stores information about the linking of a directory entry (that is, a particular name of the file) with the corresponding file. Each disk-based filesystem stores this information in its own particular way on disk. The picture below illustrates with a simple example how processes interact with files. Three different processes have opened the same file, two of them using the same hard link. In this case, each of the three processes uses its own file object, while only two dentry objects are required \u2014 one for each hard link. Both dentry objects refer to the same inode object, which identifies the superblock object and, together with the latter, the common disk file.","title":"The dentry object"},{"location":"Notes%20on%20mdraid%20Performance%20Testing/#block-devices-handling","text":"We will follow the path of a call to read() through the kernel. The service routine of the read( ) system call activates a suitable VFS function, passing to it a file descriptor and an offset inside the file. The Virtual Filesystem is the upper layer of the block device handling architecture, and it provides a common file model adopted by all filesystems supported by Linux. The VFS function determines if the requested data is already available and, if necessary, how to perform the read operation. Sometimes there is no need to access the data on disk, because the kernel keeps in RAM the data most recently read from \u2014 or written to \u2014 a block device. TODO - investigate pacge cache in chapter 15 and how VFS talks to the cache in chapter 16. Let\u2019s assume that the kernel must read the data from the block device, thus it must determine the physical location of that data. To do this, the kernel relies on the mapping layer , which typically executes two steps: 1.It determines the block size of the filesystem including the file and computes the extent of the requested data in terms of file block numbers . Essentially, the file is seen as split in many blocks, and the kernel determines the numbers (indices relative to the beginning of file) of the blocks containing the requested data. 2.Next, the mapping layer invokes a filesystem-specific function that accesses the file\u2019s disk inode and determines the position of the requested data on disk in terms of logical block numbers. Essentially, the disk is seen as split in blocks, and the kernel determines the numbers (indices relative to the beginning of the disk or partition) corresponding to the blocks storing the requested data. Because a file may be stored in nonadjacent blocks on disk, a data structure stored in the disk inode maps each file block number to a logical block number.[*] However, if the read access was done on a raw block device file, the mapping layer does not invoke a filesystem-specific method; rather, it translates the offset in the block device file to a position inside the disk \u2014 or disk partition \u2014 corresponding to the device file. The kernel can now issue the read operation on the block device. It makes use of the generic block layer , which starts the I/ O operations that transfer the requested data. In general, each I/ O operation involves a group of blocks that are adjacent on disk. Because the requested data is not necessarily adjacent on disk, the generic block layer might start several I/ O operations. Each I/ O operation is represented by a \"block I/ O\ufffd (in short, \"bio\") structure, which collects all information needed by the lower components to satisfy the request. The generic block layer hides the peculiarities of each hardware block device, thus offering an abstract view of the block devices. Because almost all block devices are disks, the generic block layer also provides some general data structures that describe \"disks\" and \"disk partitions.\" Below the generic block layer, the \"I/ O scheduler \" sorts the pending I/ O data transfer requests according to predefined kernel policies. The purpose of the scheduler is to group requests of data that lie near each other on the physical medium. Finally, the block device drivers take care of the actual data transfer by sending suitable commands to the hardware interfaces of the disk controllers.","title":"Block Devices Handling"},{"location":"Notes%20on%20mdraid%20Performance%20Testing/#block-device-sizes","text":"There are many kernel components that are concerned with data stored in block devices; each of them manages the disk data using chunks of different length: The controllers of the hardware block devices transfer data in chunks of fixed length called \"sectors.\" Therefore, the I/ O scheduler and the block device drivers must manage sectors of data. The Virtual Filesystem, the mapping layer, and the filesystems group the disk data in logical units called \"blocks.\" A block corresponds to the minimal disk storage unit inside a filesystem. Block device drivers should be able to cope with \"segments\" of data: each segment is a memory page \u2014 or a portion of a memory page \u2014 including chunks of data that are physically adjacent on disk. The disk caches work on \"pages\" of disk data, each of which fits in a page frame. The generic block layer glues together all the upper and lower components, thus it knows about sectors , blocks, segments, and pages of data. Even if there are many different chunks of data, they usually share the same physical RAM cells. For instance, Figure 14-2 shows the layout of a 4,096-byte page. The upper kernel components see the page as composed of four block buffers of 1,024 bytes each. The last three blocks of the page are being transferred by the block device driver, thus they are inserted in a segment covering the last 3,072 bytes of the page. The hard disk controller considers the segment as composed of six 512-byte sectors.","title":"Block Device Sizes"},{"location":"Notes%20on%20mdraid%20Performance%20Testing/#sectors","text":"To achieve acceptable performance, hard disks and similar devices transfer several adjacent bytes at once. Each data transfer operation for a block device acts on a group of adjacent bytes called a sector. In the following discussion, we say that groups of bytes are adjacent when they are recorded on the disk surface in such a manner that a single seek operation can access them. Although the physical geometry of a disk is usually very complicated, the hard disk controller accepts commands that refer to the disk as a large array of sectors. In most disk devices, the size of a sector is 512 bytes, although there are devices that use larger sectors (1,024 and 2,048 bytes). Notice that the sector should be considered as the basic unit of data transfer; it is never possible to transfer less than one sector, although most disk devices are capable of transferring several adjacent sectors at once. In Linux, the size of a sector is conventionally set to 512 bytes; if a block device uses larger sectors, the corresponding low-level block device driver will do the necessary conversions. Thus, a group of data stored in a block device is identified on disk by its position \u2014 the index of the first 512-byte sector \u2014 and its length as number of 512-byte sectors. Sector indices are stored in 32- or 64-bit variables of type sector_t.","title":"Sectors"},{"location":"Notes%20on%20mdraid%20Performance%20Testing/#blocks","text":"While the sector is the basic unit of data transfer for the hardware devices, the block is the basic unit of data transfer for the VFS and, consequently, for the filesystems. For example, when the kernel accesses the contents of a file, it must first read from disk a block containing the disk inode of the file (see the section \"Inode Objects\" in Chapter 12). This block on disk corresponds to one or more adjacent sectors, which are looked at by the VFS as a single data unit. In Linux, the block size must be a power of 2 and cannot be larger than a page frame. Moreover, it must be a multiple of the sector size, because each block must include an integral number of sectors. Therefore, on 80 \u00d7 86 architecture, the permitted block sizes are 512, 1,024, 2,048, and 4,096 bytes. The block size is not specific to a block device. When creating a disk-based filesystem, the administrator may select the proper block size. Thus, several partitions on the same disk might make use of different block sizes. Furthermore, each read or write operation issued on a block device file is a \"raw\" access that bypasses the disk-based filesystem; the kernel executes it by using blocks of largest size (4,096 bytes). Each block requires its own block buffer, which is a RAM memory area used by the kernel to store the block\u2019s content. When the kernel reads a block from disk, it fills the corresponding block buffer with the values obtained from the hardware device; similarly, when the kernel writes a block on disk, it updates the corresponding group of adjacent bytes on the hardware device with the actual values of the associated block buffer. The size of a block buffer always matches the size of the corresponding block. Each buffer has a \"buffer head\" descriptor of type buffer_head. This descriptor contains all the information needed by the kernel to know how to handle the buffer; thus, before operating on each buffer, the kernel checks its buffer head. We will give a detailed explanation of all fields of the buffer head in Chapter 15; in the present chapter, however, we will only consider a few fields: b_page, b_data, b_blocknr, and b_bdev. The b_page field stores the page descriptor address of the page frame that includes the block buffer. If the page frame is in high memory, the b_data field stores the offset of the block buffer inside the page; otherwise, it stores the starting linear address of the block buffer itself. The b_blocknr field stores the logical block number (i.e., the index of the block inside the disk partition). Finally, the b_bdev field identifies the block device that is using the buffer head","title":"Blocks"},{"location":"Notes%20on%20mdraid%20Performance%20Testing/#segments","text":"We know that each disk I/ O operation consists of transferring the contents of some adjacent sectors from \u2014 or to \u2014 some RAM locations. In almost all cases, the data transfer is directly performed by the disk controller with a DMA operation (see the section \"Direct Memory Access (DMA)\" in Chapter 13). The block device driver simply triggers the data transfer by sending suitable commands to the disk controller; once the data transfer is finished, the controller raises an interrupt to notify the block device driver. The data transferred by a single DMA operation must belong to sectors that are adjacent on disk. This is a physical constraint: a disk controller that allows DMA transfers to non-adjacent sectors would have a poor transfer rate, because moving a read/ write head on the disk surface is quite a slow operation. Older disk controllers support \"simple\" DMA operations only: in each such operation, data is transferred from or to memory cells that are physically contiguous in RAM. Recent disk controllers, however, may also support the so-called scatter-gather DMA transfers : in each such operation, the data can be transferred from or to several noncontiguous memory areas. For each scatter-gather DMA transfer, the block device driver must send to the disk controller: The initial disk sector number and the total number of sectors to be transferred A list of descriptors of memory areas, each of which consists of an address and a length. The disk controller takes care of the whole data transfer; for instance, in a read operation the controller fetches the data from the adjacent disk sectors and scatters it into the various memory areas. To make use of scatter-gather DMA operations, block device drivers must handle the data in units called segments . A segment is simply a memory page \u2014 or a portion of a memory page \u2014 that includes the data of some adjacent disk sectors. Thus, a scatter-gather DMA operation may involve several segments at once. Notice that a block device driver does not need to know about blocks, block sizes, and block buffers. Thus, even if a segment is seen by the higher levels as a page composed of several block buffers, the block device driver does not care about it. As we\u2019ll see, the generic block layer can merge different segments if the corresponding page frames happen to be contiguous in RAM and the corresponding chunks of disk data are adjacent on disk. The larger memory area resulting from this merge operation is called physical segment. Yet another merge operation is allowed on architectures that handle the mapping between bus addresses and physical addresses through a dedicated bus circuitry (the IO-MMU; see the section \"Direct Memory Access (DMA)\" in Chapter 13). The memory area resulting from this kind of merge operation is called hardware segment . Because we will focus on the 80 \u00d7 86 architecture, which has no such dynamic mapping between bus addresses and physical addresses, we will assume in the rest of this chapter that hardware segments always coincide with physical segments . TODO - need to go read about how DMA works","title":"Segments"},{"location":"Notes%20on%20mdraid%20Performance%20Testing/#generic-block-layer","text":"","title":"Generic Block layer"},{"location":"Notes%20on%20mdraid%20Performance%20Testing/#my-questions","text":"","title":"My Questions"},{"location":"Notes%20on%20mdraid%20Performance%20Testing/#-when-running-fio-to-what-extent-is-disk-caching-engaged","text":"","title":"- When running FIO, to what extent is disk caching engaged?"},{"location":"Notes%20on%20mdraid%20Performance%20Testing/lstopo/","text":"[root@r8402 ~]# lstopo Machine (187GB total) Package L#0 NUMANode L#0 (P#0 45GB) L3 L#0 (14MB) L2 L#0 (1024KB) + L1d L#0 (32KB) + L1i L#0 (32KB) + Core L#0 PU L#0 (P#0) PU L#1 (P#40) L2 L#1 (1024KB) + L1d L#1 (32KB) + L1i L#1 (32KB) + Core L#1 PU L#2 (P#4) PU L#3 (P#44) L2 L#2 (1024KB) + L1d L#2 (32KB) + L1i L#2 (32KB) + Core L#2 PU L#4 (P#8) PU L#5 (P#48) L2 L#3 (1024KB) + L1d L#3 (32KB) + L1i L#3 (32KB) + Core L#3 PU L#6 (P#12) PU L#7 (P#52) L2 L#4 (1024KB) + L1d L#4 (32KB) + L1i L#4 (32KB) + Core L#4 PU L#8 (P#16) PU L#9 (P#56) L2 L#5 (1024KB) + L1d L#5 (32KB) + L1i L#5 (32KB) + Core L#5 PU L#10 (P#20) PU L#11 (P#60) L2 L#6 (1024KB) + L1d L#6 (32KB) + L1i L#6 (32KB) + Core L#6 PU L#12 (P#24) PU L#13 (P#64) L2 L#7 (1024KB) + L1d L#7 (32KB) + L1i L#7 (32KB) + Core L#7 PU L#14 (P#28) PU L#15 (P#68) L2 L#8 (1024KB) + L1d L#8 (32KB) + L1i L#8 (32KB) + Core L#8 PU L#16 (P#32) PU L#17 (P#72) L2 L#9 (1024KB) + L1d L#9 (32KB) + L1i L#9 (32KB) + Core L#9 PU L#18 (P#36) PU L#19 (P#76) HostBridge PCI 00:11.5 (RAID) PCI 00:17.0 (RAID) PCIBridge PCI 01:00.0 (Ethernet) Net \"eno99\" PCI 01:00.1 (Ethernet) Net \"eno100\" PCIBridge PCIBridge PCI 03:00.0 (VGA) HostBridge PCIBridge PCI 17:00.0 (Ethernet) Net \"eno145\" PCI 17:00.1 (Ethernet) Net \"eno146\" HostBridge PCIBridge PCI 25:00.0 (SATA) Block(Disk) \"sda\" Package L#1 NUMANode L#1 (P#1 47GB) L3 L#1 (14MB) L2 L#10 (1024KB) + L1d L#10 (32KB) + L1i L#10 (32KB) + Core L#10 PU L#20 (P#1) PU L#21 (P#41) L2 L#11 (1024KB) + L1d L#11 (32KB) + L1i L#11 (32KB) + Core L#11 PU L#22 (P#5) PU L#23 (P#45) L2 L#12 (1024KB) + L1d L#12 (32KB) + L1i L#12 (32KB) + Core L#12 PU L#24 (P#9) PU L#25 (P#49) L2 L#13 (1024KB) + L1d L#13 (32KB) + L1i L#13 (32KB) + Core L#13 PU L#26 (P#13) PU L#27 (P#53) L2 L#14 (1024KB) + L1d L#14 (32KB) + L1i L#14 (32KB) + Core L#14 PU L#28 (P#17) PU L#29 (P#57) L2 L#15 (1024KB) + L1d L#15 (32KB) + L1i L#15 (32KB) + Core L#15 PU L#30 (P#21) PU L#31 (P#61) L2 L#16 (1024KB) + L1d L#16 (32KB) + L1i L#16 (32KB) + Core L#16 PU L#32 (P#25) PU L#33 (P#65) L2 L#17 (1024KB) + L1d L#17 (32KB) + L1i L#17 (32KB) + Core L#17 PU L#34 (P#29) PU L#35 (P#69) L2 L#18 (1024KB) + L1d L#18 (32KB) + L1i L#18 (32KB) + Core L#18 PU L#36 (P#33) PU L#37 (P#73) L2 L#19 (1024KB) + L1d L#19 (32KB) + L1i L#19 (32KB) + Core L#19 PU L#38 (P#37) PU L#39 (P#77) HostBridge PCIBridge PCI 48:00.0 (RAID) Package L#2 NUMANode L#2 (P#2 47GB) L3 L#2 (14MB) L2 L#20 (1024KB) + L1d L#20 (32KB) + L1i L#20 (32KB) + Core L#20 PU L#40 (P#2) PU L#41 (P#42) L2 L#21 (1024KB) + L1d L#21 (32KB) + L1i L#21 (32KB) + Core L#21 PU L#42 (P#6) PU L#43 (P#46) L2 L#22 (1024KB) + L1d L#22 (32KB) + L1i L#22 (32KB) + Core L#22 PU L#44 (P#10) PU L#45 (P#50) L2 L#23 (1024KB) + L1d L#23 (32KB) + L1i L#23 (32KB) + Core L#23 PU L#46 (P#14) PU L#47 (P#54) L2 L#24 (1024KB) + L1d L#24 (32KB) + L1i L#24 (32KB) + Core L#24 PU L#48 (P#18) PU L#49 (P#58) L2 L#25 (1024KB) + L1d L#25 (32KB) + L1i L#25 (32KB) + Core L#25 PU L#50 (P#22) PU L#51 (P#62) L2 L#26 (1024KB) + L1d L#26 (32KB) + L1i L#26 (32KB) + Core L#26 PU L#52 (P#26) PU L#53 (P#66) L2 L#27 (1024KB) + L1d L#27 (32KB) + L1i L#27 (32KB) + Core L#27 PU L#54 (P#30) PU L#55 (P#70) L2 L#28 (1024KB) + L1d L#28 (32KB) + L1i L#28 (32KB) + Core L#28 PU L#56 (P#34) PU L#57 (P#74) L2 L#29 (1024KB) + L1d L#29 (32KB) + L1i L#29 (32KB) + Core L#29 PU L#58 (P#38) PU L#59 (P#78) HostBridge PCIBridge PCI 88:00.0 (NVMExp) Block(Disk) \"nvme0n1\" PCIBridge PCI 89:00.0 (NVMExp) Block(Disk) \"nvme1n1\" HostBridge PCIBridge PCI 9b:00.0 (NVMExp) Block(Disk) \"nvme2n1\" PCIBridge PCI 9c:00.0 (NVMExp) Block(Disk) \"nvme3n1\" PCIBridge PCI 9d:00.0 (NVMExp) Block(Disk) \"nvme4n1\" PCIBridge PCI 9e:00.0 (NVMExp) Block(Disk) \"nvme5n1\" Package L#3 NUMANode L#3 (P#3 47GB) L3 L#3 (14MB) L2 L#30 (1024KB) + L1d L#30 (32KB) + L1i L#30 (32KB) + Core L#30 PU L#60 (P#3) PU L#61 (P#43) L2 L#31 (1024KB) + L1d L#31 (32KB) + L1i L#31 (32KB) + Core L#31 PU L#62 (P#7) PU L#63 (P#47) L2 L#32 (1024KB) + L1d L#32 (32KB) + L1i L#32 (32KB) + Core L#32 PU L#64 (P#11) PU L#65 (P#51) L2 L#33 (1024KB) + L1d L#33 (32KB) + L1i L#33 (32KB) + Core L#33 PU L#66 (P#15) PU L#67 (P#55) L2 L#34 (1024KB) + L1d L#34 (32KB) + L1i L#34 (32KB) + Core L#34 PU L#68 (P#19) PU L#69 (P#59) L2 L#35 (1024KB) + L1d L#35 (32KB) + L1i L#35 (32KB) + Core L#35 PU L#70 (P#23) PU L#71 (P#63) L2 L#36 (1024KB) + L1d L#36 (32KB) + L1i L#36 (32KB) + Core L#36 PU L#72 (P#27) PU L#73 (P#67) L2 L#37 (1024KB) + L1d L#37 (32KB) + L1i L#37 (32KB) + Core L#37 PU L#74 (P#31) PU L#75 (P#71) L2 L#38 (1024KB) + L1d L#38 (32KB) + L1i L#38 (32KB) + Core L#38 PU L#76 (P#35) PU L#77 (P#75) L2 L#39 (1024KB) + L1d L#39 (32KB) + L1i L#39 (32KB) + Core L#39 PU L#78 (P#39) PU L#79 (P#79) HostBridge PCIBridge PCI c8:00.0 (NVMExp) Block(Disk) \"nvme6n1\" PCIBridge PCI c9:00.0 (NVMExp) Block(Disk) \"nvme7n1\" PCIBridge PCI ca:00.0 (NVMExp) Block(Disk) \"nvme8n1\" PCIBridge PCI cb:00.0 (NVMExp) Block(Disk) \"nvme9n1\" HostBridge PCIBridge PCI db:00.0 (NVMExp) Block(Disk) \"nvme10n1\" PCIBridge PCI dc:00.0 (NVMExp) Block(Disk) \"nvme11n1\" Misc(MemoryModule) Misc(MemoryModule) Misc(MemoryModule) Misc(MemoryModule) Misc(MemoryModule) Misc(MemoryModule) Misc(MemoryModule) Misc(MemoryModule) Misc(MemoryModule) Misc(MemoryModule) Misc(MemoryModule) Misc(MemoryModule) Misc(MemoryModule) Misc(MemoryModule) Misc(MemoryModule) Misc(MemoryModule) Misc(MemoryModule) Misc(MemoryModule) Misc(MemoryModule) Misc(MemoryModule) Misc(MemoryModule) Misc(MemoryModule) Misc(MemoryModule) Misc(MemoryModule)","title":"Lstopo"},{"location":"Notes%20on%20nodejs/","text":"Notes on nodejs Notes on nodejs Required Javascript Arrow expressions Promises Resolve and Reject Async/Await setInterval() and setTimeout() Node The Node REPL (read-eval-print loop) Running a Program with Node Core Modules Console Module The Process Module The OS Module The Util Module NPM Create a new app nodemon Package Scope Global Packages Installing a Custom Package Modules Exporting Require Using Object Destructuring to be more Selective With require() The Events Module User Input and Output The Error Module Why Error First Callbacks The Buffer Module Readable Streams Further explanation Writable Streams Timers Modules HTTP Server The URL Module Routing Longer Example Returning a Status Code Express Request Object Properties Request Object Methods Response Object Knex.js How does exports.up and exports.down work Seed Files Babel ReactJS Importing React Required Code Components Create a Component Class The Render Function Create a Component Instance Use This in a Class Render Components with Components Importing Files and Exporting Functionality Importing Exporting Component Props Event Handler handleEvent, onEvent, and this.props.onEvent this.props.children Default Properties Component State this.setState from Another Function Component Lifecycle componentDidMount componentWillUnmount componentDidUpdate Stateless Functional Components Function Component Props React Hooks Comparison Class vs Function Update Function Component State Initialize State Use State Setter Outside of JSX Longer Example Set From Previous State Arrays in State Objects in State Longer Example Separate Hooks for Separate States Comparison The Effect Hook - useEffect React Hooks and Component Lifecycle Equivalent componentWillMount for react functional component? Function Component Effects Clean Up Effects Control When Effects are Called Fetch Data from a Server Rules of Hooks Separate Hooks for Separate Effects Stateless Components from Stateful Components Build a Stateful Component Class Don't Update props Child Components Update Their Parents' State More Complex Example Child Components Update Sibling Components One Sibling to Display, Another to Change Style Inline Styles Make a Style Object Variable Share Styles Across Multiple Components Separate Container Components from Presentational Components Create a Container Component Create a Presentational Component propTypes Apply PropTypes PropTypes in Function Components React Forms Input on Change Control vs Uncontrolled Update an Input's Value Set the Input's Initial State Dynamically Rendering Different Components without Switch: the Capitalized Reference Technique React Router BrowserRouter Route Routes Links URL Parameters Nested Routes Pass props to Router Components Good Practices for Calling APIs from ReactJS JSX JSX Elements JSX Elements And Their Surroundings Attributes In JSX Nested JSX JSX Outer Elements Rendering JSX ReactDOM.render() Passing a Variable to ReactDOM.render() class vs className Self-Closing Tags Javascript in JSX Variables in JSX Event Listeners in JSX JSX Conditionals Ternary Operator && .map in JSX List Keys React Create Element Required Javascript Arrow expressions Let\u2019s take a look at the code below. You will see two different functions defined. The first is anonymous (function is not named), and the second is named. When using an arrow expression, we do not use the function declaration. To define an arrow expression you simply use: () => { }. You can pass arguments to an arrow expression between the parenthesis (()). // Defining an anonymous arrow expression that simply logs a string to the console. console.log(() => console.log('Shhh, Im anonymous')); // Defining a named function by creating an arrow expression and saving it to a const variable helloWorld. const helloWorld = (name) => { console.log(`Welcome ${name} to Codecademy, this is an arrow expression.`) }; // Calling the helloWorld() function. helloWorld('Codey'); //Output: Welcome Codey to Codecademy, this is an Arrow Function Expression. Promises A Promise is a JavaScript object that represents the eventual outcome of an asynchronous operation. A Promise has three different outcomes: pending (the result is undefined and the expression is waiting for a result), fulfilled (the promise has been completed successfully and returned a value), and rejected (the promise did not successfully complete, the result is an error object). In the code below a new Promise is being defined and is passed a function that takes two arguments, a fulfilled condition, and a rejected condition. We then log the returned value of the Promise to the console and chain a .catch() method to handle errors. // Creating a new Promise and saving it to the testLuck variable. Two arguments are being passed, one for when the promise resolves, and one for if the promise gets rejected. const testLuck = new Promise((resolve, reject) => { if (Math.random() < 0.5) { resolve('Lucky winner!') } else { reject(new Error('Unlucky!')) } }); testLuck.then(message => { console.log(message) // Log the resolved value of the Promise }).catch(error => { console.error(error) // Log the rejected error of the Promise }); Resolve and Reject The Promise constructor method takes a function parameter called the executor function which runs automatically when the constructor is called. The executor function generally starts an asynchronous operation and dictates how the promise should be settled. The executor function has two function parameters, usually referred to as the resolve() and reject() functions. The resolve() and reject() functions aren\u2019t defined by the programmer. When the Promise constructor runs, JavaScript will pass its own resolve() and reject() functions into the executor function. resolve is a function with one argument. Under the hood, if invoked, resolve() will change the promise\u2019s status from pending to fulfilled, and the promise\u2019s resolved value will be set to the argument passed into resolve(). reject is a function that takes a reason or error as an argument. Under the hood, if invoked, reject() will change the promise\u2019s status from pending to rejected, and the promise\u2019s rejection reason will be set to the argument passed into reject(). Async/Await The async...await syntax allows developers to easily implement Promise-based code. The keyword async used in conjunction with a function declaration creates an async function that returns a Promise. Async functions allow us to use the keyword await to block the event loop until a given Promise resolves or rejects. The await keyword also allows us to assign the resolved value of a Promise to a variable. Let\u2019s take a look at the code below. In the code below an asynchronous arrow expression is defined with the async keyword. In the function body we are creating a new Promise which passes a function that is executed after 5 seconds, we await the Promise to resolve and save the value returned to finalResult, and the output of the Promise is logged to the console. // Creating a new promise that runs the function in the setTimeout after 5 seconds. const newPromise = new Promise((resolve, reject) => { setTimeout(() => resolve(\"All done!\"), 5000); }); // Creating an asynchronous function using an arrow expression and saving it to a the variable asyncFunction. const asyncFunction = async () => { // Awaiting the promise to resolve and saving the result to the variable finalResult. const finalResult = await newPromise; // Logging the result of the promise to the console console.log(finalResult); // Output: All done! } asyncFunction(); setInterval() and setTimeout() In addition to utilizing the async...await syntax, we can also use the setInterval() and setTimeout() functions. In the example code of the previous section, we created a setTimeout() instance in the Promise constructor. The setInterval() function executes a code block at a specified interval, in milliseconds. The setInterval() function requires two arguments: the name of the function (the code block that will be executed), and the number of milliseconds (how often the function will be executed). Optionally, we can pass additional arguments which will be supplied as parameters for the function that will be executed by setInterval(). The setInterval() function will continue to execute until the clearInterval() function is called or the node process is exited. In the code block below, the setInterval() function in the showAlert() function will display an alert box every 5000 milliseconds. // Defining a function that instantiates setInterval const showAlert = () => { // Calling setInterval() and passing a function that shows an alert every 5 seconds. setInterval(() => { alert('I show every 5 seconds!') }, 5000); }; // Calling the newInterval() function that calls the setInterval showAlert(); The setTimeout() function executes a code block after a specified amount of time (in milliseconds) and is only executed once. The setTimeout() function accepts the same arguments as the setInterval() function. Using the clearTimeout() function will prevent the function specified from being executed. In the code block below, a function named showTimeout() is declared as an arrow expression. The setTimeout() function is then defined and displays an alert box after 5 seconds. // Defining a function that calls setTimeout const showTimeout = () => { // Calling setTimeout() that passes a function that shows an alert after 5 seconds. setTimeout(() => { alert('I only show once after 5 seconds!'); }, 5000); }; // Calling the showTimeout() function showTimeout(); Node The Node REPL (read-eval-print loop) REPL is an abbreviation for read\u2013eval\u2013print loop. It\u2019s a program that loops, or repeatedly cycles, through three different states: a read state where the program reads input from a user, the eval state where the program evaluates the user\u2019s input, and the print state where the program prints out its evaluation to a console. Then it loops through these states again. It's just the equivalent of typing python except for javascript. Type node to get to it. To see global vars see Object.keys(global) . You can add to it with global.cat = 'thing' . Print with console.log(global.cat) If you\u2019re familiar with running JavaScript on the browser, you\u2019ve likely encountered the Window object. Here\u2019s one major way that Node differs: try to access the Window object (this will throw an error). The Window object is the JavaScript object in the browser that holds the DOM, since we don\u2019t have a DOM here, there\u2019s no Window object. Running a Program with Node node program Core Modules Include a module: // Require in the 'events' core module: const events = require('events'); Some core modules are actually used inside other core modules. For instance, the util module can be used in the console module to format messages. We\u2019ll cover these two modules in this lesson, as well as two other commonly used core modules: process and os. See all builtin modules: require('module').builtinModules Console Module Since console is a global module, its methods can be accessed from anywhere, and the require() function is not necessary. .log() - prints messages to the terminal .assert() - prints a message to the terminal if the value is falsey console.assert(petsArray.length > 5); .table() - prints out a table in the terminal from an object or array The Process Module Node has a global process object with useful methods and information about the current process. The console.log() method is a \"thin wrapper\" on the .stdout.write() method of the process object. The process.env property is an object which stores and controls information about the environment in which the process is currently running. For example, the process.env object contains a PWD property which holds a string with the directory in which the current process is located. It can be useful to have some if/else logic in a program depending on the current environment\u2014 a web application in a development phase might perform different tasks than when it\u2019s live to users. We could store this information on the process.env. One convention is to add a property to process.env with the key NODE_ENV and a value of either production or development. if (process.env.NODE_ENV === 'development'){ console.log('Testing! Testing! Does everything work?'); } The process.memoryUsage() returns information on the CPU demands of the current process. It returns a property that looks similar to this: { rss: 26247168, heapTotal: 5767168, heapUsed: 3573032, external: 8772 } process.argv holds an array of command line values provided when the current process was initiated. The OS Module const os = require('os'); os.type() \u2014 to return the computer\u2019s operating system. os.arch() \u2014 to return the operating system CPU architecture. os.networkInterfaces() \u2014 to return information about the network interfaces of the computer, such as IP and MAC address. os.homedir() \u2014 to return the current user\u2019s home directory. os.hostname() \u2014 to return the hostname of the operating system. os.uptime() \u2014 to return the system uptime, in seconds. Create an empty object const object = {}; Instantiate a dictionary: const os = require('os'); const server = {type: os.type(), architecture: os.arch(), uptime: os.uptime()}; console.table(server) The Util Module Developers sometimes classify outlier functions used to maintain code and debug certain aspects of a program\u2019s functionality as utility functions. Utility functions don\u2019t necessarily create new functionality in a program, but you can think of them as internal tools used to maintain and debug your code. The Node.js util core module contains methods specifically designed for these purposes. const util = require('util'); Get the type of an object : const util = require('util'); const today = new Date(); const earthDay = 'April 22, 2022'; console.log(util.types.isDate(today)); console.log(util.types.isDate(earthDay)); Turn callback functions into promises : Another important util method is .promisify(), which turns callback functions into promises. As you know, asynchronous programming is essential to Node.js. In the beginning, this asynchrony was achieved using error-first callback functions, which are still very prevalent in the Node ecosystem today. But since promises are often preferred over callbacks and especially nested callbacks, Node offers a way to turn these into promises. Let\u2019s take a look: function getUser (id, callback) { return setTimeout(() => { if (id === 5) { callback(null, { nickname: 'Teddy' }) } else { callback(new Error('User not found')) } }, 1000) } function callback (error, user) { if (error) { console.error(error.message) process.exit(1) } console.log(`User found! Their nickname is: ${user.nickname}`) } getUser(1, callback) // -> `User not found` getUser(5, callback) // -> `User found! Their nickname is: Teddy` You can convert the above to: const getUserPromise = util.promisify(getUser); getUserPromise(id) .then((user) => { console.log(`User found! Their nickname is: ${user.nickname}`); }) .catch((error) => { console.log('User not found', error); }); getUser(1) // -> `User not found` getUser(5) // -> `User found! Their nickname is: Teddy` We declare a getUserPromise variable that stores the getUser method turned into a promise using the .promisify() method. With that in place, we\u2019re able to use getUserPromise with .then() and .catch() methods (or we could also use the async...await syntax here) to resolve the promise returned or catch any errors. NPM Create a new app npm init Add -y to answer yes to everything. This will generate a package.json file: { \"name\": \"my-project\", \"version\": \"1.0.0\", \"description\": \"a basic project\", \"main\": \"index.js\", \"scripts\": { \"test\": \"echo \\\"Error: no test specified\\\" && exit 1\" }, \"author\": \"Super Coder\", \"license\": \"ISC\", \"dependencies\": { \"express\": \"^4.17.1\" }, } nodemon Automatically restart a program when a file changes. npm install nodemon The npm i <package name> command installs a package locally in a folder called node_modules/ which is created in the project directory that you ran the command from. In addition, the newly installed package will be added to the package.json file. Package Scope While most dependencies play a direct role in the functionality of your application, development dependencies are used for the purpose of making development easier or more efficient. In fact, the nodemon package is actually better suited as a development dependency since it makes developers\u2019 lives easier but makes no changes to the app itself. To install nodemon as a development dependency, we can add the --save-dev flag, or its alias, -D. npm install nodemon --save-dev Development dependencies are listed in the \"devDependencies\" field of the package.json file. This indicates that the package is being used specifically for development and will not be included in a production release of the project. { \"name\": \"my-project\", \"version\": \"1.0.0\", \"description\": \"a basic project\", \"main\": \"index.js\", \"scripts\": { \"test\": \"echo \\\"Error: no test specified\\\" && exit 1\" }, \"author\": \"\", \"license\": \"ISC\", \"dependencies\": { \"express\": \"^4.17.1\" }, \"devDependencies\": { \"nodemon\": \"^2.0.13\" } } Global Packages Typically, packages installed this way will be used in the command-line rather than imported into a project\u2019s code. One such example is the http-server package which allows you to spin up a zero-configuration server from anywhere in the command-line. To install a package globally, use the -g flag with the installation command: npm install http-server -g http-server is a good package to install globally since it is a general command-line utility and its purpose is not linked to any specific functionality within an app. Unlike local package dependencies or development dependencies, packages installed globally will not be listed in a projects package.json file and they will be stored in a separate global node_modules/ folder. Installing a Custom Package If you want to give someone else your package you can provide the package.json file and then they can install with npm i . Add --production to leave out the dev dependencies. Modules There are multiple ways of implementing modules depending on the runtime environment in which your code is executed. In JavaScript, there are two runtime environments and each has a preferred module implementation: The Node runtime environment and the module.exports and require() syntax. The browser\u2019s runtime environment and the ES6 import/export syntax. Exporting /* converters.js */ function celsiusToFahrenheit(celsius) { return celsius * (9/5) + 32; } module.exports.celsiusToFahrenheit = celsiusToFahrenheit; module.exports.fahrenheitToCelsius = function(fahrenheit) { return (fahrenheit - 32) * (5/9); }; At the top of the new file, converters.js, the function celsiusToFahrenheit() is declared. On the next line of code, the first approach for exporting a function from a module is shown. In this case, the already-defined function celsiusToFahrenheit() is assigned to module.exports.celsiusToFahrenheit. Below, an alternative approach for exporting a function from a module is shown. In this second case, a new function expression is declared and assigned to module.exports.fahrenheitToCelsius. This new method is designed to convert Fahrenheit values back to Celsius. Both approaches successfully store a function within the module.exports object. module.exports is an object that is built-in to the Node.js runtime environment. Other files can now import this object, and make use of these two functions, with another feature that is built-in to the Node.js runtime environment: the require() function. Require The require() function accepts a string as an argument. That string provides the file path to the module you would like to import. Let\u2019s update water-limits.js such that it uses require() to import the .celsiusToFahrenheit() method from the module.exports object within converters.js: /* water-limits.js */ const converters = require('./converters.js'); const freezingPointC = 0; const boilingPointC = 100; const freezingPointF = converters.celsiusToFahrenheit(freezingPointC); const boilingPointF = converters.celsiusToFahrenheit(boilingPointC); console.log(`The freezing point of water in Fahrenheit is ${freezingPointF}`); console.log(`The boiling point of water in Fahrenheit is ${boilingPointF}`); Using Object Destructuring to be more Selective With require() In many cases, modules will export a large number of functions but only one or two of them are needed. You can use object destructuring to extract only the needed functions. Let\u2019s update celsius-to-fahrenheit.js and only extract the .celsiusToFahrenheit() method, leaving .fahrenheitToCelsius() behind: /* celsius-to-fahrenheit.js */ const { celsiusToFahrenheit } = require('./converters.js'); const celsiusInput = process.argv[2]; const fahrenheitValue = celsiusToFahrenheit(celsiusInput); console.log(`${celsiusInput} degrees Celsius = ${fahrenheitValue} degrees Fahrenheit`); Notice that the first line used to be const converters = require('./converters.js'); and now it is specifying the exported function. The Events Module Node provides an EventEmitter class which we can access by requiring in the events core module: // Require in the 'events' core module let events = require('events'); // Create an instance of the EventEmitter class let myEmitter = new events.EventEmitter(); Each event emitter instance has an .on() method which assigns a listener callback function to a named event. The .on() method takes as its first argument the name of the event as a string and, as its second argument, the listener callback function. Each event emitter instance also has an .emit() method which announces a named event has occurred. The .emit() method takes as its first argument the name of the event as a string and, as its second argument, the data that should be passed let newUserListener = (data) => { console.log(`We have a new user: ${data}.`); }; // Assign the newUserListener function as the listener callback for 'new user' events myEmitter.on('new user', newUserListener) // Emit a 'new user' event myEmitter.emit('new user', 'Lily Pad') //newUserListener will be invoked with 'Lily Pad' Note There is no link between the variable data in the constructer for the event emitter and the new user name. User Input and Output Notice that for user input and output for something like stdin what you're really doing is registering a callback and then calling it on user input. Ex: process.stdin.on('data', (userInput) => { let input = userInput.toString() console.log(input) }); Notice the on and then here we're just defining an anonymous function. The Error Module The Node environment\u2019s error module has all the standard JavaScript errors such as EvalError, SyntaxError, RangeError, ReferenceError, TypeError, and URIError as well as the JavaScript Error class for creating new error instances. Within our own code, we can generate errors and throw them, and, with synchronous code in Node, we can use error handling techniques such as try...catch statements. Note that the error module is within the global scope\u2014there is no need to import the module with the require() statement. Many asynchronous Node APIs use error-first callback functions\u2014callback functions which have an error as the first expected argument and the data as the second argument. If the asynchronous task results in an error, it will be passed in as the first argument to the callback function. If no error was thrown, the first argument will be undefined. const errorFirstCallback = (err, data) => { if (err) { console.log(`There WAS an error: ${err}`); } else { // err was falsy console.log(`There was NO error. Event data: ${data}`); } } Why Error First Callbacks You need this because if you try something like: const api = require('./api.js'); // Not an error-first callback let callbackFunc = (data) => { console.log(`Something went right. Data: ${data}\\n`); }; try { api.naiveErrorProneAsyncFunction('problematic input', callbackFunc); } catch(err) { console.log(`Something went wrong. ${err}\\n`); } then the try-catch won't work because the error is thrown in the context of the separate thread spawned asynchronously and subsequently never caught because Javascript is a garbage programming language. The Buffer Module In Node.js, the Buffer module is used to handle binary data. The Buffer module is within the global scope, which means that Buffer objects can be accessed anywhere in the environment without importing the module with require(). A Buffer object represents a fixed amount of memory that can\u2019t be resized. Buffer objects are similar to an array of integers where each element in the array represents a byte of data. The buffer object will have a range of integers from 0 to 255 inclusive. The Buffer module provides a variety of methods to handle the binary data such as .alloc(), .toString(), .from(), and .concat(). The .alloc() method creates a new Buffer object with the size specified as the first parameter. .alloc() accepts three arguments: Size: Required. The size of the buffer Fill: Optional. A value to fill the buffer with. Default is 0. Encoding: Optional. Default is UTF-8. const buffer = Buffer.alloc(5); console.log(buffer); // Ouput: [0, 0, 0, 0, 0] The .toString() method translates the Buffer object into a human-readable string. It accepts three optional arguments: Encoding: Default is UTF-8. Start: The byte offset to begin translating in the Buffer object. Default is 0. End: The byte offset to end translating in the Buffer object. Default is the length of the buffer. The start and end of the buffer are similar to the start and end of an array, where the first element is 0 and increments upwards. const buffer = Buffer.alloc(5, 'a'); console.log(buffer.toString()); // Output: aaaaa The .from() method is provided to create a new Buffer object from the specified string, array, or buffer. The method accepts two arguments: Object: Required. An object to fill the buffer with. Encoding: Optional. Default is UTF-8. const buffer = Buffer.from('hello'); console.log(buffer); // Output: [104, 101, 108, 108, 111] The .concat() method joins all buffer objects passed in an array into one Buffer object. .concat() comes in handy because a Buffer object can\u2019t be resized. This method accepts two arguments: Array: Required. An array containing Buffer objects. Length: Optional. Specifies the length of the concatenated buffer. const buffer1 = Buffer.from('hello'); // Output: [104, 101, 108, 108, 111] const buffer2 = Buffer.from('world'); // Output:[119, 111, 114, 108, 100] const array = [buffer1, buffer2]; const bufferConcat = Buffer.concat(array); console.log(bufferConcat); // Output: [104, 101, 108, 108, 111, 119, 111, 114, 108, 100] Readable Streams const readline = require('readline'); const fs = require('fs'); const myInterface = readline.createInterface({ input: fs.createReadStream('shoppingList.txt') }); const printData = (data) => { console.log(`Item: ${data}`); }; myInterface.on('line', printData); Further explanation One of the simplest uses of streams is reading and writing to files line-by-line. To read files line-by-line, we can use the .createInterface() method from the readline core module. .createInterface() returns an EventEmitter set up to emit 'line' events: const readline = require('readline'); const fs = require('fs'); const myInterface = readline.createInterface({ input: fs.createReadStream('text.txt') }); myInterface.on('line', (fileLine) => { console.log(`The line read: ${fileLine}`); }); Let\u2019s walk through the above code: We require in the readline and fs core modules. We assign to myInterface the returned value from invoking readline.createInterface() with an object containing our designated input. We set our input to fs.createReadStream('text.txt') which will create a stream from the text.txt file. Next we assign a listener callback to execute when line events are emitted. A 'line' event will be emitted after each line from the file is read. Our listener callback will log to the console 'The line read: [fileLine]', where [fileLine] is the line just read. Writable Streams const readline = require('readline'); const fs = require('fs'); const myInterface = readline.createInterface({ input: fs.createReadStream('shoppingList.txt') }); const fileStream = fs.createWriteStream('shoppingResults.txt'); let transformData = (line) => { fileStream.write(`They were out of: ${line}\\n`); }; myInterface.on('line', transformData); Timers Modules You may already be familiar with some timer functions such as, setTimeout() and setInterval(). Timer functions in Node.js behave similarly to how they work in front-end JavaScript programs, but the difference is that they are added to the Node.js event loop. This means that the timer functions are scheduled and put into a queue. This queue is processed at every iteration of the event loop. If a timer function is executed outside of a module, the behavior will be random (non-deterministic). The setImmediate() function is often compared with the setTimeout() function. When setImmediate() is called, it executes the specified callback function after the current (poll phase) is completed. The method accepts two parameters: the callback function (required) and arguments for the callback function (optional). If you instantiate multiple setImmediate() functions, they will be queued for execution in the order that they were created. HTTP Server To process HTTP requests in JavaScript and Node.js, we can use the built-in http module. This core module is key in leveraging Node.js networking and is extremely useful in creating HTTP servers and processing HTTP requests. The http module comes with various methods that are useful when engaging with HTTP network requests. One of the most commonly used methods within the http module is the .createServer() method. This method is responsible for doing exactly what its namesake implies; it creates an HTTP server. To implement this method to create a server, the following code can be used: const server = http.createServer((req, res) => { res.end('Server is running!'); }); server.listen(8080, () => { const { address, port } = server.address(); console.log(`Server is listening on: http://${address}:${port}`); }) The .createServer() method takes a single argument in the form of a callback function. This callback function has two primary arguments; the request (commonly written as req) and the response (commonly written as res). The req object contains all of the information about an HTTP request ingested by the server. It exposes information such as the HTTP method (GET, POST, etc.), the pathname, headers, body, and so on. The res object contains methods and properties pertaining to the generation of a response by the HTTP server. This object contains methods such as .setHeader() (sets HTTP headers on the response), .statusCode (set the status code of the response), and .end() (dispatches the response to the client who made the request). In the example above, we use the .end() method to send the string \u2018Server is Running!\u2019 to the client, which will display on the web page. Once the .createServer() method has instantiated the server, it must begin listening for connections. This final step is accomplished by the .listen() method on the server instance. This method takes a port number as the first argument, which tells the server to listen for connections at the given port number. In our example above, the server has been set to listen on port 8080. Additionally, the .listen() method takes an optional callback function as a second argument, allowing it to carry out a task after the server has successfully started. Using this simple .createServer() method, in conjunction with the callback, provides the ability to process HTTP requests dynamically and dispatch responses back to their callers. The URL Module Typically, an HTTP server will require information from the request URL to accurately process a request. This request URL is located on the url property contained within the req object itself. To parse the different parts of this URL easily, Node.js provides the built-in url module. The core of the url module revolves around the URL class. A new URL object can be instantiated using the URL class as follows: const url = new URL('https://www.example.com/p/a/t/h?query=string'); Once instantiated, different parts of the URL can be accessed and modified via various properties, which include: hostname: Gets and sets the host name portion of the URL. pathname: Gets and sets the path portion of the URL. searchParams: Gets the search parameter object representing the query parameters contained within the URL. Returns an instance of the URLSearchParams class. You might recognize the URL and URLSearchParams classes if you are familiar with browser-based JavaScript. It\u2019s because they are actually the same thing! These classes are defined by the WHATWG URL specification. Both the browser and Node.js implement this API, which means developers can have a similar developer experience working with both client and server-side JavaScript. Using these properties, one can break the URL down into easily usable parts for processing the request. const host = url.hostname; // example.com const pathname = url.pathname; // /p/a/t/h const searchParams = url.searchParams; // {query: 'string'} While the url module can be used to deconstruct a URL into its constituent parts, it can also be used to construct a URL. Constructing a URL via this method relies on most of the same properties listed above to set values on the URL instead of retrieving them. This can be done by setting each of these values equal to a value for the newly constructed URL. Once all parts of the URL have been added, the composed URL can be obtained using the .toString() method. const createdUrl = new URL('https://www.example.com'); createdUrl.pathname = '/p/a/t/h'; createdUrl.search = '?query=string'; createUrl.toString(); // Creates https://www.example.com/p/a/t/h?query=string Routing To process and respond to requests appropriately, servers need to do more than look at a request and dispatch a response. Internally, a server needs to maintain a way to handle each request based on specific criteria such as method, pathname, etc. The process of handling requests in specific ways based on the information provided within the request is known as routing. The method is one important piece of information that can be used to route requests. Since each HTTP request contains a method such as GET and POST, it is a great way to discern different classes of requests based on the action intended for the server to carry out. Thus, all GET requests could be routed to a specific function for handling, while all POST requests are routed to another function to be handled. This also allows for the logical co-location of processing code with the specific verb to be handled. const server = http.createServer((req, res) => { const { method } = req; switch(method) { case 'GET': return handleGetRequest(req, res); case 'POST': return handlePostRequest(req, res); case 'DELETE': return handleDeleteRequest(req, res); case 'PUT': return handlePutRequest(req, res); default: throw new Error(`Unsupported request method: ${method}`); } }) In the above example, the HTTP method property is destructured from the req object and used to conditionally invoke a handler function built specifically for handling those types of requests. This is great at first glance, but it should soon become apparent that the routing is not specific enough. After all, how will one GET request be distinguished from another? We can distinguish one request from another of the same method through the use of the pathname. The pathname allows the server to understand what resource is being targeted. Let\u2019s take a look at the handleGetRequest handler function. function handleGetRequest(req, res) { const { pathname } = new URL(req.url); let data = {}; if (pathname === '/projects') { data = await getProjects(); res.setHeader('Content-Type', 'application/json'); return res.end(JSON.stringify(data)); } res.statusCode = 404; return res.end('Requested resource does not exist'); } Within the handleGetRequest() function, the pathname is being checked to match a known resource, '/projects'. If the pathname matches, the resource data is fetched and then subsequently dispatched from the server as a successful response. Otherwise, the .statusCode property is set to 404, indicating that the resource is not found, and a corresponding error message is dispatched. This pattern can be extrapolated to any number of conditional resource matches, allowing the server to handle many different types of requests to different resources. Longer Example const http = require('http'); // Handle get request const handleGetRequest = (req, res) => { const pathname = req.url; if (pathname === '/users') { res.end(JSON.stringify([])); } } // Creates server instance const server = http.createServer((req, res) => { const { method } = req; switch(method) { case 'GET': return handleGetRequest(req, res); default: throw new Error(`Unsupported request method: ${method}`); } }); // Starts server listening on specified port server.listen(4001, () => { const { address, port } = server.address(); console.log(`Server is listening on: http://${address}:${port}`); }); Returning a Status Code const http = require('http'); const handleGetRequest = (req, res) => { res.statusCode = 200; return res.end(JSON.stringify({ data: [] })); } const handlePostRequest = (req, res) => { res.statusCode = 500; return res.end(\"Unable to create record\"); } // Creates server instance const server = http.createServer((req, res) => { const { method } = req; switch(method) { case 'GET': return handleGetRequest(req, res); case 'POST': return handlePostRequest(req, res); default: throw new Error(`Unsupported request method: ${method}`); } }); // Starts server listening on specified port server.listen(4001, () => { const { address, port } = server.address(); console.log(`Server is listening on: http://${address}:${port}`); }); Express Request Object Properties Index Properties Description 1. req.app This is used to hold a reference to the instance of the express application that is using the middleware. 2. req.baseurl It specifies the URL path on which a router instance was mounted. 3. req.body It contains key-value pairs of data submitted in the request body. By default, it is undefined, and is populated when you use body-parsing middleware such as body-parser. 4. req.cookies When we use cookie-parser middleware, this property is an object that contains cookies sent by the request. 5. req.fresh It specifies that the request is \"fresh.\" it is the opposite of req.stale. 6. req.hostname It contains the hostname from the \"host\" http header. 7. req.ip It specifies the remote IP address of the request. 8. req.ips When the trust proxy setting is true, this property contains an array of IP addresses specified in the ?x-forwarded-for? request header. 9. req.originalurl This property is much like req.url; however, it retains the original request URL, allowing you to rewrite req.url freely for internal routing purposes. 10. req.params An object containing properties mapped to the named route ?parameters?. For example, if you have the route /user/:name, then the \"name\" property is available as req.params.name. This object defaults to {}. 11. req.path It contains the path part of the request URL. 12. req.protocol The request protocol string, \"http\" or \"https\" when requested with TLS. 13. req.query An object containing a property for each query string parameter in the route. 14. req.route The currently-matched route, a string. 15. req.secure A Boolean that is true if a TLS connection is established. 16. req.signedcookies When using cookie-parser middleware, this property contains signed cookies sent by the request, unsigned and ready for use. 17. req.stale It indicates whether the request is \"stale,\" and is the opposite of req.fresh. 18. req.subdomains It represents an array of subdomains in the domain name of the request. 19. req.xhr A Boolean value that is true if the request's \"x-requested-with\" header field is \"xmlhttprequest\", indicating that the request was issued by a client library such as jQuery Request Object Methods req.accepts This method is used to check whether the specified content types are acceptable, based on the request's Accept HTTP header field. req.accepts('html'); //=>?html? req.accepts('text/html'); // => ?text/html? req.get(field) This method returns the specified HTTP request header field. req.get('Content-Type'); // => \"text/plain\" req.get('content-type'); // => \"text/plain\" req.get('Something'); // => undefined req.is(type) // With Content-Type: text/html; charset=utf-8 req.is('html'); req.is('text/html'); req.is('text/*'); // => true req.param(name [,defaultValue]) This method is used to fetch the value of param name when present. // ?name=sasha req.param('name') // => \"sasha\" // POST name=sasha req.param('name') // => \"sasha\" // /user/sasha for /user/:name req.param('name') // => \"sasha\" Response Object Knex.js How does exports.up and exports.down work http://perkframework.com/v1/guides/database-migrations-knex.html Seed Files A seed file allows you to add data into your database without having to manually add it. This is most frequently used for database initialization or loading demo data. Babel What is Babel: https://babeljs.io/docs/en/ ReactJS Importing React Required Code import React from 'react'; This creates an object named React which contains methods necessary to use the React library. import ReactDOM from 'react-dom'; The methods imported from 'react-dom' are meant for interacting with the DOM. You are already familiar with one of them: ReactDOM.render(). The methods imported from 'react' don\u2019t deal with the DOM at all. They don\u2019t engage directly with anything that isn\u2019t part of React. To clarify: the DOM is used in React applications, but it isn\u2019t part of React. After all, the DOM is also used in countless non-React applications. Methods imported from 'react' are only for pure React purposes, such as creating components or writing JSX elements. Components Create a Component Class we can use a JavaScript class to define a new React component. We can also define components with JavaScript functions, but we\u2019ll focus on class components first. All class components will have some methods and properties in common (more on this later). Rather than rewriting those same properties over and over again every time, we extend the Component class from the React library. This way, we can use code that we import from the React library, without having to write it over and over again ourselves. After we define our class component, we can use it to render as many instances of that component as we want. What is React.Component, and how do you use it to make a component class? React.Component is a JavaScript class. To create your own component class, you must subclass React.Component. You can do this by using the syntax class YourComponentNameGoesHere extends React.Component {}. import React from 'react'; import ReactDOM from 'react-dom'; class MyComponentClass extends React.Component { render() { return <h1>Hello world</h1>; } } ReactDOM.render( <MyComponentClass />, document.getElementById('app') ); On line 4, you know that you are declaring a new component class, which is like a factory for building React components. You know that React.Component is a class, which you must subclass in order to create a component class of your own. You also know that React.Component is a property on the object which was returned by import React from 'react' on line 1. The Render Function A render method is a property whose name is render, and whose value is a function. The term \"render method\" can refer to the entire property, or to just the function part. class ComponentFactory extends React.Component { render() { return <h1>Hello world</h1>; } } Create a Component Instance To make a React component, you write a JSX element. Instead of naming your JSX element something like h1 or div like you\u2019ve done before, give it the same name as a component class. Voil\u00e0, there\u2019s your component instance! JSX elements can be either HTML-like, or component instances. JSX uses capitalization to distinguish between the two! That is the React-specific reason why component class names must begin with capital letters. In a JSX element, that capitalized first letter says, \"I will be a component instance and not an HTML tag.\" Whenever you make a component, that component inherits all of the methods of its component class. MyComponentClass has one method: MyComponentClass.render(). Therefore, also has a method named render. In order to render a component, that component needs to have a method named render. Your component has this! It inherited a method named render from MyComponentClass. To call a component\u2019s render method, you pass that component to ReactDOM.render(). Notice your component, being passed as ReactDOM.render()\u2018s first argument: ReactDOM.render( <MyComponentClass />, document.getElementById('app') ); ReactDOM.render() will tell to call its render method. will call its render method, which will return the JSX element Hello world . ReactDOM.render() will then take that resulting JSX element, and add it to the virtual DOM. This will make \"Hello world\" appear on the screen. Use This in a Class class IceCreamGuy extends React.Component { get food() { return 'ice cream'; } render() { return <h1>I like {this.food}.</h1>; } } Render Components with Components class OMG extends React.Component { render() { return <h1>Whooaa!</h1>; } } class Crazy extends React.Component { render() { return <OMG />; } } Importing Files and Exporting Functionality Importing The second important difference involves the contents of the string at the end of the statement: 'react' vs './NavBar.js'. If you use an import statement, and the string at the end begins with either a dot or a slash, then import will treat that string as a filepath. import will follow that filepath, and import the file that it finds. If your filepath doesn\u2019t have a file extension, then \".js\" is assumed. So the above example could be shortened: import { NavBar } from './NavBar'; One final, important note: None of this behavior is specific to React! Module systems of independent, importable files are a very popular way to organize code. React\u2019s specific module system comes from ES6. Exporting This is called a named export. export class NavBar extends React.Component { Component Props A component\u2019s props is an object. It holds information about that component. You can pass information to a prop via an attribute. import React from 'react'; import ReactDOM from 'react-dom'; class Greeting extends React.Component { render() { return <h1>Hi there, {this.props.firstName}!</h1>; } } ReactDOM.render( <Greeting firstName='Grant' />, document.getElementById('app') ); Event Handler import React from 'react'; import ReactDOM from 'react-dom'; import { Button } from './Button'; class Talker extends React.Component { talk() { let speech = ''; for (let i = 0; i < 10000; i++) { speech += 'blah '; } alert(speech); } render() { return <Button talk={this.talk}/>; ReactDOM.render( <Talker />, document.getElementById('app') ); // **************************************** // In Button.js import React from 'react'; export class Button extends React.Component { render() { return ( // TODO - why is it `this` here? <button onClick={this.props.talk}> Click me! </button> ); } } handleEvent, onEvent, and this.props.onEvent When you pass an event handler as a prop, as you just did, there are two names that you have to choose. Both naming choices occur in the parent component class - that is, in the component class that defines the event handler and passes it. The first name that you have to choose is the name of the event handler itself. Look at Talker.js, lines 6 through 12. This is our event handler. We chose to name it talk. The second name that you have to choose is the name of the prop that you will use to pass the event handler. This is the same thing as your attribute name. For our prop name, we also chose talk, as shown on line 15: return <Button talk={this.talk} />; These two names can be whatever you want. However, there is a naming convention that they often follow. You don\u2019t have to follow this convention, but you should understand it when you see it. Here\u2019s how the naming convention works: first, think about what type of event you are listening for. In our example, the event type was \"click.\" If you are listening for a \"click\" event, then you name your event handler handleClick. If you are listening for a \"keyPress\" event, then you name your event handler handleKeyPress: class MyClass extends React.Component { handleHover() { alert('I am an event handler.'); alert('I will be called in response to \"hover\" events.'); } } Your prop name should be the word on, plus your event type. If you are listening for a \"click\" event, then you name your prop onClick. If you are listening for a \"keyPress\" event, then you name your prop onKeyPress: class MyClass extends React.Component { handleHover() { alert('I am an event handler.'); alert('I will listen for a \"hover\" event.'); } render() { return <Child onHover={this.handleHover} />; } } this.props.children Every component\u2019s props object has a property named children. this.props.children will return everything in between a component\u2019s opening and closing JSX tags. For example: // List.js import React from 'react'; export class List extends React.Component { render() { let titleText = `Favorite ${this.props.type}`; if (this.props.children instanceof Array) { // Add an s to make it plural if there is more than one titleText += 's'; } return ( <div> <h1>{titleText}</h1> <ul>{this.props.children}</ul> </div> ); } } // App.js import React from 'react'; import ReactDOM from 'react-dom'; import { List } from './List'; class App extends React.Component { render() { return ( <div> <List type='Living Musician'> <li>Sachiko M</li> <li>Harvey Sid Fisher</li> </List> <List type='Living Cat Musician'> <li>Nora the Piano Cat</li> </List> </div> ); } } ReactDOM.render( <App />, document.getElementById('app') ); This will print: Favorite Living Musicians Sachiko M Harvey Sid Fisher Favorite Living Cat Musician Nora the Piano Cat Because in List.js, between the <ul></ul> you have {this.props.children} which grabs all the elements between <List></List> in the App class. Default Properties Used if nothing is passed into the property. import React from 'react'; import ReactDOM from 'react-dom'; class Button extends React.Component { render() { return ( <button> {this.props.text} </button> ); } } // defaultProps goes here: Button.defaultProps = {text: \"I am a button\"}; ReactDOM.render( <Button />, document.getElementById('app') ); Component State A React component can access dynamic information in two ways: props and state. Unlike props, a component\u2019s state is not passed in from the outside. A component decides its own state. To make a component have state, give the component a state property. This property should be declared inside of a constructor method, like this: class Example extends React.Component { constructor(props) { super(props); this.state = { mood: 'decent' }; } render() { return <div></div>; } } <Example /> // Access the state outside with this.state.mood // You can set the state with this.setState({mood: \"the mood\"}) What is super(props) Also: https://overreacted.io/why-do-we-write-super-props/ this.setState from Another Function You'll use a wrapper function to call this.setState from another function. Like this: class Example extends React.Component { constructor(props) { super(props); this.state = { weather: 'sunny' }; this.makeSomeFog = this.makeSomeFog.bind(this); } makeSomeFog() { this.setState({ weather: 'foggy' }); } } The line this.makeSomeFog = this.makeSomeFog.bind(this); is necessary because makeSomeFog()'s body contains the word this. It has to do with the way event handlers are bound in Javascript. If you use this without the line this.makeSomeFog = this.makeSomeFog.bind(this); with an event handler the this word will be lost so we have to bind it... because Javascript. If the function isn't used by an event handler then it won't matter. Full example import React from 'react'; import ReactDOM from 'react-dom'; const green = '#39D1B4'; const yellow = '#FFD712'; class Toggle extends React.Component { constructor(props) { super(props); this.state = {color: green}; this.changeColor = this.changeColor.bind(this); } changeColor() { if(this.state.color === yellow) { this.setState({color: green}); } else { this.setState({color: yellow}); } } render() { return ( <div style={{background: this.state.color}}> <h1> <button onClick={this.changeColor}> Change color </button> </h1> </div> ); } } ReactDOM.render(<Toggle />, document.getElementById('app')); NOTE : Anytime you call this.setState it automatically calls render as soon as the state has changed. This is why you don't have to call render again. Component Lifecycle We\u2019ve seen that React components can be highly dynamic. They get created, rendered, added to the DOM, updated, and removed. All of these steps are part of a component\u2019s lifecycle. The component lifecycle has three high-level parts: Mounting, when the component is being initialized and put into the DOM for the first time Updating, when the component updates as a result of changed state or changed props Unmounting, when the component is being removed from the DOM Every React component you\u2019ve ever interacted with does the first step at a minimum. If a component never mounted, you\u2019d never see it! Most interesting components are updated at some point. A purely static component\u2014like, for example, a logo\u2014might not ever update. But if a component\u2019s state changes, it updates. Or if different props are passed to a component, it updates. Finally, a component is unmounted when it\u2019s removed from the DOM. For example, if you have a button that hides a component, chances are that component will be unmounted. If your app has multiple screens, it\u2019s likely that each screen (and all of its child components) will be unmounted. If a component is \"alive\" for the entire lifetime of your app (say, a top-level component or a persistent navigation bar), it won\u2019t be unmounted. But most components can get unmounted one way or another! It\u2019s worth noting that each component instance has its own lifecycle. For example, if you have 3 buttons on a page, then there are 3 component instances, each with its own lifecycle. However, once a component instance is unmounted, that\u2019s it\u2014it will never be re-mounted, or updated again, or unmounted. React components have several methods, called lifecycle methods, that are called at different parts of a component\u2019s lifecycle. This is how you, the programmer, deal with the lifecycle of a component. You may not have known it, but you\u2019ve already used two of the most common lifecycle methods: constructor() and render()! constructor() is the first method called during the mounting phase. render() is called later during the mounting phase, to render the component for the first time, and during the updating phase, to re-render the component. Notice that lifecycle methods don\u2019t necessarily correspond one-to-one with part of the lifecycle. constructor() only executes during the mounting phase, but render() executes during both the mounting and updating phase. componentDidMount Say you want a component to update itself at a setInterval. You don't want to put it in the constructor because that would violate the single responsibility rule but you also don't want it in render because then it would be called on update AND on mounting. That's what componentDidMount is for. componentDidMount() is the final method called during the mounting phase. The order is: The constructor render() componentDidMount() In other words, it\u2019s called after the component is rendered. (Another method, getDerivedStateFromProps(), is called between the constructor and render(), but it is very rarely used and usually isn\u2019t the best way to achieve your goals. We won\u2019t be talking about it in this lesson.) import React from 'react'; import ReactDOM from 'react-dom'; class Clock extends React.Component { constructor(props) { super(props); this.state = { date: new Date() }; } render() { return <div>{this.state.date.toLocaleTimeString()}</div>; } componentDidMount() { const oneSecond = 1000; setInterval(() => { this.setState({ date: new Date() }); }, oneSecond); } } ReactDOM.render(<Clock />, document.getElementById('app')); componentWillUnmount In the case of our interval above, the problem is now that timer will never stop. If we want to remove it. We want to use clearInterval() to clean it up. We can call this during componentWillUnmount import React from 'react'; export class Clock extends React.Component { constructor(props) { super(props); this.state = { date: new Date() }; } render() { return <div>{this.state.date.toLocaleTimeString()}</div>; } componentDidMount() { const oneSecond = 1000; this.intervalID = setInterval(() => { this.setState({ date: new Date() }); }, oneSecond); } componentWillUnmount() { clearInterval(this.intervalID); } } componentDidUpdate When a component updates many things happen but there are two primary methods - render and componentDidUpdate. import React from 'react'; export class Clock extends React.Component { constructor(props) { super(props); this.state = { date: new Date() }; } render() { return ( <div> {this.props.isPrecise ? this.state.date.toISOString() : this.state.date.toLocaleTimeString()} </div> ); } startInterval() { let delay; if (this.props.isPrecise) { delay = 100; } else { delay = 1000; } this.intervalID = setInterval(() => { this.setState({ date: new Date() }); }, delay); } componentDidMount() { this.startInterval(); } componentDidUpdate(prevProps) { if (this.props.isPrecise === prevProps.isPrecise) { return; } clearInterval(this.intervalID); this.startInterval(); } componentWillUnmount() { clearInterval(this.intervalID); } } Stateless Functional Components We used to use classes for components but now we use functions. // Original class-based way of writing components import React from 'react'; import ReactDOM from 'react-dom'; export class Friend extends React.Component { render() { return <img src=\"https://content.codecademy.com/courses/React/react_photo-octopus.jpg\" />; } }; ReactDOM.render( <Friend />, document.getElementById('app') ); // Function Version import React from 'react'; import ReactDOM from 'react-dom'; export const Friend = () => { return <img src=\"https://content.codecademy.com/courses/React/react_photo-octopus.jpg\" />; } ReactDOM.render( <Friend />, document.getElementById('app') ); Function Component Props export function YesNoQuestion (props) { return ( <div> <p>{props.prompt}</p> <input value=\"Yes\" /> <input value=\"No\" /> </div> ); } ReactDOM.render( <YesNoQuestion prompt=\"Have you eaten an apple today?\" />, document.getElementById('app'); ); React Hooks With Hooks, we can use simple function components to do lots of the fancy things that we could only do with class components in the past. React Hooks, plainly put, are functions that let us manage the internal state of components and handle post-rendering side effects directly from our function components. Hooks don\u2019t work inside classes \u2014 they let us use fancy React features without classes. Keep in mind that function components and React Hooks do not replace class components. They are completely optional; just a new tool that we can take advantage of. Note: If you\u2019re familiar with lifecycle methods of class components, you could say that Hooks let us \"hook into\" state and lifecycle features directly from our function components. React offers a number of built-in Hooks. A few of these include useState(), useEffect(), useContext(), useReducer(), and useRef(). See the full list in the docs . With React, we feed static and dynamic data models to JSX to render a view to the screen Use Hooks to \u201chook into\u201d internal component state for managing dynamic data in function components We employ the State Hook by using the code below: currentState to reference the current value of state stateSetter to reference a function used to update the value of this state the initialState argument to initialize the value of state for the component\u2019s first render const [currentState, stateSetter] = useState( initialState ); Call state setters in event handlers Define simple event handlers inline with our JSX event listeners and define complex event handlers outside of our JSX Use a state setter callback function when our next value depends on our previous value Use arrays and objects to organize and manage related data that tends to change together Use the spread syntax on collections of dynamic data to copy the previous state into the next state like so: setArrayState((prev) => [ ...prev ]) and setObjectState((prev) => ({ ...prev })) Split state into multiple, simpler variables instead of throwing it all into one state object Comparison Class vs Function Class import React, { Component } from \"react\"; import NewTask from \"../Presentational/NewTask\"; import TasksList from \"../Presentational/TasksList\"; export default class AppClass extends Component { constructor(props) { super(props); this.state = { newTask: {}, allTasks: [] }; this.handleChange = this.handleChange.bind(this); this.handleSubmit = this.handleSubmit.bind(this); this.handleDelete = this.handleDelete.bind(this); } handleChange({ target }){ const { name, value } = target; this.setState((prevState) => ({ ...prevState, newTask: { ...prevState.newTask, [name]: value, id: Date.now() } })); } handleSubmit(event){ event.preventDefault(); if (!this.state.newTask.title) return; this.setState((prevState) => ({ allTasks: [prevState.newTask, ...prevState.allTasks], newTask: {} })); } handleDelete(taskIdToRemove){ this.setState((prevState) => ({ ...prevState, allTasks: prevState.allTasks.filter((task) => task.id !== taskIdToRemove) })); } render() { return ( <main> <h1>Tasks</h1> <NewTask newTask={this.state.newTask} handleChange={this.handleChange} handleSubmit={this.handleSubmit} /> <TasksList allTasks={this.state.allTasks} handleDelete={this.handleDelete} /> </main> ); } } Function import React, { useState } from \"react\"; import NewTask from \"../Presentational/NewTask\"; import TasksList from \"../Presentational/TasksList\"; export default function AppFunction() { const [newTask, setNewTask] = useState({}); const handleChange = ({ target }) => { const { name, value } = target; setNewTask((prev) => ({ ...prev, id: Date.now(), [name]: value })); }; const [allTasks, setAllTasks] = useState([]); const handleSubmit = (event) => { event.preventDefault(); if (!newTask.title) return; setAllTasks((prev) => [newTask, ...prev]); setNewTask({}); }; const handleDelete = (taskIdToRemove) => { setAllTasks((prev) => prev.filter( (task) => task.id !== taskIdToRemove )); }; return ( <main> <h1>Tasks</h1> <NewTask newTask={newTask} handleChange={handleChange} handleSubmit={handleSubmit} /> <TasksList allTasks={allTasks} handleDelete={handleDelete} /> </main> ); } Update Function Component State Let\u2019s get started with the State Hook, the most common Hook used for building React components. The State Hook is a named export from the React library, so we import it like this: import React, { useState } from 'react'; useState() is a JavaScript function defined in the React library. When we call this function, it returns an array with two values: current state - the current value of this state state setter - a function that we can use to update the value of this state Because React returns these two values in an array, we can assign them to local variables, naming them whatever we like. For example: const [toggle, setToggle] = useState(); import React, { useState } from \"react\"; function Toggle() { const [toggle, setToggle] = useState(); return ( <div> <p>The toggle is {toggle}</p> <button onClick={() => setToggle(\"On\")}>On</button> <button onClick={() => setToggle(\"Off\")}>Off</button> </div> ); } Notice how the state setter function, setToggle(), is called by our onClick event listeners. To update the value of toggle and re-render this component with the new value, all we need to do is call the setToggle() function with the next state value as an argument. No need to worry about binding functions to class instances, working with constructors, or dealing with the this keyword. With the State Hook, updating state is as simple as calling a state setter function. Calling the state setter signals to React that the component needs to re-render, so the whole function defining the component is called again. The magic of useState() is that it allows React to keep track of the current value of state from one render to the next! More complex example: import React, { useState } from 'react'; export default function ColorPicker() { const [color, setColor] = useState(); const divStyle = {backgroundColor: color}; return ( <div style={divStyle}> <p>The color is {color}</p> <button onClick={() => setColor('Aquamarine')}> Aquamarine </button> <button onClick={() => setColor('BlueViolet')}> BlueViolet </button> <button onClick={() => setColor('Chartreuse')}> Chartreuse </button> <button onClick={() => setColor('CornflowerBlue')}> CornflowerBlue </button> </div> ); } Initialize State You can set a state at the beginning with: const [color, setColor] = useState(\"Tomato\"); . There are three ways in which this code affects our component: During the first render, the initial state argument is used. When the state setter is called, React ignores the initial state argument and uses the new value. When the component re-renders for any other reason, React continues to use the same value from the previous render. Use State Setter Outside of JSX https://www.codecademy.com/courses/react-101/lessons/the-state-hook/exercises/use-state-setter-outside-of-jsx Let\u2019s see how to manage the changing value of a string as a user types into a text input field: import React, { useState } from 'react'; export default function EmailTextInput() { const [email, setEmail] = useState(''); const handleChange = (event) => { const updatedEmail = event.target.value; setEmail(updatedEmail); } return ( // Here value={email} will set the value to the current // value in e-mail in the event hook <input value={email} onChange={handleChange} /> ); } Let\u2019s break down how this code works! The square brackets on the left side of the assignment operator signal array destructuring The local variable named email is assigned the current state value at index 0 from the array returned by useState() The local variable named setEmail() is assigned a reference to the state setter function at index 1 from the array returned by useState() It\u2019s convention to name this variable using the current state variable (email) with \"set\" prepended The JSX input tag has an event listener called onChange. This event listener calls an event handler each time the user types something in this element. In the example above, our event handler is defined inside of the definition for our function component, but outside of our JSX. Earlier in this lesson, we wrote our event handlers right in our JSX. Those inline event handlers work perfectly fine, but when we want to do something more interesting than just calling the state setter with a static value, it\u2019s a good idea to separate that logic from everything else going on in our JSX. This separation of concerns makes our code easier to read, test, and modify. You can change: const updatedEmail = event.target.value; setEmail(updatedEmail); // to this const handleChange = ({target}) => setEmail(target.value); Longer Example import React, { useState } from \"react\"; // regex to match numbers between 1 and 10 digits long const validPhoneNumber = /^\\d{1,10}$/; export default function PhoneNumber() { const [phone, setPhone] = useState(''); const handleChange = ({ target })=> { const newPhone = target.value; const isValid = validPhoneNumber.test(newPhone); if (isValid) { setPhone(newPhone); } // just ignore the event, when new value is invalid }; return ( <div className='phone'> <label for='phone-input'>Phone: </label> <input value={phone} onChange={handleChange} id='phone-input' /> </div> ); } Set From Previous State Often, the next value of our state is calculated using the current state. In this case, it is best practice to update state with a callback function. If we do not, we risk capturing outdated, or \u201cstale\u201d, state values. import React, { useState } from 'react'; export default function Counter() { const [count, setCount] = useState(0); const increment = () => setCount(prevCount => prevCount + 1); return ( <div> <p>Wow, you've clicked that button: {count} times</p> <button onClick={increment}>Click here!</button> </div> ); } When the button is pressed, the increment() event handler is called. Inside of this function, we use our setCount() state setter in a new way! Because the next value of count depends on the previous value of count, we pass a callback function as the argument for setCount() instead of a value (as we\u2019ve done in previous exercises). setCount(prevCount => prevCount + 1) When our state setter calls the callback function, this state setter callback function takes our previous count as an argument. The value returned by this state setter callback function is used as the next value of count (in this case prevCount + 1). Note: We can just call setCount(count +1) and it would work the same in this example\u2026 but for reasons that are out of scope for this lesson, it is safer to use the callback method. Arrays in State import React, { useState } from \"react\"; import ItemList from \"./ItemList\"; import { produce, pantryItems } from \"./storeItems\"; export default function GroceryCart() { // declare and initialize state const [cart, setCart] = useState([]); // addItem is the event handler and will receive the item that // gets clicked const addItem = (item) => { // setCart is the state setter // and it will tell the component to update its state. // Via the magic that is the totality of Javascript, it // will magically receive the previous state to this function // We then use spread syntax to expand the previous array // and add it with the item. setCart((prev) => { return [item, ...prev]; }); }; // This removes the item at some set index. const removeItem = (targetIndex) => { setCart((prev) => { return prev.filter((item, index) => index !== targetIndex); }); }; return ( <div> <h1>Grocery Cart</h1> <ul> {cart.map((item, index) => ( <li onClick={() => removeItem(index)} key={index}> {item} </li> ))} </ul> <h2>Produce</h2> <ItemList items={produce} onItemClick={addItem} /> <h2>Pantry Items</h2> <ItemList items={pantryItems} onItemClick={addItem} /> </div> ); } Objects in State export default function Login() { const [formState, setFormState] = useState({}); const handleChange = ({ target }) => { const { name, value } = target; setFormState((prev) => ({ ...prev, [name]: value })); }; return ( <form> <input value={formState.firstName} onChange={handleChange} name=\"firstName\" type=\"text\" /> <input value={formState.password} onChange={handleChange} type=\"password\" name=\"password\" /> </form> ); } A few things to notice: We use a state setter callback function to update state based on the previous value The spread syntax is the same for objects as for arrays: { ...oldObject, newKey: newValue } We reuse our event handler across multiple inputs by using the input tag\u2019s name attribute to identify which input the change event came from Once again, when updating the state with setFormState() inside a function component, we do not modify the same object. We must copy over the values from the previous object when setting the next value of state. Thankfully, the spread syntax makes this super easy to do! Anytime one of the input values is updated, the handleChange() function will be called. Inside of this event handler, we use object destructuring to unpack the target property from our event object, then we use object destructuring again to unpack the name and value properties from the target object. Inside of our state setter callback function, we wrap our curly brackets in parentheses like so: setFormState((prev) => ({ ...prev })). This tells JavaScript that our curly brackets refer to a new object to be returned. We use ..., the spread operator, to fill in the corresponding fields from our previous state. Finally, we overwrite the appropriate key with its updated value. Did you notice the square brackets around the name? This Computed Property Name allows us to use the string value stored by the name variable as a property key! Longer Example import React, { useState } from \"react\"; export default function EditProfile() { const [profile, setProfile] = useState({}); const handleChange = ({ target }) => { const {name, value } = target; setProfile((prevProfile) => ({ ...prevProfile, [name]: value })); }; const handleSubmit = (event) => { event.preventDefault(); alert(JSON.stringify(profile, '', 2)); }; return ( <form onSubmit={handleSubmit}> <input value={profile.firstName || ''} name=\"firstName\" type=\"text\" placeholder=\"First Name\" onChange={handleChange} /> <input value={profile.lastName || ''} type=\"text\" name=\"lastName\" placeholder=\"Last Name\" onChange={handleChange} /> <input value={profile.bday || ''} type=\"date\" name=\"bday\" onChange={handleChange} /> <input value={profile.password || ''} type=\"password\" name=\"password\" placeholder=\"Password\" onChange={handleChange} /> <button type=\"submit\">Submit</button> </form> ); } Separate Hooks for Separate States While there are times when it can be helpful to store related data in a data collection like an array or object, it can also be helpful to separate data that changes separately into completely different state variables. Managing dynamic data is much easier when we keep our data models as simple as possible. For example, if we had a single object that held state for a subject you are studying at school, it might look something like this: function Subject() { const [state, setState] = useState({ currentGrade: 'B', classmates: ['Hasan', 'Sam', 'Emma'], classDetails: {topic: 'Math', teacher: 'Ms. Barry', room: 201}; exams: [{unit: 1, score: 91}, {unit: 2, score: 88}]); }); This would work, but think about how messy it could get to copy over all the other values when we need to update something in this big state object. For example, to update the grade on an exam, we would need an event handler that did something like this: // Get the previous state in and pass that to something that is going to return a new object {} setState((prev) => ({ // Expand the previous state to grab everything ...prev, // You want the previous state, except with exams you're going to grab just exams and then map // that to a new function where you'll extract just the exam you want and change the score exams: prev.exams.map((exam) => { if( exam.unit === updatedExam.unit ){ return { ...exam, score: updatedExam.score }; } else { return exam; } }), })); Yikes! Complex code like this is likely to cause bugs! Luckily, there is another option\u2026 We can make more than one call to the State Hook. In fact, we can make as many calls to useState() as we want! It\u2019s best to split state into multiple state variables based on which values tend to change together. We can rewrite the previous example as follows\u2026 function Subject() { const [currentGrade, setGrade] = useState('B'); const [classmates, setClassmates] = useState(['Hasan', 'Sam', 'Emma']); const [classDetails, setClassDetails] = useState({topic: 'Math', teacher: 'Ms. Barry', room: 201}); const [exams, setExams] = useState([{unit: 1, score: 91}, {unit: 2, score: 88}]); // ... } See https://reactjs.org/docs/hooks-state.html#tip-using-multiple-state-variables Comparison function Musical() { const [state, setState] = useState({ title: \"Best Musical Ever\", actors: [\"George Wilson\", \"Tim Hughes\", \"Larry Clements\"], locations: { Chicago: { dates: [\"1/1\", \"2/2\"], address: \"chicago theater\"}, SanFrancisco: { dates: [\"5/2\"], address: \"sf theater\" } } }) } function MusicalRefactored() { const [title, setTitle] = useState(\"Best Musical Ever\"); const [actors, setActors] = useState([\"George Wilson\", \"Tim Hughes\", \"Larry Clements\"]); const [locations, setLocations] = useState({ Chicago: { dates: [\"1/1\", \"2/2\"], address: \"chicago theater\"}, SanFrancisco: { dates: [\"5/2\"], address: \"sf theater\" } }); } The Effect Hook - useEffect Before Hooks, function components were only used to accept data in the form of props and return some JSX to be rendered. However, as we learned in the last lesson, the State Hook allows us to manage dynamic data, in the form of component state, within our function components. In this lesson, we\u2019ll use the Effect Hook to run some JavaScript code after each render, such as: fetching data from a backend service subscribing to a stream of data managing timers and intervals reading from and making changes to the DOM Why after each render? Most interesting components will re-render multiple times throughout their lifetime and these key moments present the perfect opportunity to execute these \u201cside effects\u201d. There are three key moments when the Effect Hook can be utilized: When the component is first added, or mounted, to the DOM and renders When the state or props change, causing the component to re-render When the component is removed, or unmounted, from the DOM. React Hooks and Component Lifecycle Equivalent https://stackoverflow.com/a/53254018/4427375 componentWillMount for react functional component? https://stackoverflow.com/questions/62091146/componentwillmount-for-react-functional-component Function Component Effects import React, { useState, useEffect } from 'react'; function PageTitle() { const [name, setName] = useState(''); useEffect(() => { document.title = `Hi, ${name}`; }); return ( <div> <p>Use the input field below to rename this page!</p> <input onChange={({target}) => setName(target.value)} value={name} type='text' /> </div> ); } In our effect, we assign the value of the name variable to the document.title within a string. For more on this syntax, have a look at this explanation of the document\u2019s title property. Notice how we use the current state inside of our effect. Even though our effect is called after the component renders, we still have access to the variables in the scope of our function component! When React renders our component, it will update the DOM as usual, and then run our effect after the DOM has been updated. This happens for every render, including the first and last one. Clean Up Effects useEffect(()=>{ document.addEventListener('keydown', handleKeyPress); return () => { document.removeEventListener('keydown', handleKeyPress); }; }) If our effect didn\u2019t return a cleanup function, then a new event listener would be added to the DOM\u2019s document object every time that our component re-renders. Not only would this cause bugs, but it could cause our application performance to diminish and maybe even crash! Because effects run after every render and not just once, React calls our cleanup function before each re-render and before unmounting to clean up each effect call. If our effect returns a function, then the useEffect() Hook always treats that as a cleanup function. React will call this cleanup function before the component re-renders or unmounts. Since this cleanup function is optional, it is our responsibility to return a cleanup function from our effect when our effect code could create memory leaks. import React, { useState, useEffect } from 'react'; export default function Counter() { const [clickCount, setClickCount] = useState(0); const increment = () => setClickCount((prev) => prev + 1); useEffect(() => { document.addEventListener('mousedown', increment); return () => { document.removeEventListener('mousedown', increment); }; }); return ( <h1>Document Clicks: {clickCount}</h1> ); } Control When Effects are Called It is common, when defining function components, to run an effect only when the component mounts (renders the first time), but not when the component re-renders. The Effect Hook makes this very easy for us to do! If we want to only call our effect after the first render, we pass an empty array to useEffect() as the second argument. This second argument is called the dependency array. The dependency array is used to tell the useEffect() method when to call our effect and when to skip it. Our effect is always called after the first render but only called again if something in our dependency array has changed values between renders useEffect(() => { alert(\"component rendered for the first time\"); return () => { alert(\"component is being removed from the DOM\"); }; }, []); Fetch Data from a Server Since the effect hook is called after every render we want to be extra careful when we are fetching data from a server as this will quickly sabotage the performance of our app. When the data that our components need to render doesn\u2019t change, we can pass an empty dependency array, so that the data is fetched after the first render. When the response is received from the server, we can use a state setter from the State Hook to store the data from the server\u2019s response in our local component state for future renders. Using the State Hook and the Effect Hook together in this way is a powerful pattern that saves our components from unnecessarily fetching new data after every render! An empty dependency array signals to the Effect Hook that our effect never needs to be re-run, that it doesn\u2019t depend on anything. Specifying zero dependencies means that the result of running that effect won\u2019t change and calling our effect once is enough. A dependency array that is not empty signals to the Effect Hook that it can skip calling our effect after re-renders unless the value of one of the variables in our dependency array has changed. If the value of a dependency has changed, then the Effect Hook will call our effect again! Here\u2019s a nice example from the official React docs: useEffect(() => { document.title = `You clicked ${count} times`; }, [count]); // Only re-run the effect if the value stored by count changes Rules of Hooks There are two main rules to keep in mind when using Hooks: only call Hooks at the top level only call Hooks from React functions As we have been practicing with the State Hook and the Effect Hook, we\u2019ve been following these rules with ease, but it is helpful to keep these two rules in mind as you take your new understanding of Hooks out into the wild and begin using more Hooks in your React applications. When React builds the Virtual DOM, the library calls the functions that define our components over and over again as the user interacts with the user interface. React keeps track of the data and functions that we are managing with Hooks based on their order in the function component\u2019s definition. For this reason, we always call our Hooks at the top level; we never call hooks inside of loops, conditions, or nested functions. Instead of confusing React with code like this: if (userName !== '') { useEffect(() => { localStorage.setItem('savedUserName', userName); }); } We can accomplish the same goal, while consistently calling our Hook every time: useEffect(() => { if (userName !== '') { localStorage.setItem('savedUserName', userName); } }); Secondly, Hooks can only be used in React Functions. We cannot use Hooks in class components and we cannot use Hooks in regular JavaScript functions. We\u2019ve been working with useState() and useEffect() in function components, and this is the most common use. The only other place where Hooks can be used is within custom hooks. Custom Hooks are incredibly useful for organizing and reusing stateful logic between function components. For more on this topic, head to the React Docs. Separate Hooks for Separate Effects When multiple values are closely related and change at the same time, it can make sense to group these values in a collection like an object or array. Packaging data together can also add complexity to the code responsible for managing that data. Therefore, it is a good idea to separate concerns by managing different data with different Hooks. Compare the complexity here, where data is bundled up into a single object: // Handle both position and menuItems with one useEffect hook. const [data, setData] = useState({ position: { x: 0, y: 0 } }); useEffect(() => { get('/menu').then((response) => { setData((prev) => ({ ...prev, menuItems: response.data })); }); const handleMove = (event) => setData((prev) => ({ ...prev, position: { x: event.clientX, y: event.clientY } })); window.addEventListener('mousemove', handleMove); return () => window.removeEventListener('mousemove', handleMove); }, []); To the simplicity here, where we have separated concerns: // Handle menuItems with one useEffect hook. const [menuItems, setMenuItems] = useState(null); useEffect(() => { get('/menu').then((response) => setMenuItems(response.data)); }, []); // Handle position with a separate useEffect hook. const [position, setPosition] = useState({ x: 0, y: 0 }); useEffect(() => { const handleMove = (event) => setPosition({ x: event.clientX, y: event.clientY }); window.addEventListener('mousemove', handleMove); return () => window.removeEventListener('mousemove', handleMove); }, []); Stateless Components from Stateful Components Instead of having one, very complicated, stateful, component, we have one stateful component (App) at the top level with many stateless components in a hierarchy. The stateful component will pass its state down to the stateless components. Build a Stateful Component Class Example of passing a parent's state into a stateless child // PARENT import React from 'react'; import ReactDOM from 'react-dom'; import { Child } from './Child'; class Parent extends React.Component { constructor(props) { super(props); this.state = { name: 'Frarthur' }; } render() { return <Child name={this.state.name}/>; } } ReactDOM.render(<Parent />, document.getElementById('app')); // CHILD import React from 'react'; import ReactDOM from 'react-dom'; // We have to export this since it will be rendered by // another component export class Child extends React.Component { render() { return <h1>Hey, my name is {this.props.name}!</h1>; } } This will print: Hey, my name is Frarthur! Don't Update props A React component should use props to store information that can be changed, but can only be changed by a different component. A React component should use state to store information that the component itself can change. // BAD import React from 'react'; class Bad extends React.Component { render() { this.props.message = 'yo'; // NOOOOOOOOOOOOOO!!! return <h1>{this.props.message}</h1>; } } Child Components Update Their Parents' State How does a stateless, child component update the state of the parent component? Here\u2019s how that works: 1 The parent component class defines a method that calls this.setState(). For an example, look in Step1.js at the .handleClick() method. import React from 'react'; import ReactDOM from 'react-dom'; import { ChildClass } from './ChildClass'; class ParentClass extends React.Component { constructor(props) { super(props); this.state = { totalClicks: 0 }; } handleClick() { const total = this.state.totalClicks; // calling handleClick will // result in a state change: this.setState( { totalClicks: total + 1 } ); } } 2 The parent component binds the newly-defined method to the current instance of the component in its constructor. This ensures that when we pass the method to the child component, it will still update the parent component. For an example, look in Step2.js at the end of the constructor() method. An explanation of how this/bind work How bind works: https://stackoverflow.com/a/10115970/4427375 What is the global object Once the parent has defined a method that updates its state and bound to it, the parent then passes that method down to a child. Look in Step2.js, at the prop on line 28. import React from 'react'; import ReactDOM from 'react-dom'; import { ChildClass } from './ChildClass'; class ParentClass extends React.Component { constructor(props) { super(props); this.state = { totalClicks: 0 }; this.handleClick = this.handleClick.bind(this); } handleClick() { const total = this.state.totalClicks; // calling handleClick will // result in a state change: this.setState( { totalClicks: total + 1 } ); } // The stateful component class passes down // handleClick to a stateless component class: render() { return ( <ChildClass onClick={this.handleClick} /> ); } } 3 The child receives the passed-down function, and uses it as an event handler. Look in Step3.js. When a user clicks on the , a click event will fire. This will make the passed-down function get called, which will update the parent\u2019s state. import React from 'react'; import ReactDOM from 'react-dom'; export class ChildClass extends React.Component { render() { return ( // The stateless component class uses // the passed-down handleClick function, // accessed here as this.props.onClick, // as an event handler: <button onClick={this.props.onClick}> Click Me! </button> ); } } More Complex Example WARNING this violates the rule that components should only do one thing! We fix this in One Sibling to Display, Another to Change // CHILD import React from 'react'; export class Child extends React.Component { constructor(props) { super(props); this.handleChange = this.handleChange.bind(this); } handleChange(e) { const name = e.target.value; this.props.onChange(name); } render() { return ( <div> <h1> Hey my name is {this.props.name}! </h1> <select id=\"great-names\" onChange={this.handleChange}> <option value=\"Frarthur\"> Frarthur </option> <option value=\"Gromulus\"> Gromulus </option> <option value=\"Thinkpiece\"> Thinkpiece </option> </select> </div> ); } } // PARENT import React from 'react'; import ReactDOM from 'react-dom'; import { Child } from './Child'; class Parent extends React.Component { constructor(props) { super(props); this.state = { name: 'Frarthur' }; this.changeName = this.changeName.bind(this); } changeName(newName) { this.setState({ name: newName }); } render() { return <Child name={this.state.name} onChange={this.changeName} /> } } ReactDOM.render( <Parent />, document.getElementById('app') ); Child Components Update Sibling Components The Reactions component passes an event handler to the Like component. When Like is clicked, the handler is called, which causes the parent Reactions component to send a new prop to Stats. The Stats component updates with the new information. One Sibling to Display, Another to Change You will have one stateless component display information, and a different stateless component offer the ability to change that information. A stateful component class defines a function that calls this.setState. (Parent.js, lines 15-19) The stateful component passes that function down to a stateless component. (Parent.js, line 24) That stateless component class defines a function that calls the passed-down function, and that can take an event object as an argument. (Child.js, lines 10-13) The stateless component class uses this new function as an event handler. (Child.js, line 20) When an event is detected, the parent\u2019s state updates. (A user selects a new dropdown menu item) The stateful component class passes down its state, distinct from the ability to change its state, to a different stateless component. (Parent.js, line 25) That stateless component class receives the state and displays it. (Sibling.js, lines 5-10) An instance of the stateful component class is rendered. One stateless child component displays the state, and a different stateless child component displays a way to change the state. (Parent.js, lines 23-26) // PARENT import React from 'react'; import ReactDOM from 'react-dom'; import { Child } from './Child'; import { Sibling } from './Sibling'; class Parent extends React.Component { constructor(props) { super(props); this.state = { name: 'Frarthur' }; this.changeName = this.changeName.bind(this); } changeName(newName) { this.setState({ name: newName }); } render() { return ( <div> <Child onChange={this.changeName} /> <Sibling name={this.state.name}/> </div> ); } } ReactDOM.render( <Parent />, document.getElementById('app') ); // CHILD import React from 'react'; export class Child extends React.Component { constructor(props) { super(props); this.handleChange = this.handleChange.bind(this); } handleChange(e) { const name = e.target.value; this.props.onChange(name); } render() { return ( <div> <select id=\"great-names\" onChange={this.handleChange}> <option value=\"Frarthur\">Frarthur</option> <option value=\"Gromulus\">Gromulus</option> <option value=\"Thinkpiece\">Thinkpiece</option> </select> </div> ); } } // SIBLING import React from 'react'; export class Sibling extends React.Component { render() { const name = this.props.name; return ( <div> <h1>Hey, my name is {name}!</h1> <h2>Don't you think {name} is the prettiest name ever?</h2> <h2>Sure am glad that my parents picked {name}!</h2> </div> ); } } Style Inline Styles An inline style is a style that\u2019s written as an attribute, like this: <h1 style={{ color: 'red' }}>Hello world</h1> Notice the double curly braces. What are those for? The outer curly braces inject JavaScript into JSX. They say, \u201ceverything between us should be read as JavaScript, not JSX.\u201d The inner curly braces create a JavaScript object literal. They make this a valid JavaScript object: { color: 'red' } If you inject an object literal into JSX, and your entire injection is only that object literal, then you will end up with double curly braces. There\u2019s nothing unusual about how they work, but they look funny and can be confusing. Make a Style Object Variable Notice that here we define the style at the top level as a variable and then pass it in. In React style variable names are written camelCase. NOTE : The styles in ReactJS use numbers and the px is implied. import React from 'react'; import ReactDOM from 'react-dom'; const styles = { background: 'lightblue', color: 'darkred' marginTop: 100, fontSize: 50 }; const styleMe = <h1 style={styles}>Please style me! I am so bland!</h1>; ReactDOM.render( styleMe, document.getElementById('app') ); Share Styles Across Multiple Components // STYLES.JS const fontFamily = 'Comic Sans MS, Lucida Handwriting, cursive'; const background = 'pink url(\"https://content.codecademy.com/programs/react/images/welcome-to-my-homepage.gif\") fixed'; const fontSize = '4em'; const padding = '45px 0'; const color = 'green'; export const styles = { fontFamily: fontFamily, background: background, fontSize: fontSize, padding: padding, color: color }; // ATTENTIONGRABBER.JS import React from 'react'; import { styles } from './styles'; const h1Style = { color: styles.color, fontSize: styles.fontSize, fontFamily: styles.fontFamily, padding: styles.padding, margin: 0, }; export class AttentionGrabber extends React.Component { render() { return <h1 style={h1Style}>WELCOME TO MY HOMEPAGE!</h1>; } } // HOME.JS import React from 'react'; import ReactDOM from 'react-dom'; import { AttentionGrabber } from './AttentionGrabber'; import { styles } from './styles'; const divStyle = { background: styles.background, height: '100%' }; export class Home extends React.Component { render() { return ( <div style={divStyle}> <AttentionGrabber /> <footer>THANK YOU FOR VISITING MY HOMEPAGE!</footer> </div> ); } } ReactDOM.render( <Home />, document.getElementById('app') ); Separate Container Components from Presentational Components As you continue building your React application, you will soon realize that one component has too many responsibilities, but how do you know when you have reached that point? Separating container components from presentational components helps to answer that question. It shows you when it might be a good time to divide a component into smaller components. It also shows you how to perform that division. <GuineaPigs /> \u2018s job is to render a photo carousel of guinea pigs. It does this perfectly well! And yet, it has a problem: it does too much stuff. How might we divide this into a container component and a presentational component? import React from 'react'; import ReactDOM from 'react-dom'; const GUINEAPATHS = [ 'https://content.codecademy.com/courses/React/react_photo-guineapig-1.jpg', 'https://content.codecademy.com/courses/React/react_photo-guineapig-2.jpg', 'https://content.codecademy.com/courses/React/react_photo-guineapig-3.jpg', 'https://content.codecademy.com/courses/React/react_photo-guineapig-4.jpg' ]; export class GuineaPigs extends React.Component { constructor(props) { super(props); this.state = { currentGP: 0 }; this.interval = null; this.nextGP = this.nextGP.bind(this); } nextGP() { let current = this.state.currentGP; let next = ++current % GUINEAPATHS.length; this.setState({ currentGP: next }); } componentDidMount() { this.interval = setInterval(this.nextGP, 5000); } componentWillUnmount() { clearInterval(this.interval); } render() { let src = GUINEAPATHS[this.state.currentGP]; return ( <div> <h1>Cute Guinea Pigs</h1> <img src={src} /> </div> ); } } ReactDOM.render( <GuineaPigs />, document.getElementById('app') ); Create a Container Component Separating container components from presentational components is a popular React programming pattern. It is a special application of the concepts learned in the Stateless Components From Stateful Components module. If a component has to have state, make calculations based on props, or manage any other complex logic, then that component shouldn\u2019t also have to render HTML-like JSX. The functional part of a component (state, calculations, etc.) can be separated into a container component. GuineaPigs.js contains a lot of logic! It has to select the correct guinea pig to render, wait for the right amount of time before rendering, render an image, select the next correct guinea pig, and so on. Let\u2019s separate the logic from the GuineaPigs component into a container component. Create a Presentational Component The presentational component\u2019s only job is to contain HTML-like JSX. It should be an exported component and will not render itself because a presentational component will always get rendered by a container component. As a separate example, say we have Presentational and Container components. Presentational.js must export the component class (or function, when applicable): export class Presentational extends Component { Container.js must import that component: import { Presentational } from 'Presentational.js'; // GuineaPigs.js import React from 'react'; export class GuineaPigs extends React.Component { render() { let src = this.props.src; return ( <div> <h1>Cute Guinea Pigs</h1> <img src={src} /> </div> ); } } // GuineaPigsContainer.js import React from 'react'; import ReactDOM from 'react-dom'; import { GuineaPigs } from '../components/GuineaPigs'; const GUINEAPATHS = [ 'https://content.codecademy.com/courses/React/react_photo-guineapig-1.jpg', 'https://content.codecademy.com/courses/React/react_photo-guineapig-2.jpg', 'https://content.codecademy.com/courses/React/react_photo-guineapig-3.jpg', 'https://content.codecademy.com/courses/React/react_photo-guineapig-4.jpg' ]; class GuineaPigsContainer extends React.Component { constructor(props) { super(props); this.state = { currentGP: 0 }; this.interval = null; this.nextGP = this.nextGP.bind(this); } nextGP() { let current = this.state.currentGP; let next = ++current % GUINEAPATHS.length; this.setState({ currentGP: next }); } componentDidMount() { this.interval = setInterval(this.nextGP, 5000); } componentWillUnmount() { clearInterval(this.interval); } render() { const src = GUINEAPATHS[this.state.currentGP]; return <GuineaPigs src={src} />; } } ReactDOM.render( <GuineaPigsContainer />, document.getElementById('app') ); propTypes propTypes are useful for two reasons. The first reason is prop validation. Validation can ensure that your props are doing what they\u2019re supposed to be doing. If props are missing, or if they\u2019re present but they aren\u2019t what you\u2019re expecting, then a warning will print in the console. This is useful, but reason #2 is arguably more useful: documentation. Documenting props makes it easier to glance at a file and quickly understand the component class inside. When you have a lot of files, and you will, this can be a huge benefit. Apply PropTypes The name of each property in propTypes should be the name of an expected prop. In our case, MessageDisplayer expects a prop named message, so our property\u2019s name is message. The value of each property in propTypes should fit this pattern: PropTypes.expected_data_type_goes_here import React from 'react'; import PropTypes from 'prop-types'; export class BestSeller extends React.Component { render() { return ( <li> Title: <span> {this.props.title} </span><br /> Author: <span> {this.props.author} </span><br /> Weeks: <span> {this.props.weeksOnList} </span> </li> ); } } BestSeller.propTypes = { title: PropTypes.string.isRequired, author: PropTypes.string.isRequired, weeksOnList: PropTypes.number.isRequired }; PropTypes in Function Components // Normal way to display a prop: export class MyComponentClass extends React.Component { render() { return <h1>{this.props.title}</h1>; } } // Functional component way to display a prop: export const MyComponentClass = (props) => { return <h1>{props.title}</h1>; } // Normal way to display a prop using a variable: export class MyComponentClass extends React.component { render() { let title = this.props.title; return <h1>{title}</h1>; } } // Functional component way to display a prop using a variable: export const MyComponentClass = (props) => { let title = props.title; return <h1>{title}</h1>; } React Forms Think about how forms work in a typical, non-React environment. A user types some data into a form\u2019s input fields, and the server doesn\u2019t know about it. The server remains clueless until the user hits a \u201csubmit\u201d button, which sends all of the form\u2019s data over to the server simultaneously. In React, as in many other JavaScript environments, this is not the best way of doing things. The problem is the period of time during which a form thinks that a user has typed one thing, but the server thinks that the user has typed a different thing. What if, during that time, a third part of the website needs to know what a user has typed? It could ask the form or the server and get two different answers. In a complex JavaScript app with many moving, interdependent parts, this kind of conflict can easily lead to problems. In a React form, you want the server to know about every new character or deletion, as soon as it happens. That way, your screen will always be in sync with the rest of your application. Input on Change A traditional form doesn\u2019t update the server until a user hits \u201csubmit.\u201d But you want to update the server any time a user enters or deletes any character. import React from 'react'; export class Example extends React.Component { constructor(props) { super(props); this.state = { userInput: '' }; this.handleChange = this.handleChange.bind(this); } handleChange(e) { this.setState({ userInput: e.target.value }); } render() { return ( <input onChange={this.handleChange} type=\"text\" /> ); } } Control vs Uncontrolled There are two terms that will probably come up when you talk about React forms: controlled component and uncontrolled component. Like automatic binding, controlled vs uncontrolled components is a topic that you should be familiar with, but don\u2019t need to understand deeply at this point. An uncontrolled component is a component that maintains its own internal state. A controlled component is a component that does not maintain any internal state. Since a controlled component has no state, it must be controlled by someone else. Think of a typical <input type='text' /> element. It appears onscreen as a text box. If you need to know what text is currently in the box, then you can ask the <input /> , possibly with some code like this: let input = document.querySelector('input[type=\"text\"]'); let typedText = input.value; // input.value will be equal to whatever text is currently in the text box. The important thing here is that the <input /> keeps track of its own text. You can ask it what its text is at any time, and it will be able to tell you. The fact that <input /> keeps track of information makes it an uncontrolled component. It maintains its own internal state, by remembering data about itself. A controlled component, on the other hand, has no memory. If you ask it for information about itself, then it will have to get that information through props. Most React components are controlled. In React, when you give an <input /> a value attribute, then something strange happens: the <input /> BECOMES controlled. It stops using its internal storage. This is a more \u2018React\u2019 way of doing things. Update an Input's Value When a user types or deletes in the <input /> , then that will trigger a change event, which will call handleUserInput. That\u2019s good! handleUserInput will set this.state.userInput equal to whatever text is currently in the input field. That\u2019s also good! There\u2019s only one problem: you can set this.state.userInput to whatever you want, but <input /> won\u2019t care. You need to somehow make the <input /> \u2018s text responsive to this.state.userInput. Easy enough! You can control an <input /> \u2018s text by setting its value attribute. Set the Input's Initial State Good! Any time that someone types or deletes in <input /> , the .handleUserInput() method will update this.state.userInput with the <input /> \u2018s text. Since you\u2019re using this.setState, that means that Input needs an initial state! What should this.state\u2018s initial value be? Well, this.state.userInput will be displayed in the <input /> . What should the initial text in the <input /> be, when a user first visits the page? The initial text should be blank! Otherwise it would look like someone had already typed something. Dynamically Rendering Different Components without Switch: the Capitalized Reference Technique See: https://j5bot.medium.com/react-dynamically-rendering-different-components-without-switch-the-capitalized-reference-e668d89e460b React Router https://ui.dev/react-router-tutorial BrowserRouter Naturally, in order to do its thing, React Router needs to be both aware and in control of your app's location. The way it does this is with its BrowserRouter component. Under the hood, BrowserRouter uses both the history library as well as React Context . The history library helps React Router keep track of the browsing history of the application using the browser's built-in history stack, and React Context helps make history available wherever React Router needs it. There's not much to BrowserRouter, you just need to make sure that if you're using React Router on the web, you wrap your app inside of the BrowserRouter import ReactDOM from 'react-dom' import * as React from 'react' import { BrowserRouter } from 'react-router-dom' import App from './App` ReactDOM.render( <BrowserRouter> <App /> </BrowserRouter> , document.getElementById('app)) Route Put simply, Route allows you to map your app's location to different React components. For example, say we wanted to render a Dashboard component whenever a user navigated to the /dashboard path. To do so, we'd render a Route that looked like this. <Route path=\"/dashboard\" element={<Dashboard />} /> The mental model I use for Route is that it always has to render something \u2013 either its element prop if the path matches the app's current location or null, if it doesn't. You can render as many Routes as you'd like. <Route path=\"/\" element={<Home />} /> <Route path=\"/about\" element={<About />} /> <Route path=\"/settings\" element={<Settings />} /> You can even render nested routes, which we'll talk about later on in this post. With our Route elements in this configuration, it's possible for multiple routes to match on a single URL. You might want to do that sometimes, but most often you want React Router to only render the route that matches best. Fortunately, we can easily do that with Routes. Routes You can think of Routes as the metaphorical conductor of your routes. Whenever you have one or more Routes, you'll most likely want to wrap them in a Routes. import { Routes, Route } from \"react-router-dom\"; function App() { return ( <Routes> <Route path=\"/\" element={<Home />} /> <Route path=\"/about\" element={<About />} /> <Route path=\"/settings\" element={<Settings />} /> <Route path=\"*\" element={<NotFound />} /> </Routes> ); } The reason for this is because it's Routes job is to understand all of its children Route elements, and intelligently choose which ones are the best to render. Though it's not shown in the simple example above, once we start adding more complex Routes to our application, Routes will start to do more work like enabling intelligent rendering and relative paths. We'll see these scenarios in a bit. Next up, linking between pages. Links Now that you know how to map the app's location to certain React components using Routes and Route, the next step is being able to navigate between them. This is the purpose of the Link component. To tell Link what path to take the user to when clicked, you pass it a to prop. <nav> <Link to=\"/\">Home</Link> <Link to=\"/about\">About</Link> <Link to=\"/settings\">Settings</Link> </nav> If you need more control over Link, you can also pass to as an object. Doing so allows you to add a query string via the search property or pass along any data to the new route via state. <nav> <Link to=\"/\">Home</Link> <Link to=\"/about\">About</Link> <Link to={{ pathname: \"/settings\", search: \"?sort=date\", state: { fromHome: true }, }} > Settings </Link> </nav> URL Parameters Like function parameters allow you to declare placeholders when you define a function, URL Parameters allow you to declare placeholders for portions of a URL. Take Wikipedia for example. When you visit a topic on Wikipedia, you'll notice that the URL pattern is always the same, wikipedia.com/wiki/{topicId}. Instead of defining a route for every topic on the site, they can declare one route with a placeholder for the topic's id. The way you tell React Router that a certain portion of the URL is a placeholder (or URL Parameter), is by using a : in the Route's path prop. <Route path=\"/wiki/:topicId\" element={<Article />} /> Now whenever anyone visits a URL that matches the /wiki/:topicId pattern (/wiki/javascript, /wiki/Brendan_Eich, /wiki/anything) , the Article component is rendered. Now the question becomes, how do you access the dynamic portion of the URL \u2013 in this case, topicId \u2013 in the component that's rendered? As of v5.1, React Router comes with a useParams Hook that returns an object with a mapping between the URL parameter(s) and its value. import * as React from 'react' import { useParams } from 'react-router-dom' import { getArticle } from '../utils' function Article () { const [article, setArticle] = React.useState(null) const { topicId } = useParams() React.useEffect(() => { getArticle(topicId) .then(setUser) }, [topicId]) return ( ... ) } Nested Routes Nested Routes allow the parent Route to act as a wrapper and control the rendering of a child Route. A real-life example of this UI could look similar to Twitter's /messages route. When you go to /messages, you see all of your previous conversations on the left side of the screen. Then, when you go to /messages/:id, you still see all your messages, but you also see your chat history for :id. Let's look at how we could implement this sort of nested routes pattern with React Router. We'll start off with some basic Routes. // App.js function App() { return ( <Routes> <Route path=\"/\" element={<Home />} /> <Route path=\"/messages\" element={<Messages />} /> <Route path=\"/settings\" element={<Settings />} /> </Routes> ); } Now, if we want Messages to be in control of rendering a child Routes, what's stopping us from just rendering another Routes component inside Messages? Something like this: function Messages() { return ( <Container> <Conversations /> <Routes> <Route path=\":id\" element={<Chat />} /> </Routes> </Container> ); } Now when the user navigates to /messages, React Router renders the Messages component. From there, Messages shows all our conversations via the Conversations component and then renders another Routes with a Route that maps /messages/:id to the Chat component. Relative Routes Notice that we don't have to include the full /messages/:id path in the nested Route. This is because Routes is intelligent and by leaving off the leading /, it assumes we want this path to be relative to the parent's location, /messages. Looks good, but there's one subtle issue. Can you spot it? Messages only gets rendered when the user is at /messages. When they visit a URL that matches the /messages/:id pattern, Messages no longer matches and therefore, our nested Routes never gets rendered. To fix this, naturally, we need a way to tell React Router that we want to render Messages both when the user is at /messages or any other location that matches the /messages/* pattern. Wait. What if we just update our path to be /messages/*? // App.js function App() { return ( <Routes> <Route path=\"/\" element={<Home />} /> <Route path=\"/messages/*\" element={<Messages />} /> <Route path=\"/settings\" element={<Settings />} /> </Routes> ); } Much to our delight, that'll work. By appending a / to the end of our /messages path, we're essentially telling React Router that Messages has a nested Routes component and our parent path should match for /messages as well as any other location that matches the /messages/ pattern. Exactly what we wanted. At this point, we've looked at how you can create nested routes by appending /* to our Route's path and rendering, literally, a nested Routes component. This works when you want your child Route in control of rendering the nested Routes, but what if we wanted our App component to contain all the information it needed to create our nested routes rather than having to do it inside of Messages? Because this is a common preference, React Router supports this way of creating nested routes as well. Here's what it looks like. function App() { return ( <Routes> <Route path=\"/\" element={<Home />} /> <Route path=\"/messages\" element={<Messages />}> <Route path=\":id\" element={<Chats />} /> </Route> <Route path=\"/settings\" element={<Settings />} /> </Routes> ); } You declaratively nest the child Route as a children of the parent Route. Like before, the child Route is now relative to the parent, so you don't need to include the parent (/messages) path. Now, the last thing you need to do is tell React Router where in the parent Route (Messages) should it render the child Route (Chats). To do this, you use React Router's Outlet component. import { Outlet } from \"react-router-dom\"; function Messages() { return ( <Container> <Conversations /> <Outlet /> </Container> ); } If the app's location matches the nested Route's path, this Outlet component will render the Route's element. So based on our Routes above, if we were at /messages, the Outlet component would render null, but if we were at /messages/1, it would render the component. Pass props to Router Components In previous versions of React Router (v4), this was non-trivial since React Router was in charge of creating the React element. However, with React Router v6, since you're in charge of creating the element, you just pass a prop to the component as you normally would. <Route path=\"/dashboard\" element={<Dashboard authed={true} />} /> Good Practices for Calling APIs from ReactJS https://medium.com/weekly-webtips/patterns-for-doing-api-calls-in-reactjs-8fd9a42ac7d4 JSX JSX is a syntax extension for JavaScript. It was written to be used with React. JSX code looks a lot like HTML. What does \"syntax extension\" mean? In this case, it means that JSX is not valid JavaScript. Web browsers can\u2019t read it! If a JavaScript file contains JSX code, then that file will have to be compiled. That means that before the file reaches a web browser, a JSX compiler will translate any JSX into regular JavaScript. JSX Elements A basic unit of JSX is called a JSX element. Here\u2019s an example of a JSX element: <h1>Hello world</h1> JSX Elements And Their Surroundings JSX elements are treated as JavaScript expressions. They can go anywhere that JavaScript expressions can go. That means that a JSX element can be saved in a variable, passed to a function, stored in an object or array\u2026you name it. Here\u2019s an example of a JSX element being saved in a variable: const navBar = <nav>I am a nav bar</nav>; const myTeam = { center: <li>Benzo Walli</li>, powerForward: <li>Rasha Loa</li>, smallForward: <li>Tayshaun Dasmoto</li>, shootingGuard: <li>Colmar Cumberbatch</li>, pointGuard: <li>Femi Billon</li> }; Attributes In JSX JSX elements can have attributes, just like HTML elements can. A JSX attribute is written using HTML-like syntax: a name, followed by an equals sign, followed by a value. The value should be wrapped in quotes, like this: my-attribute-name=\"my-attribute-value\" <a href='http://www.example.com'>Welcome to the Web</a>; const title = <h1 id='title'>Introduction to React.js: Part I</h1>; const panda = <img src='images/panda.jpg' alt='panda' width='500px' height='500px' />; Nested JSX If a JSX expression takes up more than one line, then you must wrap the multi-line JSX expression in parentheses. This looks strange at first, but you get used to it: const theExample = ( <a href=\"https://www.example.com\"> <h1> Click me! </h1> </a> ) JSX Outer Elements There\u2019s a rule that we haven\u2019t mentioned: a JSX expression must have exactly one outermost element. In other words, this code will work: const paragraphs = ( <div id=\"i-am-the-outermost-element\"> <p>I am a paragraph.</p> <p>I, too, am a paragraph.</p> </div> ); // But this code will not work: const paragraphs = ( <p>I am a paragraph.</p> <p>I, too, am a paragraph.</p> ); Rendering JSX The following code will render a JSX expression: ReactDOM.render(<h1>Hello world</h1>, document.getElementById('app')); ReactDOM.render() ReactDOM is the name of a JavaScript library. This library contains several React-specific methods, all of which deal with the DOM in some way or another. When a web page is loaded, the browser creates a Document Object Model of the page. The HTML DOM model is constructed as a tree of Objects: ReactDOM.render() is the most common way to render JSX. It takes a JSX expression, creates a corresponding tree of DOM nodes, and adds that tree to the DOM. That is the way to make a JSX expression appear onscreen. In the code ReactDOM.render(<h1>Render me!</h1>, document.getElementById('app')); the expression <h1>Render me!</h1> is what you want rendered. The second argument document.getElementById('app') indicates where you want to append the first argument in the DOM. Ex: if you had the following HTML: <!DOCTYPE html> <html lang=\"en\"> <head> <meta charset=\"utf-8\"> <link rel=\"stylesheet\" href=\"/styles.css\"> <title>Learn ReactJS</title> </head> <body> <main id=\"app\"></main> <script src=\"https://content.codecademy.com/courses/React/react-course-bundle.min.js\"></script> <script src=\"/app.compiled.js\"></script> </body> </html> The element with the ID would be selected and the DOM added to it. One special thing about ReactDOM.render() is that it only updates DOM elements that have changed. That means that if you render the exact same thing twice in a row, the second render will do nothing. Passing a Variable to ReactDOM.render() ReactDOM.render()\u2018s first argument should evaluate to a JSX expression, it doesn\u2019t have to literally be a JSX expression. The first argument could also be a variable, so long as that variable evaluates to a JSX expression. class vs className <h1 class=\"big\">Hey</h1> In JSX, you can\u2019t use the word class! You have to use className instead: <h1 className=\"big\">Hey</h1> Self-Closing Tags With self closing tags you MUST include the slash in JSX. Ex: <br /> . The trailing / isn't optional. Javascript in JSX Te render Javascript in JSX, you have to use curly braces. Ex: import React from 'react'; import ReactDOM from 'react-dom'; // Write code here: ReactDOM.render( <h1>{2 + 3}</h1>, document.getElementById('app') ); Variables in JSX When you inject JavaScript into JSX, that JavaScript is part of the same environment as the rest of the JavaScript in your file. That means that you can access variables while inside of a JSX expression, even if those variables were declared on the outside. You can set HTML attributes with curly braces like this: // Use a variable to set the `height` and `width` attributes: const sideLength = \"200px\"; const panda = ( <img src=\"images/panda.jpg\" alt=\"panda\" height={sideLength} width={sideLength} /> ); Event Listeners in JSX JSX elements can have event listeners, just like HTML elements can. Programming in React means constantly working with event listeners. You create an event listener by giving a JSX element a special attribute. Here\u2019s an example: <img onClick={myFunc} /> An event listener attribute\u2019s name should be something like onClick or onMouseOver: the word on, plus the type of event that you\u2019re listening for. You can see a list of valid event names here . An event listener attribute\u2019s value should be a function. The above example would only work if myFunc were a valid function that had been defined elsewhere: function myFunc() { alert('Make myFunc the pFunc... omg that was horrible i am so sorry'); } <img onClick={myFunc} /> JSX Conditionals This code will break: ( <h1> { if (purchase.complete) { 'Thank you for placing an order!' } } </h1> ) The reason why has to do with the way that JSX is compiled. You don\u2019t need to understand the mechanics of it for now, but if you\u2019re interested then you can learn more in the React documentation . How can you write a conditional, if you can\u2019t inject an if statement into JSX? Well, one option is to write an if statement, and not inject it into JSX. Look at if.js. Follow the if statement, all the way from line 6 down to line 18. if.js works, because the words if and else are not injected in between JSX tags. The if statement is on the outside, and no JavaScript injection is necessary. import React from 'react'; import ReactDOM from 'react-dom'; let message; if (user.age >= drinkingAge) { message = ( <h1> Hey, check out this alcoholic beverage! </h1> ); } else { message = ( <h1> Hey, check out these earrings I got at Claire's! </h1> ); } ReactDOM.render( message, document.getElementById('app') ); Ternary Operator Recall how it works: you write x ? y : z, where x, y, and z are all JavaScript expressions. When your code is executed, x is evaluated as either \"truthy\" or \"falsy.\" If x is truthy, then the entire ternary operator returns y. If x is falsy, then the entire ternary operator returns z. Here\u2019s a nice explanation if you need a refresher. const headline = ( <h1> { age >= drinkingAge ? 'Buy Drink' : 'Do Teen Stuff' } </h1> ); && && works best in conditionals that will sometimes do an action, but other times do nothing at all. Here\u2019s an example: const tasty = ( <ul> <li>Applesauce</li> { !baby && <li>Pizza</li> } { age > 15 && <li>Brussels Sprouts</li> } { age > 20 && <li>Oysters</li> } { age > 25 && <li>Grappa</li> } </ul> ); If the expression on the left of the && evaluates as true, then the JSX on the right of the && will be rendered. If the first expression is false, however, then the JSX to the right of the && will be ignored and not rendered. .map in JSX If you want to create a list of JSX elements, then .map() is often your best bet. It can look odd at first: const strings = ['Home', 'Shop', 'About Me']; const listItems = strings.map(string => <li>{string}</li>); <ul>{listItems}</ul> In the above example, we start out with an array of strings. We call .map() on this array of strings, and the .map() call returns a new array of s. If you want the index you can do: const listItems = strings.map((string, i) => <li>{string}</li>); List Keys When you make a list in JSX, sometimes your list will need to include something called keys: <ul> <li key=\"li-01\">Example1</li> <li key=\"li-02\">Example2</li> <li key=\"li-03\">Example3</li> </ul> A key is a JSX attribute. The attribute\u2019s name is key. The attribute\u2019s value should be something unique, similar to an id attribute. keys don\u2019t do anything that you can see! React uses them internally to keep track of lists. If you don\u2019t use keys when you\u2019re supposed to, React might accidentally scramble your list-items into the wrong order. Not all lists need to have keys. A list needs keys if either of the following are true: The list-items have memory from one render to the next. For instance, when a to-do list renders, each item must \"remember\" whether it was checked off. The items shouldn\u2019t get amnesia when they render. A list\u2019s order might be shuffled. For instance, a list of search results might be shuffled from one render to the next. import React from 'react'; import ReactDOM from 'react-dom'; const people = ['Rowe', 'Prevost', 'Gare']; const peopleLis = people.map((person, i) => // expression goes here: <li key={'person_' + i}>{person}</li> ); // ReactDOM.render goes here: ReactDOM.render(<ul>{peopleLis}</ul>, document.getElementById('app')) React Create Element You can write React code without using JSX at all! The majority of React programmers do use JSX, and we will use it for the remainder of this tutorial, but you should understand that it is possible to write React code without it. The following JSX expression: const h1 = <h1>Hello world</h1>; can be rewritten without JSX, like this: const h1 = React.createElement( \"h1\", null, \"Hello world\" ); When a JSX element is compiled, the compiler transforms the JSX element into the method that you see above: React.createElement(). Every JSX element is secretly a call to React.createElement().","title":"Notes on nodejs"},{"location":"Notes%20on%20nodejs/#notes-on-nodejs","text":"Notes on nodejs Required Javascript Arrow expressions Promises Resolve and Reject Async/Await setInterval() and setTimeout() Node The Node REPL (read-eval-print loop) Running a Program with Node Core Modules Console Module The Process Module The OS Module The Util Module NPM Create a new app nodemon Package Scope Global Packages Installing a Custom Package Modules Exporting Require Using Object Destructuring to be more Selective With require() The Events Module User Input and Output The Error Module Why Error First Callbacks The Buffer Module Readable Streams Further explanation Writable Streams Timers Modules HTTP Server The URL Module Routing Longer Example Returning a Status Code Express Request Object Properties Request Object Methods Response Object Knex.js How does exports.up and exports.down work Seed Files Babel ReactJS Importing React Required Code Components Create a Component Class The Render Function Create a Component Instance Use This in a Class Render Components with Components Importing Files and Exporting Functionality Importing Exporting Component Props Event Handler handleEvent, onEvent, and this.props.onEvent this.props.children Default Properties Component State this.setState from Another Function Component Lifecycle componentDidMount componentWillUnmount componentDidUpdate Stateless Functional Components Function Component Props React Hooks Comparison Class vs Function Update Function Component State Initialize State Use State Setter Outside of JSX Longer Example Set From Previous State Arrays in State Objects in State Longer Example Separate Hooks for Separate States Comparison The Effect Hook - useEffect React Hooks and Component Lifecycle Equivalent componentWillMount for react functional component? Function Component Effects Clean Up Effects Control When Effects are Called Fetch Data from a Server Rules of Hooks Separate Hooks for Separate Effects Stateless Components from Stateful Components Build a Stateful Component Class Don't Update props Child Components Update Their Parents' State More Complex Example Child Components Update Sibling Components One Sibling to Display, Another to Change Style Inline Styles Make a Style Object Variable Share Styles Across Multiple Components Separate Container Components from Presentational Components Create a Container Component Create a Presentational Component propTypes Apply PropTypes PropTypes in Function Components React Forms Input on Change Control vs Uncontrolled Update an Input's Value Set the Input's Initial State Dynamically Rendering Different Components without Switch: the Capitalized Reference Technique React Router BrowserRouter Route Routes Links URL Parameters Nested Routes Pass props to Router Components Good Practices for Calling APIs from ReactJS JSX JSX Elements JSX Elements And Their Surroundings Attributes In JSX Nested JSX JSX Outer Elements Rendering JSX ReactDOM.render() Passing a Variable to ReactDOM.render() class vs className Self-Closing Tags Javascript in JSX Variables in JSX Event Listeners in JSX JSX Conditionals Ternary Operator && .map in JSX List Keys React Create Element","title":"Notes on nodejs"},{"location":"Notes%20on%20nodejs/#required-javascript","text":"","title":"Required Javascript"},{"location":"Notes%20on%20nodejs/#arrow-expressions","text":"Let\u2019s take a look at the code below. You will see two different functions defined. The first is anonymous (function is not named), and the second is named. When using an arrow expression, we do not use the function declaration. To define an arrow expression you simply use: () => { }. You can pass arguments to an arrow expression between the parenthesis (()). // Defining an anonymous arrow expression that simply logs a string to the console. console.log(() => console.log('Shhh, Im anonymous')); // Defining a named function by creating an arrow expression and saving it to a const variable helloWorld. const helloWorld = (name) => { console.log(`Welcome ${name} to Codecademy, this is an arrow expression.`) }; // Calling the helloWorld() function. helloWorld('Codey'); //Output: Welcome Codey to Codecademy, this is an Arrow Function Expression.","title":"Arrow expressions"},{"location":"Notes%20on%20nodejs/#promises","text":"A Promise is a JavaScript object that represents the eventual outcome of an asynchronous operation. A Promise has three different outcomes: pending (the result is undefined and the expression is waiting for a result), fulfilled (the promise has been completed successfully and returned a value), and rejected (the promise did not successfully complete, the result is an error object). In the code below a new Promise is being defined and is passed a function that takes two arguments, a fulfilled condition, and a rejected condition. We then log the returned value of the Promise to the console and chain a .catch() method to handle errors. // Creating a new Promise and saving it to the testLuck variable. Two arguments are being passed, one for when the promise resolves, and one for if the promise gets rejected. const testLuck = new Promise((resolve, reject) => { if (Math.random() < 0.5) { resolve('Lucky winner!') } else { reject(new Error('Unlucky!')) } }); testLuck.then(message => { console.log(message) // Log the resolved value of the Promise }).catch(error => { console.error(error) // Log the rejected error of the Promise });","title":"Promises"},{"location":"Notes%20on%20nodejs/#resolve-and-reject","text":"The Promise constructor method takes a function parameter called the executor function which runs automatically when the constructor is called. The executor function generally starts an asynchronous operation and dictates how the promise should be settled. The executor function has two function parameters, usually referred to as the resolve() and reject() functions. The resolve() and reject() functions aren\u2019t defined by the programmer. When the Promise constructor runs, JavaScript will pass its own resolve() and reject() functions into the executor function. resolve is a function with one argument. Under the hood, if invoked, resolve() will change the promise\u2019s status from pending to fulfilled, and the promise\u2019s resolved value will be set to the argument passed into resolve(). reject is a function that takes a reason or error as an argument. Under the hood, if invoked, reject() will change the promise\u2019s status from pending to rejected, and the promise\u2019s rejection reason will be set to the argument passed into reject().","title":"Resolve and Reject"},{"location":"Notes%20on%20nodejs/#asyncawait","text":"The async...await syntax allows developers to easily implement Promise-based code. The keyword async used in conjunction with a function declaration creates an async function that returns a Promise. Async functions allow us to use the keyword await to block the event loop until a given Promise resolves or rejects. The await keyword also allows us to assign the resolved value of a Promise to a variable. Let\u2019s take a look at the code below. In the code below an asynchronous arrow expression is defined with the async keyword. In the function body we are creating a new Promise which passes a function that is executed after 5 seconds, we await the Promise to resolve and save the value returned to finalResult, and the output of the Promise is logged to the console. // Creating a new promise that runs the function in the setTimeout after 5 seconds. const newPromise = new Promise((resolve, reject) => { setTimeout(() => resolve(\"All done!\"), 5000); }); // Creating an asynchronous function using an arrow expression and saving it to a the variable asyncFunction. const asyncFunction = async () => { // Awaiting the promise to resolve and saving the result to the variable finalResult. const finalResult = await newPromise; // Logging the result of the promise to the console console.log(finalResult); // Output: All done! } asyncFunction();","title":"Async/Await"},{"location":"Notes%20on%20nodejs/#setinterval-and-settimeout","text":"In addition to utilizing the async...await syntax, we can also use the setInterval() and setTimeout() functions. In the example code of the previous section, we created a setTimeout() instance in the Promise constructor. The setInterval() function executes a code block at a specified interval, in milliseconds. The setInterval() function requires two arguments: the name of the function (the code block that will be executed), and the number of milliseconds (how often the function will be executed). Optionally, we can pass additional arguments which will be supplied as parameters for the function that will be executed by setInterval(). The setInterval() function will continue to execute until the clearInterval() function is called or the node process is exited. In the code block below, the setInterval() function in the showAlert() function will display an alert box every 5000 milliseconds. // Defining a function that instantiates setInterval const showAlert = () => { // Calling setInterval() and passing a function that shows an alert every 5 seconds. setInterval(() => { alert('I show every 5 seconds!') }, 5000); }; // Calling the newInterval() function that calls the setInterval showAlert(); The setTimeout() function executes a code block after a specified amount of time (in milliseconds) and is only executed once. The setTimeout() function accepts the same arguments as the setInterval() function. Using the clearTimeout() function will prevent the function specified from being executed. In the code block below, a function named showTimeout() is declared as an arrow expression. The setTimeout() function is then defined and displays an alert box after 5 seconds. // Defining a function that calls setTimeout const showTimeout = () => { // Calling setTimeout() that passes a function that shows an alert after 5 seconds. setTimeout(() => { alert('I only show once after 5 seconds!'); }, 5000); }; // Calling the showTimeout() function showTimeout();","title":"setInterval() and setTimeout()"},{"location":"Notes%20on%20nodejs/#node","text":"","title":"Node"},{"location":"Notes%20on%20nodejs/#the-node-repl-read-eval-print-loop","text":"REPL is an abbreviation for read\u2013eval\u2013print loop. It\u2019s a program that loops, or repeatedly cycles, through three different states: a read state where the program reads input from a user, the eval state where the program evaluates the user\u2019s input, and the print state where the program prints out its evaluation to a console. Then it loops through these states again. It's just the equivalent of typing python except for javascript. Type node to get to it. To see global vars see Object.keys(global) . You can add to it with global.cat = 'thing' . Print with console.log(global.cat) If you\u2019re familiar with running JavaScript on the browser, you\u2019ve likely encountered the Window object. Here\u2019s one major way that Node differs: try to access the Window object (this will throw an error). The Window object is the JavaScript object in the browser that holds the DOM, since we don\u2019t have a DOM here, there\u2019s no Window object.","title":"The Node REPL (read-eval-print loop)"},{"location":"Notes%20on%20nodejs/#running-a-program-with-node","text":"node program","title":"Running a Program with Node"},{"location":"Notes%20on%20nodejs/#core-modules","text":"Include a module: // Require in the 'events' core module: const events = require('events'); Some core modules are actually used inside other core modules. For instance, the util module can be used in the console module to format messages. We\u2019ll cover these two modules in this lesson, as well as two other commonly used core modules: process and os. See all builtin modules: require('module').builtinModules","title":"Core Modules"},{"location":"Notes%20on%20nodejs/#console-module","text":"Since console is a global module, its methods can be accessed from anywhere, and the require() function is not necessary. .log() - prints messages to the terminal .assert() - prints a message to the terminal if the value is falsey console.assert(petsArray.length > 5); .table() - prints out a table in the terminal from an object or array","title":"Console Module"},{"location":"Notes%20on%20nodejs/#the-process-module","text":"Node has a global process object with useful methods and information about the current process. The console.log() method is a \"thin wrapper\" on the .stdout.write() method of the process object. The process.env property is an object which stores and controls information about the environment in which the process is currently running. For example, the process.env object contains a PWD property which holds a string with the directory in which the current process is located. It can be useful to have some if/else logic in a program depending on the current environment\u2014 a web application in a development phase might perform different tasks than when it\u2019s live to users. We could store this information on the process.env. One convention is to add a property to process.env with the key NODE_ENV and a value of either production or development. if (process.env.NODE_ENV === 'development'){ console.log('Testing! Testing! Does everything work?'); } The process.memoryUsage() returns information on the CPU demands of the current process. It returns a property that looks similar to this: { rss: 26247168, heapTotal: 5767168, heapUsed: 3573032, external: 8772 } process.argv holds an array of command line values provided when the current process was initiated.","title":"The Process Module"},{"location":"Notes%20on%20nodejs/#the-os-module","text":"const os = require('os'); os.type() \u2014 to return the computer\u2019s operating system. os.arch() \u2014 to return the operating system CPU architecture. os.networkInterfaces() \u2014 to return information about the network interfaces of the computer, such as IP and MAC address. os.homedir() \u2014 to return the current user\u2019s home directory. os.hostname() \u2014 to return the hostname of the operating system. os.uptime() \u2014 to return the system uptime, in seconds. Create an empty object const object = {}; Instantiate a dictionary: const os = require('os'); const server = {type: os.type(), architecture: os.arch(), uptime: os.uptime()}; console.table(server)","title":"The OS Module"},{"location":"Notes%20on%20nodejs/#the-util-module","text":"Developers sometimes classify outlier functions used to maintain code and debug certain aspects of a program\u2019s functionality as utility functions. Utility functions don\u2019t necessarily create new functionality in a program, but you can think of them as internal tools used to maintain and debug your code. The Node.js util core module contains methods specifically designed for these purposes. const util = require('util'); Get the type of an object : const util = require('util'); const today = new Date(); const earthDay = 'April 22, 2022'; console.log(util.types.isDate(today)); console.log(util.types.isDate(earthDay)); Turn callback functions into promises : Another important util method is .promisify(), which turns callback functions into promises. As you know, asynchronous programming is essential to Node.js. In the beginning, this asynchrony was achieved using error-first callback functions, which are still very prevalent in the Node ecosystem today. But since promises are often preferred over callbacks and especially nested callbacks, Node offers a way to turn these into promises. Let\u2019s take a look: function getUser (id, callback) { return setTimeout(() => { if (id === 5) { callback(null, { nickname: 'Teddy' }) } else { callback(new Error('User not found')) } }, 1000) } function callback (error, user) { if (error) { console.error(error.message) process.exit(1) } console.log(`User found! Their nickname is: ${user.nickname}`) } getUser(1, callback) // -> `User not found` getUser(5, callback) // -> `User found! Their nickname is: Teddy` You can convert the above to: const getUserPromise = util.promisify(getUser); getUserPromise(id) .then((user) => { console.log(`User found! Their nickname is: ${user.nickname}`); }) .catch((error) => { console.log('User not found', error); }); getUser(1) // -> `User not found` getUser(5) // -> `User found! Their nickname is: Teddy` We declare a getUserPromise variable that stores the getUser method turned into a promise using the .promisify() method. With that in place, we\u2019re able to use getUserPromise with .then() and .catch() methods (or we could also use the async...await syntax here) to resolve the promise returned or catch any errors.","title":"The Util Module"},{"location":"Notes%20on%20nodejs/#npm","text":"","title":"NPM"},{"location":"Notes%20on%20nodejs/#create-a-new-app","text":"npm init Add -y to answer yes to everything. This will generate a package.json file: { \"name\": \"my-project\", \"version\": \"1.0.0\", \"description\": \"a basic project\", \"main\": \"index.js\", \"scripts\": { \"test\": \"echo \\\"Error: no test specified\\\" && exit 1\" }, \"author\": \"Super Coder\", \"license\": \"ISC\", \"dependencies\": { \"express\": \"^4.17.1\" }, }","title":"Create a new app"},{"location":"Notes%20on%20nodejs/#nodemon","text":"Automatically restart a program when a file changes. npm install nodemon The npm i <package name> command installs a package locally in a folder called node_modules/ which is created in the project directory that you ran the command from. In addition, the newly installed package will be added to the package.json file.","title":"nodemon"},{"location":"Notes%20on%20nodejs/#package-scope","text":"While most dependencies play a direct role in the functionality of your application, development dependencies are used for the purpose of making development easier or more efficient. In fact, the nodemon package is actually better suited as a development dependency since it makes developers\u2019 lives easier but makes no changes to the app itself. To install nodemon as a development dependency, we can add the --save-dev flag, or its alias, -D. npm install nodemon --save-dev Development dependencies are listed in the \"devDependencies\" field of the package.json file. This indicates that the package is being used specifically for development and will not be included in a production release of the project. { \"name\": \"my-project\", \"version\": \"1.0.0\", \"description\": \"a basic project\", \"main\": \"index.js\", \"scripts\": { \"test\": \"echo \\\"Error: no test specified\\\" && exit 1\" }, \"author\": \"\", \"license\": \"ISC\", \"dependencies\": { \"express\": \"^4.17.1\" }, \"devDependencies\": { \"nodemon\": \"^2.0.13\" } }","title":"Package Scope"},{"location":"Notes%20on%20nodejs/#global-packages","text":"Typically, packages installed this way will be used in the command-line rather than imported into a project\u2019s code. One such example is the http-server package which allows you to spin up a zero-configuration server from anywhere in the command-line. To install a package globally, use the -g flag with the installation command: npm install http-server -g http-server is a good package to install globally since it is a general command-line utility and its purpose is not linked to any specific functionality within an app. Unlike local package dependencies or development dependencies, packages installed globally will not be listed in a projects package.json file and they will be stored in a separate global node_modules/ folder.","title":"Global Packages"},{"location":"Notes%20on%20nodejs/#installing-a-custom-package","text":"If you want to give someone else your package you can provide the package.json file and then they can install with npm i . Add --production to leave out the dev dependencies.","title":"Installing a Custom Package"},{"location":"Notes%20on%20nodejs/#modules","text":"There are multiple ways of implementing modules depending on the runtime environment in which your code is executed. In JavaScript, there are two runtime environments and each has a preferred module implementation: The Node runtime environment and the module.exports and require() syntax. The browser\u2019s runtime environment and the ES6 import/export syntax.","title":"Modules"},{"location":"Notes%20on%20nodejs/#exporting","text":"/* converters.js */ function celsiusToFahrenheit(celsius) { return celsius * (9/5) + 32; } module.exports.celsiusToFahrenheit = celsiusToFahrenheit; module.exports.fahrenheitToCelsius = function(fahrenheit) { return (fahrenheit - 32) * (5/9); }; At the top of the new file, converters.js, the function celsiusToFahrenheit() is declared. On the next line of code, the first approach for exporting a function from a module is shown. In this case, the already-defined function celsiusToFahrenheit() is assigned to module.exports.celsiusToFahrenheit. Below, an alternative approach for exporting a function from a module is shown. In this second case, a new function expression is declared and assigned to module.exports.fahrenheitToCelsius. This new method is designed to convert Fahrenheit values back to Celsius. Both approaches successfully store a function within the module.exports object. module.exports is an object that is built-in to the Node.js runtime environment. Other files can now import this object, and make use of these two functions, with another feature that is built-in to the Node.js runtime environment: the require() function.","title":"Exporting"},{"location":"Notes%20on%20nodejs/#require","text":"The require() function accepts a string as an argument. That string provides the file path to the module you would like to import. Let\u2019s update water-limits.js such that it uses require() to import the .celsiusToFahrenheit() method from the module.exports object within converters.js: /* water-limits.js */ const converters = require('./converters.js'); const freezingPointC = 0; const boilingPointC = 100; const freezingPointF = converters.celsiusToFahrenheit(freezingPointC); const boilingPointF = converters.celsiusToFahrenheit(boilingPointC); console.log(`The freezing point of water in Fahrenheit is ${freezingPointF}`); console.log(`The boiling point of water in Fahrenheit is ${boilingPointF}`);","title":"Require"},{"location":"Notes%20on%20nodejs/#using-object-destructuring-to-be-more-selective-with-require","text":"In many cases, modules will export a large number of functions but only one or two of them are needed. You can use object destructuring to extract only the needed functions. Let\u2019s update celsius-to-fahrenheit.js and only extract the .celsiusToFahrenheit() method, leaving .fahrenheitToCelsius() behind: /* celsius-to-fahrenheit.js */ const { celsiusToFahrenheit } = require('./converters.js'); const celsiusInput = process.argv[2]; const fahrenheitValue = celsiusToFahrenheit(celsiusInput); console.log(`${celsiusInput} degrees Celsius = ${fahrenheitValue} degrees Fahrenheit`); Notice that the first line used to be const converters = require('./converters.js'); and now it is specifying the exported function.","title":"Using Object Destructuring to be more Selective With require()"},{"location":"Notes%20on%20nodejs/#the-events-module","text":"Node provides an EventEmitter class which we can access by requiring in the events core module: // Require in the 'events' core module let events = require('events'); // Create an instance of the EventEmitter class let myEmitter = new events.EventEmitter(); Each event emitter instance has an .on() method which assigns a listener callback function to a named event. The .on() method takes as its first argument the name of the event as a string and, as its second argument, the listener callback function. Each event emitter instance also has an .emit() method which announces a named event has occurred. The .emit() method takes as its first argument the name of the event as a string and, as its second argument, the data that should be passed let newUserListener = (data) => { console.log(`We have a new user: ${data}.`); }; // Assign the newUserListener function as the listener callback for 'new user' events myEmitter.on('new user', newUserListener) // Emit a 'new user' event myEmitter.emit('new user', 'Lily Pad') //newUserListener will be invoked with 'Lily Pad' Note There is no link between the variable data in the constructer for the event emitter and the new user name.","title":"The Events Module"},{"location":"Notes%20on%20nodejs/#user-input-and-output","text":"Notice that for user input and output for something like stdin what you're really doing is registering a callback and then calling it on user input. Ex: process.stdin.on('data', (userInput) => { let input = userInput.toString() console.log(input) }); Notice the on and then here we're just defining an anonymous function.","title":"User Input and Output"},{"location":"Notes%20on%20nodejs/#the-error-module","text":"The Node environment\u2019s error module has all the standard JavaScript errors such as EvalError, SyntaxError, RangeError, ReferenceError, TypeError, and URIError as well as the JavaScript Error class for creating new error instances. Within our own code, we can generate errors and throw them, and, with synchronous code in Node, we can use error handling techniques such as try...catch statements. Note that the error module is within the global scope\u2014there is no need to import the module with the require() statement. Many asynchronous Node APIs use error-first callback functions\u2014callback functions which have an error as the first expected argument and the data as the second argument. If the asynchronous task results in an error, it will be passed in as the first argument to the callback function. If no error was thrown, the first argument will be undefined. const errorFirstCallback = (err, data) => { if (err) { console.log(`There WAS an error: ${err}`); } else { // err was falsy console.log(`There was NO error. Event data: ${data}`); } }","title":"The Error Module"},{"location":"Notes%20on%20nodejs/#why-error-first-callbacks","text":"You need this because if you try something like: const api = require('./api.js'); // Not an error-first callback let callbackFunc = (data) => { console.log(`Something went right. Data: ${data}\\n`); }; try { api.naiveErrorProneAsyncFunction('problematic input', callbackFunc); } catch(err) { console.log(`Something went wrong. ${err}\\n`); } then the try-catch won't work because the error is thrown in the context of the separate thread spawned asynchronously and subsequently never caught because Javascript is a garbage programming language.","title":"Why Error First Callbacks"},{"location":"Notes%20on%20nodejs/#the-buffer-module","text":"In Node.js, the Buffer module is used to handle binary data. The Buffer module is within the global scope, which means that Buffer objects can be accessed anywhere in the environment without importing the module with require(). A Buffer object represents a fixed amount of memory that can\u2019t be resized. Buffer objects are similar to an array of integers where each element in the array represents a byte of data. The buffer object will have a range of integers from 0 to 255 inclusive. The Buffer module provides a variety of methods to handle the binary data such as .alloc(), .toString(), .from(), and .concat(). The .alloc() method creates a new Buffer object with the size specified as the first parameter. .alloc() accepts three arguments: Size: Required. The size of the buffer Fill: Optional. A value to fill the buffer with. Default is 0. Encoding: Optional. Default is UTF-8. const buffer = Buffer.alloc(5); console.log(buffer); // Ouput: [0, 0, 0, 0, 0] The .toString() method translates the Buffer object into a human-readable string. It accepts three optional arguments: Encoding: Default is UTF-8. Start: The byte offset to begin translating in the Buffer object. Default is 0. End: The byte offset to end translating in the Buffer object. Default is the length of the buffer. The start and end of the buffer are similar to the start and end of an array, where the first element is 0 and increments upwards. const buffer = Buffer.alloc(5, 'a'); console.log(buffer.toString()); // Output: aaaaa The .from() method is provided to create a new Buffer object from the specified string, array, or buffer. The method accepts two arguments: Object: Required. An object to fill the buffer with. Encoding: Optional. Default is UTF-8. const buffer = Buffer.from('hello'); console.log(buffer); // Output: [104, 101, 108, 108, 111] The .concat() method joins all buffer objects passed in an array into one Buffer object. .concat() comes in handy because a Buffer object can\u2019t be resized. This method accepts two arguments: Array: Required. An array containing Buffer objects. Length: Optional. Specifies the length of the concatenated buffer. const buffer1 = Buffer.from('hello'); // Output: [104, 101, 108, 108, 111] const buffer2 = Buffer.from('world'); // Output:[119, 111, 114, 108, 100] const array = [buffer1, buffer2]; const bufferConcat = Buffer.concat(array); console.log(bufferConcat); // Output: [104, 101, 108, 108, 111, 119, 111, 114, 108, 100]","title":"The Buffer Module"},{"location":"Notes%20on%20nodejs/#readable-streams","text":"const readline = require('readline'); const fs = require('fs'); const myInterface = readline.createInterface({ input: fs.createReadStream('shoppingList.txt') }); const printData = (data) => { console.log(`Item: ${data}`); }; myInterface.on('line', printData);","title":"Readable Streams"},{"location":"Notes%20on%20nodejs/#further-explanation","text":"One of the simplest uses of streams is reading and writing to files line-by-line. To read files line-by-line, we can use the .createInterface() method from the readline core module. .createInterface() returns an EventEmitter set up to emit 'line' events: const readline = require('readline'); const fs = require('fs'); const myInterface = readline.createInterface({ input: fs.createReadStream('text.txt') }); myInterface.on('line', (fileLine) => { console.log(`The line read: ${fileLine}`); }); Let\u2019s walk through the above code: We require in the readline and fs core modules. We assign to myInterface the returned value from invoking readline.createInterface() with an object containing our designated input. We set our input to fs.createReadStream('text.txt') which will create a stream from the text.txt file. Next we assign a listener callback to execute when line events are emitted. A 'line' event will be emitted after each line from the file is read. Our listener callback will log to the console 'The line read: [fileLine]', where [fileLine] is the line just read.","title":"Further explanation"},{"location":"Notes%20on%20nodejs/#writable-streams","text":"const readline = require('readline'); const fs = require('fs'); const myInterface = readline.createInterface({ input: fs.createReadStream('shoppingList.txt') }); const fileStream = fs.createWriteStream('shoppingResults.txt'); let transformData = (line) => { fileStream.write(`They were out of: ${line}\\n`); }; myInterface.on('line', transformData);","title":"Writable Streams"},{"location":"Notes%20on%20nodejs/#timers-modules","text":"You may already be familiar with some timer functions such as, setTimeout() and setInterval(). Timer functions in Node.js behave similarly to how they work in front-end JavaScript programs, but the difference is that they are added to the Node.js event loop. This means that the timer functions are scheduled and put into a queue. This queue is processed at every iteration of the event loop. If a timer function is executed outside of a module, the behavior will be random (non-deterministic). The setImmediate() function is often compared with the setTimeout() function. When setImmediate() is called, it executes the specified callback function after the current (poll phase) is completed. The method accepts two parameters: the callback function (required) and arguments for the callback function (optional). If you instantiate multiple setImmediate() functions, they will be queued for execution in the order that they were created.","title":"Timers Modules"},{"location":"Notes%20on%20nodejs/#http-server","text":"To process HTTP requests in JavaScript and Node.js, we can use the built-in http module. This core module is key in leveraging Node.js networking and is extremely useful in creating HTTP servers and processing HTTP requests. The http module comes with various methods that are useful when engaging with HTTP network requests. One of the most commonly used methods within the http module is the .createServer() method. This method is responsible for doing exactly what its namesake implies; it creates an HTTP server. To implement this method to create a server, the following code can be used: const server = http.createServer((req, res) => { res.end('Server is running!'); }); server.listen(8080, () => { const { address, port } = server.address(); console.log(`Server is listening on: http://${address}:${port}`); }) The .createServer() method takes a single argument in the form of a callback function. This callback function has two primary arguments; the request (commonly written as req) and the response (commonly written as res). The req object contains all of the information about an HTTP request ingested by the server. It exposes information such as the HTTP method (GET, POST, etc.), the pathname, headers, body, and so on. The res object contains methods and properties pertaining to the generation of a response by the HTTP server. This object contains methods such as .setHeader() (sets HTTP headers on the response), .statusCode (set the status code of the response), and .end() (dispatches the response to the client who made the request). In the example above, we use the .end() method to send the string \u2018Server is Running!\u2019 to the client, which will display on the web page. Once the .createServer() method has instantiated the server, it must begin listening for connections. This final step is accomplished by the .listen() method on the server instance. This method takes a port number as the first argument, which tells the server to listen for connections at the given port number. In our example above, the server has been set to listen on port 8080. Additionally, the .listen() method takes an optional callback function as a second argument, allowing it to carry out a task after the server has successfully started. Using this simple .createServer() method, in conjunction with the callback, provides the ability to process HTTP requests dynamically and dispatch responses back to their callers.","title":"HTTP Server"},{"location":"Notes%20on%20nodejs/#the-url-module","text":"Typically, an HTTP server will require information from the request URL to accurately process a request. This request URL is located on the url property contained within the req object itself. To parse the different parts of this URL easily, Node.js provides the built-in url module. The core of the url module revolves around the URL class. A new URL object can be instantiated using the URL class as follows: const url = new URL('https://www.example.com/p/a/t/h?query=string'); Once instantiated, different parts of the URL can be accessed and modified via various properties, which include: hostname: Gets and sets the host name portion of the URL. pathname: Gets and sets the path portion of the URL. searchParams: Gets the search parameter object representing the query parameters contained within the URL. Returns an instance of the URLSearchParams class. You might recognize the URL and URLSearchParams classes if you are familiar with browser-based JavaScript. It\u2019s because they are actually the same thing! These classes are defined by the WHATWG URL specification. Both the browser and Node.js implement this API, which means developers can have a similar developer experience working with both client and server-side JavaScript. Using these properties, one can break the URL down into easily usable parts for processing the request. const host = url.hostname; // example.com const pathname = url.pathname; // /p/a/t/h const searchParams = url.searchParams; // {query: 'string'} While the url module can be used to deconstruct a URL into its constituent parts, it can also be used to construct a URL. Constructing a URL via this method relies on most of the same properties listed above to set values on the URL instead of retrieving them. This can be done by setting each of these values equal to a value for the newly constructed URL. Once all parts of the URL have been added, the composed URL can be obtained using the .toString() method. const createdUrl = new URL('https://www.example.com'); createdUrl.pathname = '/p/a/t/h'; createdUrl.search = '?query=string'; createUrl.toString(); // Creates https://www.example.com/p/a/t/h?query=string","title":"The URL Module"},{"location":"Notes%20on%20nodejs/#routing","text":"To process and respond to requests appropriately, servers need to do more than look at a request and dispatch a response. Internally, a server needs to maintain a way to handle each request based on specific criteria such as method, pathname, etc. The process of handling requests in specific ways based on the information provided within the request is known as routing. The method is one important piece of information that can be used to route requests. Since each HTTP request contains a method such as GET and POST, it is a great way to discern different classes of requests based on the action intended for the server to carry out. Thus, all GET requests could be routed to a specific function for handling, while all POST requests are routed to another function to be handled. This also allows for the logical co-location of processing code with the specific verb to be handled. const server = http.createServer((req, res) => { const { method } = req; switch(method) { case 'GET': return handleGetRequest(req, res); case 'POST': return handlePostRequest(req, res); case 'DELETE': return handleDeleteRequest(req, res); case 'PUT': return handlePutRequest(req, res); default: throw new Error(`Unsupported request method: ${method}`); } }) In the above example, the HTTP method property is destructured from the req object and used to conditionally invoke a handler function built specifically for handling those types of requests. This is great at first glance, but it should soon become apparent that the routing is not specific enough. After all, how will one GET request be distinguished from another? We can distinguish one request from another of the same method through the use of the pathname. The pathname allows the server to understand what resource is being targeted. Let\u2019s take a look at the handleGetRequest handler function. function handleGetRequest(req, res) { const { pathname } = new URL(req.url); let data = {}; if (pathname === '/projects') { data = await getProjects(); res.setHeader('Content-Type', 'application/json'); return res.end(JSON.stringify(data)); } res.statusCode = 404; return res.end('Requested resource does not exist'); } Within the handleGetRequest() function, the pathname is being checked to match a known resource, '/projects'. If the pathname matches, the resource data is fetched and then subsequently dispatched from the server as a successful response. Otherwise, the .statusCode property is set to 404, indicating that the resource is not found, and a corresponding error message is dispatched. This pattern can be extrapolated to any number of conditional resource matches, allowing the server to handle many different types of requests to different resources.","title":"Routing"},{"location":"Notes%20on%20nodejs/#longer-example","text":"const http = require('http'); // Handle get request const handleGetRequest = (req, res) => { const pathname = req.url; if (pathname === '/users') { res.end(JSON.stringify([])); } } // Creates server instance const server = http.createServer((req, res) => { const { method } = req; switch(method) { case 'GET': return handleGetRequest(req, res); default: throw new Error(`Unsupported request method: ${method}`); } }); // Starts server listening on specified port server.listen(4001, () => { const { address, port } = server.address(); console.log(`Server is listening on: http://${address}:${port}`); });","title":"Longer Example"},{"location":"Notes%20on%20nodejs/#returning-a-status-code","text":"const http = require('http'); const handleGetRequest = (req, res) => { res.statusCode = 200; return res.end(JSON.stringify({ data: [] })); } const handlePostRequest = (req, res) => { res.statusCode = 500; return res.end(\"Unable to create record\"); } // Creates server instance const server = http.createServer((req, res) => { const { method } = req; switch(method) { case 'GET': return handleGetRequest(req, res); case 'POST': return handlePostRequest(req, res); default: throw new Error(`Unsupported request method: ${method}`); } }); // Starts server listening on specified port server.listen(4001, () => { const { address, port } = server.address(); console.log(`Server is listening on: http://${address}:${port}`); });","title":"Returning a Status Code"},{"location":"Notes%20on%20nodejs/#express","text":"","title":"Express"},{"location":"Notes%20on%20nodejs/#request-object-properties","text":"Index Properties Description 1. req.app This is used to hold a reference to the instance of the express application that is using the middleware. 2. req.baseurl It specifies the URL path on which a router instance was mounted. 3. req.body It contains key-value pairs of data submitted in the request body. By default, it is undefined, and is populated when you use body-parsing middleware such as body-parser. 4. req.cookies When we use cookie-parser middleware, this property is an object that contains cookies sent by the request. 5. req.fresh It specifies that the request is \"fresh.\" it is the opposite of req.stale. 6. req.hostname It contains the hostname from the \"host\" http header. 7. req.ip It specifies the remote IP address of the request. 8. req.ips When the trust proxy setting is true, this property contains an array of IP addresses specified in the ?x-forwarded-for? request header. 9. req.originalurl This property is much like req.url; however, it retains the original request URL, allowing you to rewrite req.url freely for internal routing purposes. 10. req.params An object containing properties mapped to the named route ?parameters?. For example, if you have the route /user/:name, then the \"name\" property is available as req.params.name. This object defaults to {}. 11. req.path It contains the path part of the request URL. 12. req.protocol The request protocol string, \"http\" or \"https\" when requested with TLS. 13. req.query An object containing a property for each query string parameter in the route. 14. req.route The currently-matched route, a string. 15. req.secure A Boolean that is true if a TLS connection is established. 16. req.signedcookies When using cookie-parser middleware, this property contains signed cookies sent by the request, unsigned and ready for use. 17. req.stale It indicates whether the request is \"stale,\" and is the opposite of req.fresh. 18. req.subdomains It represents an array of subdomains in the domain name of the request. 19. req.xhr A Boolean value that is true if the request's \"x-requested-with\" header field is \"xmlhttprequest\", indicating that the request was issued by a client library such as jQuery","title":"Request Object Properties"},{"location":"Notes%20on%20nodejs/#request-object-methods","text":"req.accepts This method is used to check whether the specified content types are acceptable, based on the request's Accept HTTP header field. req.accepts('html'); //=>?html? req.accepts('text/html'); // => ?text/html? req.get(field) This method returns the specified HTTP request header field. req.get('Content-Type'); // => \"text/plain\" req.get('content-type'); // => \"text/plain\" req.get('Something'); // => undefined req.is(type) // With Content-Type: text/html; charset=utf-8 req.is('html'); req.is('text/html'); req.is('text/*'); // => true req.param(name [,defaultValue]) This method is used to fetch the value of param name when present. // ?name=sasha req.param('name') // => \"sasha\" // POST name=sasha req.param('name') // => \"sasha\" // /user/sasha for /user/:name req.param('name') // => \"sasha\"","title":"Request Object Methods"},{"location":"Notes%20on%20nodejs/#response-object","text":"","title":"Response Object"},{"location":"Notes%20on%20nodejs/#knexjs","text":"","title":"Knex.js"},{"location":"Notes%20on%20nodejs/#how-does-exportsup-and-exportsdown-work","text":"http://perkframework.com/v1/guides/database-migrations-knex.html","title":"How does exports.up and exports.down work"},{"location":"Notes%20on%20nodejs/#seed-files","text":"A seed file allows you to add data into your database without having to manually add it. This is most frequently used for database initialization or loading demo data.","title":"Seed Files"},{"location":"Notes%20on%20nodejs/#babel","text":"What is Babel: https://babeljs.io/docs/en/","title":"Babel"},{"location":"Notes%20on%20nodejs/#reactjs","text":"","title":"ReactJS"},{"location":"Notes%20on%20nodejs/#importing-react-required-code","text":"import React from 'react'; This creates an object named React which contains methods necessary to use the React library. import ReactDOM from 'react-dom'; The methods imported from 'react-dom' are meant for interacting with the DOM. You are already familiar with one of them: ReactDOM.render(). The methods imported from 'react' don\u2019t deal with the DOM at all. They don\u2019t engage directly with anything that isn\u2019t part of React. To clarify: the DOM is used in React applications, but it isn\u2019t part of React. After all, the DOM is also used in countless non-React applications. Methods imported from 'react' are only for pure React purposes, such as creating components or writing JSX elements.","title":"Importing React Required Code"},{"location":"Notes%20on%20nodejs/#components","text":"","title":"Components"},{"location":"Notes%20on%20nodejs/#create-a-component-class","text":"we can use a JavaScript class to define a new React component. We can also define components with JavaScript functions, but we\u2019ll focus on class components first. All class components will have some methods and properties in common (more on this later). Rather than rewriting those same properties over and over again every time, we extend the Component class from the React library. This way, we can use code that we import from the React library, without having to write it over and over again ourselves. After we define our class component, we can use it to render as many instances of that component as we want. What is React.Component, and how do you use it to make a component class? React.Component is a JavaScript class. To create your own component class, you must subclass React.Component. You can do this by using the syntax class YourComponentNameGoesHere extends React.Component {}. import React from 'react'; import ReactDOM from 'react-dom'; class MyComponentClass extends React.Component { render() { return <h1>Hello world</h1>; } } ReactDOM.render( <MyComponentClass />, document.getElementById('app') ); On line 4, you know that you are declaring a new component class, which is like a factory for building React components. You know that React.Component is a class, which you must subclass in order to create a component class of your own. You also know that React.Component is a property on the object which was returned by import React from 'react' on line 1.","title":"Create a Component Class"},{"location":"Notes%20on%20nodejs/#the-render-function","text":"A render method is a property whose name is render, and whose value is a function. The term \"render method\" can refer to the entire property, or to just the function part. class ComponentFactory extends React.Component { render() { return <h1>Hello world</h1>; } }","title":"The Render Function"},{"location":"Notes%20on%20nodejs/#create-a-component-instance","text":"To make a React component, you write a JSX element. Instead of naming your JSX element something like h1 or div like you\u2019ve done before, give it the same name as a component class. Voil\u00e0, there\u2019s your component instance! JSX elements can be either HTML-like, or component instances. JSX uses capitalization to distinguish between the two! That is the React-specific reason why component class names must begin with capital letters. In a JSX element, that capitalized first letter says, \"I will be a component instance and not an HTML tag.\" Whenever you make a component, that component inherits all of the methods of its component class. MyComponentClass has one method: MyComponentClass.render(). Therefore, also has a method named render. In order to render a component, that component needs to have a method named render. Your component has this! It inherited a method named render from MyComponentClass. To call a component\u2019s render method, you pass that component to ReactDOM.render(). Notice your component, being passed as ReactDOM.render()\u2018s first argument: ReactDOM.render( <MyComponentClass />, document.getElementById('app') ); ReactDOM.render() will tell to call its render method. will call its render method, which will return the JSX element","title":"Create a Component Instance"},{"location":"Notes%20on%20nodejs/#use-this-in-a-class","text":"class IceCreamGuy extends React.Component { get food() { return 'ice cream'; } render() { return <h1>I like {this.food}.</h1>; } }","title":"Use This in a Class"},{"location":"Notes%20on%20nodejs/#render-components-with-components","text":"class OMG extends React.Component { render() { return <h1>Whooaa!</h1>; } } class Crazy extends React.Component { render() { return <OMG />; } }","title":"Render Components with Components"},{"location":"Notes%20on%20nodejs/#importing-files-and-exporting-functionality","text":"","title":"Importing Files and Exporting Functionality"},{"location":"Notes%20on%20nodejs/#importing","text":"The second important difference involves the contents of the string at the end of the statement: 'react' vs './NavBar.js'. If you use an import statement, and the string at the end begins with either a dot or a slash, then import will treat that string as a filepath. import will follow that filepath, and import the file that it finds. If your filepath doesn\u2019t have a file extension, then \".js\" is assumed. So the above example could be shortened: import { NavBar } from './NavBar'; One final, important note: None of this behavior is specific to React! Module systems of independent, importable files are a very popular way to organize code. React\u2019s specific module system comes from ES6.","title":"Importing"},{"location":"Notes%20on%20nodejs/#exporting_1","text":"This is called a named export. export class NavBar extends React.Component {","title":"Exporting"},{"location":"Notes%20on%20nodejs/#component-props","text":"A component\u2019s props is an object. It holds information about that component. You can pass information to a prop via an attribute. import React from 'react'; import ReactDOM from 'react-dom'; class Greeting extends React.Component { render() { return <h1>Hi there, {this.props.firstName}!</h1>; } } ReactDOM.render( <Greeting firstName='Grant' />, document.getElementById('app') );","title":"Component Props"},{"location":"Notes%20on%20nodejs/#event-handler","text":"import React from 'react'; import ReactDOM from 'react-dom'; import { Button } from './Button'; class Talker extends React.Component { talk() { let speech = ''; for (let i = 0; i < 10000; i++) { speech += 'blah '; } alert(speech); } render() { return <Button talk={this.talk}/>; ReactDOM.render( <Talker />, document.getElementById('app') ); // **************************************** // In Button.js import React from 'react'; export class Button extends React.Component { render() { return ( // TODO - why is it `this` here? <button onClick={this.props.talk}> Click me! </button> ); } }","title":"Event Handler"},{"location":"Notes%20on%20nodejs/#handleevent-onevent-and-thispropsonevent","text":"When you pass an event handler as a prop, as you just did, there are two names that you have to choose. Both naming choices occur in the parent component class - that is, in the component class that defines the event handler and passes it. The first name that you have to choose is the name of the event handler itself. Look at Talker.js, lines 6 through 12. This is our event handler. We chose to name it talk. The second name that you have to choose is the name of the prop that you will use to pass the event handler. This is the same thing as your attribute name. For our prop name, we also chose talk, as shown on line 15: return <Button talk={this.talk} />; These two names can be whatever you want. However, there is a naming convention that they often follow. You don\u2019t have to follow this convention, but you should understand it when you see it. Here\u2019s how the naming convention works: first, think about what type of event you are listening for. In our example, the event type was \"click.\" If you are listening for a \"click\" event, then you name your event handler handleClick. If you are listening for a \"keyPress\" event, then you name your event handler handleKeyPress: class MyClass extends React.Component { handleHover() { alert('I am an event handler.'); alert('I will be called in response to \"hover\" events.'); } } Your prop name should be the word on, plus your event type. If you are listening for a \"click\" event, then you name your prop onClick. If you are listening for a \"keyPress\" event, then you name your prop onKeyPress: class MyClass extends React.Component { handleHover() { alert('I am an event handler.'); alert('I will listen for a \"hover\" event.'); } render() { return <Child onHover={this.handleHover} />; } }","title":"handleEvent, onEvent, and this.props.onEvent"},{"location":"Notes%20on%20nodejs/#thispropschildren","text":"Every component\u2019s props object has a property named children. this.props.children will return everything in between a component\u2019s opening and closing JSX tags. For example: // List.js import React from 'react'; export class List extends React.Component { render() { let titleText = `Favorite ${this.props.type}`; if (this.props.children instanceof Array) { // Add an s to make it plural if there is more than one titleText += 's'; } return ( <div> <h1>{titleText}</h1> <ul>{this.props.children}</ul> </div> ); } } // App.js import React from 'react'; import ReactDOM from 'react-dom'; import { List } from './List'; class App extends React.Component { render() { return ( <div> <List type='Living Musician'> <li>Sachiko M</li> <li>Harvey Sid Fisher</li> </List> <List type='Living Cat Musician'> <li>Nora the Piano Cat</li> </List> </div> ); } } ReactDOM.render( <App />, document.getElementById('app') ); This will print: Favorite Living Musicians Sachiko M Harvey Sid Fisher Favorite Living Cat Musician Nora the Piano Cat Because in List.js, between the <ul></ul> you have {this.props.children} which grabs all the elements between <List></List> in the App class.","title":"this.props.children"},{"location":"Notes%20on%20nodejs/#default-properties","text":"Used if nothing is passed into the property. import React from 'react'; import ReactDOM from 'react-dom'; class Button extends React.Component { render() { return ( <button> {this.props.text} </button> ); } } // defaultProps goes here: Button.defaultProps = {text: \"I am a button\"}; ReactDOM.render( <Button />, document.getElementById('app') );","title":"Default Properties"},{"location":"Notes%20on%20nodejs/#component-state","text":"A React component can access dynamic information in two ways: props and state. Unlike props, a component\u2019s state is not passed in from the outside. A component decides its own state. To make a component have state, give the component a state property. This property should be declared inside of a constructor method, like this: class Example extends React.Component { constructor(props) { super(props); this.state = { mood: 'decent' }; } render() { return <div></div>; } } <Example /> // Access the state outside with this.state.mood // You can set the state with this.setState({mood: \"the mood\"}) What is super(props) Also: https://overreacted.io/why-do-we-write-super-props/","title":"Component State"},{"location":"Notes%20on%20nodejs/#thissetstate-from-another-function","text":"You'll use a wrapper function to call this.setState from another function. Like this: class Example extends React.Component { constructor(props) { super(props); this.state = { weather: 'sunny' }; this.makeSomeFog = this.makeSomeFog.bind(this); } makeSomeFog() { this.setState({ weather: 'foggy' }); } } The line this.makeSomeFog = this.makeSomeFog.bind(this); is necessary because makeSomeFog()'s body contains the word this. It has to do with the way event handlers are bound in Javascript. If you use this without the line this.makeSomeFog = this.makeSomeFog.bind(this); with an event handler the this word will be lost so we have to bind it... because Javascript. If the function isn't used by an event handler then it won't matter. Full example import React from 'react'; import ReactDOM from 'react-dom'; const green = '#39D1B4'; const yellow = '#FFD712'; class Toggle extends React.Component { constructor(props) { super(props); this.state = {color: green}; this.changeColor = this.changeColor.bind(this); } changeColor() { if(this.state.color === yellow) { this.setState({color: green}); } else { this.setState({color: yellow}); } } render() { return ( <div style={{background: this.state.color}}> <h1> <button onClick={this.changeColor}> Change color </button> </h1> </div> ); } } ReactDOM.render(<Toggle />, document.getElementById('app')); NOTE : Anytime you call this.setState it automatically calls render as soon as the state has changed. This is why you don't have to call render again.","title":"this.setState from Another Function"},{"location":"Notes%20on%20nodejs/#component-lifecycle","text":"We\u2019ve seen that React components can be highly dynamic. They get created, rendered, added to the DOM, updated, and removed. All of these steps are part of a component\u2019s lifecycle. The component lifecycle has three high-level parts: Mounting, when the component is being initialized and put into the DOM for the first time Updating, when the component updates as a result of changed state or changed props Unmounting, when the component is being removed from the DOM Every React component you\u2019ve ever interacted with does the first step at a minimum. If a component never mounted, you\u2019d never see it! Most interesting components are updated at some point. A purely static component\u2014like, for example, a logo\u2014might not ever update. But if a component\u2019s state changes, it updates. Or if different props are passed to a component, it updates. Finally, a component is unmounted when it\u2019s removed from the DOM. For example, if you have a button that hides a component, chances are that component will be unmounted. If your app has multiple screens, it\u2019s likely that each screen (and all of its child components) will be unmounted. If a component is \"alive\" for the entire lifetime of your app (say, a top-level component or a persistent navigation bar), it won\u2019t be unmounted. But most components can get unmounted one way or another! It\u2019s worth noting that each component instance has its own lifecycle. For example, if you have 3 buttons on a page, then there are 3 component instances, each with its own lifecycle. However, once a component instance is unmounted, that\u2019s it\u2014it will never be re-mounted, or updated again, or unmounted. React components have several methods, called lifecycle methods, that are called at different parts of a component\u2019s lifecycle. This is how you, the programmer, deal with the lifecycle of a component. You may not have known it, but you\u2019ve already used two of the most common lifecycle methods: constructor() and render()! constructor() is the first method called during the mounting phase. render() is called later during the mounting phase, to render the component for the first time, and during the updating phase, to re-render the component. Notice that lifecycle methods don\u2019t necessarily correspond one-to-one with part of the lifecycle. constructor() only executes during the mounting phase, but render() executes during both the mounting and updating phase.","title":"Component Lifecycle"},{"location":"Notes%20on%20nodejs/#componentdidmount","text":"Say you want a component to update itself at a setInterval. You don't want to put it in the constructor because that would violate the single responsibility rule but you also don't want it in render because then it would be called on update AND on mounting. That's what componentDidMount is for. componentDidMount() is the final method called during the mounting phase. The order is: The constructor render() componentDidMount() In other words, it\u2019s called after the component is rendered. (Another method, getDerivedStateFromProps(), is called between the constructor and render(), but it is very rarely used and usually isn\u2019t the best way to achieve your goals. We won\u2019t be talking about it in this lesson.) import React from 'react'; import ReactDOM from 'react-dom'; class Clock extends React.Component { constructor(props) { super(props); this.state = { date: new Date() }; } render() { return <div>{this.state.date.toLocaleTimeString()}</div>; } componentDidMount() { const oneSecond = 1000; setInterval(() => { this.setState({ date: new Date() }); }, oneSecond); } } ReactDOM.render(<Clock />, document.getElementById('app'));","title":"componentDidMount"},{"location":"Notes%20on%20nodejs/#componentwillunmount","text":"In the case of our interval above, the problem is now that timer will never stop. If we want to remove it. We want to use clearInterval() to clean it up. We can call this during componentWillUnmount import React from 'react'; export class Clock extends React.Component { constructor(props) { super(props); this.state = { date: new Date() }; } render() { return <div>{this.state.date.toLocaleTimeString()}</div>; } componentDidMount() { const oneSecond = 1000; this.intervalID = setInterval(() => { this.setState({ date: new Date() }); }, oneSecond); } componentWillUnmount() { clearInterval(this.intervalID); } }","title":"componentWillUnmount"},{"location":"Notes%20on%20nodejs/#componentdidupdate","text":"When a component updates many things happen but there are two primary methods - render and componentDidUpdate. import React from 'react'; export class Clock extends React.Component { constructor(props) { super(props); this.state = { date: new Date() }; } render() { return ( <div> {this.props.isPrecise ? this.state.date.toISOString() : this.state.date.toLocaleTimeString()} </div> ); } startInterval() { let delay; if (this.props.isPrecise) { delay = 100; } else { delay = 1000; } this.intervalID = setInterval(() => { this.setState({ date: new Date() }); }, delay); } componentDidMount() { this.startInterval(); } componentDidUpdate(prevProps) { if (this.props.isPrecise === prevProps.isPrecise) { return; } clearInterval(this.intervalID); this.startInterval(); } componentWillUnmount() { clearInterval(this.intervalID); } }","title":"componentDidUpdate"},{"location":"Notes%20on%20nodejs/#stateless-functional-components","text":"We used to use classes for components but now we use functions. // Original class-based way of writing components import React from 'react'; import ReactDOM from 'react-dom'; export class Friend extends React.Component { render() { return <img src=\"https://content.codecademy.com/courses/React/react_photo-octopus.jpg\" />; } }; ReactDOM.render( <Friend />, document.getElementById('app') ); // Function Version import React from 'react'; import ReactDOM from 'react-dom'; export const Friend = () => { return <img src=\"https://content.codecademy.com/courses/React/react_photo-octopus.jpg\" />; } ReactDOM.render( <Friend />, document.getElementById('app') );","title":"Stateless Functional Components"},{"location":"Notes%20on%20nodejs/#function-component-props","text":"export function YesNoQuestion (props) { return ( <div> <p>{props.prompt}</p> <input value=\"Yes\" /> <input value=\"No\" /> </div> ); } ReactDOM.render( <YesNoQuestion prompt=\"Have you eaten an apple today?\" />, document.getElementById('app'); );","title":"Function Component Props"},{"location":"Notes%20on%20nodejs/#react-hooks","text":"With Hooks, we can use simple function components to do lots of the fancy things that we could only do with class components in the past. React Hooks, plainly put, are functions that let us manage the internal state of components and handle post-rendering side effects directly from our function components. Hooks don\u2019t work inside classes \u2014 they let us use fancy React features without classes. Keep in mind that function components and React Hooks do not replace class components. They are completely optional; just a new tool that we can take advantage of. Note: If you\u2019re familiar with lifecycle methods of class components, you could say that Hooks let us \"hook into\" state and lifecycle features directly from our function components. React offers a number of built-in Hooks. A few of these include useState(), useEffect(), useContext(), useReducer(), and useRef(). See the full list in the docs . With React, we feed static and dynamic data models to JSX to render a view to the screen Use Hooks to \u201chook into\u201d internal component state for managing dynamic data in function components We employ the State Hook by using the code below: currentState to reference the current value of state stateSetter to reference a function used to update the value of this state the initialState argument to initialize the value of state for the component\u2019s first render const [currentState, stateSetter] = useState( initialState ); Call state setters in event handlers Define simple event handlers inline with our JSX event listeners and define complex event handlers outside of our JSX Use a state setter callback function when our next value depends on our previous value Use arrays and objects to organize and manage related data that tends to change together Use the spread syntax on collections of dynamic data to copy the previous state into the next state like so: setArrayState((prev) => [ ...prev ]) and setObjectState((prev) => ({ ...prev })) Split state into multiple, simpler variables instead of throwing it all into one state object","title":"React Hooks"},{"location":"Notes%20on%20nodejs/#comparison-class-vs-function","text":"Class import React, { Component } from \"react\"; import NewTask from \"../Presentational/NewTask\"; import TasksList from \"../Presentational/TasksList\"; export default class AppClass extends Component { constructor(props) { super(props); this.state = { newTask: {}, allTasks: [] }; this.handleChange = this.handleChange.bind(this); this.handleSubmit = this.handleSubmit.bind(this); this.handleDelete = this.handleDelete.bind(this); } handleChange({ target }){ const { name, value } = target; this.setState((prevState) => ({ ...prevState, newTask: { ...prevState.newTask, [name]: value, id: Date.now() } })); } handleSubmit(event){ event.preventDefault(); if (!this.state.newTask.title) return; this.setState((prevState) => ({ allTasks: [prevState.newTask, ...prevState.allTasks], newTask: {} })); } handleDelete(taskIdToRemove){ this.setState((prevState) => ({ ...prevState, allTasks: prevState.allTasks.filter((task) => task.id !== taskIdToRemove) })); } render() { return ( <main> <h1>Tasks</h1> <NewTask newTask={this.state.newTask} handleChange={this.handleChange} handleSubmit={this.handleSubmit} /> <TasksList allTasks={this.state.allTasks} handleDelete={this.handleDelete} /> </main> ); } } Function import React, { useState } from \"react\"; import NewTask from \"../Presentational/NewTask\"; import TasksList from \"../Presentational/TasksList\"; export default function AppFunction() { const [newTask, setNewTask] = useState({}); const handleChange = ({ target }) => { const { name, value } = target; setNewTask((prev) => ({ ...prev, id: Date.now(), [name]: value })); }; const [allTasks, setAllTasks] = useState([]); const handleSubmit = (event) => { event.preventDefault(); if (!newTask.title) return; setAllTasks((prev) => [newTask, ...prev]); setNewTask({}); }; const handleDelete = (taskIdToRemove) => { setAllTasks((prev) => prev.filter( (task) => task.id !== taskIdToRemove )); }; return ( <main> <h1>Tasks</h1> <NewTask newTask={newTask} handleChange={handleChange} handleSubmit={handleSubmit} /> <TasksList allTasks={allTasks} handleDelete={handleDelete} /> </main> ); }","title":"Comparison Class vs Function"},{"location":"Notes%20on%20nodejs/#update-function-component-state","text":"Let\u2019s get started with the State Hook, the most common Hook used for building React components. The State Hook is a named export from the React library, so we import it like this: import React, { useState } from 'react'; useState() is a JavaScript function defined in the React library. When we call this function, it returns an array with two values: current state - the current value of this state state setter - a function that we can use to update the value of this state Because React returns these two values in an array, we can assign them to local variables, naming them whatever we like. For example: const [toggle, setToggle] = useState(); import React, { useState } from \"react\"; function Toggle() { const [toggle, setToggle] = useState(); return ( <div> <p>The toggle is {toggle}</p> <button onClick={() => setToggle(\"On\")}>On</button> <button onClick={() => setToggle(\"Off\")}>Off</button> </div> ); } Notice how the state setter function, setToggle(), is called by our onClick event listeners. To update the value of toggle and re-render this component with the new value, all we need to do is call the setToggle() function with the next state value as an argument. No need to worry about binding functions to class instances, working with constructors, or dealing with the this keyword. With the State Hook, updating state is as simple as calling a state setter function. Calling the state setter signals to React that the component needs to re-render, so the whole function defining the component is called again. The magic of useState() is that it allows React to keep track of the current value of state from one render to the next! More complex example: import React, { useState } from 'react'; export default function ColorPicker() { const [color, setColor] = useState(); const divStyle = {backgroundColor: color}; return ( <div style={divStyle}> <p>The color is {color}</p> <button onClick={() => setColor('Aquamarine')}> Aquamarine </button> <button onClick={() => setColor('BlueViolet')}> BlueViolet </button> <button onClick={() => setColor('Chartreuse')}> Chartreuse </button> <button onClick={() => setColor('CornflowerBlue')}> CornflowerBlue </button> </div> ); }","title":"Update Function Component State"},{"location":"Notes%20on%20nodejs/#initialize-state","text":"You can set a state at the beginning with: const [color, setColor] = useState(\"Tomato\"); . There are three ways in which this code affects our component: During the first render, the initial state argument is used. When the state setter is called, React ignores the initial state argument and uses the new value. When the component re-renders for any other reason, React continues to use the same value from the previous render.","title":"Initialize State"},{"location":"Notes%20on%20nodejs/#use-state-setter-outside-of-jsx","text":"https://www.codecademy.com/courses/react-101/lessons/the-state-hook/exercises/use-state-setter-outside-of-jsx Let\u2019s see how to manage the changing value of a string as a user types into a text input field: import React, { useState } from 'react'; export default function EmailTextInput() { const [email, setEmail] = useState(''); const handleChange = (event) => { const updatedEmail = event.target.value; setEmail(updatedEmail); } return ( // Here value={email} will set the value to the current // value in e-mail in the event hook <input value={email} onChange={handleChange} /> ); } Let\u2019s break down how this code works! The square brackets on the left side of the assignment operator signal array destructuring The local variable named email is assigned the current state value at index 0 from the array returned by useState() The local variable named setEmail() is assigned a reference to the state setter function at index 1 from the array returned by useState() It\u2019s convention to name this variable using the current state variable (email) with \"set\" prepended The JSX input tag has an event listener called onChange. This event listener calls an event handler each time the user types something in this element. In the example above, our event handler is defined inside of the definition for our function component, but outside of our JSX. Earlier in this lesson, we wrote our event handlers right in our JSX. Those inline event handlers work perfectly fine, but when we want to do something more interesting than just calling the state setter with a static value, it\u2019s a good idea to separate that logic from everything else going on in our JSX. This separation of concerns makes our code easier to read, test, and modify. You can change: const updatedEmail = event.target.value; setEmail(updatedEmail); // to this const handleChange = ({target}) => setEmail(target.value);","title":"Use State Setter Outside of JSX"},{"location":"Notes%20on%20nodejs/#longer-example_1","text":"import React, { useState } from \"react\"; // regex to match numbers between 1 and 10 digits long const validPhoneNumber = /^\\d{1,10}$/; export default function PhoneNumber() { const [phone, setPhone] = useState(''); const handleChange = ({ target })=> { const newPhone = target.value; const isValid = validPhoneNumber.test(newPhone); if (isValid) { setPhone(newPhone); } // just ignore the event, when new value is invalid }; return ( <div className='phone'> <label for='phone-input'>Phone: </label> <input value={phone} onChange={handleChange} id='phone-input' /> </div> ); }","title":"Longer Example"},{"location":"Notes%20on%20nodejs/#set-from-previous-state","text":"Often, the next value of our state is calculated using the current state. In this case, it is best practice to update state with a callback function. If we do not, we risk capturing outdated, or \u201cstale\u201d, state values. import React, { useState } from 'react'; export default function Counter() { const [count, setCount] = useState(0); const increment = () => setCount(prevCount => prevCount + 1); return ( <div> <p>Wow, you've clicked that button: {count} times</p> <button onClick={increment}>Click here!</button> </div> ); } When the button is pressed, the increment() event handler is called. Inside of this function, we use our setCount() state setter in a new way! Because the next value of count depends on the previous value of count, we pass a callback function as the argument for setCount() instead of a value (as we\u2019ve done in previous exercises). setCount(prevCount => prevCount + 1) When our state setter calls the callback function, this state setter callback function takes our previous count as an argument. The value returned by this state setter callback function is used as the next value of count (in this case prevCount + 1). Note: We can just call setCount(count +1) and it would work the same in this example\u2026 but for reasons that are out of scope for this lesson, it is safer to use the callback method.","title":"Set From Previous State"},{"location":"Notes%20on%20nodejs/#arrays-in-state","text":"import React, { useState } from \"react\"; import ItemList from \"./ItemList\"; import { produce, pantryItems } from \"./storeItems\"; export default function GroceryCart() { // declare and initialize state const [cart, setCart] = useState([]); // addItem is the event handler and will receive the item that // gets clicked const addItem = (item) => { // setCart is the state setter // and it will tell the component to update its state. // Via the magic that is the totality of Javascript, it // will magically receive the previous state to this function // We then use spread syntax to expand the previous array // and add it with the item. setCart((prev) => { return [item, ...prev]; }); }; // This removes the item at some set index. const removeItem = (targetIndex) => { setCart((prev) => { return prev.filter((item, index) => index !== targetIndex); }); }; return ( <div> <h1>Grocery Cart</h1> <ul> {cart.map((item, index) => ( <li onClick={() => removeItem(index)} key={index}> {item} </li> ))} </ul> <h2>Produce</h2> <ItemList items={produce} onItemClick={addItem} /> <h2>Pantry Items</h2> <ItemList items={pantryItems} onItemClick={addItem} /> </div> ); }","title":"Arrays in State"},{"location":"Notes%20on%20nodejs/#objects-in-state","text":"export default function Login() { const [formState, setFormState] = useState({}); const handleChange = ({ target }) => { const { name, value } = target; setFormState((prev) => ({ ...prev, [name]: value })); }; return ( <form> <input value={formState.firstName} onChange={handleChange} name=\"firstName\" type=\"text\" /> <input value={formState.password} onChange={handleChange} type=\"password\" name=\"password\" /> </form> ); } A few things to notice: We use a state setter callback function to update state based on the previous value The spread syntax is the same for objects as for arrays: { ...oldObject, newKey: newValue } We reuse our event handler across multiple inputs by using the input tag\u2019s name attribute to identify which input the change event came from Once again, when updating the state with setFormState() inside a function component, we do not modify the same object. We must copy over the values from the previous object when setting the next value of state. Thankfully, the spread syntax makes this super easy to do! Anytime one of the input values is updated, the handleChange() function will be called. Inside of this event handler, we use object destructuring to unpack the target property from our event object, then we use object destructuring again to unpack the name and value properties from the target object. Inside of our state setter callback function, we wrap our curly brackets in parentheses like so: setFormState((prev) => ({ ...prev })). This tells JavaScript that our curly brackets refer to a new object to be returned. We use ..., the spread operator, to fill in the corresponding fields from our previous state. Finally, we overwrite the appropriate key with its updated value. Did you notice the square brackets around the name? This Computed Property Name allows us to use the string value stored by the name variable as a property key!","title":"Objects in State"},{"location":"Notes%20on%20nodejs/#longer-example_2","text":"import React, { useState } from \"react\"; export default function EditProfile() { const [profile, setProfile] = useState({}); const handleChange = ({ target }) => { const {name, value } = target; setProfile((prevProfile) => ({ ...prevProfile, [name]: value })); }; const handleSubmit = (event) => { event.preventDefault(); alert(JSON.stringify(profile, '', 2)); }; return ( <form onSubmit={handleSubmit}> <input value={profile.firstName || ''} name=\"firstName\" type=\"text\" placeholder=\"First Name\" onChange={handleChange} /> <input value={profile.lastName || ''} type=\"text\" name=\"lastName\" placeholder=\"Last Name\" onChange={handleChange} /> <input value={profile.bday || ''} type=\"date\" name=\"bday\" onChange={handleChange} /> <input value={profile.password || ''} type=\"password\" name=\"password\" placeholder=\"Password\" onChange={handleChange} /> <button type=\"submit\">Submit</button> </form> ); }","title":"Longer Example"},{"location":"Notes%20on%20nodejs/#separate-hooks-for-separate-states","text":"While there are times when it can be helpful to store related data in a data collection like an array or object, it can also be helpful to separate data that changes separately into completely different state variables. Managing dynamic data is much easier when we keep our data models as simple as possible. For example, if we had a single object that held state for a subject you are studying at school, it might look something like this: function Subject() { const [state, setState] = useState({ currentGrade: 'B', classmates: ['Hasan', 'Sam', 'Emma'], classDetails: {topic: 'Math', teacher: 'Ms. Barry', room: 201}; exams: [{unit: 1, score: 91}, {unit: 2, score: 88}]); }); This would work, but think about how messy it could get to copy over all the other values when we need to update something in this big state object. For example, to update the grade on an exam, we would need an event handler that did something like this: // Get the previous state in and pass that to something that is going to return a new object {} setState((prev) => ({ // Expand the previous state to grab everything ...prev, // You want the previous state, except with exams you're going to grab just exams and then map // that to a new function where you'll extract just the exam you want and change the score exams: prev.exams.map((exam) => { if( exam.unit === updatedExam.unit ){ return { ...exam, score: updatedExam.score }; } else { return exam; } }), })); Yikes! Complex code like this is likely to cause bugs! Luckily, there is another option\u2026 We can make more than one call to the State Hook. In fact, we can make as many calls to useState() as we want! It\u2019s best to split state into multiple state variables based on which values tend to change together. We can rewrite the previous example as follows\u2026 function Subject() { const [currentGrade, setGrade] = useState('B'); const [classmates, setClassmates] = useState(['Hasan', 'Sam', 'Emma']); const [classDetails, setClassDetails] = useState({topic: 'Math', teacher: 'Ms. Barry', room: 201}); const [exams, setExams] = useState([{unit: 1, score: 91}, {unit: 2, score: 88}]); // ... } See https://reactjs.org/docs/hooks-state.html#tip-using-multiple-state-variables","title":"Separate Hooks for Separate States"},{"location":"Notes%20on%20nodejs/#comparison","text":"function Musical() { const [state, setState] = useState({ title: \"Best Musical Ever\", actors: [\"George Wilson\", \"Tim Hughes\", \"Larry Clements\"], locations: { Chicago: { dates: [\"1/1\", \"2/2\"], address: \"chicago theater\"}, SanFrancisco: { dates: [\"5/2\"], address: \"sf theater\" } } }) } function MusicalRefactored() { const [title, setTitle] = useState(\"Best Musical Ever\"); const [actors, setActors] = useState([\"George Wilson\", \"Tim Hughes\", \"Larry Clements\"]); const [locations, setLocations] = useState({ Chicago: { dates: [\"1/1\", \"2/2\"], address: \"chicago theater\"}, SanFrancisco: { dates: [\"5/2\"], address: \"sf theater\" } }); }","title":"Comparison"},{"location":"Notes%20on%20nodejs/#the-effect-hook-useeffect","text":"Before Hooks, function components were only used to accept data in the form of props and return some JSX to be rendered. However, as we learned in the last lesson, the State Hook allows us to manage dynamic data, in the form of component state, within our function components. In this lesson, we\u2019ll use the Effect Hook to run some JavaScript code after each render, such as: fetching data from a backend service subscribing to a stream of data managing timers and intervals reading from and making changes to the DOM Why after each render? Most interesting components will re-render multiple times throughout their lifetime and these key moments present the perfect opportunity to execute these \u201cside effects\u201d. There are three key moments when the Effect Hook can be utilized: When the component is first added, or mounted, to the DOM and renders When the state or props change, causing the component to re-render When the component is removed, or unmounted, from the DOM.","title":"The Effect Hook - useEffect"},{"location":"Notes%20on%20nodejs/#react-hooks-and-component-lifecycle-equivalent","text":"https://stackoverflow.com/a/53254018/4427375","title":"React Hooks and Component Lifecycle Equivalent"},{"location":"Notes%20on%20nodejs/#componentwillmount-for-react-functional-component","text":"https://stackoverflow.com/questions/62091146/componentwillmount-for-react-functional-component","title":"componentWillMount for react functional component?"},{"location":"Notes%20on%20nodejs/#function-component-effects","text":"import React, { useState, useEffect } from 'react'; function PageTitle() { const [name, setName] = useState(''); useEffect(() => { document.title = `Hi, ${name}`; }); return ( <div> <p>Use the input field below to rename this page!</p> <input onChange={({target}) => setName(target.value)} value={name} type='text' /> </div> ); } In our effect, we assign the value of the name variable to the document.title within a string. For more on this syntax, have a look at this explanation of the document\u2019s title property. Notice how we use the current state inside of our effect. Even though our effect is called after the component renders, we still have access to the variables in the scope of our function component! When React renders our component, it will update the DOM as usual, and then run our effect after the DOM has been updated. This happens for every render, including the first and last one.","title":"Function Component Effects"},{"location":"Notes%20on%20nodejs/#clean-up-effects","text":"useEffect(()=>{ document.addEventListener('keydown', handleKeyPress); return () => { document.removeEventListener('keydown', handleKeyPress); }; }) If our effect didn\u2019t return a cleanup function, then a new event listener would be added to the DOM\u2019s document object every time that our component re-renders. Not only would this cause bugs, but it could cause our application performance to diminish and maybe even crash! Because effects run after every render and not just once, React calls our cleanup function before each re-render and before unmounting to clean up each effect call. If our effect returns a function, then the useEffect() Hook always treats that as a cleanup function. React will call this cleanup function before the component re-renders or unmounts. Since this cleanup function is optional, it is our responsibility to return a cleanup function from our effect when our effect code could create memory leaks. import React, { useState, useEffect } from 'react'; export default function Counter() { const [clickCount, setClickCount] = useState(0); const increment = () => setClickCount((prev) => prev + 1); useEffect(() => { document.addEventListener('mousedown', increment); return () => { document.removeEventListener('mousedown', increment); }; }); return ( <h1>Document Clicks: {clickCount}</h1> ); }","title":"Clean Up Effects"},{"location":"Notes%20on%20nodejs/#control-when-effects-are-called","text":"It is common, when defining function components, to run an effect only when the component mounts (renders the first time), but not when the component re-renders. The Effect Hook makes this very easy for us to do! If we want to only call our effect after the first render, we pass an empty array to useEffect() as the second argument. This second argument is called the dependency array. The dependency array is used to tell the useEffect() method when to call our effect and when to skip it. Our effect is always called after the first render but only called again if something in our dependency array has changed values between renders useEffect(() => { alert(\"component rendered for the first time\"); return () => { alert(\"component is being removed from the DOM\"); }; }, []);","title":"Control When Effects are Called"},{"location":"Notes%20on%20nodejs/#fetch-data-from-a-server","text":"Since the effect hook is called after every render we want to be extra careful when we are fetching data from a server as this will quickly sabotage the performance of our app. When the data that our components need to render doesn\u2019t change, we can pass an empty dependency array, so that the data is fetched after the first render. When the response is received from the server, we can use a state setter from the State Hook to store the data from the server\u2019s response in our local component state for future renders. Using the State Hook and the Effect Hook together in this way is a powerful pattern that saves our components from unnecessarily fetching new data after every render! An empty dependency array signals to the Effect Hook that our effect never needs to be re-run, that it doesn\u2019t depend on anything. Specifying zero dependencies means that the result of running that effect won\u2019t change and calling our effect once is enough. A dependency array that is not empty signals to the Effect Hook that it can skip calling our effect after re-renders unless the value of one of the variables in our dependency array has changed. If the value of a dependency has changed, then the Effect Hook will call our effect again! Here\u2019s a nice example from the official React docs: useEffect(() => { document.title = `You clicked ${count} times`; }, [count]); // Only re-run the effect if the value stored by count changes","title":"Fetch Data from a Server"},{"location":"Notes%20on%20nodejs/#rules-of-hooks","text":"There are two main rules to keep in mind when using Hooks: only call Hooks at the top level only call Hooks from React functions As we have been practicing with the State Hook and the Effect Hook, we\u2019ve been following these rules with ease, but it is helpful to keep these two rules in mind as you take your new understanding of Hooks out into the wild and begin using more Hooks in your React applications. When React builds the Virtual DOM, the library calls the functions that define our components over and over again as the user interacts with the user interface. React keeps track of the data and functions that we are managing with Hooks based on their order in the function component\u2019s definition. For this reason, we always call our Hooks at the top level; we never call hooks inside of loops, conditions, or nested functions. Instead of confusing React with code like this: if (userName !== '') { useEffect(() => { localStorage.setItem('savedUserName', userName); }); } We can accomplish the same goal, while consistently calling our Hook every time: useEffect(() => { if (userName !== '') { localStorage.setItem('savedUserName', userName); } }); Secondly, Hooks can only be used in React Functions. We cannot use Hooks in class components and we cannot use Hooks in regular JavaScript functions. We\u2019ve been working with useState() and useEffect() in function components, and this is the most common use. The only other place where Hooks can be used is within custom hooks. Custom Hooks are incredibly useful for organizing and reusing stateful logic between function components. For more on this topic, head to the React Docs.","title":"Rules of Hooks"},{"location":"Notes%20on%20nodejs/#separate-hooks-for-separate-effects","text":"When multiple values are closely related and change at the same time, it can make sense to group these values in a collection like an object or array. Packaging data together can also add complexity to the code responsible for managing that data. Therefore, it is a good idea to separate concerns by managing different data with different Hooks. Compare the complexity here, where data is bundled up into a single object: // Handle both position and menuItems with one useEffect hook. const [data, setData] = useState({ position: { x: 0, y: 0 } }); useEffect(() => { get('/menu').then((response) => { setData((prev) => ({ ...prev, menuItems: response.data })); }); const handleMove = (event) => setData((prev) => ({ ...prev, position: { x: event.clientX, y: event.clientY } })); window.addEventListener('mousemove', handleMove); return () => window.removeEventListener('mousemove', handleMove); }, []); To the simplicity here, where we have separated concerns: // Handle menuItems with one useEffect hook. const [menuItems, setMenuItems] = useState(null); useEffect(() => { get('/menu').then((response) => setMenuItems(response.data)); }, []); // Handle position with a separate useEffect hook. const [position, setPosition] = useState({ x: 0, y: 0 }); useEffect(() => { const handleMove = (event) => setPosition({ x: event.clientX, y: event.clientY }); window.addEventListener('mousemove', handleMove); return () => window.removeEventListener('mousemove', handleMove); }, []);","title":"Separate Hooks for Separate Effects"},{"location":"Notes%20on%20nodejs/#stateless-components-from-stateful-components","text":"Instead of having one, very complicated, stateful, component, we have one stateful component (App) at the top level with many stateless components in a hierarchy. The stateful component will pass its state down to the stateless components.","title":"Stateless Components from Stateful Components"},{"location":"Notes%20on%20nodejs/#build-a-stateful-component-class","text":"Example of passing a parent's state into a stateless child // PARENT import React from 'react'; import ReactDOM from 'react-dom'; import { Child } from './Child'; class Parent extends React.Component { constructor(props) { super(props); this.state = { name: 'Frarthur' }; } render() { return <Child name={this.state.name}/>; } } ReactDOM.render(<Parent />, document.getElementById('app')); // CHILD import React from 'react'; import ReactDOM from 'react-dom'; // We have to export this since it will be rendered by // another component export class Child extends React.Component { render() { return <h1>Hey, my name is {this.props.name}!</h1>; } } This will print: Hey, my name is Frarthur!","title":"Build a Stateful Component Class"},{"location":"Notes%20on%20nodejs/#dont-update-props","text":"A React component should use props to store information that can be changed, but can only be changed by a different component. A React component should use state to store information that the component itself can change. // BAD import React from 'react'; class Bad extends React.Component { render() { this.props.message = 'yo'; // NOOOOOOOOOOOOOO!!! return <h1>{this.props.message}</h1>; } }","title":"Don't Update props"},{"location":"Notes%20on%20nodejs/#child-components-update-their-parents-state","text":"How does a stateless, child component update the state of the parent component? Here\u2019s how that works: 1 The parent component class defines a method that calls this.setState(). For an example, look in Step1.js at the .handleClick() method. import React from 'react'; import ReactDOM from 'react-dom'; import { ChildClass } from './ChildClass'; class ParentClass extends React.Component { constructor(props) { super(props); this.state = { totalClicks: 0 }; } handleClick() { const total = this.state.totalClicks; // calling handleClick will // result in a state change: this.setState( { totalClicks: total + 1 } ); } } 2 The parent component binds the newly-defined method to the current instance of the component in its constructor. This ensures that when we pass the method to the child component, it will still update the parent component. For an example, look in Step2.js at the end of the constructor() method. An explanation of how this/bind work How bind works: https://stackoverflow.com/a/10115970/4427375 What is the global object Once the parent has defined a method that updates its state and bound to it, the parent then passes that method down to a child. Look in Step2.js, at the prop on line 28. import React from 'react'; import ReactDOM from 'react-dom'; import { ChildClass } from './ChildClass'; class ParentClass extends React.Component { constructor(props) { super(props); this.state = { totalClicks: 0 }; this.handleClick = this.handleClick.bind(this); } handleClick() { const total = this.state.totalClicks; // calling handleClick will // result in a state change: this.setState( { totalClicks: total + 1 } ); } // The stateful component class passes down // handleClick to a stateless component class: render() { return ( <ChildClass onClick={this.handleClick} /> ); } } 3 The child receives the passed-down function, and uses it as an event handler. Look in Step3.js. When a user clicks on the , a click event will fire. This will make the passed-down function get called, which will update the parent\u2019s state. import React from 'react'; import ReactDOM from 'react-dom'; export class ChildClass extends React.Component { render() { return ( // The stateless component class uses // the passed-down handleClick function, // accessed here as this.props.onClick, // as an event handler: <button onClick={this.props.onClick}> Click Me! </button> ); } }","title":"Child Components Update Their Parents' State"},{"location":"Notes%20on%20nodejs/#more-complex-example","text":"WARNING this violates the rule that components should only do one thing! We fix this in One Sibling to Display, Another to Change // CHILD import React from 'react'; export class Child extends React.Component { constructor(props) { super(props); this.handleChange = this.handleChange.bind(this); } handleChange(e) { const name = e.target.value; this.props.onChange(name); } render() { return ( <div> <h1> Hey my name is {this.props.name}! </h1> <select id=\"great-names\" onChange={this.handleChange}> <option value=\"Frarthur\"> Frarthur </option> <option value=\"Gromulus\"> Gromulus </option> <option value=\"Thinkpiece\"> Thinkpiece </option> </select> </div> ); } } // PARENT import React from 'react'; import ReactDOM from 'react-dom'; import { Child } from './Child'; class Parent extends React.Component { constructor(props) { super(props); this.state = { name: 'Frarthur' }; this.changeName = this.changeName.bind(this); } changeName(newName) { this.setState({ name: newName }); } render() { return <Child name={this.state.name} onChange={this.changeName} /> } } ReactDOM.render( <Parent />, document.getElementById('app') );","title":"More Complex Example"},{"location":"Notes%20on%20nodejs/#child-components-update-sibling-components","text":"The Reactions component passes an event handler to the Like component. When Like is clicked, the handler is called, which causes the parent Reactions component to send a new prop to Stats. The Stats component updates with the new information.","title":"Child Components Update Sibling Components"},{"location":"Notes%20on%20nodejs/#one-sibling-to-display-another-to-change","text":"You will have one stateless component display information, and a different stateless component offer the ability to change that information. A stateful component class defines a function that calls this.setState. (Parent.js, lines 15-19) The stateful component passes that function down to a stateless component. (Parent.js, line 24) That stateless component class defines a function that calls the passed-down function, and that can take an event object as an argument. (Child.js, lines 10-13) The stateless component class uses this new function as an event handler. (Child.js, line 20) When an event is detected, the parent\u2019s state updates. (A user selects a new dropdown menu item) The stateful component class passes down its state, distinct from the ability to change its state, to a different stateless component. (Parent.js, line 25) That stateless component class receives the state and displays it. (Sibling.js, lines 5-10) An instance of the stateful component class is rendered. One stateless child component displays the state, and a different stateless child component displays a way to change the state. (Parent.js, lines 23-26) // PARENT import React from 'react'; import ReactDOM from 'react-dom'; import { Child } from './Child'; import { Sibling } from './Sibling'; class Parent extends React.Component { constructor(props) { super(props); this.state = { name: 'Frarthur' }; this.changeName = this.changeName.bind(this); } changeName(newName) { this.setState({ name: newName }); } render() { return ( <div> <Child onChange={this.changeName} /> <Sibling name={this.state.name}/> </div> ); } } ReactDOM.render( <Parent />, document.getElementById('app') ); // CHILD import React from 'react'; export class Child extends React.Component { constructor(props) { super(props); this.handleChange = this.handleChange.bind(this); } handleChange(e) { const name = e.target.value; this.props.onChange(name); } render() { return ( <div> <select id=\"great-names\" onChange={this.handleChange}> <option value=\"Frarthur\">Frarthur</option> <option value=\"Gromulus\">Gromulus</option> <option value=\"Thinkpiece\">Thinkpiece</option> </select> </div> ); } } // SIBLING import React from 'react'; export class Sibling extends React.Component { render() { const name = this.props.name; return ( <div> <h1>Hey, my name is {name}!</h1> <h2>Don't you think {name} is the prettiest name ever?</h2> <h2>Sure am glad that my parents picked {name}!</h2> </div> ); } }","title":"One Sibling to Display, Another to Change"},{"location":"Notes%20on%20nodejs/#style","text":"","title":"Style"},{"location":"Notes%20on%20nodejs/#inline-styles","text":"An inline style is a style that\u2019s written as an attribute, like this: <h1 style={{ color: 'red' }}>Hello world</h1> Notice the double curly braces. What are those for? The outer curly braces inject JavaScript into JSX. They say, \u201ceverything between us should be read as JavaScript, not JSX.\u201d The inner curly braces create a JavaScript object literal. They make this a valid JavaScript object: { color: 'red' } If you inject an object literal into JSX, and your entire injection is only that object literal, then you will end up with double curly braces. There\u2019s nothing unusual about how they work, but they look funny and can be confusing.","title":"Inline Styles"},{"location":"Notes%20on%20nodejs/#make-a-style-object-variable","text":"Notice that here we define the style at the top level as a variable and then pass it in. In React style variable names are written camelCase. NOTE : The styles in ReactJS use numbers and the px is implied. import React from 'react'; import ReactDOM from 'react-dom'; const styles = { background: 'lightblue', color: 'darkred' marginTop: 100, fontSize: 50 }; const styleMe = <h1 style={styles}>Please style me! I am so bland!</h1>; ReactDOM.render( styleMe, document.getElementById('app') );","title":"Make a Style Object Variable"},{"location":"Notes%20on%20nodejs/#share-styles-across-multiple-components","text":"// STYLES.JS const fontFamily = 'Comic Sans MS, Lucida Handwriting, cursive'; const background = 'pink url(\"https://content.codecademy.com/programs/react/images/welcome-to-my-homepage.gif\") fixed'; const fontSize = '4em'; const padding = '45px 0'; const color = 'green'; export const styles = { fontFamily: fontFamily, background: background, fontSize: fontSize, padding: padding, color: color }; // ATTENTIONGRABBER.JS import React from 'react'; import { styles } from './styles'; const h1Style = { color: styles.color, fontSize: styles.fontSize, fontFamily: styles.fontFamily, padding: styles.padding, margin: 0, }; export class AttentionGrabber extends React.Component { render() { return <h1 style={h1Style}>WELCOME TO MY HOMEPAGE!</h1>; } } // HOME.JS import React from 'react'; import ReactDOM from 'react-dom'; import { AttentionGrabber } from './AttentionGrabber'; import { styles } from './styles'; const divStyle = { background: styles.background, height: '100%' }; export class Home extends React.Component { render() { return ( <div style={divStyle}> <AttentionGrabber /> <footer>THANK YOU FOR VISITING MY HOMEPAGE!</footer> </div> ); } } ReactDOM.render( <Home />, document.getElementById('app') );","title":"Share Styles Across Multiple Components"},{"location":"Notes%20on%20nodejs/#separate-container-components-from-presentational-components","text":"As you continue building your React application, you will soon realize that one component has too many responsibilities, but how do you know when you have reached that point? Separating container components from presentational components helps to answer that question. It shows you when it might be a good time to divide a component into smaller components. It also shows you how to perform that division. <GuineaPigs /> \u2018s job is to render a photo carousel of guinea pigs. It does this perfectly well! And yet, it has a problem: it does too much stuff. How might we divide this into a container component and a presentational component? import React from 'react'; import ReactDOM from 'react-dom'; const GUINEAPATHS = [ 'https://content.codecademy.com/courses/React/react_photo-guineapig-1.jpg', 'https://content.codecademy.com/courses/React/react_photo-guineapig-2.jpg', 'https://content.codecademy.com/courses/React/react_photo-guineapig-3.jpg', 'https://content.codecademy.com/courses/React/react_photo-guineapig-4.jpg' ]; export class GuineaPigs extends React.Component { constructor(props) { super(props); this.state = { currentGP: 0 }; this.interval = null; this.nextGP = this.nextGP.bind(this); } nextGP() { let current = this.state.currentGP; let next = ++current % GUINEAPATHS.length; this.setState({ currentGP: next }); } componentDidMount() { this.interval = setInterval(this.nextGP, 5000); } componentWillUnmount() { clearInterval(this.interval); } render() { let src = GUINEAPATHS[this.state.currentGP]; return ( <div> <h1>Cute Guinea Pigs</h1> <img src={src} /> </div> ); } } ReactDOM.render( <GuineaPigs />, document.getElementById('app') );","title":"Separate Container Components from Presentational Components"},{"location":"Notes%20on%20nodejs/#create-a-container-component","text":"Separating container components from presentational components is a popular React programming pattern. It is a special application of the concepts learned in the Stateless Components From Stateful Components module. If a component has to have state, make calculations based on props, or manage any other complex logic, then that component shouldn\u2019t also have to render HTML-like JSX. The functional part of a component (state, calculations, etc.) can be separated into a container component. GuineaPigs.js contains a lot of logic! It has to select the correct guinea pig to render, wait for the right amount of time before rendering, render an image, select the next correct guinea pig, and so on. Let\u2019s separate the logic from the GuineaPigs component into a container component.","title":"Create a Container Component"},{"location":"Notes%20on%20nodejs/#create-a-presentational-component","text":"The presentational component\u2019s only job is to contain HTML-like JSX. It should be an exported component and will not render itself because a presentational component will always get rendered by a container component. As a separate example, say we have Presentational and Container components. Presentational.js must export the component class (or function, when applicable): export class Presentational extends Component { Container.js must import that component: import { Presentational } from 'Presentational.js'; // GuineaPigs.js import React from 'react'; export class GuineaPigs extends React.Component { render() { let src = this.props.src; return ( <div> <h1>Cute Guinea Pigs</h1> <img src={src} /> </div> ); } } // GuineaPigsContainer.js import React from 'react'; import ReactDOM from 'react-dom'; import { GuineaPigs } from '../components/GuineaPigs'; const GUINEAPATHS = [ 'https://content.codecademy.com/courses/React/react_photo-guineapig-1.jpg', 'https://content.codecademy.com/courses/React/react_photo-guineapig-2.jpg', 'https://content.codecademy.com/courses/React/react_photo-guineapig-3.jpg', 'https://content.codecademy.com/courses/React/react_photo-guineapig-4.jpg' ]; class GuineaPigsContainer extends React.Component { constructor(props) { super(props); this.state = { currentGP: 0 }; this.interval = null; this.nextGP = this.nextGP.bind(this); } nextGP() { let current = this.state.currentGP; let next = ++current % GUINEAPATHS.length; this.setState({ currentGP: next }); } componentDidMount() { this.interval = setInterval(this.nextGP, 5000); } componentWillUnmount() { clearInterval(this.interval); } render() { const src = GUINEAPATHS[this.state.currentGP]; return <GuineaPigs src={src} />; } } ReactDOM.render( <GuineaPigsContainer />, document.getElementById('app') );","title":"Create a Presentational Component"},{"location":"Notes%20on%20nodejs/#proptypes","text":"propTypes are useful for two reasons. The first reason is prop validation. Validation can ensure that your props are doing what they\u2019re supposed to be doing. If props are missing, or if they\u2019re present but they aren\u2019t what you\u2019re expecting, then a warning will print in the console. This is useful, but reason #2 is arguably more useful: documentation. Documenting props makes it easier to glance at a file and quickly understand the component class inside. When you have a lot of files, and you will, this can be a huge benefit.","title":"propTypes"},{"location":"Notes%20on%20nodejs/#apply-proptypes","text":"The name of each property in propTypes should be the name of an expected prop. In our case, MessageDisplayer expects a prop named message, so our property\u2019s name is message. The value of each property in propTypes should fit this pattern: PropTypes.expected_data_type_goes_here import React from 'react'; import PropTypes from 'prop-types'; export class BestSeller extends React.Component { render() { return ( <li> Title: <span> {this.props.title} </span><br /> Author: <span> {this.props.author} </span><br /> Weeks: <span> {this.props.weeksOnList} </span> </li> ); } } BestSeller.propTypes = { title: PropTypes.string.isRequired, author: PropTypes.string.isRequired, weeksOnList: PropTypes.number.isRequired };","title":"Apply PropTypes"},{"location":"Notes%20on%20nodejs/#proptypes-in-function-components","text":"// Normal way to display a prop: export class MyComponentClass extends React.Component { render() { return <h1>{this.props.title}</h1>; } } // Functional component way to display a prop: export const MyComponentClass = (props) => { return <h1>{props.title}</h1>; } // Normal way to display a prop using a variable: export class MyComponentClass extends React.component { render() { let title = this.props.title; return <h1>{title}</h1>; } } // Functional component way to display a prop using a variable: export const MyComponentClass = (props) => { let title = props.title; return <h1>{title}</h1>; }","title":"PropTypes in Function Components"},{"location":"Notes%20on%20nodejs/#react-forms","text":"Think about how forms work in a typical, non-React environment. A user types some data into a form\u2019s input fields, and the server doesn\u2019t know about it. The server remains clueless until the user hits a \u201csubmit\u201d button, which sends all of the form\u2019s data over to the server simultaneously. In React, as in many other JavaScript environments, this is not the best way of doing things. The problem is the period of time during which a form thinks that a user has typed one thing, but the server thinks that the user has typed a different thing. What if, during that time, a third part of the website needs to know what a user has typed? It could ask the form or the server and get two different answers. In a complex JavaScript app with many moving, interdependent parts, this kind of conflict can easily lead to problems. In a React form, you want the server to know about every new character or deletion, as soon as it happens. That way, your screen will always be in sync with the rest of your application.","title":"React Forms"},{"location":"Notes%20on%20nodejs/#input-on-change","text":"A traditional form doesn\u2019t update the server until a user hits \u201csubmit.\u201d But you want to update the server any time a user enters or deletes any character. import React from 'react'; export class Example extends React.Component { constructor(props) { super(props); this.state = { userInput: '' }; this.handleChange = this.handleChange.bind(this); } handleChange(e) { this.setState({ userInput: e.target.value }); } render() { return ( <input onChange={this.handleChange} type=\"text\" /> ); } }","title":"Input on Change"},{"location":"Notes%20on%20nodejs/#control-vs-uncontrolled","text":"There are two terms that will probably come up when you talk about React forms: controlled component and uncontrolled component. Like automatic binding, controlled vs uncontrolled components is a topic that you should be familiar with, but don\u2019t need to understand deeply at this point. An uncontrolled component is a component that maintains its own internal state. A controlled component is a component that does not maintain any internal state. Since a controlled component has no state, it must be controlled by someone else. Think of a typical <input type='text' /> element. It appears onscreen as a text box. If you need to know what text is currently in the box, then you can ask the <input /> , possibly with some code like this: let input = document.querySelector('input[type=\"text\"]'); let typedText = input.value; // input.value will be equal to whatever text is currently in the text box. The important thing here is that the <input /> keeps track of its own text. You can ask it what its text is at any time, and it will be able to tell you. The fact that <input /> keeps track of information makes it an uncontrolled component. It maintains its own internal state, by remembering data about itself. A controlled component, on the other hand, has no memory. If you ask it for information about itself, then it will have to get that information through props. Most React components are controlled. In React, when you give an <input /> a value attribute, then something strange happens: the <input /> BECOMES controlled. It stops using its internal storage. This is a more \u2018React\u2019 way of doing things.","title":"Control vs Uncontrolled"},{"location":"Notes%20on%20nodejs/#update-an-inputs-value","text":"When a user types or deletes in the <input /> , then that will trigger a change event, which will call handleUserInput. That\u2019s good! handleUserInput will set this.state.userInput equal to whatever text is currently in the input field. That\u2019s also good! There\u2019s only one problem: you can set this.state.userInput to whatever you want, but <input /> won\u2019t care. You need to somehow make the <input /> \u2018s text responsive to this.state.userInput. Easy enough! You can control an <input /> \u2018s text by setting its value attribute.","title":"Update an Input's Value"},{"location":"Notes%20on%20nodejs/#set-the-inputs-initial-state","text":"Good! Any time that someone types or deletes in <input /> , the .handleUserInput() method will update this.state.userInput with the <input /> \u2018s text. Since you\u2019re using this.setState, that means that Input needs an initial state! What should this.state\u2018s initial value be? Well, this.state.userInput will be displayed in the <input /> . What should the initial text in the <input /> be, when a user first visits the page? The initial text should be blank! Otherwise it would look like someone had already typed something.","title":"Set the Input's Initial State"},{"location":"Notes%20on%20nodejs/#dynamically-rendering-different-components-without-switch-the-capitalized-reference-technique","text":"See: https://j5bot.medium.com/react-dynamically-rendering-different-components-without-switch-the-capitalized-reference-e668d89e460b","title":"Dynamically Rendering Different Components without Switch: the Capitalized Reference Technique"},{"location":"Notes%20on%20nodejs/#react-router","text":"https://ui.dev/react-router-tutorial","title":"React Router"},{"location":"Notes%20on%20nodejs/#browserrouter","text":"Naturally, in order to do its thing, React Router needs to be both aware and in control of your app's location. The way it does this is with its BrowserRouter component. Under the hood, BrowserRouter uses both the history library as well as React Context . The history library helps React Router keep track of the browsing history of the application using the browser's built-in history stack, and React Context helps make history available wherever React Router needs it. There's not much to BrowserRouter, you just need to make sure that if you're using React Router on the web, you wrap your app inside of the BrowserRouter import ReactDOM from 'react-dom' import * as React from 'react' import { BrowserRouter } from 'react-router-dom' import App from './App` ReactDOM.render( <BrowserRouter> <App /> </BrowserRouter> , document.getElementById('app))","title":"BrowserRouter"},{"location":"Notes%20on%20nodejs/#route","text":"Put simply, Route allows you to map your app's location to different React components. For example, say we wanted to render a Dashboard component whenever a user navigated to the /dashboard path. To do so, we'd render a Route that looked like this. <Route path=\"/dashboard\" element={<Dashboard />} /> The mental model I use for Route is that it always has to render something \u2013 either its element prop if the path matches the app's current location or null, if it doesn't. You can render as many Routes as you'd like. <Route path=\"/\" element={<Home />} /> <Route path=\"/about\" element={<About />} /> <Route path=\"/settings\" element={<Settings />} /> You can even render nested routes, which we'll talk about later on in this post. With our Route elements in this configuration, it's possible for multiple routes to match on a single URL. You might want to do that sometimes, but most often you want React Router to only render the route that matches best. Fortunately, we can easily do that with Routes.","title":"Route"},{"location":"Notes%20on%20nodejs/#routes","text":"You can think of Routes as the metaphorical conductor of your routes. Whenever you have one or more Routes, you'll most likely want to wrap them in a Routes. import { Routes, Route } from \"react-router-dom\"; function App() { return ( <Routes> <Route path=\"/\" element={<Home />} /> <Route path=\"/about\" element={<About />} /> <Route path=\"/settings\" element={<Settings />} /> <Route path=\"*\" element={<NotFound />} /> </Routes> ); } The reason for this is because it's Routes job is to understand all of its children Route elements, and intelligently choose which ones are the best to render. Though it's not shown in the simple example above, once we start adding more complex Routes to our application, Routes will start to do more work like enabling intelligent rendering and relative paths. We'll see these scenarios in a bit. Next up, linking between pages.","title":"Routes"},{"location":"Notes%20on%20nodejs/#links","text":"Now that you know how to map the app's location to certain React components using Routes and Route, the next step is being able to navigate between them. This is the purpose of the Link component. To tell Link what path to take the user to when clicked, you pass it a to prop. <nav> <Link to=\"/\">Home</Link> <Link to=\"/about\">About</Link> <Link to=\"/settings\">Settings</Link> </nav> If you need more control over Link, you can also pass to as an object. Doing so allows you to add a query string via the search property or pass along any data to the new route via state. <nav> <Link to=\"/\">Home</Link> <Link to=\"/about\">About</Link> <Link to={{ pathname: \"/settings\", search: \"?sort=date\", state: { fromHome: true }, }} > Settings </Link> </nav>","title":"Links"},{"location":"Notes%20on%20nodejs/#url-parameters","text":"Like function parameters allow you to declare placeholders when you define a function, URL Parameters allow you to declare placeholders for portions of a URL. Take Wikipedia for example. When you visit a topic on Wikipedia, you'll notice that the URL pattern is always the same, wikipedia.com/wiki/{topicId}. Instead of defining a route for every topic on the site, they can declare one route with a placeholder for the topic's id. The way you tell React Router that a certain portion of the URL is a placeholder (or URL Parameter), is by using a : in the Route's path prop. <Route path=\"/wiki/:topicId\" element={<Article />} /> Now whenever anyone visits a URL that matches the /wiki/:topicId pattern (/wiki/javascript, /wiki/Brendan_Eich, /wiki/anything) , the Article component is rendered. Now the question becomes, how do you access the dynamic portion of the URL \u2013 in this case, topicId \u2013 in the component that's rendered? As of v5.1, React Router comes with a useParams Hook that returns an object with a mapping between the URL parameter(s) and its value. import * as React from 'react' import { useParams } from 'react-router-dom' import { getArticle } from '../utils' function Article () { const [article, setArticle] = React.useState(null) const { topicId } = useParams() React.useEffect(() => { getArticle(topicId) .then(setUser) }, [topicId]) return ( ... ) }","title":"URL Parameters"},{"location":"Notes%20on%20nodejs/#nested-routes","text":"Nested Routes allow the parent Route to act as a wrapper and control the rendering of a child Route. A real-life example of this UI could look similar to Twitter's /messages route. When you go to /messages, you see all of your previous conversations on the left side of the screen. Then, when you go to /messages/:id, you still see all your messages, but you also see your chat history for :id. Let's look at how we could implement this sort of nested routes pattern with React Router. We'll start off with some basic Routes. // App.js function App() { return ( <Routes> <Route path=\"/\" element={<Home />} /> <Route path=\"/messages\" element={<Messages />} /> <Route path=\"/settings\" element={<Settings />} /> </Routes> ); } Now, if we want Messages to be in control of rendering a child Routes, what's stopping us from just rendering another Routes component inside Messages? Something like this: function Messages() { return ( <Container> <Conversations /> <Routes> <Route path=\":id\" element={<Chat />} /> </Routes> </Container> ); } Now when the user navigates to /messages, React Router renders the Messages component. From there, Messages shows all our conversations via the Conversations component and then renders another Routes with a Route that maps /messages/:id to the Chat component. Relative Routes Notice that we don't have to include the full /messages/:id path in the nested Route. This is because Routes is intelligent and by leaving off the leading /, it assumes we want this path to be relative to the parent's location, /messages. Looks good, but there's one subtle issue. Can you spot it? Messages only gets rendered when the user is at /messages. When they visit a URL that matches the /messages/:id pattern, Messages no longer matches and therefore, our nested Routes never gets rendered. To fix this, naturally, we need a way to tell React Router that we want to render Messages both when the user is at /messages or any other location that matches the /messages/* pattern. Wait. What if we just update our path to be /messages/*? // App.js function App() { return ( <Routes> <Route path=\"/\" element={<Home />} /> <Route path=\"/messages/*\" element={<Messages />} /> <Route path=\"/settings\" element={<Settings />} /> </Routes> ); } Much to our delight, that'll work. By appending a / to the end of our /messages path, we're essentially telling React Router that Messages has a nested Routes component and our parent path should match for /messages as well as any other location that matches the /messages/ pattern. Exactly what we wanted. At this point, we've looked at how you can create nested routes by appending /* to our Route's path and rendering, literally, a nested Routes component. This works when you want your child Route in control of rendering the nested Routes, but what if we wanted our App component to contain all the information it needed to create our nested routes rather than having to do it inside of Messages? Because this is a common preference, React Router supports this way of creating nested routes as well. Here's what it looks like. function App() { return ( <Routes> <Route path=\"/\" element={<Home />} /> <Route path=\"/messages\" element={<Messages />}> <Route path=\":id\" element={<Chats />} /> </Route> <Route path=\"/settings\" element={<Settings />} /> </Routes> ); } You declaratively nest the child Route as a children of the parent Route. Like before, the child Route is now relative to the parent, so you don't need to include the parent (/messages) path. Now, the last thing you need to do is tell React Router where in the parent Route (Messages) should it render the child Route (Chats). To do this, you use React Router's Outlet component. import { Outlet } from \"react-router-dom\"; function Messages() { return ( <Container> <Conversations /> <Outlet /> </Container> ); } If the app's location matches the nested Route's path, this Outlet component will render the Route's element. So based on our Routes above, if we were at /messages, the Outlet component would render null, but if we were at /messages/1, it would render the component.","title":"Nested Routes"},{"location":"Notes%20on%20nodejs/#pass-props-to-router-components","text":"In previous versions of React Router (v4), this was non-trivial since React Router was in charge of creating the React element. However, with React Router v6, since you're in charge of creating the element, you just pass a prop to the component as you normally would. <Route path=\"/dashboard\" element={<Dashboard authed={true} />} />","title":"Pass props to Router Components"},{"location":"Notes%20on%20nodejs/#good-practices-for-calling-apis-from-reactjs","text":"https://medium.com/weekly-webtips/patterns-for-doing-api-calls-in-reactjs-8fd9a42ac7d4","title":"Good Practices for Calling APIs from ReactJS"},{"location":"Notes%20on%20nodejs/#jsx","text":"JSX is a syntax extension for JavaScript. It was written to be used with React. JSX code looks a lot like HTML. What does \"syntax extension\" mean? In this case, it means that JSX is not valid JavaScript. Web browsers can\u2019t read it! If a JavaScript file contains JSX code, then that file will have to be compiled. That means that before the file reaches a web browser, a JSX compiler will translate any JSX into regular JavaScript.","title":"JSX"},{"location":"Notes%20on%20nodejs/#jsx-elements","text":"A basic unit of JSX is called a JSX element. Here\u2019s an example of a JSX element: <h1>Hello world</h1>","title":"JSX Elements"},{"location":"Notes%20on%20nodejs/#jsx-elements-and-their-surroundings","text":"JSX elements are treated as JavaScript expressions. They can go anywhere that JavaScript expressions can go. That means that a JSX element can be saved in a variable, passed to a function, stored in an object or array\u2026you name it. Here\u2019s an example of a JSX element being saved in a variable: const navBar = <nav>I am a nav bar</nav>; const myTeam = { center: <li>Benzo Walli</li>, powerForward: <li>Rasha Loa</li>, smallForward: <li>Tayshaun Dasmoto</li>, shootingGuard: <li>Colmar Cumberbatch</li>, pointGuard: <li>Femi Billon</li> };","title":"JSX Elements And Their Surroundings"},{"location":"Notes%20on%20nodejs/#attributes-in-jsx","text":"JSX elements can have attributes, just like HTML elements can. A JSX attribute is written using HTML-like syntax: a name, followed by an equals sign, followed by a value. The value should be wrapped in quotes, like this: my-attribute-name=\"my-attribute-value\" <a href='http://www.example.com'>Welcome to the Web</a>; const title = <h1 id='title'>Introduction to React.js: Part I</h1>; const panda = <img src='images/panda.jpg' alt='panda' width='500px' height='500px' />;","title":"Attributes In JSX"},{"location":"Notes%20on%20nodejs/#nested-jsx","text":"If a JSX expression takes up more than one line, then you must wrap the multi-line JSX expression in parentheses. This looks strange at first, but you get used to it: const theExample = ( <a href=\"https://www.example.com\"> <h1> Click me! </h1> </a> )","title":"Nested JSX"},{"location":"Notes%20on%20nodejs/#jsx-outer-elements","text":"There\u2019s a rule that we haven\u2019t mentioned: a JSX expression must have exactly one outermost element. In other words, this code will work: const paragraphs = ( <div id=\"i-am-the-outermost-element\"> <p>I am a paragraph.</p> <p>I, too, am a paragraph.</p> </div> ); // But this code will not work: const paragraphs = ( <p>I am a paragraph.</p> <p>I, too, am a paragraph.</p> );","title":"JSX Outer Elements"},{"location":"Notes%20on%20nodejs/#rendering-jsx","text":"The following code will render a JSX expression: ReactDOM.render(<h1>Hello world</h1>, document.getElementById('app'));","title":"Rendering JSX"},{"location":"Notes%20on%20nodejs/#reactdomrender","text":"ReactDOM is the name of a JavaScript library. This library contains several React-specific methods, all of which deal with the DOM in some way or another. When a web page is loaded, the browser creates a Document Object Model of the page. The HTML DOM model is constructed as a tree of Objects: ReactDOM.render() is the most common way to render JSX. It takes a JSX expression, creates a corresponding tree of DOM nodes, and adds that tree to the DOM. That is the way to make a JSX expression appear onscreen. In the code ReactDOM.render(<h1>Render me!</h1>, document.getElementById('app')); the expression <h1>Render me!</h1> is what you want rendered. The second argument document.getElementById('app') indicates where you want to append the first argument in the DOM. Ex: if you had the following HTML: <!DOCTYPE html> <html lang=\"en\"> <head> <meta charset=\"utf-8\"> <link rel=\"stylesheet\" href=\"/styles.css\"> <title>Learn ReactJS</title> </head> <body> <main id=\"app\"></main> <script src=\"https://content.codecademy.com/courses/React/react-course-bundle.min.js\"></script> <script src=\"/app.compiled.js\"></script> </body> </html> The element with the ID would be selected and the DOM added to it. One special thing about ReactDOM.render() is that it only updates DOM elements that have changed. That means that if you render the exact same thing twice in a row, the second render will do nothing.","title":"ReactDOM.render()"},{"location":"Notes%20on%20nodejs/#passing-a-variable-to-reactdomrender","text":"ReactDOM.render()\u2018s first argument should evaluate to a JSX expression, it doesn\u2019t have to literally be a JSX expression. The first argument could also be a variable, so long as that variable evaluates to a JSX expression.","title":"Passing a Variable to ReactDOM.render()"},{"location":"Notes%20on%20nodejs/#class-vs-classname","text":"<h1 class=\"big\">Hey</h1> In JSX, you can\u2019t use the word class! You have to use className instead: <h1 className=\"big\">Hey</h1>","title":"class vs className"},{"location":"Notes%20on%20nodejs/#self-closing-tags","text":"With self closing tags you MUST include the slash in JSX. Ex: <br /> . The trailing / isn't optional.","title":"Self-Closing Tags"},{"location":"Notes%20on%20nodejs/#javascript-in-jsx","text":"Te render Javascript in JSX, you have to use curly braces. Ex: import React from 'react'; import ReactDOM from 'react-dom'; // Write code here: ReactDOM.render( <h1>{2 + 3}</h1>, document.getElementById('app') );","title":"Javascript in JSX"},{"location":"Notes%20on%20nodejs/#variables-in-jsx","text":"When you inject JavaScript into JSX, that JavaScript is part of the same environment as the rest of the JavaScript in your file. That means that you can access variables while inside of a JSX expression, even if those variables were declared on the outside. You can set HTML attributes with curly braces like this: // Use a variable to set the `height` and `width` attributes: const sideLength = \"200px\"; const panda = ( <img src=\"images/panda.jpg\" alt=\"panda\" height={sideLength} width={sideLength} /> );","title":"Variables in JSX"},{"location":"Notes%20on%20nodejs/#event-listeners-in-jsx","text":"JSX elements can have event listeners, just like HTML elements can. Programming in React means constantly working with event listeners. You create an event listener by giving a JSX element a special attribute. Here\u2019s an example: <img onClick={myFunc} /> An event listener attribute\u2019s name should be something like onClick or onMouseOver: the word on, plus the type of event that you\u2019re listening for. You can see a list of valid event names here . An event listener attribute\u2019s value should be a function. The above example would only work if myFunc were a valid function that had been defined elsewhere: function myFunc() { alert('Make myFunc the pFunc... omg that was horrible i am so sorry'); } <img onClick={myFunc} />","title":"Event Listeners in JSX"},{"location":"Notes%20on%20nodejs/#jsx-conditionals","text":"This code will break: ( <h1> { if (purchase.complete) { 'Thank you for placing an order!' } } </h1> ) The reason why has to do with the way that JSX is compiled. You don\u2019t need to understand the mechanics of it for now, but if you\u2019re interested then you can learn more in the React documentation . How can you write a conditional, if you can\u2019t inject an if statement into JSX? Well, one option is to write an if statement, and not inject it into JSX. Look at if.js. Follow the if statement, all the way from line 6 down to line 18. if.js works, because the words if and else are not injected in between JSX tags. The if statement is on the outside, and no JavaScript injection is necessary. import React from 'react'; import ReactDOM from 'react-dom'; let message; if (user.age >= drinkingAge) { message = ( <h1> Hey, check out this alcoholic beverage! </h1> ); } else { message = ( <h1> Hey, check out these earrings I got at Claire's! </h1> ); } ReactDOM.render( message, document.getElementById('app') );","title":"JSX Conditionals"},{"location":"Notes%20on%20nodejs/#ternary-operator","text":"Recall how it works: you write x ? y : z, where x, y, and z are all JavaScript expressions. When your code is executed, x is evaluated as either \"truthy\" or \"falsy.\" If x is truthy, then the entire ternary operator returns y. If x is falsy, then the entire ternary operator returns z. Here\u2019s a nice explanation if you need a refresher. const headline = ( <h1> { age >= drinkingAge ? 'Buy Drink' : 'Do Teen Stuff' } </h1> );","title":"Ternary Operator"},{"location":"Notes%20on%20nodejs/#_1","text":"&& works best in conditionals that will sometimes do an action, but other times do nothing at all. Here\u2019s an example: const tasty = ( <ul> <li>Applesauce</li> { !baby && <li>Pizza</li> } { age > 15 && <li>Brussels Sprouts</li> } { age > 20 && <li>Oysters</li> } { age > 25 && <li>Grappa</li> } </ul> ); If the expression on the left of the && evaluates as true, then the JSX on the right of the && will be rendered. If the first expression is false, however, then the JSX to the right of the && will be ignored and not rendered.","title":"&amp;&amp;"},{"location":"Notes%20on%20nodejs/#map-in-jsx","text":"If you want to create a list of JSX elements, then .map() is often your best bet. It can look odd at first: const strings = ['Home', 'Shop', 'About Me']; const listItems = strings.map(string => <li>{string}</li>); <ul>{listItems}</ul> In the above example, we start out with an array of strings. We call .map() on this array of strings, and the .map() call returns a new array of s. If you want the index you can do: const listItems = strings.map((string, i) => <li>{string}</li>);","title":".map in JSX"},{"location":"Notes%20on%20nodejs/#list-keys","text":"When you make a list in JSX, sometimes your list will need to include something called keys: <ul> <li key=\"li-01\">Example1</li> <li key=\"li-02\">Example2</li> <li key=\"li-03\">Example3</li> </ul> A key is a JSX attribute. The attribute\u2019s name is key. The attribute\u2019s value should be something unique, similar to an id attribute. keys don\u2019t do anything that you can see! React uses them internally to keep track of lists. If you don\u2019t use keys when you\u2019re supposed to, React might accidentally scramble your list-items into the wrong order. Not all lists need to have keys. A list needs keys if either of the following are true: The list-items have memory from one render to the next. For instance, when a to-do list renders, each item must \"remember\" whether it was checked off. The items shouldn\u2019t get amnesia when they render. A list\u2019s order might be shuffled. For instance, a list of search results might be shuffled from one render to the next. import React from 'react'; import ReactDOM from 'react-dom'; const people = ['Rowe', 'Prevost', 'Gare']; const peopleLis = people.map((person, i) => // expression goes here: <li key={'person_' + i}>{person}</li> ); // ReactDOM.render goes here: ReactDOM.render(<ul>{peopleLis}</ul>, document.getElementById('app'))","title":"List Keys"},{"location":"Notes%20on%20nodejs/#react-create-element","text":"You can write React code without using JSX at all! The majority of React programmers do use JSX, and we will use it for the remainder of this tutorial, but you should understand that it is possible to write React code without it. The following JSX expression: const h1 = <h1>Hello world</h1>; can be rewritten without JSX, like this: const h1 = React.createElement( \"h1\", null, \"Hello world\" ); When a JSX element is compiled, the compiler transforms the JSX element into the method that you see above: React.createElement(). Every JSX element is secretly a call to React.createElement().","title":"React Create Element"},{"location":"Notes%20on%20nodejs/README_temp/","text":"Notes on nodejs","title":"Notes on nodejs"},{"location":"Notes%20on%20nodejs/README_temp/#notes-on-nodejs","text":"","title":"Notes on nodejs"},{"location":"Nvidia%20GPUDirect/","text":"Nvidia GPUDirect Nvidia GPUDirect Background Research Helpful Links Source Code Description of the Problem Lab Configuration Hardware Configuration RHEL Version GCC Version Nvidia SMI CUDA Version Setting Up the Code Environment CUDA Development Packages Prepare the Code Compiling and Running the App Debugging Background Research See Background Research for background information I studied before doing this. Helpful Links RDMA Aware Programming Typical Application Flow Source Code See the test file Description of the Problem What we are doing in the below steps is playing a packet from one Mellanox card directly into another. Before playing the packet, we write to a region in GPU memory with a specific pattern and in the packet we send we have a different pattern. As a proof of concept we expect that the packet's data overwrites this memory buffer. See this post for the original description of the problem. A queue pair and its associated resources are established exactly as described in the (generic application flow)[https://docs.nvidia.com/networking/display/RDMAAwareProgrammingv17/Typical+Application] Lines 0-192 of the attached code Register a region of host memory and fill it with a known pattern 1.lines 192-195 \u200bRegister a region of GPU memory 1.Lines 197-223 Send a packet containing a known pattern from one Mellanox device to another 1.Lines 223-375 Copy the data from the GPU device's memory region into the host system memory which we expect to overwrite the host system memory's bit pattern with the one we just sent 1.Line 375-380 Confirm that the memory patterns match. The idea being that we just sent a new pattern from one Mellanox device to the other and then told it to overwrite the pattern that was already in system memory with what the GPU received. The logic being that we expect the pattern which was in system memory to be overwritten by what was just sent. 1.This happens in lines 391-396\u200b At no point does the CUDA toolkit issue any errors. Everything returns as a success however, the pattern in system memory is not overwritten. Need to determine why this simple POC does not work in order to move forward with customer. Lab Configuration Hardware Configuration Dell R750 with a Mellanox MLX6 as the transmitting device and a MLX5 as the receiving device. Worth noting is that at the time of writing there is no special MLX6 driver. All device names will appear is MLX5. RHEL Version NAME=\"Red Hat Enterprise Linux\" VERSION=\"8.5 (Ootpa)\" ID=\"rhel\" ID_LIKE=\"fedora\" VERSION_ID=\"8.5\" PLATFORM_ID=\"platform:el8\" PRETTY_NAME=\"Red Hat Enterprise Linux 8.5 (Ootpa)\" ANSI_COLOR=\"0;31\" CPE_NAME=\"cpe:/o:redhat:enterprise_linux:8::baseos\" HOME_URL=\"https://www.redhat.com/\" DOCUMENTATION_URL=\"https://access.redhat.com/documentation/red_hat_enterprise_linux/8/\" BUG_REPORT_URL=\"https://bugzilla.redhat.com/\" REDHAT_BUGZILLA_PRODUCT=\"Red Hat Enterprise Linux 8\" REDHAT_BUGZILLA_PRODUCT_VERSION=8.5 REDHAT_SUPPORT_PRODUCT=\"Red Hat Enterprise Linux\" REDHAT_SUPPORT_PRODUCT_VERSION=\"8.5\" Red Hat Enterprise Linux release 8.5 (Ootpa) Red Hat Enterprise Linux release 8.5 (Ootpa) GCC Version [root@gputest ~]# gcc --version gcc (GCC) 8.5.0 20210514 (Red Hat 8.5.0-4) Copyright (C) 2018 Free Software Foundation, Inc. This is free software; see the source for copying conditions. There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. Nvidia SMI +-----------------------------------------------------------------------------+ | NVIDIA-SMI 495.29.05 Driver Version: 495.29.05 CUDA Version: 11.5 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 NVIDIA A100-PCI... Off | 00000000:CA:00.0 Off | 0 | | N/A 26C P0 35W / 250W | 541MiB / 40536MiB | 0% Default | | | | Disabled | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | 0 N/A N/A 4969 G /usr/libexec/Xorg 26MiB | | 0 N/A N/A 5345 G /usr/bin/gnome-shell 98MiB | | 0 N/A N/A 606994 C /tmp/test/rdma-loopback 413MiB | +-----------------------------------------------------------------------------+ CUDA Version [root@gputest ~]# nvcc --version nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 2005-2021 NVIDIA Corporation Built on Thu_Nov_18_09:45:30_PST_2021 Cuda compilation tools, release 11.5, V11.5.119 Build cuda_11.5.r11.5/compiler.30672275_0 ` ### MLX Config See [MLX5_0](images/mlx5_0.log) and [MLX5_2](images/mlx5_2.log) ## Installation ### MLNX_OFED 1. Download MLNX_OFED drivers from https://www.mellanox.com/products/infiniband-drivers/linux/mlnx_ofed 1.MLNX_OFED is version dependent. I suggest you use `subscription-manager release --set=8.4` to ensure your version of RHEL stays at the version for which MLNX_OFED was compiled 2. Upload the ISO to the target box 3. Run: ```bash dnf group install \"Development Tools\" -y dnf install -y tk tcsh tcl gcc-gfortran kernel-modules-extra gcc-g++ gdb rsync ninja-build make zip mount MLNX* /mnt cd /mnt ./mlnxofedinstall Setting Up the Code Environment CUDA Development Packages Make sure you have an Nvidia GPU that shows on the device with lspci | grep -i nvidia Make sure you have the kernel dev headers for your kernel. with rpm -qa | grep devel | grep kernel && uname -r Run the following (See Nvidia's instructions for details): dnf install -y https://dl.fedoraproject.org/pub/epel/epel-release-latest-8.noarch.rpm subscription-manager repos --enable=rhel-8-for-x86_64-appstream-rpms subscription-manager repos --enable=rhel-8-for-x86_64-baseos-rpms subscription-manager repos --enable=codeready-builder-for-rhel-8-x86_64-rpms sudo rpm -i cuda-repo-rhel8-11-5-local-11.5.1_495.29.05-1.x86_64.rpm sudo dnf config-manager --add-repo https://developer.download.nvidia.com/compute/cuda/repos/rhel8/x86_64/cuda-rhel8.repo sudo dnf clean expire-cache sudo dnf module install -y nvidia-driver:latest-dkms sudo dnf install -y cuda export PATH=/usr/local/cuda-11.5/bin${PATH:+:${PATH}} echo 'export PATH=/usr/local/cuda-11.5/bin${PATH:+:${PATH}}' >> /root/.bashrc modprobe nvidia-peermem # YOU MUST RUN THIS MANUALLY # Below is just for debugging. You don't have to install them. # Make sure, even though it is RHEL 8, you use the word yum here. yum debuginfo-install libgcc-8.5.0-4.el8_5.x86_64 libibverbs-55mlnx37-1.55103.x86_64 libnl3-3.5.0-1.el8.x86_64 libstdc++-8.5.0-4.el8_5.x86_64 nvidia-driver-cuda-libs-495.29.05-1.el8.x86_64 WARNING Whenever you want to run this code you must manually load the nvidia-peermem module. See Nvidia peermem . Load with modprobe nvidia-peermem Prepare the Code First we need to make some manual adjustments to some parameters in the code. For this you need the MAC addresses My first challenge was that the system was entirely remote so I had to figure out how to determine exactly which interfaces belonged to which card. To find the MAC addresses remotely you can use lspci -v | grep -i ethernet and compare that with the output of ethtool -i <interface_name> . This will allow you to corelate the model name of the NIC to the interface/MAC using the PCIe bus number. Ex: [root@gputest ~]# ip a s 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2: eno8303: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP group default qlen 1000 link/ether b0:7b:25:f8:44:d2 brd ff:ff:ff:ff:ff:ff inet 172.28.1.40/24 brd 172.28.1.255 scope global dynamic noprefixroute eno8303 valid_lft 71017sec preferred_lft 71017sec inet6 fe80::b27b:25ff:fef8:44d2/64 scope link noprefixroute valid_lft forever preferred_lft forever 3: eno8403: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc mq state DOWN group default qlen 1000 link/ether b0:7b:25:f8:44:d3 brd ff:ff:ff:ff:ff:ff 4: eno12399: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN group default qlen 1000 link/ether b4:96:91:cd:e8:ac brd ff:ff:ff:ff:ff:ff 5: eno12409: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN group default qlen 1000 link/ether b4:96:91:cd:e8:ad brd ff:ff:ff:ff:ff:ff 6: ens6f0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc mq state DOWN group default qlen 1000 link/ether b8:ce:f6:cc:9e:dc brd ff:ff:ff:ff:ff:ff 7: ens6f1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP group default qlen 1000 link/ether b8:ce:f6:cc:9e:dd brd ff:ff:ff:ff:ff:ff 8: ens5f0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP group default qlen 1000 link/ether 0c:42:a1:73:8d:e6 brd ff:ff:ff:ff:ff:ff 9: ens5f1: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc mq state DOWN group default qlen 1000 link/ether 0c:42:a1:73:8d:e7 brd ff:ff:ff:ff:ff:ff 10: virbr0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default qlen 1000 link/ether 52:54:00:b7:e9:a7 brd ff:ff:ff:ff:ff:ff inet 192.168.122.1/24 brd 192.168.122.255 scope global virbr0 valid_lft forever preferred_lft forever 11: virbr0-nic: <BROADCAST,MULTICAST> mtu 1500 qdisc fq_codel master virbr0 state DOWN group default qlen 1000 link/ether 52:54:00:b7:e9:a7 brd ff:ff:ff:ff:ff:ff [root@gputest test_code]# lspci -v | grep -i ethernet 04:00.0 Ethernet controller: Broadcom Inc. and subsidiaries NetXtreme BCM5720 Gigabit Ethernet PCIe 04:00.1 Ethernet controller: Broadcom Inc. and subsidiaries NetXtreme BCM5720 Gigabit Ethernet PCIe 31:00.0 Ethernet controller: Intel Corporation Ethernet Controller E810-XXV for SFP (rev 02) Subsystem: Intel Corporation Ethernet 25G 2P E810-XXV OCP 31:00.1 Ethernet controller: Intel Corporation Ethernet Controller E810-XXV for SFP (rev 02) Subsystem: Intel Corporation Ethernet 25G 2P E810-XXV OCP 98:00.0 Ethernet controller: Mellanox Technologies MT2892 Family [ConnectX-6 Dx] 98:00.1 Ethernet controller: Mellanox Technologies MT2892 Family [ConnectX-6 Dx] b1:00.0 Ethernet controller: Mellanox Technologies MT28800 Family [ConnectX-5 Ex] b1:00.1 Ethernet controller: Mellanox Technologies MT28800 Family [ConnectX-5 Ex] [root@gputest test_code]# ethtool -i ens6f1 driver: mlx5_core version: 5.5-1.0.3 firmware-version: 22.31.1014 (DEL0000000027) expansion-rom-version: bus-info: 0000:98:00.1 supports-statistics: yes supports-test: yes supports-eeprom-access: no supports-register-dump: no supports-priv-flags: yes [root@gputest ~]# ethtool -i ens5f0 driver: mlx5_core version: 5.5-1.0.3 firmware-version: 16.27.6106 (DEL0000000004) expansion-rom-version: bus-info: 0000:b1:00.0 supports-statistics: yes supports-test: yes supports-eeprom-access: no supports-register-dump: no supports-priv-flags: yes So here we can see from the bus numbers that in my case MLX6 device is ens6f0/ens6f1 and the MLX5 is ens5f0/ensf1. My transmit interface will be b8:ce:f6:cc:9e:dd/ens6f1 and my receive is 0c:42:a1:73:8d:e6/ens5f0. Next you have to figure out the Mellanox device numbers. You can do this using mlxconfig. To get this listing run mlxconfig -d mlx5_0 q . In the header of each block you should see something like this: Device #1: ---------- Device type: ConnectX6DX Name: 0F6FXM_08P2T2_Ax Description: Mellanox ConnectX-6 Dx Dual Port 100 GbE QSFP56 Network Adapter Device: mlx5_0 From my experimentation they are in order. So mlx5_0 is the first interface on the MLX6 device which lines up with ens6f0. This means mlx5_1 is the second interface and then mlx5_3 would be the first interface of the MLX5 device which we can confirm with mlxconfig -d mlx5_3 q Device #1: ---------- Device type: ConnectX5 Name: 09FTMY_071C1T_Ax Description: Mellanox ConnectX-5 Ex Dual Port 100 GbE QSFP Network Adapter Device: mlx5_3 Compiling and Running the App g++ rdma-loopback.cc -o rdma-loopback -libverbs -I/usr/local/cuda/include -L/usr/local/cuda/lib64 -lcudart Debugging gdb --args rdma-loopback 0 set print pretty on is helpful To get the config of the Mellanox devices run mlxconfig -d mlx5_0 q > mlx5_0.log . Replace mlx5 with your device name.","title":"Nvidia GPUDirect"},{"location":"Nvidia%20GPUDirect/#nvidia-gpudirect","text":"Nvidia GPUDirect Background Research Helpful Links Source Code Description of the Problem Lab Configuration Hardware Configuration RHEL Version GCC Version Nvidia SMI CUDA Version Setting Up the Code Environment CUDA Development Packages Prepare the Code Compiling and Running the App Debugging","title":"Nvidia GPUDirect"},{"location":"Nvidia%20GPUDirect/#background-research","text":"See Background Research for background information I studied before doing this.","title":"Background Research"},{"location":"Nvidia%20GPUDirect/#helpful-links","text":"RDMA Aware Programming Typical Application Flow","title":"Helpful Links"},{"location":"Nvidia%20GPUDirect/#source-code","text":"See the test file","title":"Source Code"},{"location":"Nvidia%20GPUDirect/#description-of-the-problem","text":"What we are doing in the below steps is playing a packet from one Mellanox card directly into another. Before playing the packet, we write to a region in GPU memory with a specific pattern and in the packet we send we have a different pattern. As a proof of concept we expect that the packet's data overwrites this memory buffer. See this post for the original description of the problem. A queue pair and its associated resources are established exactly as described in the (generic application flow)[https://docs.nvidia.com/networking/display/RDMAAwareProgrammingv17/Typical+Application] Lines 0-192 of the attached code Register a region of host memory and fill it with a known pattern 1.lines 192-195 \u200bRegister a region of GPU memory 1.Lines 197-223 Send a packet containing a known pattern from one Mellanox device to another 1.Lines 223-375 Copy the data from the GPU device's memory region into the host system memory which we expect to overwrite the host system memory's bit pattern with the one we just sent 1.Line 375-380 Confirm that the memory patterns match. The idea being that we just sent a new pattern from one Mellanox device to the other and then told it to overwrite the pattern that was already in system memory with what the GPU received. The logic being that we expect the pattern which was in system memory to be overwritten by what was just sent. 1.This happens in lines 391-396\u200b At no point does the CUDA toolkit issue any errors. Everything returns as a success however, the pattern in system memory is not overwritten. Need to determine why this simple POC does not work in order to move forward with customer.","title":"Description of the Problem"},{"location":"Nvidia%20GPUDirect/#lab-configuration","text":"","title":"Lab Configuration"},{"location":"Nvidia%20GPUDirect/#hardware-configuration","text":"Dell R750 with a Mellanox MLX6 as the transmitting device and a MLX5 as the receiving device. Worth noting is that at the time of writing there is no special MLX6 driver. All device names will appear is MLX5.","title":"Hardware Configuration"},{"location":"Nvidia%20GPUDirect/#rhel-version","text":"NAME=\"Red Hat Enterprise Linux\" VERSION=\"8.5 (Ootpa)\" ID=\"rhel\" ID_LIKE=\"fedora\" VERSION_ID=\"8.5\" PLATFORM_ID=\"platform:el8\" PRETTY_NAME=\"Red Hat Enterprise Linux 8.5 (Ootpa)\" ANSI_COLOR=\"0;31\" CPE_NAME=\"cpe:/o:redhat:enterprise_linux:8::baseos\" HOME_URL=\"https://www.redhat.com/\" DOCUMENTATION_URL=\"https://access.redhat.com/documentation/red_hat_enterprise_linux/8/\" BUG_REPORT_URL=\"https://bugzilla.redhat.com/\" REDHAT_BUGZILLA_PRODUCT=\"Red Hat Enterprise Linux 8\" REDHAT_BUGZILLA_PRODUCT_VERSION=8.5 REDHAT_SUPPORT_PRODUCT=\"Red Hat Enterprise Linux\" REDHAT_SUPPORT_PRODUCT_VERSION=\"8.5\" Red Hat Enterprise Linux release 8.5 (Ootpa) Red Hat Enterprise Linux release 8.5 (Ootpa)","title":"RHEL Version"},{"location":"Nvidia%20GPUDirect/#gcc-version","text":"[root@gputest ~]# gcc --version gcc (GCC) 8.5.0 20210514 (Red Hat 8.5.0-4) Copyright (C) 2018 Free Software Foundation, Inc. This is free software; see the source for copying conditions. There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.","title":"GCC Version"},{"location":"Nvidia%20GPUDirect/#nvidia-smi","text":"+-----------------------------------------------------------------------------+ | NVIDIA-SMI 495.29.05 Driver Version: 495.29.05 CUDA Version: 11.5 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 NVIDIA A100-PCI... Off | 00000000:CA:00.0 Off | 0 | | N/A 26C P0 35W / 250W | 541MiB / 40536MiB | 0% Default | | | | Disabled | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | 0 N/A N/A 4969 G /usr/libexec/Xorg 26MiB | | 0 N/A N/A 5345 G /usr/bin/gnome-shell 98MiB | | 0 N/A N/A 606994 C /tmp/test/rdma-loopback 413MiB | +-----------------------------------------------------------------------------+","title":"Nvidia SMI"},{"location":"Nvidia%20GPUDirect/#cuda-version","text":"[root@gputest ~]# nvcc --version nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 2005-2021 NVIDIA Corporation Built on Thu_Nov_18_09:45:30_PST_2021 Cuda compilation tools, release 11.5, V11.5.119 Build cuda_11.5.r11.5/compiler.30672275_0 ` ### MLX Config See [MLX5_0](images/mlx5_0.log) and [MLX5_2](images/mlx5_2.log) ## Installation ### MLNX_OFED 1. Download MLNX_OFED drivers from https://www.mellanox.com/products/infiniband-drivers/linux/mlnx_ofed 1.MLNX_OFED is version dependent. I suggest you use `subscription-manager release --set=8.4` to ensure your version of RHEL stays at the version for which MLNX_OFED was compiled 2. Upload the ISO to the target box 3. Run: ```bash dnf group install \"Development Tools\" -y dnf install -y tk tcsh tcl gcc-gfortran kernel-modules-extra gcc-g++ gdb rsync ninja-build make zip mount MLNX* /mnt cd /mnt ./mlnxofedinstall","title":"CUDA Version"},{"location":"Nvidia%20GPUDirect/#setting-up-the-code-environment","text":"","title":"Setting Up the Code Environment"},{"location":"Nvidia%20GPUDirect/#cuda-development-packages","text":"Make sure you have an Nvidia GPU that shows on the device with lspci | grep -i nvidia Make sure you have the kernel dev headers for your kernel. with rpm -qa | grep devel | grep kernel && uname -r Run the following (See Nvidia's instructions for details): dnf install -y https://dl.fedoraproject.org/pub/epel/epel-release-latest-8.noarch.rpm subscription-manager repos --enable=rhel-8-for-x86_64-appstream-rpms subscription-manager repos --enable=rhel-8-for-x86_64-baseos-rpms subscription-manager repos --enable=codeready-builder-for-rhel-8-x86_64-rpms sudo rpm -i cuda-repo-rhel8-11-5-local-11.5.1_495.29.05-1.x86_64.rpm sudo dnf config-manager --add-repo https://developer.download.nvidia.com/compute/cuda/repos/rhel8/x86_64/cuda-rhel8.repo sudo dnf clean expire-cache sudo dnf module install -y nvidia-driver:latest-dkms sudo dnf install -y cuda export PATH=/usr/local/cuda-11.5/bin${PATH:+:${PATH}} echo 'export PATH=/usr/local/cuda-11.5/bin${PATH:+:${PATH}}' >> /root/.bashrc modprobe nvidia-peermem # YOU MUST RUN THIS MANUALLY # Below is just for debugging. You don't have to install them. # Make sure, even though it is RHEL 8, you use the word yum here. yum debuginfo-install libgcc-8.5.0-4.el8_5.x86_64 libibverbs-55mlnx37-1.55103.x86_64 libnl3-3.5.0-1.el8.x86_64 libstdc++-8.5.0-4.el8_5.x86_64 nvidia-driver-cuda-libs-495.29.05-1.el8.x86_64 WARNING Whenever you want to run this code you must manually load the nvidia-peermem module. See Nvidia peermem . Load with modprobe nvidia-peermem","title":"CUDA Development Packages"},{"location":"Nvidia%20GPUDirect/#prepare-the-code","text":"First we need to make some manual adjustments to some parameters in the code. For this you need the MAC addresses My first challenge was that the system was entirely remote so I had to figure out how to determine exactly which interfaces belonged to which card. To find the MAC addresses remotely you can use lspci -v | grep -i ethernet and compare that with the output of ethtool -i <interface_name> . This will allow you to corelate the model name of the NIC to the interface/MAC using the PCIe bus number. Ex: [root@gputest ~]# ip a s 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2: eno8303: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP group default qlen 1000 link/ether b0:7b:25:f8:44:d2 brd ff:ff:ff:ff:ff:ff inet 172.28.1.40/24 brd 172.28.1.255 scope global dynamic noprefixroute eno8303 valid_lft 71017sec preferred_lft 71017sec inet6 fe80::b27b:25ff:fef8:44d2/64 scope link noprefixroute valid_lft forever preferred_lft forever 3: eno8403: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc mq state DOWN group default qlen 1000 link/ether b0:7b:25:f8:44:d3 brd ff:ff:ff:ff:ff:ff 4: eno12399: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN group default qlen 1000 link/ether b4:96:91:cd:e8:ac brd ff:ff:ff:ff:ff:ff 5: eno12409: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN group default qlen 1000 link/ether b4:96:91:cd:e8:ad brd ff:ff:ff:ff:ff:ff 6: ens6f0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc mq state DOWN group default qlen 1000 link/ether b8:ce:f6:cc:9e:dc brd ff:ff:ff:ff:ff:ff 7: ens6f1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP group default qlen 1000 link/ether b8:ce:f6:cc:9e:dd brd ff:ff:ff:ff:ff:ff 8: ens5f0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP group default qlen 1000 link/ether 0c:42:a1:73:8d:e6 brd ff:ff:ff:ff:ff:ff 9: ens5f1: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc mq state DOWN group default qlen 1000 link/ether 0c:42:a1:73:8d:e7 brd ff:ff:ff:ff:ff:ff 10: virbr0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default qlen 1000 link/ether 52:54:00:b7:e9:a7 brd ff:ff:ff:ff:ff:ff inet 192.168.122.1/24 brd 192.168.122.255 scope global virbr0 valid_lft forever preferred_lft forever 11: virbr0-nic: <BROADCAST,MULTICAST> mtu 1500 qdisc fq_codel master virbr0 state DOWN group default qlen 1000 link/ether 52:54:00:b7:e9:a7 brd ff:ff:ff:ff:ff:ff [root@gputest test_code]# lspci -v | grep -i ethernet 04:00.0 Ethernet controller: Broadcom Inc. and subsidiaries NetXtreme BCM5720 Gigabit Ethernet PCIe 04:00.1 Ethernet controller: Broadcom Inc. and subsidiaries NetXtreme BCM5720 Gigabit Ethernet PCIe 31:00.0 Ethernet controller: Intel Corporation Ethernet Controller E810-XXV for SFP (rev 02) Subsystem: Intel Corporation Ethernet 25G 2P E810-XXV OCP 31:00.1 Ethernet controller: Intel Corporation Ethernet Controller E810-XXV for SFP (rev 02) Subsystem: Intel Corporation Ethernet 25G 2P E810-XXV OCP 98:00.0 Ethernet controller: Mellanox Technologies MT2892 Family [ConnectX-6 Dx] 98:00.1 Ethernet controller: Mellanox Technologies MT2892 Family [ConnectX-6 Dx] b1:00.0 Ethernet controller: Mellanox Technologies MT28800 Family [ConnectX-5 Ex] b1:00.1 Ethernet controller: Mellanox Technologies MT28800 Family [ConnectX-5 Ex] [root@gputest test_code]# ethtool -i ens6f1 driver: mlx5_core version: 5.5-1.0.3 firmware-version: 22.31.1014 (DEL0000000027) expansion-rom-version: bus-info: 0000:98:00.1 supports-statistics: yes supports-test: yes supports-eeprom-access: no supports-register-dump: no supports-priv-flags: yes [root@gputest ~]# ethtool -i ens5f0 driver: mlx5_core version: 5.5-1.0.3 firmware-version: 16.27.6106 (DEL0000000004) expansion-rom-version: bus-info: 0000:b1:00.0 supports-statistics: yes supports-test: yes supports-eeprom-access: no supports-register-dump: no supports-priv-flags: yes So here we can see from the bus numbers that in my case MLX6 device is ens6f0/ens6f1 and the MLX5 is ens5f0/ensf1. My transmit interface will be b8:ce:f6:cc:9e:dd/ens6f1 and my receive is 0c:42:a1:73:8d:e6/ens5f0. Next you have to figure out the Mellanox device numbers. You can do this using mlxconfig. To get this listing run mlxconfig -d mlx5_0 q . In the header of each block you should see something like this: Device #1: ---------- Device type: ConnectX6DX Name: 0F6FXM_08P2T2_Ax Description: Mellanox ConnectX-6 Dx Dual Port 100 GbE QSFP56 Network Adapter Device: mlx5_0 From my experimentation they are in order. So mlx5_0 is the first interface on the MLX6 device which lines up with ens6f0. This means mlx5_1 is the second interface and then mlx5_3 would be the first interface of the MLX5 device which we can confirm with mlxconfig -d mlx5_3 q Device #1: ---------- Device type: ConnectX5 Name: 09FTMY_071C1T_Ax Description: Mellanox ConnectX-5 Ex Dual Port 100 GbE QSFP Network Adapter Device: mlx5_3","title":"Prepare the Code"},{"location":"Nvidia%20GPUDirect/#compiling-and-running-the-app","text":"g++ rdma-loopback.cc -o rdma-loopback -libverbs -I/usr/local/cuda/include -L/usr/local/cuda/lib64 -lcudart","title":"Compiling and Running the App"},{"location":"Nvidia%20GPUDirect/#debugging","text":"gdb --args rdma-loopback 0 set print pretty on is helpful To get the config of the Mellanox devices run mlxconfig -d mlx5_0 q > mlx5_0.log . Replace mlx5 with your device name.","title":"Debugging"},{"location":"Nvidia%20GPUDirect/background_research/","text":"Helpful Links Nvidia GPUDirect Overview Nvidia System Management Interface GPUDirect RDMA Access Example Code Mellanox GPUDirect RDMA Example NVIDIA CUDA Basics GPUDirect Benchmark Tests RDMA Aware Programming User Manual (has a great RDMA architecture overview) NOTE This also includes how Infiniband works. Packet Capture with GPUDirect How Infiniband Works https://stackoverflow.com/questions/52125610/visual-studio-remote-linux-headers Using Nvidia SMI Run from Windows: c:\\Windows\\System32\\DriverStore\\FileRepository\\nvdm.inf_amd64_3349a8117b680632>nvidia-smi.exe OUTPUT c:\\Windows\\System32\\DriverStore\\FileRepository\\nvdm.inf_amd64_3349a8117b680632>nvidia-smi.exe Wed Nov 10 15:47:38 2021 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 471.35 Driver Version: 471.35 CUDA Version: 11.4 | |-------------------------------+----------------------+----------------------+ | GPU Name TCC/WDDM | Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 Quadro T1000 WDDM | 00000000:01:00.0 Off | N/A | | N/A 52C P8 2W / N/A | 287MiB / 4096MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | 0 N/A N/A 3360 C+G ...ser\\Application\\brave.exe N/A | +-----------------------------------------------------------------------------+ Processes is a list of processes having compute or graphics context on the device. Compute processes are reported on all the fully supported products. Reporting for Graphics processes is limited to the supported products starting with Kepler architecture. Each Entry is of format <GPU Index> <PID> <Type> <Process Name> <GPUMemory Usage>\" GPU Index - Represents NVML index of the device? PID Represents process ID corresponding to the active compute or graphics context? Type - Displayed as \"C\" for compute process, \"G\" for graphics process, and \"C+G\" for the procses having both compute and graphics contexts Process Name - Represents process name for the compute or graphics process GPU Memory Usage - Amount of memory used on the device by the context. Not available on Windows when running in WDDM mode because Windows KMD manages all memory not Nvidia driver Notes on CUDA Programming See CUDA C Programming Guide Overview At its core CUDA has three key abstractions: - a hierarchy of thread groups - shared memories - barrier synchronization This decomposition preserves language expressivity by allowing threads to cooperate when solving each sub-problem, and at the same time enables automatic scalability. Indeed, each block of threads can be scheduled on any of the available multiprocessors within a GPU, in any order, concurrently or sequentially, so that a compiled CUDA program can execute on any number of multiprocessors as illustrated by Figure 3, and only the runtime system needs to know the physical multiprocessor count. Note : A GPU is built around an array of Streaming Multiprocessors (SMs) (see Hardware Implementation for more details). A multithreaded program is partitioned into blocks of threads that execute independently from each other, so that a GPU with more multiprocessors will automatically execute the program in less time than a GPU with fewer multiprocessors. Kernels CUDA C++ extends C++ by allowing the programmer to define C++ functions, called kernels, that, when called, are executed N times in parallel by N different CUDA threads, as opposed to only once like regular C++ functions. The following sample code, using the built-in variable threadIdx, adds two vectors A and B of size N and stores the result into vector C. // Kernel definition __global__ void VecAdd(float* A, float* B, float* C) { int i = threadIdx.x; C[i] = A[i] + B[i]; } int main() { ... // Kernel invocation with N threads VecAdd<<<1, N>>>(A, B, C); ... } Global Keyword The global keyword indicates a function that runs on the device and is called from host code How nvcc separates code nvcc separates source code into host and device components. Device functions (e.g. mykernel()) are proccesed by the Nvidia compiler while host functions (e.g. main()) are processed by standard host compiler (gcc, cl, etc) Thread Hierarchy For convenience, threadIdx is a 3-component vector, so that threads can be identified using a one-dimensional, two-dimensional, or three-dimensional thread index, forming a one-dimensional, two-dimensional, or three-dimensional block of threads, called a thread block. This provides a natural way to invoke computation across the elements in a domain such as a vector, matrix, or volume. CUDA Runtime The runtime is introduced in CUDA Runtime. It provides C and C++ functions that execute on the host to allocate and deallocate device memory, transfer data between host memory and device memory, manage systems with multiple devices, etc. A complete description of the runtime can be found in the CUDA reference manual. The runtime is built on top of a lower-level C API, the CUDA driver API, which is also accessible by the application. The driver API provides an additional level of control by exposing lower-level concepts such as CUDA contexts - the analogue of host processes for the device - and CUDA modules - the analogue of dynamically loaded libraries for the device. Most applications do not use the driver API as they do not need this additional level of control and when using the runtime, context and module management are implicit, resulting in more concise code. As the runtime is interoperable with the driver API, most applications that need some driver API features can default to use the runtime API and only use the driver API where needed. The driver API is introduced in Driver API and fully described in the reference manual. The runtime is implemented in the cudart library, which is linked to the application, either statically via cudart.lib or libcudart.a, or dynamically via cudart.dll or libcudart.so. Applications that require cudart.dll and/or cudart.so for dynamic linking typically include them as part of the application installation package. It is only safe to pass the address of CUDA runtime symbols between components that link to the same instance of the CUDA runtime. Questions What is NVML Index? What is WDDM mode? What is Windows KMD? What is a Graphics Context? See https://stackoverflow.com/a/23087951/4427375 A GPU context is described here . It represents all the state (data, variables, conditions, etc.) that are collectively required and instantiated to perform certain tasks (e.g. CUDA compute, graphics, H.264 encode, etc). A CUDA context is instantiated to perform CUDA compute activities on the GPU, either implicitly by the CUDA runtime API, or explicitly by the CUDA device API. What is a PCIe Root Complex? https://www.quora.com/What-is-a-PCIe-root-complex?share=1 How does PCIe Enumeration Work? https://www.quora.com/What-is-PCIE-enumeration/answer/Satish-Kumar-525?ch=15&oid=31389493&share=44585235&target_type=answer NVMe over PCIe vs Other Protocols https://www.quora.com/Is-NVMe-faster-than-PCIe/answer/Mike-Jones-169?ch=15&oid=193548046&share=a587ff45&target_type=answer What is a PCIe Function? https://www.quora.com/What-is-a-PCIe-function/answer/Udit-Khanna-2?ch=15&oid=58319695&share=c7f066e5&target_type=answer PCIe-Bus and NUMA Node Correlation https://social.msdn.microsoft.com/Forums/en-US/fabb05b7-eb3f-4a7c-91c5-1ced90af3d0c/pciebus-and-numanode-correlation How does the root complex work? https://codywu2010.wordpress.com/2015/11/29/how-modern-multi-processor-multi-root-complex-system-assigns-pci-bus-number/ What is PCIe P2P? https://xilinx.github.io/XRT/master/html/p2p.html What is the difference between a thread and a block? https://stackoverflow.com/questions/16635587/whats-the-difference-between-a-thread-in-a-block-and-a-warp32-threads A block is used to run things in parallel and for each block there is one thread. Each parallel invocation of a kernel is a block and a set of blocks is a grid. You can also run multiple threads per block.","title":"Background research"},{"location":"Nvidia%20GPUDirect/background_research/#helpful-links","text":"Nvidia GPUDirect Overview Nvidia System Management Interface GPUDirect RDMA Access Example Code Mellanox GPUDirect RDMA Example NVIDIA CUDA Basics GPUDirect Benchmark Tests RDMA Aware Programming User Manual (has a great RDMA architecture overview) NOTE This also includes how Infiniband works. Packet Capture with GPUDirect How Infiniband Works https://stackoverflow.com/questions/52125610/visual-studio-remote-linux-headers","title":"Helpful Links"},{"location":"Nvidia%20GPUDirect/background_research/#using-nvidia-smi","text":"Run from Windows: c:\\Windows\\System32\\DriverStore\\FileRepository\\nvdm.inf_amd64_3349a8117b680632>nvidia-smi.exe OUTPUT c:\\Windows\\System32\\DriverStore\\FileRepository\\nvdm.inf_amd64_3349a8117b680632>nvidia-smi.exe Wed Nov 10 15:47:38 2021 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 471.35 Driver Version: 471.35 CUDA Version: 11.4 | |-------------------------------+----------------------+----------------------+ | GPU Name TCC/WDDM | Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 Quadro T1000 WDDM | 00000000:01:00.0 Off | N/A | | N/A 52C P8 2W / N/A | 287MiB / 4096MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | 0 N/A N/A 3360 C+G ...ser\\Application\\brave.exe N/A | +-----------------------------------------------------------------------------+ Processes is a list of processes having compute or graphics context on the device. Compute processes are reported on all the fully supported products. Reporting for Graphics processes is limited to the supported products starting with Kepler architecture. Each Entry is of format <GPU Index> <PID> <Type> <Process Name> <GPUMemory Usage>\" GPU Index - Represents NVML index of the device? PID Represents process ID corresponding to the active compute or graphics context? Type - Displayed as \"C\" for compute process, \"G\" for graphics process, and \"C+G\" for the procses having both compute and graphics contexts Process Name - Represents process name for the compute or graphics process GPU Memory Usage - Amount of memory used on the device by the context. Not available on Windows when running in WDDM mode because Windows KMD manages all memory not Nvidia driver","title":"Using Nvidia SMI"},{"location":"Nvidia%20GPUDirect/background_research/#notes-on-cuda-programming","text":"See CUDA C Programming Guide","title":"Notes on CUDA Programming"},{"location":"Nvidia%20GPUDirect/background_research/#overview","text":"At its core CUDA has three key abstractions: - a hierarchy of thread groups - shared memories - barrier synchronization This decomposition preserves language expressivity by allowing threads to cooperate when solving each sub-problem, and at the same time enables automatic scalability. Indeed, each block of threads can be scheduled on any of the available multiprocessors within a GPU, in any order, concurrently or sequentially, so that a compiled CUDA program can execute on any number of multiprocessors as illustrated by Figure 3, and only the runtime system needs to know the physical multiprocessor count. Note : A GPU is built around an array of Streaming Multiprocessors (SMs) (see Hardware Implementation for more details). A multithreaded program is partitioned into blocks of threads that execute independently from each other, so that a GPU with more multiprocessors will automatically execute the program in less time than a GPU with fewer multiprocessors.","title":"Overview"},{"location":"Nvidia%20GPUDirect/background_research/#kernels","text":"CUDA C++ extends C++ by allowing the programmer to define C++ functions, called kernels, that, when called, are executed N times in parallel by N different CUDA threads, as opposed to only once like regular C++ functions. The following sample code, using the built-in variable threadIdx, adds two vectors A and B of size N and stores the result into vector C. // Kernel definition __global__ void VecAdd(float* A, float* B, float* C) { int i = threadIdx.x; C[i] = A[i] + B[i]; } int main() { ... // Kernel invocation with N threads VecAdd<<<1, N>>>(A, B, C); ... }","title":"Kernels"},{"location":"Nvidia%20GPUDirect/background_research/#global-keyword","text":"The global keyword indicates a function that runs on the device and is called from host code","title":"Global Keyword"},{"location":"Nvidia%20GPUDirect/background_research/#how-nvcc-separates-code","text":"nvcc separates source code into host and device components. Device functions (e.g. mykernel()) are proccesed by the Nvidia compiler while host functions (e.g. main()) are processed by standard host compiler (gcc, cl, etc)","title":"How nvcc separates code"},{"location":"Nvidia%20GPUDirect/background_research/#thread-hierarchy","text":"For convenience, threadIdx is a 3-component vector, so that threads can be identified using a one-dimensional, two-dimensional, or three-dimensional thread index, forming a one-dimensional, two-dimensional, or three-dimensional block of threads, called a thread block. This provides a natural way to invoke computation across the elements in a domain such as a vector, matrix, or volume.","title":"Thread Hierarchy"},{"location":"Nvidia%20GPUDirect/background_research/#cuda-runtime","text":"The runtime is introduced in CUDA Runtime. It provides C and C++ functions that execute on the host to allocate and deallocate device memory, transfer data between host memory and device memory, manage systems with multiple devices, etc. A complete description of the runtime can be found in the CUDA reference manual. The runtime is built on top of a lower-level C API, the CUDA driver API, which is also accessible by the application. The driver API provides an additional level of control by exposing lower-level concepts such as CUDA contexts - the analogue of host processes for the device - and CUDA modules - the analogue of dynamically loaded libraries for the device. Most applications do not use the driver API as they do not need this additional level of control and when using the runtime, context and module management are implicit, resulting in more concise code. As the runtime is interoperable with the driver API, most applications that need some driver API features can default to use the runtime API and only use the driver API where needed. The driver API is introduced in Driver API and fully described in the reference manual. The runtime is implemented in the cudart library, which is linked to the application, either statically via cudart.lib or libcudart.a, or dynamically via cudart.dll or libcudart.so. Applications that require cudart.dll and/or cudart.so for dynamic linking typically include them as part of the application installation package. It is only safe to pass the address of CUDA runtime symbols between components that link to the same instance of the CUDA runtime.","title":"CUDA Runtime"},{"location":"Nvidia%20GPUDirect/background_research/#questions","text":"What is NVML Index? What is WDDM mode? What is Windows KMD?","title":"Questions"},{"location":"Nvidia%20GPUDirect/background_research/#what-is-a-graphics-context","text":"See https://stackoverflow.com/a/23087951/4427375 A GPU context is described here . It represents all the state (data, variables, conditions, etc.) that are collectively required and instantiated to perform certain tasks (e.g. CUDA compute, graphics, H.264 encode, etc). A CUDA context is instantiated to perform CUDA compute activities on the GPU, either implicitly by the CUDA runtime API, or explicitly by the CUDA device API.","title":"What is a Graphics Context?"},{"location":"Nvidia%20GPUDirect/background_research/#what-is-a-pcie-root-complex","text":"https://www.quora.com/What-is-a-PCIe-root-complex?share=1","title":"What is a PCIe Root Complex?"},{"location":"Nvidia%20GPUDirect/background_research/#how-does-pcie-enumeration-work","text":"https://www.quora.com/What-is-PCIE-enumeration/answer/Satish-Kumar-525?ch=15&oid=31389493&share=44585235&target_type=answer","title":"How does PCIe Enumeration Work?"},{"location":"Nvidia%20GPUDirect/background_research/#nvme-over-pcie-vs-other-protocols","text":"https://www.quora.com/Is-NVMe-faster-than-PCIe/answer/Mike-Jones-169?ch=15&oid=193548046&share=a587ff45&target_type=answer","title":"NVMe over PCIe vs Other Protocols"},{"location":"Nvidia%20GPUDirect/background_research/#what-is-a-pcie-function","text":"https://www.quora.com/What-is-a-PCIe-function/answer/Udit-Khanna-2?ch=15&oid=58319695&share=c7f066e5&target_type=answer","title":"What is a PCIe Function?"},{"location":"Nvidia%20GPUDirect/background_research/#pcie-bus-and-numa-node-correlation","text":"https://social.msdn.microsoft.com/Forums/en-US/fabb05b7-eb3f-4a7c-91c5-1ced90af3d0c/pciebus-and-numanode-correlation","title":"PCIe-Bus and NUMA Node Correlation"},{"location":"Nvidia%20GPUDirect/background_research/#how-does-the-root-complex-work","text":"https://codywu2010.wordpress.com/2015/11/29/how-modern-multi-processor-multi-root-complex-system-assigns-pci-bus-number/","title":"How does the root complex work?"},{"location":"Nvidia%20GPUDirect/background_research/#what-is-pcie-p2p","text":"https://xilinx.github.io/XRT/master/html/p2p.html","title":"What is PCIe P2P?"},{"location":"Nvidia%20GPUDirect/background_research/#what-is-the-difference-between-a-thread-and-a-block","text":"https://stackoverflow.com/questions/16635587/whats-the-difference-between-a-thread-in-a-block-and-a-warp32-threads A block is used to run things in parallel and for each block there is one thread. Each parallel invocation of a kernel is a block and a set of blocks is a grid. You can also run multiple threads per block.","title":"What is the difference between a thread and a block?"},{"location":"Nvidia%20GRID%20Notes/","text":"Nvidia GRID Notes Nvidia GRID Notes Useful Resources GPUs which Support vGPU Creation on VMWare vGPU Architecture Diagram How does SR-IOV work? Multi-Instance GPU Nvidia Professional Technologies How Does GRID Manager Integrate with the Hypervisor Where do profiles fit in the stack? What is a Profile Selecting a GPU Two General Types of Users Which Type of GPU? Dell GPUs and Upgrade Path Nvidia GPUs GPUs for Power Users Desktop Hosting Models vGPU License Decision Tree Useful Resources Nvidia vGPU Validated Solutions Nvidia Product Support Matrix for Operating Systems Add an NVIDIA GRID vGPU to a Virtual Machine Introduction to Nvidia Virtual GPU - Part 1 - Intro, Which GPU & License? NVIDIA Virtual GPU Software Packaging, Pricing, and Licensing Guide GPUs which Support vGPU Creation on VMWare Here are all the GPUs which support creating vGPU instances on VMWare: https://docs.nvidia.com/grid/13.0/grid-vgpu-release-notes-vmware-vsphere/index.html#hardware-configuration vGPU Architecture Diagram vGPU is exposed via SR-IOV . SR-IOV is an extension of the PCIe specification. It allows you to take one physical entity like a GPU or NIC and split it up into multiple virtual devices. How does SR-IOV work? In the case of SR-IOV, things are split into a Physical Function (PF) and a PCIe Virtual Function (VF) which represent the physical device and a virtual instance of that device which exposes some subset of the physical resources respectively. Usually a PCIe device has a single Requester ID (RID) which allows it to communicate over PCIe. This functions more or less like an IP address. However, with SR-IOV each physical function and virtual function gets its own RID. This allows the I/O Memory Management Unit (IOMMU) to differentiate between the different VFs. Note: The IOMMU connects any device with DMA capability (ex: NIC/GPU) to main memory directly instead of routing it through the CPU. This system allows the hypervisor to deliver IO from the VF directly to a VM without going through any software switching in the hypervisor. Multi-Instance GPU Nvidia calls this segmentation of the GPU Multi-Instance GPU (MIG). MIG enables a physical GPU to be securely partitioned into multiple separate GPU instances, providing multiple users with separate GPU resources to accelerate their applications. Nvidia Professional Technologies This is from Introduction to NVIDIA Virtual GPU - Part 1 - Intro, Which GPU & License? While a bit out of date it does a good job of showing where what technologies fit and what licenses are relevant. How Does GRID Manager Integrate with the Hypervisor Each vGPU in a VM gets a time slice on the actual GPU along with its own dedicated memory (the frame buffer). You\u2019ll have the aforementioned GPU manager that\u2019s part of the hypervisor and then each VM will run a standard Nvidia driver. Nvidia says that doing full PCIe passthrough vs giving the same resources via a vGPU is a negligible performance difference - a couple of percentage points max. Nvidia maintains a compatibility matrix on their website with what platforms and card combinations are available Where do profiles fit in the stack? This images is helpful because it shows where GPU profiles fit in the stack. What is a Profile Profiles are the means by which we define how much resources and the types of capabilities a vGPU has. Selecting a GPU Two General Types of Users Which Type of GPU? Dell GPUs and Upgrade Path Nvidia GPUs NOTE : These are from the aforementioned YouTube lecture which is a bit out of date but I thought it was helpful to see it broken out. GPUs for Power Users Desktop Hosting Models There are two general ways to present virtual desktops XenApp you have one instance of Windows Server and multiple people using it. You generally need a large frame buffer for this. Everyone else uses this model where you have multiple desktops running simultaneously. Licensing: There\u2019s an Nvidia license server that runs and as you create virtual desktops it will consume licenses. Which license depends on what you\u2019re doing and the type of physical GPU. vGPU License Decision Tree","title":"Nvidia GRID Notes"},{"location":"Nvidia%20GRID%20Notes/#nvidia-grid-notes","text":"Nvidia GRID Notes Useful Resources GPUs which Support vGPU Creation on VMWare vGPU Architecture Diagram How does SR-IOV work? Multi-Instance GPU Nvidia Professional Technologies How Does GRID Manager Integrate with the Hypervisor Where do profiles fit in the stack? What is a Profile Selecting a GPU Two General Types of Users Which Type of GPU? Dell GPUs and Upgrade Path Nvidia GPUs GPUs for Power Users Desktop Hosting Models vGPU License Decision Tree","title":"Nvidia GRID Notes"},{"location":"Nvidia%20GRID%20Notes/#useful-resources","text":"Nvidia vGPU Validated Solutions Nvidia Product Support Matrix for Operating Systems Add an NVIDIA GRID vGPU to a Virtual Machine Introduction to Nvidia Virtual GPU - Part 1 - Intro, Which GPU & License? NVIDIA Virtual GPU Software Packaging, Pricing, and Licensing Guide","title":"Useful Resources"},{"location":"Nvidia%20GRID%20Notes/#gpus-which-support-vgpu-creation-on-vmware","text":"Here are all the GPUs which support creating vGPU instances on VMWare: https://docs.nvidia.com/grid/13.0/grid-vgpu-release-notes-vmware-vsphere/index.html#hardware-configuration","title":"GPUs which Support vGPU Creation on VMWare"},{"location":"Nvidia%20GRID%20Notes/#vgpu-architecture-diagram","text":"vGPU is exposed via SR-IOV . SR-IOV is an extension of the PCIe specification. It allows you to take one physical entity like a GPU or NIC and split it up into multiple virtual devices.","title":"vGPU Architecture Diagram"},{"location":"Nvidia%20GRID%20Notes/#how-does-sr-iov-work","text":"In the case of SR-IOV, things are split into a Physical Function (PF) and a PCIe Virtual Function (VF) which represent the physical device and a virtual instance of that device which exposes some subset of the physical resources respectively. Usually a PCIe device has a single Requester ID (RID) which allows it to communicate over PCIe. This functions more or less like an IP address. However, with SR-IOV each physical function and virtual function gets its own RID. This allows the I/O Memory Management Unit (IOMMU) to differentiate between the different VFs. Note: The IOMMU connects any device with DMA capability (ex: NIC/GPU) to main memory directly instead of routing it through the CPU. This system allows the hypervisor to deliver IO from the VF directly to a VM without going through any software switching in the hypervisor.","title":"How does SR-IOV work?"},{"location":"Nvidia%20GRID%20Notes/#multi-instance-gpu","text":"Nvidia calls this segmentation of the GPU Multi-Instance GPU (MIG). MIG enables a physical GPU to be securely partitioned into multiple separate GPU instances, providing multiple users with separate GPU resources to accelerate their applications.","title":"Multi-Instance GPU"},{"location":"Nvidia%20GRID%20Notes/#nvidia-professional-technologies","text":"This is from Introduction to NVIDIA Virtual GPU - Part 1 - Intro, Which GPU & License? While a bit out of date it does a good job of showing where what technologies fit and what licenses are relevant.","title":"Nvidia Professional Technologies"},{"location":"Nvidia%20GRID%20Notes/#how-does-grid-manager-integrate-with-the-hypervisor","text":"Each vGPU in a VM gets a time slice on the actual GPU along with its own dedicated memory (the frame buffer). You\u2019ll have the aforementioned GPU manager that\u2019s part of the hypervisor and then each VM will run a standard Nvidia driver. Nvidia says that doing full PCIe passthrough vs giving the same resources via a vGPU is a negligible performance difference - a couple of percentage points max. Nvidia maintains a compatibility matrix on their website with what platforms and card combinations are available","title":"How Does GRID Manager Integrate with the Hypervisor"},{"location":"Nvidia%20GRID%20Notes/#where-do-profiles-fit-in-the-stack","text":"This images is helpful because it shows where GPU profiles fit in the stack.","title":"Where do profiles fit in the stack?"},{"location":"Nvidia%20GRID%20Notes/#what-is-a-profile","text":"Profiles are the means by which we define how much resources and the types of capabilities a vGPU has.","title":"What is a Profile"},{"location":"Nvidia%20GRID%20Notes/#selecting-a-gpu","text":"","title":"Selecting a GPU"},{"location":"Nvidia%20GRID%20Notes/#two-general-types-of-users","text":"","title":"Two General Types of Users"},{"location":"Nvidia%20GRID%20Notes/#which-type-of-gpu","text":"","title":"Which Type of GPU?"},{"location":"Nvidia%20GRID%20Notes/#dell-gpus-and-upgrade-path","text":"","title":"Dell GPUs and Upgrade Path"},{"location":"Nvidia%20GRID%20Notes/#nvidia-gpus","text":"NOTE : These are from the aforementioned YouTube lecture which is a bit out of date but I thought it was helpful to see it broken out.","title":"Nvidia GPUs"},{"location":"Nvidia%20GRID%20Notes/#gpus-for-power-users","text":"","title":"GPUs for Power Users"},{"location":"Nvidia%20GRID%20Notes/#desktop-hosting-models","text":"There are two general ways to present virtual desktops XenApp you have one instance of Windows Server and multiple people using it. You generally need a large frame buffer for this. Everyone else uses this model where you have multiple desktops running simultaneously. Licensing: There\u2019s an Nvidia license server that runs and as you create virtual desktops it will consume licenses. Which license depends on what you\u2019re doing and the type of physical GPU.","title":"Desktop Hosting Models"},{"location":"Nvidia%20GRID%20Notes/#vgpu-license-decision-tree","text":"","title":"vGPU License Decision Tree"},{"location":"OME%20Bug/","text":"OME Bug OME Version 3.7.0 (Build 82) Description OME Does not correctly handle the @odata.nextLink attribute for /api/DeviceService/Devices when passed specific IDs. For example: https://10.55.160.130/api/DeviceService/Devices?Id=13878,16669,16697,16698,16700,16701,16705,16707,16711,16712,16803,16846,16852,16975,16976,16982,16983,17002,17006,17007,17008,17020,17021,17255,17288,17289,17750,17752,17753,17755,17780,17781,17821,17822,17824,17825,17826,17829,17834,17835,17836,17838,17847,17848,17850,17851,17852,17856,17857,17864,17865,17868,17869,17870,17892,17893,17895,17896,17897,17902,17903,17904,17924,17934,17956,17970,17971,17992,18016,18017,18021,18023,18027,18075,18167,18417,18418,18419,18420,18421,18422,18423,18442,18443,18444,18445,18446,18447,18448,18449,18450,18451,18452,18453,18454,18455,18456,18457,18458,18459,18460,18461,18462,18463,18464,18465,18466,18467,18468,18469,18470,18471,18472,18473,18474,18475,18476,18477,18478,18479,18480,18481,18482,18483,18484,18485,18486,18487,18488,18489,18490,18491,18492,18493,18494,18495,18496,18497,18498,18499,18500,18501,18502,18503,18504,18505,18506,18507,18509,18510,18511,18512,18513,18514 The first page of 50 results works as expected. However, after retrieval of the first set of 50 machines the nextLink url is trunkated to '/api/DeviceService/Devices?$skip=50&$top=50' instead of properly including the IDs listed in the original URL. This leads to automated code looping over the totality of the OME inventory instead of some list of specified IDs.","title":"OME Bug"},{"location":"OME%20Bug/#ome-bug","text":"","title":"OME Bug"},{"location":"OME%20Bug/#ome-version","text":"3.7.0 (Build 82)","title":"OME Version"},{"location":"OME%20Bug/#description","text":"OME Does not correctly handle the @odata.nextLink attribute for /api/DeviceService/Devices when passed specific IDs. For example: https://10.55.160.130/api/DeviceService/Devices?Id=13878,16669,16697,16698,16700,16701,16705,16707,16711,16712,16803,16846,16852,16975,16976,16982,16983,17002,17006,17007,17008,17020,17021,17255,17288,17289,17750,17752,17753,17755,17780,17781,17821,17822,17824,17825,17826,17829,17834,17835,17836,17838,17847,17848,17850,17851,17852,17856,17857,17864,17865,17868,17869,17870,17892,17893,17895,17896,17897,17902,17903,17904,17924,17934,17956,17970,17971,17992,18016,18017,18021,18023,18027,18075,18167,18417,18418,18419,18420,18421,18422,18423,18442,18443,18444,18445,18446,18447,18448,18449,18450,18451,18452,18453,18454,18455,18456,18457,18458,18459,18460,18461,18462,18463,18464,18465,18466,18467,18468,18469,18470,18471,18472,18473,18474,18475,18476,18477,18478,18479,18480,18481,18482,18483,18484,18485,18486,18487,18488,18489,18490,18491,18492,18493,18494,18495,18496,18497,18498,18499,18500,18501,18502,18503,18504,18505,18506,18507,18509,18510,18511,18512,18513,18514 The first page of 50 results works as expected. However, after retrieval of the first set of 50 machines the nextLink url is trunkated to '/api/DeviceService/Devices?$skip=50&$top=50' instead of properly including the IDs listed in the original URL. This leads to automated code looping over the totality of the OME inventory instead of some list of specified IDs.","title":"Description"},{"location":"OME%20Integration%20for%20VMWare/","text":"OME Integration for VMWare Installation Download from https://www.dell.com/support/kbdoc/en-us/000176981/openmanage-integration-for-vmware-vcenter#Downloads Open the ZIP file and run the installer. This is a self unpacking executable","title":"OME Integration for VMWare"},{"location":"OME%20Integration%20for%20VMWare/#ome-integration-for-vmware","text":"","title":"OME Integration for VMWare"},{"location":"OME%20Integration%20for%20VMWare/#installation","text":"Download from https://www.dell.com/support/kbdoc/en-us/000176981/openmanage-integration-for-vmware-vcenter#Downloads Open the ZIP file and run the installer. This is a self unpacking executable","title":"Installation"},{"location":"OS10%20Password%20Recovery%20Bug/","text":"OS10 Password Recovery Bug YouTube Reproduction https://youtu.be/b5MJiLTl9KE Update It looks like the instructions are set up to take care of this, but they need to be updated - step 8 tells you to run a sed command that looks like it is targeted at reloving the problem, but it only tells you to run it on 10.5.1.0. When I went through this procedure I just ignored it because the customer was on 10.5.1.3 Description Follow Password recovery instructions to get into the switch at boot. During step 9: /opt/dell/os10/bin/recover_linuxadmin_password.sh produces mount: special device /dev/mapper/OS10-CONFIG does not exist . This is due to a previous configuration of OS10 when tho configuration was mounted on its own logical volume. It has since been moved to SYSROOT. Removing the below lines resolves the issue:","title":"OS10 Password Recovery Bug"},{"location":"OS10%20Password%20Recovery%20Bug/#os10-password-recovery-bug","text":"","title":"OS10 Password Recovery Bug"},{"location":"OS10%20Password%20Recovery%20Bug/#youtube-reproduction","text":"https://youtu.be/b5MJiLTl9KE","title":"YouTube Reproduction"},{"location":"OS10%20Password%20Recovery%20Bug/#update","text":"It looks like the instructions are set up to take care of this, but they need to be updated - step 8 tells you to run a sed command that looks like it is targeted at reloving the problem, but it only tells you to run it on 10.5.1.0. When I went through this procedure I just ignored it because the customer was on 10.5.1.3","title":"Update"},{"location":"OS10%20Password%20Recovery%20Bug/#description","text":"Follow Password recovery instructions to get into the switch at boot. During step 9: /opt/dell/os10/bin/recover_linuxadmin_password.sh produces mount: special device /dev/mapper/OS10-CONFIG does not exist . This is due to a previous configuration of OS10 when tho configuration was mounted on its own logical volume. It has since been moved to SYSROOT. Removing the below lines resolves the issue:","title":"Description"},{"location":"Offline%20Updates%20with%20OpenManage%20Enterprise/","text":"Offline Updates with OpenManage Enterprise Versions My Operating System [root@dellrepo html]# cat /etc/*-release CentOS Linux release 8.3.2011 NAME=\"CentOS Linux\" VERSION=\"8\" ID=\"centos\" ID_LIKE=\"rhel fedora\" VERSION_ID=\"8\" PLATFORM_ID=\"platform:el8\" PRETTY_NAME=\"CentOS Linux 8\" ANSI_COLOR=\"0;31\" CPE_NAME=\"cpe:/o:centos:centos:8\" HOME_URL=\"https://centos.org/\" BUG_REPORT_URL=\"https://bugs.centos.org/\" CENTOS_MANTISBT_PROJECT=\"CentOS-8\" CENTOS_MANTISBT_PROJECT_VERSION=\"8\" CentOS Linux release 8.3.2011 CentOS Linux release 8.3.2011 OME Version Instructions Download Dell Repository Manager Run it with /opt/dell/dellemcrepositorymanager/drm.sh NOTE: Running it with root does not work! You will get an error: GUI interface is not supported by this operating system. Click add repository Select the systems for which you want to download updates under select systems Make sure you select Windows-64 as one of the types which will be available chmod +x <binary name> then run with ./<binary_name> . I chose to distribute the repository using HTTP with Apache dnf install httpd sudo systemctl start --now httpd Next you have to synchronize the repository by clicking download. I downloaded my files to /opt/dell/catalogs/fc640 You then have to go open the Dell EMC Repository Manager -> Export -> Export. You will need to select which repositories you want to export and then you will want to select share and a save location. WARNING You have to download the Windows 64 bit versions of the updates for it to work! Even if you are using Linux the idrac only accepts the Windows EXE files. The export will generate a catalog file when you export. This is what OME will need to reference when you add the catalog. I have included a copy of mine so you can see what it looks like. You can see the progress of the export in the jobs manager: Now go to OME catalog management and hit add (Firmware compliance -> catalog Management -> Add) Configure your repository Share Address: (nothing else) Catalog File Path: /catalog.xml (cannot have anything else) 1. NOTE: The catalog my have a different name depending on how you exported it! Go back to Firmware Compliance -> Create Baseline Select your local catalog Give it a name Add the hosts you discovered Here is what it looks like in action https://youtu.be/p7pxMX-UAJw Example With Subfolder I wanted to confirm an old bug had been cleared out so I also ran it using a subfolder. See https://www.youtube.com/watch?v=iKuCgkBAzu0 to see a BIOS upgrade from start to finish. Note : In all instances the baseLocation field for me was empty.","title":"Offline Updates with OpenManage Enterprise"},{"location":"Offline%20Updates%20with%20OpenManage%20Enterprise/#offline-updates-with-openmanage-enterprise","text":"","title":"Offline Updates with OpenManage Enterprise"},{"location":"Offline%20Updates%20with%20OpenManage%20Enterprise/#versions","text":"","title":"Versions"},{"location":"Offline%20Updates%20with%20OpenManage%20Enterprise/#my-operating-system","text":"[root@dellrepo html]# cat /etc/*-release CentOS Linux release 8.3.2011 NAME=\"CentOS Linux\" VERSION=\"8\" ID=\"centos\" ID_LIKE=\"rhel fedora\" VERSION_ID=\"8\" PLATFORM_ID=\"platform:el8\" PRETTY_NAME=\"CentOS Linux 8\" ANSI_COLOR=\"0;31\" CPE_NAME=\"cpe:/o:centos:centos:8\" HOME_URL=\"https://centos.org/\" BUG_REPORT_URL=\"https://bugs.centos.org/\" CENTOS_MANTISBT_PROJECT=\"CentOS-8\" CENTOS_MANTISBT_PROJECT_VERSION=\"8\" CentOS Linux release 8.3.2011 CentOS Linux release 8.3.2011","title":"My Operating System"},{"location":"Offline%20Updates%20with%20OpenManage%20Enterprise/#ome-version","text":"","title":"OME Version"},{"location":"Offline%20Updates%20with%20OpenManage%20Enterprise/#instructions","text":"Download Dell Repository Manager Run it with /opt/dell/dellemcrepositorymanager/drm.sh NOTE: Running it with root does not work! You will get an error: GUI interface is not supported by this operating system. Click add repository Select the systems for which you want to download updates under select systems Make sure you select Windows-64 as one of the types which will be available chmod +x <binary name> then run with ./<binary_name> . I chose to distribute the repository using HTTP with Apache dnf install httpd sudo systemctl start --now httpd Next you have to synchronize the repository by clicking download. I downloaded my files to /opt/dell/catalogs/fc640 You then have to go open the Dell EMC Repository Manager -> Export -> Export. You will need to select which repositories you want to export and then you will want to select share and a save location. WARNING You have to download the Windows 64 bit versions of the updates for it to work! Even if you are using Linux the idrac only accepts the Windows EXE files. The export will generate a catalog file when you export. This is what OME will need to reference when you add the catalog. I have included a copy of mine so you can see what it looks like. You can see the progress of the export in the jobs manager: Now go to OME catalog management and hit add (Firmware compliance -> catalog Management -> Add) Configure your repository Share Address: (nothing else) Catalog File Path: /catalog.xml (cannot have anything else) 1. NOTE: The catalog my have a different name depending on how you exported it! Go back to Firmware Compliance -> Create Baseline Select your local catalog Give it a name Add the hosts you discovered Here is what it looks like in action https://youtu.be/p7pxMX-UAJw","title":"Instructions"},{"location":"Offline%20Updates%20with%20OpenManage%20Enterprise/#example-with-subfolder","text":"I wanted to confirm an old bug had been cleared out so I also ran it using a subfolder. See https://www.youtube.com/watch?v=iKuCgkBAzu0 to see a BIOS upgrade from start to finish. Note : In all instances the baseLocation field for me was empty.","title":"Example With Subfolder"},{"location":"OpenFlow%20on%204112F-ON/","text":"OpenFlow on 4112F-ON Create OpenFlow Load Balancer Files Reading Material Overview My Configuration Switch Version Info Setup Setup Controller On Host Workstation Setup OpenFlow on the Switch Enable OpenFlow Configure Management Configure OpenFlow Controller Running the Code Supported Protocols Helpful Commands Personal Notes Things We Want Protocols Things to mention Use Cases Problems Files See here for a listing of files and source code. Reading Material Open Flow Switch Specification v1.3.1 Dell OpenFlow Deployment and User Guide 3.0 OS10 Setup Instructions Overview My Configuration Controller is running on Windows in PyCharm while I'm testing. I'll move it to RHEL when I'm done. I am using a S4112F-ON I am using a Ryu OpenFlow controller Switch Version Info Dell EMC Networking OS10 Enterprise Copyright (c) 1999-2020 by Dell Inc. All Rights Reserved. OS Version: 10.5.1.0 Build Version: 10.5.1.0.124 Build Time: 2020-02-12T09:05:20+0000 System Type: S4112F-ON Architecture: x86_64 Up Time: 00:03:52 Setup Setup Controller pip install -r requirements.txt On Host Workstation ** Make sure you use sudo or things will go wrong ** curl -sL https://deb.nodesource.com/setup_14.x | sudo -E bash - sudo apt-get install -y nodejs sudo npm install -g @angular/cli sudo ng add @angular/material You can drop the -g if you want to install angular locally in the directory instead of globally. You will have to prefix your commands with npx -p @angular/cli ng To setup debugging do the following: Go to https://marketplace.visualstudio.com/items?itemName=msjsdiag.debugger-for-chrome and install the addon for Visual Studio Code Go to the debugging tab in Visual Studio code, hit the down arrow next to launch program and click launch Chrome. I used the following configuration: { // Use IntelliSense to learn about possible attributes. // Hover to view descriptions of existing attributes. // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387 \"version\": \"0.2.0\", \"configurations\": [ { \"type\": \"chrome\", \"request\": \"launch\", \"name\": \"Launch Chrome against localhost\", \"url\": \"http://localhost:4200\", \"webRoot\": \"c:\\\\Users\\\\grant\\\\Documents\\\\trafficshaper\\\\angular\" } ] } Setup OpenFlow on the Switch Enable OpenFlow On the switch run: OS10# configure terminal OS10(config)# openflow OS10(config-openflow)# mode openflow-only Configurations not relevant to openflow mode will be removed from the startup-configuration and system will be rebooted. Do you want to proceed? [confirm yes/no]:yes Configure Management OS10(conf-if-ma-1/1/1)# interface mgmt 1/1/1 OS10(conf-if-ma-1/1/1)# ip address <SOME MANAGEMENT IP>/24 OS10(conf-if-ma-1/1/1)# no shutdown OS10(conf-if-ma-1/1/1)# exit Configure OpenFlow Controller OS10# configure terminal OS10(config)# openflow OS10(config-openflow)# switch of-switch-1 OS10(config-openflow-switch)# controller ipv4 <YOUR_CONTROLLER_IP> port 6633 OS10(config-openflow-switch)# protocol-version 1.3 OS10(config-openflow-switch)# no shutdown Running the Code python main.py Supported Protocols TCP UDP ICMP Helpful Commands Personal Notes Things We Want Protocols HTTP TLS DNS SSH Things to mention Inline decryption possibilities Use Cases I want to tie a sensor directly to a DC. So all things for that DC go to one sensor A couple of dropdown boxes in a statement and an execute button. One of those things could be an IP address, or a port, or a protocol, physical port Problems need to make sure we don't receive a reject message need to make it so outports and inports persist if something is an input port do we want to stop them from using redirect port I need to go back and make sure that when compressed tiles move to the next line I need to handle getting flows for the openflow controller's interface Need to add error handling if the server is unavailable Need to update the getPorts documentation","title":"OpenFlow on 4112F-ON"},{"location":"OpenFlow%20on%204112F-ON/#openflow-on-4112f-on","text":"Create OpenFlow Load Balancer Files Reading Material Overview My Configuration Switch Version Info Setup Setup Controller On Host Workstation Setup OpenFlow on the Switch Enable OpenFlow Configure Management Configure OpenFlow Controller Running the Code Supported Protocols Helpful Commands Personal Notes Things We Want Protocols Things to mention Use Cases Problems","title":"OpenFlow on 4112F-ON"},{"location":"OpenFlow%20on%204112F-ON/#files","text":"See here for a listing of files and source code.","title":"Files"},{"location":"OpenFlow%20on%204112F-ON/#reading-material","text":"Open Flow Switch Specification v1.3.1 Dell OpenFlow Deployment and User Guide 3.0 OS10 Setup Instructions","title":"Reading Material"},{"location":"OpenFlow%20on%204112F-ON/#overview","text":"","title":"Overview"},{"location":"OpenFlow%20on%204112F-ON/#my-configuration","text":"Controller is running on Windows in PyCharm while I'm testing. I'll move it to RHEL when I'm done. I am using a S4112F-ON I am using a Ryu OpenFlow controller","title":"My Configuration"},{"location":"OpenFlow%20on%204112F-ON/#switch-version-info","text":"Dell EMC Networking OS10 Enterprise Copyright (c) 1999-2020 by Dell Inc. All Rights Reserved. OS Version: 10.5.1.0 Build Version: 10.5.1.0.124 Build Time: 2020-02-12T09:05:20+0000 System Type: S4112F-ON Architecture: x86_64 Up Time: 00:03:52","title":"Switch Version Info"},{"location":"OpenFlow%20on%204112F-ON/#setup","text":"","title":"Setup"},{"location":"OpenFlow%20on%204112F-ON/#setup-controller","text":"pip install -r requirements.txt","title":"Setup Controller"},{"location":"OpenFlow%20on%204112F-ON/#on-host-workstation","text":"** Make sure you use sudo or things will go wrong ** curl -sL https://deb.nodesource.com/setup_14.x | sudo -E bash - sudo apt-get install -y nodejs sudo npm install -g @angular/cli sudo ng add @angular/material You can drop the -g if you want to install angular locally in the directory instead of globally. You will have to prefix your commands with npx -p @angular/cli ng To setup debugging do the following: Go to https://marketplace.visualstudio.com/items?itemName=msjsdiag.debugger-for-chrome and install the addon for Visual Studio Code Go to the debugging tab in Visual Studio code, hit the down arrow next to launch program and click launch Chrome. I used the following configuration: { // Use IntelliSense to learn about possible attributes. // Hover to view descriptions of existing attributes. // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387 \"version\": \"0.2.0\", \"configurations\": [ { \"type\": \"chrome\", \"request\": \"launch\", \"name\": \"Launch Chrome against localhost\", \"url\": \"http://localhost:4200\", \"webRoot\": \"c:\\\\Users\\\\grant\\\\Documents\\\\trafficshaper\\\\angular\" } ] }","title":"On Host Workstation"},{"location":"OpenFlow%20on%204112F-ON/#setup-openflow-on-the-switch","text":"","title":"Setup OpenFlow on the Switch"},{"location":"OpenFlow%20on%204112F-ON/#enable-openflow","text":"On the switch run: OS10# configure terminal OS10(config)# openflow OS10(config-openflow)# mode openflow-only Configurations not relevant to openflow mode will be removed from the startup-configuration and system will be rebooted. Do you want to proceed? [confirm yes/no]:yes","title":"Enable OpenFlow"},{"location":"OpenFlow%20on%204112F-ON/#configure-management","text":"OS10(conf-if-ma-1/1/1)# interface mgmt 1/1/1 OS10(conf-if-ma-1/1/1)# ip address <SOME MANAGEMENT IP>/24 OS10(conf-if-ma-1/1/1)# no shutdown OS10(conf-if-ma-1/1/1)# exit","title":"Configure Management"},{"location":"OpenFlow%20on%204112F-ON/#configure-openflow-controller","text":"OS10# configure terminal OS10(config)# openflow OS10(config-openflow)# switch of-switch-1 OS10(config-openflow-switch)# controller ipv4 <YOUR_CONTROLLER_IP> port 6633 OS10(config-openflow-switch)# protocol-version 1.3 OS10(config-openflow-switch)# no shutdown","title":"Configure OpenFlow Controller"},{"location":"OpenFlow%20on%204112F-ON/#running-the-code","text":"python main.py","title":"Running the Code"},{"location":"OpenFlow%20on%204112F-ON/#supported-protocols","text":"TCP UDP ICMP","title":"Supported Protocols"},{"location":"OpenFlow%20on%204112F-ON/#helpful-commands","text":"","title":"Helpful Commands"},{"location":"OpenFlow%20on%204112F-ON/#personal-notes","text":"","title":"Personal Notes"},{"location":"OpenFlow%20on%204112F-ON/#things-we-want","text":"","title":"Things We Want"},{"location":"OpenFlow%20on%204112F-ON/#protocols","text":"HTTP TLS DNS SSH","title":"Protocols"},{"location":"OpenFlow%20on%204112F-ON/#things-to-mention","text":"Inline decryption possibilities","title":"Things to mention"},{"location":"OpenFlow%20on%204112F-ON/#use-cases","text":"I want to tie a sensor directly to a DC. So all things for that DC go to one sensor A couple of dropdown boxes in a statement and an execute button. One of those things could be an IP address, or a port, or a protocol, physical port","title":"Use Cases"},{"location":"OpenFlow%20on%204112F-ON/#problems","text":"need to make sure we don't receive a reject message need to make it so outports and inports persist if something is an input port do we want to stop them from using redirect port I need to go back and make sure that when compressed tiles move to the next line I need to handle getting flows for the openflow controller's interface Need to add error handling if the server is unavailable Need to update the getPorts documentation","title":"Problems"},{"location":"OpenFlow%20on%204112F-ON/angular/","text":"Trafficshapergui This project was generated with Angular CLI version 9.1.1. Development server Run ng serve for a dev server. Navigate to http://localhost:4200/ . The app will automatically reload if you change any of the source files. Code scaffolding Run ng generate component component-name to generate a new component. You can also use ng generate directive|pipe|service|class|guard|interface|enum|module . Build Run ng build to build the project. The build artifacts will be stored in the dist/ directory. Use the --prod flag for a production build. Running unit tests Run ng test to execute the unit tests via Karma . Running end-to-end tests Run ng e2e to execute the end-to-end tests via Protractor . Further help To get more help on the Angular CLI use ng help or go check out the Angular CLI README .","title":"Trafficshapergui"},{"location":"OpenFlow%20on%204112F-ON/angular/#trafficshapergui","text":"This project was generated with Angular CLI version 9.1.1.","title":"Trafficshapergui"},{"location":"OpenFlow%20on%204112F-ON/angular/#development-server","text":"Run ng serve for a dev server. Navigate to http://localhost:4200/ . The app will automatically reload if you change any of the source files.","title":"Development server"},{"location":"OpenFlow%20on%204112F-ON/angular/#code-scaffolding","text":"Run ng generate component component-name to generate a new component. You can also use ng generate directive|pipe|service|class|guard|interface|enum|module .","title":"Code scaffolding"},{"location":"OpenFlow%20on%204112F-ON/angular/#build","text":"Run ng build to build the project. The build artifacts will be stored in the dist/ directory. Use the --prod flag for a production build.","title":"Build"},{"location":"OpenFlow%20on%204112F-ON/angular/#running-unit-tests","text":"Run ng test to execute the unit tests via Karma .","title":"Running unit tests"},{"location":"OpenFlow%20on%204112F-ON/angular/#running-end-to-end-tests","text":"Run ng e2e to execute the end-to-end tests via Protractor .","title":"Running end-to-end tests"},{"location":"OpenFlow%20on%204112F-ON/angular/#further-help","text":"To get more help on the Angular CLI use ng help or go check out the Angular CLI README .","title":"Further help"},{"location":"OpenFlow%20on%204112F-ON/openflow_bug/","text":"Bug in Ryu datapath_id Overview Problem Ryu incorrectly truncates datapath_id from 16 characters to 15. I found the problem while testing this Ryu example . Proof of Concept Running the code in debug mode produces the below: You can see that the switch ID 150013889525632 which is only 15 characters instead of the required 16. To confirm that the problem was not with what the switch was sending I captured the response in Wireshark. You can see the switch correctly adheres to the 64bit datapath_id requirement. Detailed Troubleshooting I found the problem when none of the routes ran when browsing to the address: http://127.0.0.1:8080/simpleswitch/mactable/150013889525632 I eventually realized it is because of the following line: @route('/simpleswitch', url, methods=['PUT'], requirements={'dpid': dpid_lib.DPID_PATTERN}) DPID_PATTERN's definition is as follows: _DPID_LEN = 16 _DPID_FMT = '%0{0}x'.format(_DPID_LEN) DPID_PATTERN = r'[0-9a-f]{%d}' % _DPID_LEN You can see this more directly by looking at the regex as it is used in the WSGI call produced from the above line. As you can see from {16} the switch ID Ryu produces does not match because it is a character short. You can fix the problem by using the URL: http://127.0.0.1:8080/simpleswitch/mactable/0150013889525632 However, that then causes other code to fail because it is is looking for the original switch ID of 150013889525632. Reproducing My Configuration Controller is running on Windows in PyCharm Controller: Ryu Switch: 4112F-ON Switch Version Info Dell EMC Networking OS10 Enterprise Copyright (c) 1999-2020 by Dell Inc. All Rights Reserved. OS Version: 10.5.1.0 Build Version: 10.5.1.0.124 Build Time: 2020-02-12T09:05:20+0000 System Type: S4112F-ON Architecture: x86_64 Up Time: 4 days 09:16:43 Setup Enable OpenFlow on the Switch On the switch run: OS10# configure terminal OS10(config)# openflow OS10(config-openflow)# mode openflow-only Configurations not relevant to openflow mode will be removed from the startup-configuration and system will be rebooted. Do you want to proceed? [confirm yes/no]:yes Configure OpenFlow OS10# configure terminal OS10(config)# openflow OS10(config-openflow)# switch of-switch-1 OS10(config-openflow-switch)# controller ipv4 <YOUR_CONTROLLER_IP> port 6633 OS10(config-openflow-switch)# no shutdown See the switch config for details. Run the Code Run pip install ryu to install Ryu and its dependencies. I have included my Ryu app as it currently was when I found the bug in the file main.py . I used PyCharm to perform debugging which required me to adjust the debug configuration to the below: This will allow you to use PyCharm's debugger. Alternatively, you can delete everything after line 358 in main.py and use ryu-manager to run the application. To run the code there is an application called ryu-manager . To run the code you have to run ryu-manager main.py .","title":"Bug in Ryu datapath_id"},{"location":"OpenFlow%20on%204112F-ON/openflow_bug/#bug-in-ryu-datapath_id","text":"","title":"Bug in Ryu datapath_id"},{"location":"OpenFlow%20on%204112F-ON/openflow_bug/#overview","text":"","title":"Overview"},{"location":"OpenFlow%20on%204112F-ON/openflow_bug/#problem","text":"Ryu incorrectly truncates datapath_id from 16 characters to 15. I found the problem while testing this Ryu example .","title":"Problem"},{"location":"OpenFlow%20on%204112F-ON/openflow_bug/#proof-of-concept","text":"Running the code in debug mode produces the below: You can see that the switch ID 150013889525632 which is only 15 characters instead of the required 16. To confirm that the problem was not with what the switch was sending I captured the response in Wireshark. You can see the switch correctly adheres to the 64bit datapath_id requirement.","title":"Proof of Concept"},{"location":"OpenFlow%20on%204112F-ON/openflow_bug/#detailed-troubleshooting","text":"I found the problem when none of the routes ran when browsing to the address: http://127.0.0.1:8080/simpleswitch/mactable/150013889525632 I eventually realized it is because of the following line: @route('/simpleswitch', url, methods=['PUT'], requirements={'dpid': dpid_lib.DPID_PATTERN}) DPID_PATTERN's definition is as follows: _DPID_LEN = 16 _DPID_FMT = '%0{0}x'.format(_DPID_LEN) DPID_PATTERN = r'[0-9a-f]{%d}' % _DPID_LEN You can see this more directly by looking at the regex as it is used in the WSGI call produced from the above line. As you can see from {16} the switch ID Ryu produces does not match because it is a character short. You can fix the problem by using the URL: http://127.0.0.1:8080/simpleswitch/mactable/0150013889525632 However, that then causes other code to fail because it is is looking for the original switch ID of 150013889525632.","title":"Detailed Troubleshooting"},{"location":"OpenFlow%20on%204112F-ON/openflow_bug/#reproducing","text":"","title":"Reproducing"},{"location":"OpenFlow%20on%204112F-ON/openflow_bug/#my-configuration","text":"Controller is running on Windows in PyCharm Controller: Ryu Switch: 4112F-ON","title":"My Configuration"},{"location":"OpenFlow%20on%204112F-ON/openflow_bug/#switch-version-info","text":"Dell EMC Networking OS10 Enterprise Copyright (c) 1999-2020 by Dell Inc. All Rights Reserved. OS Version: 10.5.1.0 Build Version: 10.5.1.0.124 Build Time: 2020-02-12T09:05:20+0000 System Type: S4112F-ON Architecture: x86_64 Up Time: 4 days 09:16:43","title":"Switch Version Info"},{"location":"OpenFlow%20on%204112F-ON/openflow_bug/#setup","text":"","title":"Setup"},{"location":"OpenFlow%20on%204112F-ON/openflow_bug/#enable-openflow-on-the-switch","text":"On the switch run: OS10# configure terminal OS10(config)# openflow OS10(config-openflow)# mode openflow-only Configurations not relevant to openflow mode will be removed from the startup-configuration and system will be rebooted. Do you want to proceed? [confirm yes/no]:yes","title":"Enable OpenFlow on the Switch"},{"location":"OpenFlow%20on%204112F-ON/openflow_bug/#configure-openflow","text":"OS10# configure terminal OS10(config)# openflow OS10(config-openflow)# switch of-switch-1 OS10(config-openflow-switch)# controller ipv4 <YOUR_CONTROLLER_IP> port 6633 OS10(config-openflow-switch)# no shutdown See the switch config for details.","title":"Configure OpenFlow"},{"location":"OpenFlow%20on%204112F-ON/openflow_bug/#run-the-code","text":"Run pip install ryu to install Ryu and its dependencies. I have included my Ryu app as it currently was when I found the bug in the file main.py . I used PyCharm to perform debugging which required me to adjust the debug configuration to the below: This will allow you to use PyCharm's debugger. Alternatively, you can delete everything after line 358 in main.py and use ryu-manager to run the application. To run the code there is an application called ryu-manager . To run the code you have to run ryu-manager main.py .","title":"Run the Code"},{"location":"PCIev3%20vs%20v4/","text":"PCIev3 vs v4 PCIe v3 vs v4 PCIev3 Speed Per lane: 1GB/s unidirectional PCIev3 Max unidirectional speed: 16GB/s PCIev4 Speed per lane: 2GB/s unidirectional PCIev4 Max unidirectional speed: 32GB/s Samsung NF1 Drive According to this article a single NF1 drive runs at 3,000MB/s sequential read speed and a 1900 MB/s sequential write speed. Xeon Second Gen Scalable Procs According to this article a second gen xeon proc provides 48 PCIe v3 lanes. Two procs would mean 96. This means that a 2 proc server with these chips could theoretically run up to 96GB/s unidirectional speed. 96-36 = 60 64 SuperMicro 1029P-NMR36L Dual socket Xeon gen 2 procs with space for 32 hot-swap pci-e3 nf1 drives plus 4 SATA 3 M.2 drive bays. In a scenario where only drives were in the box and under theoretical perfect conditions, you could see a 96GB/s read speed. (Bascially impossible you'd ever actually see that)","title":"PCIev3 vs v4"},{"location":"PCIev3%20vs%20v4/#pciev3-vs-v4","text":"","title":"PCIev3 vs v4"},{"location":"PCIev3%20vs%20v4/#pcie-v3-vs-v4","text":"PCIev3 Speed Per lane: 1GB/s unidirectional PCIev3 Max unidirectional speed: 16GB/s PCIev4 Speed per lane: 2GB/s unidirectional PCIev4 Max unidirectional speed: 32GB/s","title":"PCIe v3 vs v4"},{"location":"PCIev3%20vs%20v4/#samsung-nf1-drive","text":"According to this article a single NF1 drive runs at 3,000MB/s sequential read speed and a 1900 MB/s sequential write speed.","title":"Samsung NF1 Drive"},{"location":"PCIev3%20vs%20v4/#xeon-second-gen-scalable-procs","text":"According to this article a second gen xeon proc provides 48 PCIe v3 lanes. Two procs would mean 96. This means that a 2 proc server with these chips could theoretically run up to 96GB/s unidirectional speed. 96-36 = 60 64","title":"Xeon Second Gen Scalable Procs"},{"location":"PCIev3%20vs%20v4/#supermicro-1029p-nmr36l","text":"Dual socket Xeon gen 2 procs with space for 32 hot-swap pci-e3 nf1 drives plus 4 SATA 3 M.2 drive bays. In a scenario where only drives were in the box and under theoretical perfect conditions, you could see a 96GB/s read speed. (Bascially impossible you'd ever actually see that)","title":"SuperMicro 1029P-NMR36L"},{"location":"Playing%20with%20virsh/","text":"Playing with virsh VMs List all VMs virsh list --all Network Stuff Get network info virsh net-info Dump network info virsh net-dumpxml xhubnet List all networks virsh net-list --all IPables Heads up, iptables -L does not truly list all the rules. It just lists the rules in the current table. If you want to see the NAT rules you can run: iptables -t nat -L Heads up, iptables will by default try to resolve names. To skip this do: iptables -t nat -vnL Adding a destination NAT rule: iptables -t nat -A PREROUTING -p tcp --dport 8000 -j DNAT --to 10.125.120.21 Delete a rule from iptables: iptables -t nat -D POSTROUTING -p tcp --dport 50000 -j SNAT --to 5.136.13.37","title":"Playing with virsh"},{"location":"Playing%20with%20virsh/#playing-with-virsh","text":"","title":"Playing with virsh"},{"location":"Playing%20with%20virsh/#vms","text":"List all VMs virsh list --all","title":"VMs"},{"location":"Playing%20with%20virsh/#network-stuff","text":"Get network info virsh net-info Dump network info virsh net-dumpxml xhubnet List all networks virsh net-list --all","title":"Network Stuff"},{"location":"Playing%20with%20virsh/#ipables","text":"Heads up, iptables -L does not truly list all the rules. It just lists the rules in the current table. If you want to see the NAT rules you can run: iptables -t nat -L Heads up, iptables will by default try to resolve names. To skip this do: iptables -t nat -vnL Adding a destination NAT rule: iptables -t nat -A PREROUTING -p tcp --dport 8000 -j DNAT --to 10.125.120.21 Delete a rule from iptables: iptables -t nat -D POSTROUTING -p tcp --dport 50000 -j SNAT --to 5.136.13.37","title":"IPables"},{"location":"PowerScale%20-%20Configure%20with%20Kubernetes%20%28Incomplete%29/","text":"PowerScale - Configure with Kubernetes (Incomplete) RKE2 advertises itself as an automatic K8s installer. That is... sort of true based on my experience. It is certainly simpler than what I had to do 7 years ago, but significant assembly by someone who knows kubernetes and networking was still required. It was still a pretty large PITA. Install RKE2 on Server I recommend just making life easy and doing an su - and just doing everything as root. Note: after heavy experimentation to include writing the below code that does this I still found flannel choked with firewalld on so ultimately I just ran systemctl disable --now firewalld . See Troubleshooting Flannel Issues . Since it's a lab I decided the juice wasn't worth the squeeze because I think the problem is in the internal masquerade rules. curl -sfL https://get.rke2.io | sudo sh - # Kubernetes API Server firewall-cmd --permanent --add-port=6443/tcp # RKE2 Server firewall-cmd --permanent --add-port=9345/tcp # etcd server client API firewall-cmd --permanent --add-port=2379/tcp firewall-cmd --permanent --add-port=2380/tcp # HTTPS firewall-cmd --permanent --add-port=443/tcp # NodePort Services firewall-cmd --permanent --add-port=30000-32767/tcp # Kubelet API firewall-cmd --permanent --add-port=10250/tcp # kube-scheduler firewall-cmd --permanent --add-port=10251/tcp # kube-controller-manager firewall-cmd --permanent --add-port=10252/tcp # Flannel firewall-cmd --permanent --add-port=8285/udp firewall-cmd --permanent --add-port=8472/udp # Additional ports required for Kubernetes firewall-cmd --permanent --add-port=10255/tcp # Read-only Kubelet API firewall-cmd --permanent --add-port=30000-32767/tcp # NodePort Services range firewall-cmd --permanent --add-port=6783/tcp # Flannel firewall-cmd --permanent --add-port=6783/udp # Flannel firewall-cmd --permanent --add-port=6784/udp # Flannel firewall-cmd --add-masquerade --permanent firewall-cmd --reload systemctl restart firewalld sudo systemctl enable rke2-server.service sudo systemctl start rke2-server.service cd /var/lib/rancher/rke2/bin echo 'export KUBECONFIG=/etc/rancher/rke2/rke2.yaml' >> ~/.bashrc echo 'export PATH=$PATH:/var/lib/rancher/rke2/bin' >> ~/.bashrc source ~/.bashrc The rke2 server process listens on port 9345 for new nodes to register. The Kubernetes API is still served on port 6443, as normal. Set Up a K8s Node sudo curl -sfL https://get.rke2.io | sudo INSTALL_RKE2_TYPE=\"agent\" sh - # Kubelet API and Flannel ports firewall-cmd --permanent --add-port=10250/tcp firewall-cmd --permanent --add-port=8285/udp firewall-cmd --permanent --add-port=8472/udp # NodePort Services firewall-cmd --permanent --add-port=30000-32767/tcp # Additional ports required for Kubernetes firewall-cmd --permanent --add-port=10255/tcp # Read-only Kubelet API firewall-cmd --permanent --add-port=6783/tcp # Flannel firewall-cmd --permanent --add-port=6783/udp # Flannel firewall-cmd --permanent --add-port=6784/udp # Flannel firewall-cmd --add-masquerade --permanent firewall-cmd --reload systemctl restart firewalld sudo systemctl enable rke2-agent.service mkdir -p /etc/rancher/rke2/ echo 'export PATH=$PATH:/var/lib/rancher/rke2/bin' >> ~/.bashrc source ~/.bashrc vim /etc/rancher/rke2/config.yaml After the server setup I noticed it took quite some time to come up. You can track progress with journalctl -u rke2-server -f . My logs looked like this: Nov 29 14:21:37 k8s-server.lan rke2[1016]: time=\"2023-11-29T14:21:37-05:00\" level=info msg=\"Pod for kube-apiserver not synced (waiting for termination of old pod sandbox), retrying\" Nov 29 14:21:38 k8s-server.lan rke2[1016]: time=\"2023-11-29T14:21:38-05:00\" level=info msg=\"Waiting to retrieve kube-proxy configuration; server is not ready: https://127.0.0.1:9345/v1-rke2/readyz: 500 Internal Server Error\" Nov 29 14:21:43 k8s-server.lan rke2[1016]: time=\"2023-11-29T14:21:43-05:00\" level=info msg=\"Waiting to retrieve kube-proxy configuration; server is not ready: https://127.0.0.1:9345/v1-rke2/readyz: 500 Internal Server Error\" Nov 29 14:21:48 k8s-server.lan rke2[1016]: time=\"2023-11-29T14:21:48-05:00\" level=info msg=\"Waiting to retrieve kube-proxy configuration; server is not ready: https://127.0.0.1:9345/v1-rke2/readyz: 500 Internal Server Error\" Nov 29 14:21:53 k8s-server.lan rke2[1016]: time=\"2023-11-29T14:21:53-05:00\" level=info msg=\"Waiting to retrieve kube-proxy configuration; server is not ready: https://127.0.0.1:9345/v1-rke2/readyz: 500 Internal Server Error\" Nov 29 14:21:57 k8s-server.lan rke2[1016]: time=\"2023-11-29T14:21:57-05:00\" level=info msg=\"Pod for etcd is synced\" Nov 29 14:21:57 k8s-server.lan rke2[1016]: time=\"2023-11-29T14:21:57-05:00\" level=info msg=\"Pod for kube-apiserver not synced (waiting for termination of old pod sandbox), retrying\" Nov 29 14:21:58 k8s-server.lan rke2[1016]: time=\"2023-11-29T14:21:58-05:00\" level=info msg=\"Waiting to retrieve kube-proxy configuration; server is not ready: https://127.0.0.1:9345/v1-rke2/readyz: 500 Internal Server Error\" Nov 29 14:22:03 k8s-server.lan rke2[1016]: time=\"2023-11-29T14:22:03-05:00\" level=info msg=\"Waiting to retrieve kube-proxy configuration; server is not ready: https://127.0.0.1:9345/v1-rke2/readyz: 500 Internal Server Error\" Nov 29 14:22:08 k8s-server.lan rke2[1016]: time=\"2023-11-29T14:22:08-05:00\" level=info msg=\"Waiting to retrieve kube-proxy configuration; server is not ready: https://127.0.0.1:9345/v1-rke2/readyz: 500 Internal Server Error\" Nov 29 14:22:13 k8s-server.lan rke2[1016]: time=\"2023-11-29T14:22:13-05:00\" level=info msg=\"Waiting to retrieve kube-proxy configuration; server is not ready: https://127.0.0.1:9345/v1-rke2/readyz: 500 Internal Server Error\" Nov 29 14:22:17 k8s-server.lan rke2[1016]: time=\"2023-11-29T14:22:17-05:00\" level=info msg=\"Pod for etcd is synced\" Nov 29 14:22:17 k8s-server.lan rke2[1016]: time=\"2023-11-29T14:22:17-05:00\" level=info msg=\"Pod for kube-apiserver is synced\" Nov 29 14:22:17 k8s-server.lan rke2[1016]: time=\"2023-11-29T14:22:17-05:00\" level=info msg=\"ETCD server is now running\" Nov 29 14:22:17 k8s-server.lan rke2[1016]: time=\"2023-11-29T14:22:17-05:00\" level=info msg=\"rke2 is up and running\" Nov 29 14:22:17 k8s-server.lan systemd[1]: Started Rancher Kubernetes Engine v2 (server). Nov 29 14:22:17 k8s-server.lan rke2[1016]: time=\"2023-11-29T14:22:17-05:00\" level=info msg=\"Failed to get existing traefik HelmChart\" error=\"helmcharts.helm.cattle.io \\\"traefik\\\" not found\" Nov 29 14:22:17 k8s-server.lan rke2[1016]: time=\"2023-11-29T14:22:17-05:00\" level=info msg=\"Reconciling ETCDSnapshotFile resources\" Nov 29 14:22:17 k8s-server.lan rke2[1016]: time=\"2023-11-29T14:22:17-05:00\" level=info msg=\"Tunnel server egress proxy mode: agent\" Nov 29 14:22:17 k8s-server.lan rke2[1016]: time=\"2023-11-29T14:22:17-05:00\" level=info msg=\"Starting managed etcd node metadata controller\" Nov 29 14:22:17 k8s-server.lan rke2[1016]: time=\"2023-11-29T14:22:17-05:00\" level=info msg=\"Reconciliation of ETCDSnapshotFile resources complete\" Nov 29 14:22:17 k8s-server.lan rke2[1016]: time=\"2023-11-29T14:22:17-05:00\" level=info msg=\"Starting k3s.cattle.io/v1, Kind=Addon controller\" You can see you get constant 500 errors until it eventually fixes itself. When everything has settled down make sure that you see nodes: [root@k8s-server bin]# kubectl get nodes NAME STATUS ROLES AGE VERSION k8s-agent1.lan Ready <none> 11m v1.26.10+rke2r2 k8s-server.lan Ready control-plane,etcd,master 60m v1.26.10+rke2r2 Install Helm cd /tmp wget https://get.helm.sh/helm-v3.13.2-linux-amd64.tar.gz # Update version as needed tar xzf helm-v3.13.2-linux-amd64.tar.gz sudo mv linux-amd64/helm /usr/local/bin/helm helm version Install Rancher helm repo add rancher-stable https://releases.rancher.com/server-charts/stable kubectl create namespace cattle-system helm install rancher rancher-stable/rancher --namespace cattle-system --set hostname=<your-rancher-hostname> --set bootstrapPassword=<your-admin-password> --set ingress.tls.source=rancher # YOU HAVE TO UPDATE THIS echo https://k8s-server.lan/dashboard/?setup=$(kubectl get secret --namespace cattle-system bootstrap-secret -o go-template='{{.data.bootstrapPassword|base64decode}}') Troubleshooting Flannel Issues Warning: Sassy commentary ahead. I haven't built K8s from scratch since 2017 and I'm glad to see that getting flannel to work is still a huge mess. With that said, here's how to go about troubleshooting it when it inevitably fails. (Seriously, it has been 7 years, how is it still this bad?) Rancher Woes My rancher install failed with no output from the installer. You can manually pull the logs by examining the rancher pod with kubectl logs -n cattle-system rancher-64cf6ddd96-2x2ms This got me: 2023/11/29 21:04:33 [ERROR] [updateClusterHealth] Failed to update cluster [local]: Internal error occurred: failed calling webhook \"rancher.cattle.io.clusters.management.cattle.io\": failed to call webhook: Post \"https://rancher-webhook.cattle-system.svc:443/v1/webhook/mutation/clusters.management.cattle.io?timeout=10s\": context deadline exceeded 2023/11/29 21:04:33 [ERROR] Failed to connect to peer wss://10.42.0.8/v3/connect [local ID=10.42.1.20]: dial tcp 10.42.0.8:443: connect: no route to host 2023/11/29 21:04:34 [ERROR] Failed to connect to peer wss://10.42.1.21/v3/connect [local ID=10.42.1.20]: dial tcp 10.42.1.21:443: connect: no route to host 2023/11/29 21:04:38 [ERROR] Failed to connect to peer wss://10.42.0.8/v3/connect [local ID=10.42.1.20]: dial tcp 10.42.0.8:443: connect: no route to host 2023/11/29 21:04:39 [ERROR] Failed to connect to peer wss://10.42.1.21/v3/connect [local ID=10.42.1.20]: dial tcp 10.42.1.21:443: connect: no route to host 2023/11/29 21:04:43 [ERROR] Failed to connect to peer wss://10.42.0.8/v3/connect [local ID=10.42.1.20]: dial tcp 10.42.0.8:443: connect: no route to host 2023/11/29 21:04:44 [ERROR] Failed to connect to peer wss://10.42.1.21/v3/connect [local ID=10.42.1.20]: dial tcp 10.42.1.21:443: connect: no route to host on repeat. 10.42.1.21 is an internal flannel address so the next step is to figure out who owns it with kubectl get pods --all-namespaces -o wide : [root@k8s-server ~]# kubectl get pods --all-namespaces -o wide NAMESPACE NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES cattle-fleet-system fleet-controller-56968b86b6-tctjr 1/1 Running 0 44m 10.42.1.24 k8s-agent1.lan <none> <none> cattle-fleet-system gitjob-7d68454468-bk7fh 1/1 Running 0 44m 10.42.1.25 k8s-agent1.lan <none> <none> cattle-provisioning-capi-system capi-controller-manager-6f87d6bd74-v489n 1/1 Running 0 41m 10.42.1.30 k8s-agent1.lan <none> <none> cattle-system helm-operation-64xf7 0/2 Completed 0 42m 10.42.1.29 k8s-agent1.lan <none> <none> cattle-system helm-operation-h88vn 1/2 Error 0 41m 10.42.1.34 k8s-agent1.lan <none> <none> cattle-system helm-operation-jndl9 1/2 Error 0 41m 10.42.1.33 k8s-agent1.lan <none> <none> cattle-system helm-operation-k757h 0/2 Completed 0 44m 10.42.1.23 k8s-agent1.lan <none> <none> cattle-system helm-operation-ldnkm 0/2 Completed 0 45m 10.42.1.22 k8s-agent1.lan <none> <none> cattle-system helm-operation-sv5ts 0/2 Completed 0 43m 10.42.1.28 k8s-agent1.lan <none> <none> cattle-system helm-operation-thct7 0/2 Completed 0 43m 10.42.1.27 k8s-agent1.lan <none> <none> cattle-system rancher-64cf6ddd96-2x2ms 1/1 Running 1 (45m ago) 46m 10.42.1.20 k8s-agent1.lan <none> <none> cattle-system rancher-64cf6ddd96-drrzr 1/1 Running 0 46m 10.42.0.8 k8s-server.lan <none> <none> cattle-system rancher-64cf6ddd96-qq64g 1/1 Running 0 46m 10.42.1.21 k8s-agent1.lan <none> <none> cattle-system rancher-webhook-58d68fb97d-b5sn8 1/1 Running 0 41m 10.42.1.32 k8s-agent1.lan <none> <none> cert-manager cert-manager-startupapicheck-fvp9t 0/1 Completed 1 52m 10.42.1.19 k8s-agent1.lan <none> <none> kube-system cloud-controller-manager-k8s-server.lan 1/1 Running 3 (105m ago) 139m 10.10.25.135 k8s-server.lan <none> <none> kube-system etcd-k8s-server.lan 1/1 Running 1 139m 10.10.25.135 k8s-server.lan <none> <none> kube-system helm-install-rke2-canal-k8b4d 0/1 Completed 0 139m 10.10.25.135 k8s-server.lan <none> <none> kube-system helm-install-rke2-coredns-f59dz 0/1 Completed 0 139m 10.10.25.135 k8s-server.lan <none> <none> kube-system helm-install-rke2-ingress-nginx-gpt7q 0/1 Completed 0 139m 10.42.0.2 k8s-server.lan <none> <none> kube-system helm-install-rke2-metrics-server-q9jwf 0/1 Completed 0 139m 10.42.0.6 k8s-server.lan <none> <none> kube-system helm-install-rke2-snapshot-controller-6pqpg 0/1 Completed 2 139m 10.42.0.4 k8s-server.lan <none> <none> kube-system helm-install-rke2-snapshot-controller-crd-k6klp 0/1 Completed 0 139m 10.42.0.10 k8s-server.lan <none> <none> kube-system helm-install-rke2-snapshot-validation-webhook-hrv5n 0/1 Completed 0 139m 10.42.0.3 k8s-server.lan <none> <none> kube-system kube-apiserver-k8s-server.lan 1/1 Running 1 139m 10.10.25.135 k8s-server.lan <none> <none> kube-system kube-controller-manager-k8s-server.lan 1/1 Running 2 (105m ago) 139m 10.10.25.135 k8s-server.lan <none> <none> kube-system kube-proxy-k8s-agent1.lan 1/1 Running 0 90m 10.10.25.136 k8s-agent1.lan <none> <none> kube-system kube-proxy-k8s-server.lan 1/1 Running 2 (104m ago) 103m 10.10.25.135 k8s-server.lan <none> <none> kube-system kube-scheduler-k8s-server.lan 1/1 Running 1 (105m ago) 139m 10.10.25.135 k8s-server.lan <none> <none> kube-system rke2-canal-7p5hz 2/2 Running 2 (105m ago) 139m 10.10.25.135 k8s-server.lan <none> <none> kube-system rke2-canal-9wg57 2/2 Running 0 90m 10.10.25.136 k8s-agent1.lan <none> <none> kube-system rke2-coredns-rke2-coredns-565dfc7d75-n96xs 1/1 Running 0 90m 10.42.1.2 k8s-agent1.lan <none> <none> kube-system rke2-coredns-rke2-coredns-565dfc7d75-xv92q 1/1 Running 1 (105m ago) 139m 10.42.0.3 k8s-server.lan <none> <none> kube-system rke2-coredns-rke2-coredns-autoscaler-6c48c95bf9-mh279 1/1 Running 1 (105m ago) 139m 10.42.0.2 k8s-server.lan <none> <none> kube-system rke2-ingress-nginx-controller-89d4c 1/1 Running 0 89m 10.42.1.3 k8s-agent1.lan <none> <none> kube-system rke2-ingress-nginx-controller-zctxb 1/1 Running 1 (105m ago) 139m 10.42.0.5 k8s-server.lan <none> <none> kube-system rke2-metrics-server-c9c78bd66-ndcxs 1/1 Running 1 (105m ago) 139m 10.42.0.4 k8s-server.lan <none> <none> kube-system rke2-snapshot-controller-6f7bbb497d-xfk9x 1/1 Running 1 (105m ago) 139m 10.42.0.6 k8s-server.lan <none> <none> kube-system rke2-snapshot-validation-webhook-65b5675d5c-sfqb2 1/1 Running 1 (105m ago) 139m 10.42.0.7 k8s-server.lan <none> <none> We can see that 10.42.0.8 and 10.42.1.21 are the two rancher containers which confirms for us that as per usual, flannel is not able to complete even its most basic of functions (VXLAN) successfully and its up to us to fix it. cattle-system rancher-64cf6ddd96-drrzr 1/1 Running 0 46m 10.42.0.8 k8s-server.lan <none> <none> cattle-system rancher-64cf6ddd96-qq64g 1/1 Running 0 46m 10.42.1.21 k8s-agent1.lan <none> <none> We can get shells in these containers with kubectl exec -it -n cattle-system rancher-64cf6ddd96-drrzr -- /bin/bash . I fished around in here and found nothing. Ultimately I tcpdumped the flannel network and discovered that we were missing some other specific ports it needed: [root@k8s-agent1 ~]# tcpdump -i flannel.1 dropped privs to tcpdump tcpdump: verbose output suppressed, use -v[v]... for full protocol decode listening on flannel.1, link-type EN10MB (Ethernet), snapshot length 262144 bytes 16:17:38.793476 IP 10.42.0.0.58822 > 10.42.1.32.tungsten-https: Flags [S], seq 3219974389, win 64860, options [mss 1410,sackOK,TS val 2506628321 ecr 0,nop,wscale 7], length 0 16:17:38.793509 IP k8s-agent1.lan > 10.42.0.0: ICMP host 10.42.1.32 unreachable - admin prohibited filter, length 68 16:17:39.143985 IP 10.42.0.0.41502 > 10.42.1.32.tungsten-https: Flags [S], seq 1965118956, win 64860, options [mss 1410,sackOK,TS val 2506628671 ecr 0,nop,wscale 7], length 0 16:17:39.144008 IP k8s-agent1.lan > 10.42.0.0: ICMP host 10.42.1.32 unreachable - admin prohibited filter, length 68 16:17:39.847986 IP 10.42.0.0.58822 > 10.42.1.32.tungsten-https: Flags [S], seq 3219974389, win 64860, options [mss 1410,sackOK,TS val 2506629375 ecr 0,nop,wscale 7], length 0 16:17:39.848008 IP k8s-agent1.lan > 10.42.0.0: ICMP host 10.42.1.32 unreachable - admin prohibited filter, length 68 16:17:40.679004 IP 10.42.0.0.47680 > 10.42.1.32.tungsten-https: Flags [S], seq 1291962979, win 64860, options [mss 1410,sackOK,TS val 2506630206 ecr 0,nop,wscale 7], length 0 16:17:40.679028 IP k8s-agent1.lan > 10.42.0.0: ICMP host 10.42.1.32 unreachable - admin prohibited filter, length 68 16:17:41.894997 IP 10.42.0.0.58822 > 10.42.1.32.tungsten-https: Flags [S], seq 3219974389, win 64860, options [mss 1410,sackOK,TS val 2506631422 ecr 0,nop,wscale 7], length 0 16:17:41.895024 IP k8s-agent1.lan > 10.42.0.0: ICMP host 10.42.1.32 unreachable - admin prohibited filter, length 68 16:17:42.727005 IP 10.42.0.0.54136 > 10.42.1.32.tungsten-https: Flags [S], seq 1383176303, win 64860, options [mss 1410,sackOK The moral of the story is that if you see that, there's probably a firewall port you've missed. Why isn't that built into the installer so you aren't manually looking up and opening more than a dozen ports I hear you asking? Great question.","title":"PowerScale - Configure with Kubernetes (Incomplete)"},{"location":"PowerScale%20-%20Configure%20with%20Kubernetes%20%28Incomplete%29/#powerscale-configure-with-kubernetes-incomplete","text":"RKE2 advertises itself as an automatic K8s installer. That is... sort of true based on my experience. It is certainly simpler than what I had to do 7 years ago, but significant assembly by someone who knows kubernetes and networking was still required. It was still a pretty large PITA.","title":"PowerScale - Configure with Kubernetes (Incomplete)"},{"location":"PowerScale%20-%20Configure%20with%20Kubernetes%20%28Incomplete%29/#install-rke2-on-server","text":"I recommend just making life easy and doing an su - and just doing everything as root. Note: after heavy experimentation to include writing the below code that does this I still found flannel choked with firewalld on so ultimately I just ran systemctl disable --now firewalld . See Troubleshooting Flannel Issues . Since it's a lab I decided the juice wasn't worth the squeeze because I think the problem is in the internal masquerade rules. curl -sfL https://get.rke2.io | sudo sh - # Kubernetes API Server firewall-cmd --permanent --add-port=6443/tcp # RKE2 Server firewall-cmd --permanent --add-port=9345/tcp # etcd server client API firewall-cmd --permanent --add-port=2379/tcp firewall-cmd --permanent --add-port=2380/tcp # HTTPS firewall-cmd --permanent --add-port=443/tcp # NodePort Services firewall-cmd --permanent --add-port=30000-32767/tcp # Kubelet API firewall-cmd --permanent --add-port=10250/tcp # kube-scheduler firewall-cmd --permanent --add-port=10251/tcp # kube-controller-manager firewall-cmd --permanent --add-port=10252/tcp # Flannel firewall-cmd --permanent --add-port=8285/udp firewall-cmd --permanent --add-port=8472/udp # Additional ports required for Kubernetes firewall-cmd --permanent --add-port=10255/tcp # Read-only Kubelet API firewall-cmd --permanent --add-port=30000-32767/tcp # NodePort Services range firewall-cmd --permanent --add-port=6783/tcp # Flannel firewall-cmd --permanent --add-port=6783/udp # Flannel firewall-cmd --permanent --add-port=6784/udp # Flannel firewall-cmd --add-masquerade --permanent firewall-cmd --reload systemctl restart firewalld sudo systemctl enable rke2-server.service sudo systemctl start rke2-server.service cd /var/lib/rancher/rke2/bin echo 'export KUBECONFIG=/etc/rancher/rke2/rke2.yaml' >> ~/.bashrc echo 'export PATH=$PATH:/var/lib/rancher/rke2/bin' >> ~/.bashrc source ~/.bashrc The rke2 server process listens on port 9345 for new nodes to register. The Kubernetes API is still served on port 6443, as normal.","title":"Install RKE2 on Server"},{"location":"PowerScale%20-%20Configure%20with%20Kubernetes%20%28Incomplete%29/#set-up-a-k8s-node","text":"sudo curl -sfL https://get.rke2.io | sudo INSTALL_RKE2_TYPE=\"agent\" sh - # Kubelet API and Flannel ports firewall-cmd --permanent --add-port=10250/tcp firewall-cmd --permanent --add-port=8285/udp firewall-cmd --permanent --add-port=8472/udp # NodePort Services firewall-cmd --permanent --add-port=30000-32767/tcp # Additional ports required for Kubernetes firewall-cmd --permanent --add-port=10255/tcp # Read-only Kubelet API firewall-cmd --permanent --add-port=6783/tcp # Flannel firewall-cmd --permanent --add-port=6783/udp # Flannel firewall-cmd --permanent --add-port=6784/udp # Flannel firewall-cmd --add-masquerade --permanent firewall-cmd --reload systemctl restart firewalld sudo systemctl enable rke2-agent.service mkdir -p /etc/rancher/rke2/ echo 'export PATH=$PATH:/var/lib/rancher/rke2/bin' >> ~/.bashrc source ~/.bashrc vim /etc/rancher/rke2/config.yaml After the server setup I noticed it took quite some time to come up. You can track progress with journalctl -u rke2-server -f . My logs looked like this: Nov 29 14:21:37 k8s-server.lan rke2[1016]: time=\"2023-11-29T14:21:37-05:00\" level=info msg=\"Pod for kube-apiserver not synced (waiting for termination of old pod sandbox), retrying\" Nov 29 14:21:38 k8s-server.lan rke2[1016]: time=\"2023-11-29T14:21:38-05:00\" level=info msg=\"Waiting to retrieve kube-proxy configuration; server is not ready: https://127.0.0.1:9345/v1-rke2/readyz: 500 Internal Server Error\" Nov 29 14:21:43 k8s-server.lan rke2[1016]: time=\"2023-11-29T14:21:43-05:00\" level=info msg=\"Waiting to retrieve kube-proxy configuration; server is not ready: https://127.0.0.1:9345/v1-rke2/readyz: 500 Internal Server Error\" Nov 29 14:21:48 k8s-server.lan rke2[1016]: time=\"2023-11-29T14:21:48-05:00\" level=info msg=\"Waiting to retrieve kube-proxy configuration; server is not ready: https://127.0.0.1:9345/v1-rke2/readyz: 500 Internal Server Error\" Nov 29 14:21:53 k8s-server.lan rke2[1016]: time=\"2023-11-29T14:21:53-05:00\" level=info msg=\"Waiting to retrieve kube-proxy configuration; server is not ready: https://127.0.0.1:9345/v1-rke2/readyz: 500 Internal Server Error\" Nov 29 14:21:57 k8s-server.lan rke2[1016]: time=\"2023-11-29T14:21:57-05:00\" level=info msg=\"Pod for etcd is synced\" Nov 29 14:21:57 k8s-server.lan rke2[1016]: time=\"2023-11-29T14:21:57-05:00\" level=info msg=\"Pod for kube-apiserver not synced (waiting for termination of old pod sandbox), retrying\" Nov 29 14:21:58 k8s-server.lan rke2[1016]: time=\"2023-11-29T14:21:58-05:00\" level=info msg=\"Waiting to retrieve kube-proxy configuration; server is not ready: https://127.0.0.1:9345/v1-rke2/readyz: 500 Internal Server Error\" Nov 29 14:22:03 k8s-server.lan rke2[1016]: time=\"2023-11-29T14:22:03-05:00\" level=info msg=\"Waiting to retrieve kube-proxy configuration; server is not ready: https://127.0.0.1:9345/v1-rke2/readyz: 500 Internal Server Error\" Nov 29 14:22:08 k8s-server.lan rke2[1016]: time=\"2023-11-29T14:22:08-05:00\" level=info msg=\"Waiting to retrieve kube-proxy configuration; server is not ready: https://127.0.0.1:9345/v1-rke2/readyz: 500 Internal Server Error\" Nov 29 14:22:13 k8s-server.lan rke2[1016]: time=\"2023-11-29T14:22:13-05:00\" level=info msg=\"Waiting to retrieve kube-proxy configuration; server is not ready: https://127.0.0.1:9345/v1-rke2/readyz: 500 Internal Server Error\" Nov 29 14:22:17 k8s-server.lan rke2[1016]: time=\"2023-11-29T14:22:17-05:00\" level=info msg=\"Pod for etcd is synced\" Nov 29 14:22:17 k8s-server.lan rke2[1016]: time=\"2023-11-29T14:22:17-05:00\" level=info msg=\"Pod for kube-apiserver is synced\" Nov 29 14:22:17 k8s-server.lan rke2[1016]: time=\"2023-11-29T14:22:17-05:00\" level=info msg=\"ETCD server is now running\" Nov 29 14:22:17 k8s-server.lan rke2[1016]: time=\"2023-11-29T14:22:17-05:00\" level=info msg=\"rke2 is up and running\" Nov 29 14:22:17 k8s-server.lan systemd[1]: Started Rancher Kubernetes Engine v2 (server). Nov 29 14:22:17 k8s-server.lan rke2[1016]: time=\"2023-11-29T14:22:17-05:00\" level=info msg=\"Failed to get existing traefik HelmChart\" error=\"helmcharts.helm.cattle.io \\\"traefik\\\" not found\" Nov 29 14:22:17 k8s-server.lan rke2[1016]: time=\"2023-11-29T14:22:17-05:00\" level=info msg=\"Reconciling ETCDSnapshotFile resources\" Nov 29 14:22:17 k8s-server.lan rke2[1016]: time=\"2023-11-29T14:22:17-05:00\" level=info msg=\"Tunnel server egress proxy mode: agent\" Nov 29 14:22:17 k8s-server.lan rke2[1016]: time=\"2023-11-29T14:22:17-05:00\" level=info msg=\"Starting managed etcd node metadata controller\" Nov 29 14:22:17 k8s-server.lan rke2[1016]: time=\"2023-11-29T14:22:17-05:00\" level=info msg=\"Reconciliation of ETCDSnapshotFile resources complete\" Nov 29 14:22:17 k8s-server.lan rke2[1016]: time=\"2023-11-29T14:22:17-05:00\" level=info msg=\"Starting k3s.cattle.io/v1, Kind=Addon controller\" You can see you get constant 500 errors until it eventually fixes itself. When everything has settled down make sure that you see nodes: [root@k8s-server bin]# kubectl get nodes NAME STATUS ROLES AGE VERSION k8s-agent1.lan Ready <none> 11m v1.26.10+rke2r2 k8s-server.lan Ready control-plane,etcd,master 60m v1.26.10+rke2r2","title":"Set Up a K8s Node"},{"location":"PowerScale%20-%20Configure%20with%20Kubernetes%20%28Incomplete%29/#install-helm","text":"cd /tmp wget https://get.helm.sh/helm-v3.13.2-linux-amd64.tar.gz # Update version as needed tar xzf helm-v3.13.2-linux-amd64.tar.gz sudo mv linux-amd64/helm /usr/local/bin/helm helm version","title":"Install Helm"},{"location":"PowerScale%20-%20Configure%20with%20Kubernetes%20%28Incomplete%29/#install-rancher","text":"helm repo add rancher-stable https://releases.rancher.com/server-charts/stable kubectl create namespace cattle-system helm install rancher rancher-stable/rancher --namespace cattle-system --set hostname=<your-rancher-hostname> --set bootstrapPassword=<your-admin-password> --set ingress.tls.source=rancher # YOU HAVE TO UPDATE THIS echo https://k8s-server.lan/dashboard/?setup=$(kubectl get secret --namespace cattle-system bootstrap-secret -o go-template='{{.data.bootstrapPassword|base64decode}}')","title":"Install Rancher"},{"location":"PowerScale%20-%20Configure%20with%20Kubernetes%20%28Incomplete%29/#troubleshooting-flannel-issues","text":"Warning: Sassy commentary ahead. I haven't built K8s from scratch since 2017 and I'm glad to see that getting flannel to work is still a huge mess. With that said, here's how to go about troubleshooting it when it inevitably fails. (Seriously, it has been 7 years, how is it still this bad?)","title":"Troubleshooting Flannel Issues"},{"location":"PowerScale%20-%20Configure%20with%20Kubernetes%20%28Incomplete%29/#rancher-woes","text":"My rancher install failed with no output from the installer. You can manually pull the logs by examining the rancher pod with kubectl logs -n cattle-system rancher-64cf6ddd96-2x2ms This got me: 2023/11/29 21:04:33 [ERROR] [updateClusterHealth] Failed to update cluster [local]: Internal error occurred: failed calling webhook \"rancher.cattle.io.clusters.management.cattle.io\": failed to call webhook: Post \"https://rancher-webhook.cattle-system.svc:443/v1/webhook/mutation/clusters.management.cattle.io?timeout=10s\": context deadline exceeded 2023/11/29 21:04:33 [ERROR] Failed to connect to peer wss://10.42.0.8/v3/connect [local ID=10.42.1.20]: dial tcp 10.42.0.8:443: connect: no route to host 2023/11/29 21:04:34 [ERROR] Failed to connect to peer wss://10.42.1.21/v3/connect [local ID=10.42.1.20]: dial tcp 10.42.1.21:443: connect: no route to host 2023/11/29 21:04:38 [ERROR] Failed to connect to peer wss://10.42.0.8/v3/connect [local ID=10.42.1.20]: dial tcp 10.42.0.8:443: connect: no route to host 2023/11/29 21:04:39 [ERROR] Failed to connect to peer wss://10.42.1.21/v3/connect [local ID=10.42.1.20]: dial tcp 10.42.1.21:443: connect: no route to host 2023/11/29 21:04:43 [ERROR] Failed to connect to peer wss://10.42.0.8/v3/connect [local ID=10.42.1.20]: dial tcp 10.42.0.8:443: connect: no route to host 2023/11/29 21:04:44 [ERROR] Failed to connect to peer wss://10.42.1.21/v3/connect [local ID=10.42.1.20]: dial tcp 10.42.1.21:443: connect: no route to host on repeat. 10.42.1.21 is an internal flannel address so the next step is to figure out who owns it with kubectl get pods --all-namespaces -o wide : [root@k8s-server ~]# kubectl get pods --all-namespaces -o wide NAMESPACE NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES cattle-fleet-system fleet-controller-56968b86b6-tctjr 1/1 Running 0 44m 10.42.1.24 k8s-agent1.lan <none> <none> cattle-fleet-system gitjob-7d68454468-bk7fh 1/1 Running 0 44m 10.42.1.25 k8s-agent1.lan <none> <none> cattle-provisioning-capi-system capi-controller-manager-6f87d6bd74-v489n 1/1 Running 0 41m 10.42.1.30 k8s-agent1.lan <none> <none> cattle-system helm-operation-64xf7 0/2 Completed 0 42m 10.42.1.29 k8s-agent1.lan <none> <none> cattle-system helm-operation-h88vn 1/2 Error 0 41m 10.42.1.34 k8s-agent1.lan <none> <none> cattle-system helm-operation-jndl9 1/2 Error 0 41m 10.42.1.33 k8s-agent1.lan <none> <none> cattle-system helm-operation-k757h 0/2 Completed 0 44m 10.42.1.23 k8s-agent1.lan <none> <none> cattle-system helm-operation-ldnkm 0/2 Completed 0 45m 10.42.1.22 k8s-agent1.lan <none> <none> cattle-system helm-operation-sv5ts 0/2 Completed 0 43m 10.42.1.28 k8s-agent1.lan <none> <none> cattle-system helm-operation-thct7 0/2 Completed 0 43m 10.42.1.27 k8s-agent1.lan <none> <none> cattle-system rancher-64cf6ddd96-2x2ms 1/1 Running 1 (45m ago) 46m 10.42.1.20 k8s-agent1.lan <none> <none> cattle-system rancher-64cf6ddd96-drrzr 1/1 Running 0 46m 10.42.0.8 k8s-server.lan <none> <none> cattle-system rancher-64cf6ddd96-qq64g 1/1 Running 0 46m 10.42.1.21 k8s-agent1.lan <none> <none> cattle-system rancher-webhook-58d68fb97d-b5sn8 1/1 Running 0 41m 10.42.1.32 k8s-agent1.lan <none> <none> cert-manager cert-manager-startupapicheck-fvp9t 0/1 Completed 1 52m 10.42.1.19 k8s-agent1.lan <none> <none> kube-system cloud-controller-manager-k8s-server.lan 1/1 Running 3 (105m ago) 139m 10.10.25.135 k8s-server.lan <none> <none> kube-system etcd-k8s-server.lan 1/1 Running 1 139m 10.10.25.135 k8s-server.lan <none> <none> kube-system helm-install-rke2-canal-k8b4d 0/1 Completed 0 139m 10.10.25.135 k8s-server.lan <none> <none> kube-system helm-install-rke2-coredns-f59dz 0/1 Completed 0 139m 10.10.25.135 k8s-server.lan <none> <none> kube-system helm-install-rke2-ingress-nginx-gpt7q 0/1 Completed 0 139m 10.42.0.2 k8s-server.lan <none> <none> kube-system helm-install-rke2-metrics-server-q9jwf 0/1 Completed 0 139m 10.42.0.6 k8s-server.lan <none> <none> kube-system helm-install-rke2-snapshot-controller-6pqpg 0/1 Completed 2 139m 10.42.0.4 k8s-server.lan <none> <none> kube-system helm-install-rke2-snapshot-controller-crd-k6klp 0/1 Completed 0 139m 10.42.0.10 k8s-server.lan <none> <none> kube-system helm-install-rke2-snapshot-validation-webhook-hrv5n 0/1 Completed 0 139m 10.42.0.3 k8s-server.lan <none> <none> kube-system kube-apiserver-k8s-server.lan 1/1 Running 1 139m 10.10.25.135 k8s-server.lan <none> <none> kube-system kube-controller-manager-k8s-server.lan 1/1 Running 2 (105m ago) 139m 10.10.25.135 k8s-server.lan <none> <none> kube-system kube-proxy-k8s-agent1.lan 1/1 Running 0 90m 10.10.25.136 k8s-agent1.lan <none> <none> kube-system kube-proxy-k8s-server.lan 1/1 Running 2 (104m ago) 103m 10.10.25.135 k8s-server.lan <none> <none> kube-system kube-scheduler-k8s-server.lan 1/1 Running 1 (105m ago) 139m 10.10.25.135 k8s-server.lan <none> <none> kube-system rke2-canal-7p5hz 2/2 Running 2 (105m ago) 139m 10.10.25.135 k8s-server.lan <none> <none> kube-system rke2-canal-9wg57 2/2 Running 0 90m 10.10.25.136 k8s-agent1.lan <none> <none> kube-system rke2-coredns-rke2-coredns-565dfc7d75-n96xs 1/1 Running 0 90m 10.42.1.2 k8s-agent1.lan <none> <none> kube-system rke2-coredns-rke2-coredns-565dfc7d75-xv92q 1/1 Running 1 (105m ago) 139m 10.42.0.3 k8s-server.lan <none> <none> kube-system rke2-coredns-rke2-coredns-autoscaler-6c48c95bf9-mh279 1/1 Running 1 (105m ago) 139m 10.42.0.2 k8s-server.lan <none> <none> kube-system rke2-ingress-nginx-controller-89d4c 1/1 Running 0 89m 10.42.1.3 k8s-agent1.lan <none> <none> kube-system rke2-ingress-nginx-controller-zctxb 1/1 Running 1 (105m ago) 139m 10.42.0.5 k8s-server.lan <none> <none> kube-system rke2-metrics-server-c9c78bd66-ndcxs 1/1 Running 1 (105m ago) 139m 10.42.0.4 k8s-server.lan <none> <none> kube-system rke2-snapshot-controller-6f7bbb497d-xfk9x 1/1 Running 1 (105m ago) 139m 10.42.0.6 k8s-server.lan <none> <none> kube-system rke2-snapshot-validation-webhook-65b5675d5c-sfqb2 1/1 Running 1 (105m ago) 139m 10.42.0.7 k8s-server.lan <none> <none> We can see that 10.42.0.8 and 10.42.1.21 are the two rancher containers which confirms for us that as per usual, flannel is not able to complete even its most basic of functions (VXLAN) successfully and its up to us to fix it. cattle-system rancher-64cf6ddd96-drrzr 1/1 Running 0 46m 10.42.0.8 k8s-server.lan <none> <none> cattle-system rancher-64cf6ddd96-qq64g 1/1 Running 0 46m 10.42.1.21 k8s-agent1.lan <none> <none> We can get shells in these containers with kubectl exec -it -n cattle-system rancher-64cf6ddd96-drrzr -- /bin/bash . I fished around in here and found nothing. Ultimately I tcpdumped the flannel network and discovered that we were missing some other specific ports it needed: [root@k8s-agent1 ~]# tcpdump -i flannel.1 dropped privs to tcpdump tcpdump: verbose output suppressed, use -v[v]... for full protocol decode listening on flannel.1, link-type EN10MB (Ethernet), snapshot length 262144 bytes 16:17:38.793476 IP 10.42.0.0.58822 > 10.42.1.32.tungsten-https: Flags [S], seq 3219974389, win 64860, options [mss 1410,sackOK,TS val 2506628321 ecr 0,nop,wscale 7], length 0 16:17:38.793509 IP k8s-agent1.lan > 10.42.0.0: ICMP host 10.42.1.32 unreachable - admin prohibited filter, length 68 16:17:39.143985 IP 10.42.0.0.41502 > 10.42.1.32.tungsten-https: Flags [S], seq 1965118956, win 64860, options [mss 1410,sackOK,TS val 2506628671 ecr 0,nop,wscale 7], length 0 16:17:39.144008 IP k8s-agent1.lan > 10.42.0.0: ICMP host 10.42.1.32 unreachable - admin prohibited filter, length 68 16:17:39.847986 IP 10.42.0.0.58822 > 10.42.1.32.tungsten-https: Flags [S], seq 3219974389, win 64860, options [mss 1410,sackOK,TS val 2506629375 ecr 0,nop,wscale 7], length 0 16:17:39.848008 IP k8s-agent1.lan > 10.42.0.0: ICMP host 10.42.1.32 unreachable - admin prohibited filter, length 68 16:17:40.679004 IP 10.42.0.0.47680 > 10.42.1.32.tungsten-https: Flags [S], seq 1291962979, win 64860, options [mss 1410,sackOK,TS val 2506630206 ecr 0,nop,wscale 7], length 0 16:17:40.679028 IP k8s-agent1.lan > 10.42.0.0: ICMP host 10.42.1.32 unreachable - admin prohibited filter, length 68 16:17:41.894997 IP 10.42.0.0.58822 > 10.42.1.32.tungsten-https: Flags [S], seq 3219974389, win 64860, options [mss 1410,sackOK,TS val 2506631422 ecr 0,nop,wscale 7], length 0 16:17:41.895024 IP k8s-agent1.lan > 10.42.0.0: ICMP host 10.42.1.32 unreachable - admin prohibited filter, length 68 16:17:42.727005 IP 10.42.0.0.54136 > 10.42.1.32.tungsten-https: Flags [S], seq 1383176303, win 64860, options [mss 1410,sackOK The moral of the story is that if you see that, there's probably a firewall port you've missed. Why isn't that built into the installer so you aren't manually looking up and opening more than a dozen ports I hear you asking? Great question.","title":"Rancher Woes"},{"location":"PowerScale%20Failed%20Authentication/","text":"PowerScale Failed Authentication PowerScale Failed Authentication Problem Summary Problem Details Expected Behavior Reproduction Demonstration Cluster Setup Rebuild Initial Setup Code for Testing Authentication Mechanisms Concepts Super Block Quorum How Do Session Teardowns Work? Problem Summary PowerScale OneOS inaccurately reports authentication failures when the number of concurrent sessions is exceeded. Problem Details If --concurrent-session-limit=LIMIT is set with isi auth settings global modify --concurrent-session-limit=15 and that limit is exceeded the logs will say: HTTP Error Log tail -f /var/log/apache2/webui_httpd_error.log 2023-11-28T17:39:39.572651+00:00 <18.3> grantcluster-1(id1) httpd[98700]: [auth_isilon:error] [pid 98700:tid 34421640960] [client 172.16.5.155:62570] (STATUS_ACCESS_DENIED (0xC0000022) HTTP error: 401) Failed issuing a new JWT from the JWT service., referer: https://10.10.25.80:8080 2023-11-28T17:39:39.572673+00:00 <18.3> grantcluster-1(id1) httpd[98700]: [auth_isilon:error] [pid 98700:tid 34421640960] [client 172.16.5.155:62570] (401) Unable to create session., referer: https://10.10.25.80:8080 ...SNIP... 2023-11-28T17:39:39.603718+00:00 <18.3> grantcluster-1(id1) httpd[98700]: [auth_isilon:error] [pid 98700:tid 34422848768] [client 172.16.5.155:62559] (STATUS_ACCESS_DENIED (0xC0000022) HTTP error: 401) Failed issuing a new JWT from the JWT service., referer: https://10.10.25.80:8080 2023-11-28T17:39:39.603728+00:00 <18.3> grantcluster-1(id1) httpd[98700]: [auth_isilon:error] [pid 98700:tid 34422848768] [client 172.16.5.155:62559] (401) Unable to create session., referer: https://10.10.25.80:8080 HTTP Access Log tail -f /var/log/apache2/webui_httpd_access.log 2023-11-28T17:41:43.101276+00:00 <19.6> grantcluster-1(id1) httpd[98697]: 172.16.5.155 - - [28/Nov/2023:17:41:43 +0000] \"POST /session/1/session HTTP/1.1\" 401 40 \"https://10.10.25.80:8080\" \"python-requests/2.28.1\" REST API Response Total Successful Sessions: 0 Authentication Failed: Status Code 401, Error: Unable to create session. While this is accurate, technically, it is extremely misleading and has lead to a substantial waste of resources investigating authentication failures when in reality the problem is that concurrent sessions was exceeded. Expected Behavior The errors in Problem Details are misleading to both technicians and users. If the number of concurrent sessions is exceeded both the logs and the API responses should reflect that the issue is that the concurrent sessions have been exceeded instead of reporting an authentication error even if generating JWT tokens is the actual product of exceeding concurrent sessions. The error message should make it so technicians resolve the problem without having to rely on developer support. Reproduction The below Python script will reproduce the problem. Replace the credentials with your PowerScale credentials and then run. It will generate 30 threads each of which will hold a session open for 10 seconds. If the number of concurrent sessions is below 30 it will fail. import requests import threading import time import urllib3 urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning) def session_cookie_authentication(ip_address, username, password): \"\"\" Authenticate using Session Cookie and return the session cookies if successful. \"\"\" base_url = f\"https://{ip_address}:8080\" session_url = f\"{base_url}/session/1/session\" credentials = {\"username\": username, \"password\": password, \"services\": [\"platform\", \"namespace\"]} headers = {\"Content-Type\": \"application/json\", \"Referer\": base_url} try: response = requests.post(session_url, headers=headers, json=credentials, verify=False) if response.status_code == 201 and 'isisessid' in response.cookies: return response.cookies else: error_message = response.json().get('message', 'No detailed error message provided.') return f\"Authentication Failed: Status Code {response.status_code}, Error: {error_message}\" except Exception as e: return f\"Error during Session Cookie Authentication: {e}\" def create_and_hold_session(ip_address, username, password, hold_time, results): \"\"\" Create a session and hold it open for a specified duration. \"\"\" session_result = session_cookie_authentication(ip_address, username, password) if isinstance(session_result, requests.cookies.RequestsCookieJar): time.sleep(hold_time) # Hold the session results.append(\"Session created and held successfully.\") else: results.append(session_result) def main(): ip_address = \"10.10.25.80\" # Replace with the actual IP address of the PowerScale username = \"root\" password = \"YOUR_PASSWORD\" hold_time = 10 # Hold time in seconds session_threads = [] results = [] # Create 30 concurrent sessions for _ in range(30): thread = threading.Thread(target=create_and_hold_session, args=(ip_address, username, password, hold_time, results)) thread.start() session_threads.append(thread) # Wait for all threads to complete for thread in session_threads: thread.join() # Analyze results and print summary success_count = results.count(\"Session created and held successfully.\") print(f\"Total Successful Sessions: {success_count}\") error_messages = set([result for result in results if result != \"Session created and held successfully.\"]) for error in error_messages: print(error) if __name__ == \"__main__\": main() Demonstration Confirm concurrent sessions is fixed at 15: grantcluster-1# isi auth settings global view Send NTLMv2: No Space Replacement: Workgroup: WORKGROUP Provider Hostname Lookup: disabled Alloc Retries: 5 User Object Cache Size: 47.68M On Disk Identity: native RPC Block Time: Now RPC Max Requests: 64 RPC Timeout: 30s Default LDAP TLS Revocation Check Level: none System GID Threshold: 80 System UID Threshold: 80 Min Mapped Rid: 2147483648 Group UID: 4294967292 Null GID: 4294967293 Null UID: 4294967293 Unknown GID: 4294967294 Unknown UID: 4294967294 Failed Login Delay Time: Now Concurrent Session Limit: 15 Now we run the above Python script: C:\\Users\\grant\\AppData\\Local\\Programs\\Python\\Python310\\python.exe \"C:\\Users\\grant\\Documents\\code\\grantcurell.github.io\\docs\\PowerScale Failed Authentication\\multiple_sessions_test.py\" Total Successful Sessions: 0 Authentication Failed: Status Code 401, Error: Unable to create session. Change the concurrent sessions to 31: grantcluster-1# isi auth settings global modify --concurrent-session-limit=31 grantcluster-1# isi auth settings global view Send NTLMv2: No Space Replacement: Workgroup: WORKGROUP Provider Hostname Lookup: disabled Alloc Retries: 5 User Object Cache Size: 47.68M On Disk Identity: native RPC Block Time: Now RPC Max Requests: 64 RPC Timeout: 30s Default LDAP TLS Revocation Check Level: none System GID Threshold: 80 System UID Threshold: 80 Min Mapped Rid: 2147483648 Group UID: 4294967292 Null GID: 4294967293 Null UID: 4294967293 Unknown GID: 4294967294 Unknown UID: 4294967294 Failed Login Delay Time: Now Concurrent Session Limit: 31 Rerun the script: C:\\Users\\grant\\AppData\\Local\\Programs\\Python\\Python310\\python.exe \"C:\\Users\\grant\\Documents\\code\\grantcurell.github.io\\docs\\PowerScale Failed Authentication\\multiple_sessions_test.py\" Total Successful Sessions: 30 Process finished with exit code 0 Cluster Setup Rebuild I hopped on an old cluster I used for testing and ran isi_reformat_node Initial Setup These are the settings I used for my build. Since I was building this in a lab I told it to use the internal IP addresses for external as well instead of making them separate sets. Configuration Item Value Cluster name grantcluster Encoding utf-8 int-a netmask 255.255.255.0 int-a IP ranges { 10.10.25.80-10.10.25.89 } int-a IP range { 10.10.25.80-10.10.25.89 } int-a gateway 10.10.25.1 SmartConnect zone name onefs DNS servers { 10.10.25.120 } Search domains { grant.lan, lan } After I joined the nodes together I confirmed they had a quorum: grantcluster-1# sysctl efs.gmp.has_quorum efs.gmp.has_quorum: 1 grantcluster-1# sysctl efs.gmp.has_super_block_quorum efs.gmp.has_super_block_quorum: 1 1 indicates success whereas 0 indicates that there is no quorum. Super Blocks are described here . Code for Testing Authentication Mechanisms I used this code to test the different authentication mechanisms to confirm valid credentials. Concepts Super Block Quorum Referred to as efs.gmp.has_super_block_quorum , is a property that ensures the file system's integrity by requiring more than half of the nodes in the cluster to be available and in agreement over the internal network. This quorum prevents data conflicts, such as conflicting versions of the same file if two groups of nodes become unsynchronized. If a node is unreachable, OneFS will separate it from the cluster, known as splitting. Operations can continue as long as a quorum of nodes remains connected. If the split nodes can reconnect and re-synchronize, they rejoin the majority group in a process known as merging. The superblock quorum status can be checked by connecting to a node via SSH and running the sysctl efs.gmp.has_super_block_quorum command-line tool as root. How Do Session Teardowns Work? See Session Teardown Reverse Engineering","title":"PowerScale Failed Authentication"},{"location":"PowerScale%20Failed%20Authentication/#powerscale-failed-authentication","text":"PowerScale Failed Authentication Problem Summary Problem Details Expected Behavior Reproduction Demonstration Cluster Setup Rebuild Initial Setup Code for Testing Authentication Mechanisms Concepts Super Block Quorum How Do Session Teardowns Work?","title":"PowerScale Failed Authentication"},{"location":"PowerScale%20Failed%20Authentication/#problem-summary","text":"PowerScale OneOS inaccurately reports authentication failures when the number of concurrent sessions is exceeded.","title":"Problem Summary"},{"location":"PowerScale%20Failed%20Authentication/#problem-details","text":"If --concurrent-session-limit=LIMIT is set with isi auth settings global modify --concurrent-session-limit=15 and that limit is exceeded the logs will say: HTTP Error Log tail -f /var/log/apache2/webui_httpd_error.log 2023-11-28T17:39:39.572651+00:00 <18.3> grantcluster-1(id1) httpd[98700]: [auth_isilon:error] [pid 98700:tid 34421640960] [client 172.16.5.155:62570] (STATUS_ACCESS_DENIED (0xC0000022) HTTP error: 401) Failed issuing a new JWT from the JWT service., referer: https://10.10.25.80:8080 2023-11-28T17:39:39.572673+00:00 <18.3> grantcluster-1(id1) httpd[98700]: [auth_isilon:error] [pid 98700:tid 34421640960] [client 172.16.5.155:62570] (401) Unable to create session., referer: https://10.10.25.80:8080 ...SNIP... 2023-11-28T17:39:39.603718+00:00 <18.3> grantcluster-1(id1) httpd[98700]: [auth_isilon:error] [pid 98700:tid 34422848768] [client 172.16.5.155:62559] (STATUS_ACCESS_DENIED (0xC0000022) HTTP error: 401) Failed issuing a new JWT from the JWT service., referer: https://10.10.25.80:8080 2023-11-28T17:39:39.603728+00:00 <18.3> grantcluster-1(id1) httpd[98700]: [auth_isilon:error] [pid 98700:tid 34422848768] [client 172.16.5.155:62559] (401) Unable to create session., referer: https://10.10.25.80:8080 HTTP Access Log tail -f /var/log/apache2/webui_httpd_access.log 2023-11-28T17:41:43.101276+00:00 <19.6> grantcluster-1(id1) httpd[98697]: 172.16.5.155 - - [28/Nov/2023:17:41:43 +0000] \"POST /session/1/session HTTP/1.1\" 401 40 \"https://10.10.25.80:8080\" \"python-requests/2.28.1\" REST API Response Total Successful Sessions: 0 Authentication Failed: Status Code 401, Error: Unable to create session. While this is accurate, technically, it is extremely misleading and has lead to a substantial waste of resources investigating authentication failures when in reality the problem is that concurrent sessions was exceeded.","title":"Problem Details"},{"location":"PowerScale%20Failed%20Authentication/#expected-behavior","text":"The errors in Problem Details are misleading to both technicians and users. If the number of concurrent sessions is exceeded both the logs and the API responses should reflect that the issue is that the concurrent sessions have been exceeded instead of reporting an authentication error even if generating JWT tokens is the actual product of exceeding concurrent sessions. The error message should make it so technicians resolve the problem without having to rely on developer support.","title":"Expected Behavior"},{"location":"PowerScale%20Failed%20Authentication/#reproduction","text":"The below Python script will reproduce the problem. Replace the credentials with your PowerScale credentials and then run. It will generate 30 threads each of which will hold a session open for 10 seconds. If the number of concurrent sessions is below 30 it will fail. import requests import threading import time import urllib3 urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning) def session_cookie_authentication(ip_address, username, password): \"\"\" Authenticate using Session Cookie and return the session cookies if successful. \"\"\" base_url = f\"https://{ip_address}:8080\" session_url = f\"{base_url}/session/1/session\" credentials = {\"username\": username, \"password\": password, \"services\": [\"platform\", \"namespace\"]} headers = {\"Content-Type\": \"application/json\", \"Referer\": base_url} try: response = requests.post(session_url, headers=headers, json=credentials, verify=False) if response.status_code == 201 and 'isisessid' in response.cookies: return response.cookies else: error_message = response.json().get('message', 'No detailed error message provided.') return f\"Authentication Failed: Status Code {response.status_code}, Error: {error_message}\" except Exception as e: return f\"Error during Session Cookie Authentication: {e}\" def create_and_hold_session(ip_address, username, password, hold_time, results): \"\"\" Create a session and hold it open for a specified duration. \"\"\" session_result = session_cookie_authentication(ip_address, username, password) if isinstance(session_result, requests.cookies.RequestsCookieJar): time.sleep(hold_time) # Hold the session results.append(\"Session created and held successfully.\") else: results.append(session_result) def main(): ip_address = \"10.10.25.80\" # Replace with the actual IP address of the PowerScale username = \"root\" password = \"YOUR_PASSWORD\" hold_time = 10 # Hold time in seconds session_threads = [] results = [] # Create 30 concurrent sessions for _ in range(30): thread = threading.Thread(target=create_and_hold_session, args=(ip_address, username, password, hold_time, results)) thread.start() session_threads.append(thread) # Wait for all threads to complete for thread in session_threads: thread.join() # Analyze results and print summary success_count = results.count(\"Session created and held successfully.\") print(f\"Total Successful Sessions: {success_count}\") error_messages = set([result for result in results if result != \"Session created and held successfully.\"]) for error in error_messages: print(error) if __name__ == \"__main__\": main()","title":"Reproduction"},{"location":"PowerScale%20Failed%20Authentication/#demonstration","text":"Confirm concurrent sessions is fixed at 15: grantcluster-1# isi auth settings global view Send NTLMv2: No Space Replacement: Workgroup: WORKGROUP Provider Hostname Lookup: disabled Alloc Retries: 5 User Object Cache Size: 47.68M On Disk Identity: native RPC Block Time: Now RPC Max Requests: 64 RPC Timeout: 30s Default LDAP TLS Revocation Check Level: none System GID Threshold: 80 System UID Threshold: 80 Min Mapped Rid: 2147483648 Group UID: 4294967292 Null GID: 4294967293 Null UID: 4294967293 Unknown GID: 4294967294 Unknown UID: 4294967294 Failed Login Delay Time: Now Concurrent Session Limit: 15 Now we run the above Python script: C:\\Users\\grant\\AppData\\Local\\Programs\\Python\\Python310\\python.exe \"C:\\Users\\grant\\Documents\\code\\grantcurell.github.io\\docs\\PowerScale Failed Authentication\\multiple_sessions_test.py\" Total Successful Sessions: 0 Authentication Failed: Status Code 401, Error: Unable to create session. Change the concurrent sessions to 31: grantcluster-1# isi auth settings global modify --concurrent-session-limit=31 grantcluster-1# isi auth settings global view Send NTLMv2: No Space Replacement: Workgroup: WORKGROUP Provider Hostname Lookup: disabled Alloc Retries: 5 User Object Cache Size: 47.68M On Disk Identity: native RPC Block Time: Now RPC Max Requests: 64 RPC Timeout: 30s Default LDAP TLS Revocation Check Level: none System GID Threshold: 80 System UID Threshold: 80 Min Mapped Rid: 2147483648 Group UID: 4294967292 Null GID: 4294967293 Null UID: 4294967293 Unknown GID: 4294967294 Unknown UID: 4294967294 Failed Login Delay Time: Now Concurrent Session Limit: 31 Rerun the script: C:\\Users\\grant\\AppData\\Local\\Programs\\Python\\Python310\\python.exe \"C:\\Users\\grant\\Documents\\code\\grantcurell.github.io\\docs\\PowerScale Failed Authentication\\multiple_sessions_test.py\" Total Successful Sessions: 30 Process finished with exit code 0","title":"Demonstration"},{"location":"PowerScale%20Failed%20Authentication/#cluster-setup","text":"","title":"Cluster Setup"},{"location":"PowerScale%20Failed%20Authentication/#rebuild","text":"I hopped on an old cluster I used for testing and ran isi_reformat_node","title":"Rebuild"},{"location":"PowerScale%20Failed%20Authentication/#initial-setup","text":"These are the settings I used for my build. Since I was building this in a lab I told it to use the internal IP addresses for external as well instead of making them separate sets. Configuration Item Value Cluster name grantcluster Encoding utf-8 int-a netmask 255.255.255.0 int-a IP ranges { 10.10.25.80-10.10.25.89 } int-a IP range { 10.10.25.80-10.10.25.89 } int-a gateway 10.10.25.1 SmartConnect zone name onefs DNS servers { 10.10.25.120 } Search domains { grant.lan, lan } After I joined the nodes together I confirmed they had a quorum: grantcluster-1# sysctl efs.gmp.has_quorum efs.gmp.has_quorum: 1 grantcluster-1# sysctl efs.gmp.has_super_block_quorum efs.gmp.has_super_block_quorum: 1 1 indicates success whereas 0 indicates that there is no quorum. Super Blocks are described here .","title":"Initial Setup"},{"location":"PowerScale%20Failed%20Authentication/#code-for-testing-authentication-mechanisms","text":"I used this code to test the different authentication mechanisms to confirm valid credentials.","title":"Code for Testing Authentication Mechanisms"},{"location":"PowerScale%20Failed%20Authentication/#concepts","text":"","title":"Concepts"},{"location":"PowerScale%20Failed%20Authentication/#super-block-quorum","text":"Referred to as efs.gmp.has_super_block_quorum , is a property that ensures the file system's integrity by requiring more than half of the nodes in the cluster to be available and in agreement over the internal network. This quorum prevents data conflicts, such as conflicting versions of the same file if two groups of nodes become unsynchronized. If a node is unreachable, OneFS will separate it from the cluster, known as splitting. Operations can continue as long as a quorum of nodes remains connected. If the split nodes can reconnect and re-synchronize, they rejoin the majority group in a process known as merging. The superblock quorum status can be checked by connecting to a node via SSH and running the sysctl efs.gmp.has_super_block_quorum command-line tool as root.","title":"Super Block Quorum"},{"location":"PowerScale%20Failed%20Authentication/#how-do-session-teardowns-work","text":"See Session Teardown Reverse Engineering","title":"How Do Session Teardowns Work?"},{"location":"PowerScale%20Failed%20Authentication/session_teardown_reverse_engineering/","text":"How Does Session Teardown Work? I received a question about how the PowerScale does session teardown so below I walk through a mid-level overview of what that looks like. PowerScale leverages Apache so let's first understand what session teardown looks like from the webserver perspective. Session Teardown in Apache A highly detailed technical explanation of how Apache server handles session teardown, leading to a 204 response, involves understanding both the HTTP protocol and the specific implementation of session management in Apache. HTTP Protocol and the 204 Response 204 No Content : In HTTP, a 204 status code indicates that the server has successfully processed the request, but is not returning any content. This is often used in situations where the server's response itself is not important, but the confirmation of successful processing is. Request Reception : When a request to terminate a session (typically a DELETE request) is received, Apache first parses and interprets the HTTP request. Session Identification : Apache identifies the session to be terminated. This is done with a session identifier. Session Management : Apache uses modules for session management. These modules are responsible for creating, maintaining, and destroying sessions. When a session teardown is requested, the relevant module locates the session in its storage (which could be memory, a database, etc.). Session Validation : Before proceeding with the teardown, Apache checks if the session exists and whether the client making the request has the right to terminate it. Resource Cleanup : Upon successful validation, Apache instructs the session management module to release any resources associated with the session. This includes things like freeing memory, deleting session data from any running applications, and revoking authentication tokens. Session Destruction : The session is then marked for destruction. This means it's effectively invalidated and cannot be used for further requests. Client Notification : After the session is terminated, Apache sends a response back to the client. If there is no additional content to return (which is typical for session teardown), a 204 No Content response is used. This response is merely an acknowledgment that the request was successfully processed and the session was terminated. Logging and Monitoring : Apache logs this interaction for administrative and security purposes. This could include information about the request, the client IP, the session identifier, and the outcome of the operation. To demonstrate this I wrote trace_teardown.py . It sets up a session with the PowerScale and then tears it down. Now we can't see the guts of the PowerScale with this code (I'll get to that) but we can see all the Apache-side handling. C:\\Users\\grant\\AppData\\Local\\Programs\\Python\\Python310\\python.exe \"C:\\Users\\grant\\Documents\\code\\grantcurell.github.io\\docs\\PowerScale Failed Authentication\\trace_teardown.py\" DEBUG:__main__:Attempting to authenticate and create a session... DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): 10.10.25.80:8080 DEBUG:urllib3.connectionpool:https://10.10.25.80:8080 \"POST /session/1/session HTTP/1.1\" 201 104 DEBUG:__main__:Authentication response: 201, {\"services\":[\"platform\",\"namespace\"],\"timeout_absolute\":14400,\"timeout_inactive\":900,\"username\":\"root\"} DEBUG:__main__:Session successfully created. Cookies: {'isicsrf': '25cd0346-4204-4c5d-be1d-49630c78c4ff', 'isisessid': 'eyJhbGciOiJQUzUxMiJ9.eyJhdWQiOlsicGxhdGZvcm0iLCJuYW1lc3BhY2UiXSwiZXhwIjoxNzAxMjA5NDM4LCJpYXQiOjE3MDExOTUwMzgsIm9uZWZzL2NzcmYiOiIyNWNkMDM0Ni00MjA0LTRjNWQtYmUxZC00OTYzMGM3OGM0ZmYiLCJvbmVmcy9pcCI6IjE3Mi4xNi41LjE1NSIsIm9uZWZzL25vbmNlIjozOTM4NzI5NDg2NTQ2MDY1NjU2LCJvbmVmcy9zZXNzaW9uIjoiZThkYzYyM2ItOWM5Yy00YmE3LWE2MjAtYzQzZTdkNzBhMzlhIiwib25lZnMvdWEiOiJweXRob24tcmVxdWVzdHMvMi4yOC4xIiwib25lZnMvemlkIjoxLCJzdWIiOiJyb290In0K.nnaDPBOw6ZSHT66mguImZZL57PsMkodUG9S2Nop0_B3r3oSwgX-1CkL4R70_oGRDMPztaskMOVSbc0YsvHXERI9IqEdE2jZaseIAZIOmUEaqPDgpjEZzkBMAiqsAcp9kFAxhDInZHzJobJn7kSa3RBKFD1rr6fi8_MljtennkX8IWpwPOWutkVMN6MNGM0YUAPPLD6qnQ1VcbFeYZU6unljhj7-n7eLrvoOfWprWjTh6vtT1jHF-Ecu_uD5Tue5IMkivsAEFnti5-TCas1qatmWYG2jlXrOoHEH7q0fEv5ZUWO6T-jGxGLZtp4E01EBJzudlaSfCZqTpL4JPaZOgbJEyFiQU2BQp7Ik0lRkLqTUO40f1lJDDnIK4xDbiN_4cIGowQjq0yTcKlu-FW9hIC2xGajoICIAuMBIJz9mEh4R_Gcvap_K1vPo796Hib3xyM-fgdeUzS3GwTVBuBWPczZQP2UMLmKSFeJuuMDbsB3_tu4V_xqnsmxWSyP-E_Btudwi75lEOJNnA7N-vrU1VK8IyBEsNU2TIJ4f_BjTE3gMvyRsk-BXayqmQ4u9PwPOTISLKGpDGuAAgYEryb9N48O_AKojglMcGEdonHLI-ep4ajWV3es6KWCmpvblQ2tpP8mTVhIRVi4xefYUG_ZCF23CrtOgoYur843lz7Mx5iS0%3D'} DEBUG:__main__:Attempting to close the session... DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): 10.10.25.80:8080 DEBUG:urllib3.connectionpool:https://10.10.25.80:8080 \"DELETE /session/1/session HTTP/1.1\" 204 0 DEBUG:__main__:Session closure response: 204, DEBUG:__main__:Session successfully closed. Here you can see the steps listed above play out to include visibility on both the session ID and the Cross-Site Request Forgery (CSRF) token. Now in addition to the above PowerScale is going to do a whole bunch of things internally and tracing each line of what happens would be an extremely long essay so we'll hit the highlights. Below is a copy of PowerScale's webui_http.conf which defines Apache's behaviors for any given thing. From this we have a very clear idea of how session teardown works on the PowerScale. The provided configuration file for PowerScale, which is leveraging Apache, offers insights into how Apache is configured and potentially how it handles sessions in this environment. Let\u2019s break down some key parts of the configuration related to session management and their implications: Session Authentication Modules : mod_auth_isilon - PowerScale's custom auth handler. This is handled by the shared library mod_auth_isilon.so . Now we don't have visibility into the code here, but realistically, these things follow design patterns that are pretty constant. If we were so inclined we could attach a debugger here, but the reality is we can make some pretty safe assumptions just based on how everyone does auth with Apache. The module is going to have all the code related to user authentication incluing the JWT tokens mentioned here README.md , it will handle session management and expiry, and it will likely make calls out to some other service for things like RBAC. IsiAuthSessionSecurity ip_check agent_check indicates additional security checks for session validation, like IP address and user-agent consistency. Worker Module Configuration : mpm_worker_module is used, which tells us it's a multi-threaded processing model. This module allows efficient handling of multiple concurrent connections, crucial for session management in a high-traffic environment. FastCGI for External Handlers : FastCGI ( mod_fastcgi ) is configured to communicate with external servers/processes like isi_papi_d , isi_rsapi_d , and isi_object_d . These external servers are responsible for handling other parts of the session lifecycle. Session-Specific Directives : <Location /session/1/session> tells us that session management (creation, validation, teardown) is handled by a specific handler ( SetHandler session-service ). IsiAuthSessionTimeoutInactive and IsiAuthSessionTimeoutAbsolute directives give session timeout settings Virtual Host Configuration : Within the <VirtualHost> section, you can see session-specific details. We see that SSL is enforced CSP is present There are a series of custom error documents which can be returned There are a lot of custom URL rewrites Specific URLs are handled by FastCGI . This includes all the APIs ErrorDocument Directives : Custom error documents for various HTTP status codes, including those that might be relevant to session handling (e.g., 401 Unauthorized). Understanding Session Teardown Apache itself, as configured here, doesn't directly reveal the mechanisms of session teardown. However, the combination of the worker module, SSL settings, custom authentication module, and external FastCGI processes suggests a complex session handling mechanism that is likely controlled both by Apache and additional PowerScale components. Teardown Process: Session Termination Request : A request to terminate a session (a DELETE request to the session endpoint) is received by Apache. Request Handling : Apache, through its worker module, accepts and processes the request Custom Authentication Module Processing : The mod_auth_isilon module, along with any session-related directives, validate the session and the request, ensuring that it's legitimate and authorized. Communication with External Handlers : If session management is partially offloaded to external processes (as seen in the FastCGI configuration), Apache forwards the relevant information to these processes. Session Invalidating and Cleanup : The responsible component (Apache module or external handler) invalidates the session, cleans up associated resources, and updates any necessary data stores. Response to Client : Once the session is successfully terminated, a response is sent back to the client. In the case of a successful teardown without further content, a 204 No Content response. Apache Config # X: ---------------- # X: This file is automatically generated and should not be # X: edited directly. If you must make changes to the # X: contents of this file it should be done via the PowerScale # X: Web UI, or via the template file located at # X: /etc/mcp/templates/webui_httpd.conf # X: ---------------- # ================================================= # Basic settings # ================================================= Listen 8080 https # ================================================= # Modules # ================================================= LoadModule unixd_module modules/mod_unixd.so LoadModule ssl_module modules/mod_ssl.so LoadModule authz_core_module modules/mod_authz_core.so LoadModule authn_core_module modules/mod_authn_core.so LoadModule authz_host_module modules/mod_authz_host.so LoadModule authz_user_module modules/mod_authz_user.so LoadModule mpm_worker_module modules/mod_mpm_worker.so LoadModule auth_isilon_module modules/mod_auth_isilon.so LoadModule cgid_module modules/mod_cgid.so LoadModule fastcgi_module modules/mod_fastcgi.so LoadModule alias_module modules/mod_alias.so LoadModule deflate_module modules/mod_deflate.so LoadModule dir_module modules/mod_dir.so LoadModule filter_module modules/mod_filter.so LoadModule headers_module modules/mod_headers.so LoadModule log_config_module modules/mod_log_config.so LoadModule mime_module modules/mod_mime.so LoadModule reqtimeout_module modules/mod_reqtimeout.so LoadModule rewrite_module modules/mod_rewrite.so LoadModule setenvif_module modules/mod_setenvif.so User daemon Group daemon UseCanonicalName On ServerRoot \"/usr/local/apache2\" DocumentRoot \"/usr/local/www/static\" PidFile \"/var/apache2/run/webui_httpd.pid\" Mutex flock:/var/apache2/run mpm-accept EnableMMAP Off EnableSendfile On KeepAlive On MaxKeepAliveRequests 500 KeepAliveTimeout 15 ## Implementing Clickjacking protection Header always set X-Frame-Options \"sameorigin\" ## MIME types advertised in the Content-Type headers should not be changed ## requires mod_headers.so (don't comment this line out) Header always set X-Content-Type-Options \"nosniff\" Header always set X-XSS-Protection \"1; mode=block\" #Hiding apache version in http header ServerTokens Prod ServerSignature Off ### Fix for CVE-2003-1567: HTTP TRACE / TRACK Methods Allowed ### TraceEnable off # Enable/Disable IsiAuthSessionSecurity agent_check and ip_check # IsiAuthSessionSecurity ip_check -> ip_check enabled # IsiAuthSessionSecurity -ip_check -> ip_check disabled # IsiAuthSessionSecurity agent_check -> agent_check enabled # IsiAuthSessionSecurity -agent_check -> agent_check disabled IsiAuthSessionSecurity ip_check agent_check # We must use a single process, multi-threaded server to correctly # cache user credentials (fds) across Platform API sessions. # StartServers: initial number of server processes to start # MaxRequestWorkers: maximum number of simultaneous client connections # MinSpareThreads: minimum number of worker threads which are kept spare # MaxSpareThreads: maximum number of worker threads which are kept spare # ThreadsPerChild: constant number of worker threads in each server process # MaxConnectionsPerChild: maximum number of requests a server process serves <IfModule mpm_worker_module> ServerLimit 1 StartServers 1 MaxRequestWorkers 64 MinSpareThreads 10 MaxSpareThreads 25 ThreadsPerChild 64 MaxConnectionsPerChild 0 </IfModule> # ================================================= # REQTIMEOUT # ================================================= <IfModule reqtimeout_module> RequestReadTimeout handshake=0 header=20-40,MinRate=500 body=20,MinRate=500 </IfModule> # ================================================= # Logging, Errors, User Agent # ================================================= LogFormat \"%h %l %u %t \\\"%r\\\" %>s %b \\\"%{Referer}i\\\" \\\"%{User-Agent}i\\\"\" combined # Log errors with syslog to /var/log/apache2/webui_httpd_error.log ErrorLog syslog:local2 LogLevel error ErrorDocument 500 /httpd/500Text.html ErrorDocument 400 /httpd/400Text.html SetEnvIf User-Agent \".*MSIE.*\" \\ nokeepalive ssl-unclean-shutdown \\ downgrade-1.0 force-response-1.0 # ================================================= # Platform API # ================================================= <IfModule fastcgi_module> FastCgiExternalServer /usr/sbin/isi_papi_d -socket /var/run/papi.sock -pass-cred -idle-timeout 600 </IfModule> # ================================================= # Remote-Service API # ================================================= <IfModule fastcgi_module> FastCgiExternalServer /usr/sbin/isi_rsapi_d -socket /var/run/isi_rsapi_d.sock -pass-cred -idle-timeout 600 </IfModule> # ================================================= # Object API # ================================================= <IfModule fastcgi_module> FastCgiExternalServer /usr/sbin/isi_object_d -pass-cred -idle-timeout 2147483647 -socket /var/run/isi_object_d.sock </IfModule> # ================================================= # Files # ================================================= AcceptFilter http none AcceptFilter https none <IfModule dir_module> DirectoryIndex isilonnoindex.none </IfModule> <Directory /> Options FollowSymLinks AllowOverride None Require all denied </Directory> <Directory \"/usr/local/www/static\"> Options Indexes FollowSymLinks MultiViews AllowOverride None <LimitExcept GET POST DELETE HEAD> Require all denied </LimitExcept> </Directory> <FilesMatch \"^\\.ht\"> Require all denied </FilesMatch> <IfModule mime_module> TypesConfig conf/mime.types AddEncoding gzip .gz AddEncoding x-compress .Z AddType application/x-compress .Z AddType application/x-gzip .gz .tgz </IfModule> # ================================================= # SSL # ================================================= <IfModule ssl_module> AddType application/x-x509-ca-cert .crt AddType application/x-pkcs7-crl .crl SSLPassPhraseDialog builtin #SSLSessionCache dbm:/var/run/ssl_scache SSLSessionCache none SSLSessionCacheTimeout 300 Mutex flock:/var/apache2/run ssl-cache SSLRandomSeed startup file:/dev/urandom 1024 SSLRandomSeed connect file:/dev/urandom 1024 SSLProtocol -all -TLSv1.1 +TLSv1.2 SSLCipherSuite ECDHE+aRSA+AES:DHE+aRSA+AES:ECDHE+ECDSA+AES:@STRENGTH SSLHonorCipherOrder on SSLCompression off SSLSessionTickets off SSLPassPhraseDialog exec:/etc/mcp/scripts/httpd_keypass.py SSLProxyCheckPeerCN off SSLProxyCheckPeerName off SSLProxyCheckPeerExpire off SSLOpenSSLConfCmd DHParameters /usr/local/apache24/conf/webui_dhparams.pem SSLOpenSSLConfCmd Curves prime256v1:secp384r1:secp521r1 </IfModule> # ================================================= # Platform API Virtual Hosts # ================================================= <VirtualHost _default_:8080> SSLEngine on SSLCertificateFile /ifs/.ifsvar/modules/isi_certs/system/server/zone_1/certs/794429aa484f2be3114356307ea2ff0bf004a8fbaa66e3e9c8647a923720f1ba.crt SSLCertificateKeyFile /ifs/.ifsvar/modules/isi_certs/system/server/zone_1/private/794429aa484f2be3114356307ea2ff0bf004a8fbaa66e3e9c8647a923720f1ba.key DocumentRoot \"/usr/local/www/static\" ServerAdmin support@isilon.com Header set Content-Security-Policy \"default-src 'self' 'unsafe-inline' 'unsafe-eval' data:; script-src 'self' 'unsafe-eval'; style-src 'unsafe-inline' 'self'; \" # Log access with syslog to /var/log/apache2/webui_httpd_access.log CustomLog \"|$ logger -t httpd -p local3.info\" combined AddOutputFilterByType DEFLATE text/css text/javascript application/javascript application/x-javascript BrowserMatch ^Mozilla/4 gzip-only-text/html BrowserMatch ^Mozilla/4\\.0[678] no-gzip BrowserMatch \\bMSIE !no-gzip !gzip-only-text/html AllowEncodedSlashes On ErrorDocument 503 /httpd/503Text.html ErrorDocument 400 /httpd/400WebUIText.html ErrorDocument 401 /httpd/401WebUIText.html ErrorDocument 403 /httpd/403WebUIText.html ErrorDocument 404 /httpd/404WebUIText.html ErrorDocument 405 /httpd/405WebUIText.html ErrorDocument 422 /httpd/422WebUIText.html ErrorDocument 423 /httpd/423WebUIText.html ErrorDocument 424 /httpd/424WebUIText.html ErrorDocument 500 /httpd/500WebUIText.html <IfModule rewrite_module> RewriteEngine On #Uncomment these lines to debug rewrite rules #RewriteLog \"/var/log/apache2/webui_httpd_error.log\" #RewriteLogLevel 9 # Redirect internal subrequests to an error to prevent them from being # redirected to another module and causing another authentication. RewriteCond %{IS_SUBREQ} t RewriteRule ^ - [G] # Restrict unnecessary http methods RewriteCond %{REQUEST_METHOD} ^(PATCH|OPTIONS|TRACK|CONNECT|TRACE|COPY|LINK|UNLINK|PURGE|LOCK|UNLOCK|PROPFIND|VIEW) RewriteRule .* - [L,R=405] RewriteCond %{REQUEST_METHOD} ^(PATCH|OPTIONS|TRACK|CONNECT|TRACE|COPY|LINK|UNLINK|PURGE|LOCK|UNLOCK|PROPFIND|VIEW) RewriteRule .* - [L,R=405] RewriteCond %{REQUEST_METHOD} ^(PATCH|OPTIONS|TRACK|CONNECT|TRACE|COPY|LINK|UNLINK|PURGE|LOCK|UNLOCK|PROPFIND|VIEW) RewriteRule .* - [L,R=405] # Older UI versions will redirect to (legacy) Login # after an upgrade. Force them into the new system RewriteRule ^/$ /v2/html/OneFS.html RewriteRule ^/Login.* /v2/html/OneFS.html RewriteRule ^/cloudpool_eula.* /v2/html/cloudpool_eula.txt RewriteRule ^/OneFS$ /v2/html/OneFS.html # Reroute legacy /Status to base URI RewriteRule ^/Status* https://%{SERVER_ADDR}:8080/ [L,R] RewriteCond %{DOCUMENT_ROOT}%{REQUEST_FILENAME}.gz -f RewriteRule (.*\\.(html|js|css))$ $1.gz [L] # Rewrite all request past /OneFS to v3 RewriteRule ^/OneFS/(.*) /v3/$1 # For v3 requests, if the file exists on disk, serve it RewriteCond %{REQUEST_FILENAME} ^/v3 [NC] RewriteCond %{DOCUMENT_ROOT}%{REQUEST_FILENAME}.gz -f RewriteRule (.*)$ $1.gz [L] RewriteCond %{REQUEST_FILENAME} ^/v3 [NC] RewriteCond %{DOCUMENT_ROOT}%{REQUEST_FILENAME} -f RewriteRule ^ - [L] # For all request to v3 that do no exists (like 404) return the index file so # the app can handle them RewriteRule ^/v3 /v3/index.html.gz [L] # Stop requests for static files RewriteRule ^/(json|v2|vasa-catalog|httpd)/.* $0 [L] RewriteRule ^/namespace(.*) /namespace$1 [PT] RewriteRule ^/object(.*) - [R=503] RewriteRule ^/webhdfs(.*) - [R=503] RewriteRule ^/imagetransfer(.*) - [R=503] RewriteRule ^/jmx(.*) - [R=503] RewriteRule ^/authenticate /session/1/session [P] RewriteRule ^/objectstore(.*) /object$1 [PT] RewriteRule ^/favicon.ico /v2/images/favicon.ico [L] RewriteRule ^/MIBs/(.*) \"/usr/share/snmp/mibs/$1\" [L] RewriteRule ^/MIBs-DEFs/(.*) \"/usr/share/snmp/defs/$1\" [L] RewriteRule ^/platform(.*) /platform$1 [PT] RewriteRule ^/remote-service(.*) /remote-service$1 [PT] RewriteRule ^/session(.*) /session$1 [PT] RewriteRule ^/mod_ssl:error:HTTP-request https://%{SERVER_ADDR}:8080/ [L,R] </IfModule> <FilesMatch .*\\.html.gz> ForceType text/html </FilesMatch> <FilesMatch .*\\.css.gz> ForceType text/css </FilesMatch> <FilesMatch .*\\.js.gz> ForceType application/javascript </FilesMatch> <Directory \"/usr/share/snmp/mibs\"> Options Indexes MultiViews ForceType application/octet-stream AllowOverride None Require all granted </Directory> <Directory \"/usr/share/snmp/defs\"> Options Indexes MultiViews ForceType application/octet-stream AllowOverride None Require all granted </Directory> <Location /webhdfs> RemoveEncoding .gz .Z </Location> Header always add Strict-Transport-Security: \"max-age=31536000;\" # ================================================= # Platform API # ================================================= Alias /platform /usr/sbin/isi_papi_d <Location /platform> AuthType Isilon IsiAuthName \"platform\" IsiAuthTypeBasic Off IsiAuthTypeSessionCookie On IsiDisabledZoneAllow Off IsiMultiZoneAllow On IsiCsrfCheck On Require valid-user SetHandler fastcgi-script Options +ExecCGI ErrorDocument 400 /httpd/400PAPIText.html ErrorDocument 401 /httpd/401PAPIText.html ErrorDocument 403 /httpd/403PAPIText.html ErrorDocument 404 /httpd/404PAPIText.html ErrorDocument 405 /httpd/405PAPIText.html ErrorDocument 422 /httpd/422PAPIText.html ErrorDocument 423 /httpd/423PAPIText.html ErrorDocument 424 /httpd/424PAPIText.html ErrorDocument 500 /httpd/500PAPIText.html </Location> <Location /session/1/session> SetHandler session-service IsiAuthServices platform remote-service namespace ForceType text/plain ErrorDocument 401 /json/401.json </Location> <Location /session/1/saml/logout/slostatus> SetHandler saml-logout-slo ErrorDocument 401 /json/401.json </Location> <Location /session/1/saml/logout/session> SetHandler saml-logout-session ErrorDocument 401 /json/401.json </Location> # Authentication is not required to access these resources <Location /platform/*/cluster/identity> IsiAuthIgnore GET </Location> <Location /platform/*/cluster/identity/> IsiAuthIgnore GET </Location> <Location /platform/*/cluster/brand> IsiAuthIgnore GET </Location> <Location /platform/*/auth/users/*/change-password> IsiAuthIgnore PUT </Location> <Location /platform/*/auth/providers/saml-services/settings> IsiAuthIgnore GET </Location> <Location /platform/*/auth/providers/saml-services> IsiAuthIgnore GET </Location> <Location /platform/*/upgrade/cluster/mixed-mode> IsiAuthIgnore GET </Location> <Location /platform/*/cluster/version> IsiAuthIgnore GET </Location> # ================================================= # Object API # ================================================= Alias /namespace /usr/sbin/isi_object_d <Location /namespace> AuthType Isilon IsiAuthName \"namespace\" IsiAuthTypeBasic Off IsiAuthTypeSessionCookie On IsiDisabledZoneAllow Off IsiMultiZoneAllow On IsiCsrfCheck On Require valid-user SetHandler fastcgi-script Options +ExecCGI ErrorDocument 401 /json/401.json Header set Content-Security-Policy \"default-src 'none'\" </Location> # ================================================= # Remote-Service API # ================================================= Alias /remote-service /usr/sbin/isi_rsapi_d <Location /remote-service> AuthType Isilon IsiAuthName \"remote-service\" IsiAuthTypeBasic Off IsiAuthTypeSessionCookie On Require valid-user SetHandler fastcgi-script Options +ExecCGI ErrorDocument 401 /json/401.json </Location> # Session timeouts IsiAuthSessionTimeoutInactive 900 IsiAuthSessionTimeoutAbsolute 14400 Timeout 500 </VirtualHost>","title":"How Does Session Teardown Work?"},{"location":"PowerScale%20Failed%20Authentication/session_teardown_reverse_engineering/#how-does-session-teardown-work","text":"I received a question about how the PowerScale does session teardown so below I walk through a mid-level overview of what that looks like. PowerScale leverages Apache so let's first understand what session teardown looks like from the webserver perspective.","title":"How Does Session Teardown Work?"},{"location":"PowerScale%20Failed%20Authentication/session_teardown_reverse_engineering/#session-teardown-in-apache","text":"A highly detailed technical explanation of how Apache server handles session teardown, leading to a 204 response, involves understanding both the HTTP protocol and the specific implementation of session management in Apache.","title":"Session Teardown in Apache"},{"location":"PowerScale%20Failed%20Authentication/session_teardown_reverse_engineering/#http-protocol-and-the-204-response","text":"204 No Content : In HTTP, a 204 status code indicates that the server has successfully processed the request, but is not returning any content. This is often used in situations where the server's response itself is not important, but the confirmation of successful processing is. Request Reception : When a request to terminate a session (typically a DELETE request) is received, Apache first parses and interprets the HTTP request. Session Identification : Apache identifies the session to be terminated. This is done with a session identifier. Session Management : Apache uses modules for session management. These modules are responsible for creating, maintaining, and destroying sessions. When a session teardown is requested, the relevant module locates the session in its storage (which could be memory, a database, etc.). Session Validation : Before proceeding with the teardown, Apache checks if the session exists and whether the client making the request has the right to terminate it. Resource Cleanup : Upon successful validation, Apache instructs the session management module to release any resources associated with the session. This includes things like freeing memory, deleting session data from any running applications, and revoking authentication tokens. Session Destruction : The session is then marked for destruction. This means it's effectively invalidated and cannot be used for further requests. Client Notification : After the session is terminated, Apache sends a response back to the client. If there is no additional content to return (which is typical for session teardown), a 204 No Content response is used. This response is merely an acknowledgment that the request was successfully processed and the session was terminated. Logging and Monitoring : Apache logs this interaction for administrative and security purposes. This could include information about the request, the client IP, the session identifier, and the outcome of the operation. To demonstrate this I wrote trace_teardown.py . It sets up a session with the PowerScale and then tears it down. Now we can't see the guts of the PowerScale with this code (I'll get to that) but we can see all the Apache-side handling. C:\\Users\\grant\\AppData\\Local\\Programs\\Python\\Python310\\python.exe \"C:\\Users\\grant\\Documents\\code\\grantcurell.github.io\\docs\\PowerScale Failed Authentication\\trace_teardown.py\" DEBUG:__main__:Attempting to authenticate and create a session... DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): 10.10.25.80:8080 DEBUG:urllib3.connectionpool:https://10.10.25.80:8080 \"POST /session/1/session HTTP/1.1\" 201 104 DEBUG:__main__:Authentication response: 201, {\"services\":[\"platform\",\"namespace\"],\"timeout_absolute\":14400,\"timeout_inactive\":900,\"username\":\"root\"} DEBUG:__main__:Session successfully created. Cookies: {'isicsrf': '25cd0346-4204-4c5d-be1d-49630c78c4ff', 'isisessid': 'eyJhbGciOiJQUzUxMiJ9.eyJhdWQiOlsicGxhdGZvcm0iLCJuYW1lc3BhY2UiXSwiZXhwIjoxNzAxMjA5NDM4LCJpYXQiOjE3MDExOTUwMzgsIm9uZWZzL2NzcmYiOiIyNWNkMDM0Ni00MjA0LTRjNWQtYmUxZC00OTYzMGM3OGM0ZmYiLCJvbmVmcy9pcCI6IjE3Mi4xNi41LjE1NSIsIm9uZWZzL25vbmNlIjozOTM4NzI5NDg2NTQ2MDY1NjU2LCJvbmVmcy9zZXNzaW9uIjoiZThkYzYyM2ItOWM5Yy00YmE3LWE2MjAtYzQzZTdkNzBhMzlhIiwib25lZnMvdWEiOiJweXRob24tcmVxdWVzdHMvMi4yOC4xIiwib25lZnMvemlkIjoxLCJzdWIiOiJyb290In0K.nnaDPBOw6ZSHT66mguImZZL57PsMkodUG9S2Nop0_B3r3oSwgX-1CkL4R70_oGRDMPztaskMOVSbc0YsvHXERI9IqEdE2jZaseIAZIOmUEaqPDgpjEZzkBMAiqsAcp9kFAxhDInZHzJobJn7kSa3RBKFD1rr6fi8_MljtennkX8IWpwPOWutkVMN6MNGM0YUAPPLD6qnQ1VcbFeYZU6unljhj7-n7eLrvoOfWprWjTh6vtT1jHF-Ecu_uD5Tue5IMkivsAEFnti5-TCas1qatmWYG2jlXrOoHEH7q0fEv5ZUWO6T-jGxGLZtp4E01EBJzudlaSfCZqTpL4JPaZOgbJEyFiQU2BQp7Ik0lRkLqTUO40f1lJDDnIK4xDbiN_4cIGowQjq0yTcKlu-FW9hIC2xGajoICIAuMBIJz9mEh4R_Gcvap_K1vPo796Hib3xyM-fgdeUzS3GwTVBuBWPczZQP2UMLmKSFeJuuMDbsB3_tu4V_xqnsmxWSyP-E_Btudwi75lEOJNnA7N-vrU1VK8IyBEsNU2TIJ4f_BjTE3gMvyRsk-BXayqmQ4u9PwPOTISLKGpDGuAAgYEryb9N48O_AKojglMcGEdonHLI-ep4ajWV3es6KWCmpvblQ2tpP8mTVhIRVi4xefYUG_ZCF23CrtOgoYur843lz7Mx5iS0%3D'} DEBUG:__main__:Attempting to close the session... DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): 10.10.25.80:8080 DEBUG:urllib3.connectionpool:https://10.10.25.80:8080 \"DELETE /session/1/session HTTP/1.1\" 204 0 DEBUG:__main__:Session closure response: 204, DEBUG:__main__:Session successfully closed. Here you can see the steps listed above play out to include visibility on both the session ID and the Cross-Site Request Forgery (CSRF) token. Now in addition to the above PowerScale is going to do a whole bunch of things internally and tracing each line of what happens would be an extremely long essay so we'll hit the highlights. Below is a copy of PowerScale's webui_http.conf which defines Apache's behaviors for any given thing. From this we have a very clear idea of how session teardown works on the PowerScale. The provided configuration file for PowerScale, which is leveraging Apache, offers insights into how Apache is configured and potentially how it handles sessions in this environment. Let\u2019s break down some key parts of the configuration related to session management and their implications: Session Authentication Modules : mod_auth_isilon - PowerScale's custom auth handler. This is handled by the shared library mod_auth_isilon.so . Now we don't have visibility into the code here, but realistically, these things follow design patterns that are pretty constant. If we were so inclined we could attach a debugger here, but the reality is we can make some pretty safe assumptions just based on how everyone does auth with Apache. The module is going to have all the code related to user authentication incluing the JWT tokens mentioned here README.md , it will handle session management and expiry, and it will likely make calls out to some other service for things like RBAC. IsiAuthSessionSecurity ip_check agent_check indicates additional security checks for session validation, like IP address and user-agent consistency. Worker Module Configuration : mpm_worker_module is used, which tells us it's a multi-threaded processing model. This module allows efficient handling of multiple concurrent connections, crucial for session management in a high-traffic environment. FastCGI for External Handlers : FastCGI ( mod_fastcgi ) is configured to communicate with external servers/processes like isi_papi_d , isi_rsapi_d , and isi_object_d . These external servers are responsible for handling other parts of the session lifecycle. Session-Specific Directives : <Location /session/1/session> tells us that session management (creation, validation, teardown) is handled by a specific handler ( SetHandler session-service ). IsiAuthSessionTimeoutInactive and IsiAuthSessionTimeoutAbsolute directives give session timeout settings Virtual Host Configuration : Within the <VirtualHost> section, you can see session-specific details. We see that SSL is enforced CSP is present There are a series of custom error documents which can be returned There are a lot of custom URL rewrites Specific URLs are handled by FastCGI . This includes all the APIs ErrorDocument Directives : Custom error documents for various HTTP status codes, including those that might be relevant to session handling (e.g., 401 Unauthorized).","title":"HTTP Protocol and the 204 Response"},{"location":"PowerScale%20Failed%20Authentication/session_teardown_reverse_engineering/#understanding-session-teardown","text":"Apache itself, as configured here, doesn't directly reveal the mechanisms of session teardown. However, the combination of the worker module, SSL settings, custom authentication module, and external FastCGI processes suggests a complex session handling mechanism that is likely controlled both by Apache and additional PowerScale components.","title":"Understanding Session Teardown"},{"location":"PowerScale%20Failed%20Authentication/session_teardown_reverse_engineering/#teardown-process","text":"Session Termination Request : A request to terminate a session (a DELETE request to the session endpoint) is received by Apache. Request Handling : Apache, through its worker module, accepts and processes the request Custom Authentication Module Processing : The mod_auth_isilon module, along with any session-related directives, validate the session and the request, ensuring that it's legitimate and authorized. Communication with External Handlers : If session management is partially offloaded to external processes (as seen in the FastCGI configuration), Apache forwards the relevant information to these processes. Session Invalidating and Cleanup : The responsible component (Apache module or external handler) invalidates the session, cleans up associated resources, and updates any necessary data stores. Response to Client : Once the session is successfully terminated, a response is sent back to the client. In the case of a successful teardown without further content, a 204 No Content response.","title":"Teardown Process:"},{"location":"PowerScale%20Failed%20Authentication/session_teardown_reverse_engineering/#apache-config","text":"# X: ---------------- # X: This file is automatically generated and should not be # X: edited directly. If you must make changes to the # X: contents of this file it should be done via the PowerScale # X: Web UI, or via the template file located at # X: /etc/mcp/templates/webui_httpd.conf # X: ---------------- # ================================================= # Basic settings # ================================================= Listen 8080 https # ================================================= # Modules # ================================================= LoadModule unixd_module modules/mod_unixd.so LoadModule ssl_module modules/mod_ssl.so LoadModule authz_core_module modules/mod_authz_core.so LoadModule authn_core_module modules/mod_authn_core.so LoadModule authz_host_module modules/mod_authz_host.so LoadModule authz_user_module modules/mod_authz_user.so LoadModule mpm_worker_module modules/mod_mpm_worker.so LoadModule auth_isilon_module modules/mod_auth_isilon.so LoadModule cgid_module modules/mod_cgid.so LoadModule fastcgi_module modules/mod_fastcgi.so LoadModule alias_module modules/mod_alias.so LoadModule deflate_module modules/mod_deflate.so LoadModule dir_module modules/mod_dir.so LoadModule filter_module modules/mod_filter.so LoadModule headers_module modules/mod_headers.so LoadModule log_config_module modules/mod_log_config.so LoadModule mime_module modules/mod_mime.so LoadModule reqtimeout_module modules/mod_reqtimeout.so LoadModule rewrite_module modules/mod_rewrite.so LoadModule setenvif_module modules/mod_setenvif.so User daemon Group daemon UseCanonicalName On ServerRoot \"/usr/local/apache2\" DocumentRoot \"/usr/local/www/static\" PidFile \"/var/apache2/run/webui_httpd.pid\" Mutex flock:/var/apache2/run mpm-accept EnableMMAP Off EnableSendfile On KeepAlive On MaxKeepAliveRequests 500 KeepAliveTimeout 15 ## Implementing Clickjacking protection Header always set X-Frame-Options \"sameorigin\" ## MIME types advertised in the Content-Type headers should not be changed ## requires mod_headers.so (don't comment this line out) Header always set X-Content-Type-Options \"nosniff\" Header always set X-XSS-Protection \"1; mode=block\" #Hiding apache version in http header ServerTokens Prod ServerSignature Off ### Fix for CVE-2003-1567: HTTP TRACE / TRACK Methods Allowed ### TraceEnable off # Enable/Disable IsiAuthSessionSecurity agent_check and ip_check # IsiAuthSessionSecurity ip_check -> ip_check enabled # IsiAuthSessionSecurity -ip_check -> ip_check disabled # IsiAuthSessionSecurity agent_check -> agent_check enabled # IsiAuthSessionSecurity -agent_check -> agent_check disabled IsiAuthSessionSecurity ip_check agent_check # We must use a single process, multi-threaded server to correctly # cache user credentials (fds) across Platform API sessions. # StartServers: initial number of server processes to start # MaxRequestWorkers: maximum number of simultaneous client connections # MinSpareThreads: minimum number of worker threads which are kept spare # MaxSpareThreads: maximum number of worker threads which are kept spare # ThreadsPerChild: constant number of worker threads in each server process # MaxConnectionsPerChild: maximum number of requests a server process serves <IfModule mpm_worker_module> ServerLimit 1 StartServers 1 MaxRequestWorkers 64 MinSpareThreads 10 MaxSpareThreads 25 ThreadsPerChild 64 MaxConnectionsPerChild 0 </IfModule> # ================================================= # REQTIMEOUT # ================================================= <IfModule reqtimeout_module> RequestReadTimeout handshake=0 header=20-40,MinRate=500 body=20,MinRate=500 </IfModule> # ================================================= # Logging, Errors, User Agent # ================================================= LogFormat \"%h %l %u %t \\\"%r\\\" %>s %b \\\"%{Referer}i\\\" \\\"%{User-Agent}i\\\"\" combined # Log errors with syslog to /var/log/apache2/webui_httpd_error.log ErrorLog syslog:local2 LogLevel error ErrorDocument 500 /httpd/500Text.html ErrorDocument 400 /httpd/400Text.html SetEnvIf User-Agent \".*MSIE.*\" \\ nokeepalive ssl-unclean-shutdown \\ downgrade-1.0 force-response-1.0 # ================================================= # Platform API # ================================================= <IfModule fastcgi_module> FastCgiExternalServer /usr/sbin/isi_papi_d -socket /var/run/papi.sock -pass-cred -idle-timeout 600 </IfModule> # ================================================= # Remote-Service API # ================================================= <IfModule fastcgi_module> FastCgiExternalServer /usr/sbin/isi_rsapi_d -socket /var/run/isi_rsapi_d.sock -pass-cred -idle-timeout 600 </IfModule> # ================================================= # Object API # ================================================= <IfModule fastcgi_module> FastCgiExternalServer /usr/sbin/isi_object_d -pass-cred -idle-timeout 2147483647 -socket /var/run/isi_object_d.sock </IfModule> # ================================================= # Files # ================================================= AcceptFilter http none AcceptFilter https none <IfModule dir_module> DirectoryIndex isilonnoindex.none </IfModule> <Directory /> Options FollowSymLinks AllowOverride None Require all denied </Directory> <Directory \"/usr/local/www/static\"> Options Indexes FollowSymLinks MultiViews AllowOverride None <LimitExcept GET POST DELETE HEAD> Require all denied </LimitExcept> </Directory> <FilesMatch \"^\\.ht\"> Require all denied </FilesMatch> <IfModule mime_module> TypesConfig conf/mime.types AddEncoding gzip .gz AddEncoding x-compress .Z AddType application/x-compress .Z AddType application/x-gzip .gz .tgz </IfModule> # ================================================= # SSL # ================================================= <IfModule ssl_module> AddType application/x-x509-ca-cert .crt AddType application/x-pkcs7-crl .crl SSLPassPhraseDialog builtin #SSLSessionCache dbm:/var/run/ssl_scache SSLSessionCache none SSLSessionCacheTimeout 300 Mutex flock:/var/apache2/run ssl-cache SSLRandomSeed startup file:/dev/urandom 1024 SSLRandomSeed connect file:/dev/urandom 1024 SSLProtocol -all -TLSv1.1 +TLSv1.2 SSLCipherSuite ECDHE+aRSA+AES:DHE+aRSA+AES:ECDHE+ECDSA+AES:@STRENGTH SSLHonorCipherOrder on SSLCompression off SSLSessionTickets off SSLPassPhraseDialog exec:/etc/mcp/scripts/httpd_keypass.py SSLProxyCheckPeerCN off SSLProxyCheckPeerName off SSLProxyCheckPeerExpire off SSLOpenSSLConfCmd DHParameters /usr/local/apache24/conf/webui_dhparams.pem SSLOpenSSLConfCmd Curves prime256v1:secp384r1:secp521r1 </IfModule> # ================================================= # Platform API Virtual Hosts # ================================================= <VirtualHost _default_:8080> SSLEngine on SSLCertificateFile /ifs/.ifsvar/modules/isi_certs/system/server/zone_1/certs/794429aa484f2be3114356307ea2ff0bf004a8fbaa66e3e9c8647a923720f1ba.crt SSLCertificateKeyFile /ifs/.ifsvar/modules/isi_certs/system/server/zone_1/private/794429aa484f2be3114356307ea2ff0bf004a8fbaa66e3e9c8647a923720f1ba.key DocumentRoot \"/usr/local/www/static\" ServerAdmin support@isilon.com Header set Content-Security-Policy \"default-src 'self' 'unsafe-inline' 'unsafe-eval' data:; script-src 'self' 'unsafe-eval'; style-src 'unsafe-inline' 'self'; \" # Log access with syslog to /var/log/apache2/webui_httpd_access.log CustomLog \"|$ logger -t httpd -p local3.info\" combined AddOutputFilterByType DEFLATE text/css text/javascript application/javascript application/x-javascript BrowserMatch ^Mozilla/4 gzip-only-text/html BrowserMatch ^Mozilla/4\\.0[678] no-gzip BrowserMatch \\bMSIE !no-gzip !gzip-only-text/html AllowEncodedSlashes On ErrorDocument 503 /httpd/503Text.html ErrorDocument 400 /httpd/400WebUIText.html ErrorDocument 401 /httpd/401WebUIText.html ErrorDocument 403 /httpd/403WebUIText.html ErrorDocument 404 /httpd/404WebUIText.html ErrorDocument 405 /httpd/405WebUIText.html ErrorDocument 422 /httpd/422WebUIText.html ErrorDocument 423 /httpd/423WebUIText.html ErrorDocument 424 /httpd/424WebUIText.html ErrorDocument 500 /httpd/500WebUIText.html <IfModule rewrite_module> RewriteEngine On #Uncomment these lines to debug rewrite rules #RewriteLog \"/var/log/apache2/webui_httpd_error.log\" #RewriteLogLevel 9 # Redirect internal subrequests to an error to prevent them from being # redirected to another module and causing another authentication. RewriteCond %{IS_SUBREQ} t RewriteRule ^ - [G] # Restrict unnecessary http methods RewriteCond %{REQUEST_METHOD} ^(PATCH|OPTIONS|TRACK|CONNECT|TRACE|COPY|LINK|UNLINK|PURGE|LOCK|UNLOCK|PROPFIND|VIEW) RewriteRule .* - [L,R=405] RewriteCond %{REQUEST_METHOD} ^(PATCH|OPTIONS|TRACK|CONNECT|TRACE|COPY|LINK|UNLINK|PURGE|LOCK|UNLOCK|PROPFIND|VIEW) RewriteRule .* - [L,R=405] RewriteCond %{REQUEST_METHOD} ^(PATCH|OPTIONS|TRACK|CONNECT|TRACE|COPY|LINK|UNLINK|PURGE|LOCK|UNLOCK|PROPFIND|VIEW) RewriteRule .* - [L,R=405] # Older UI versions will redirect to (legacy) Login # after an upgrade. Force them into the new system RewriteRule ^/$ /v2/html/OneFS.html RewriteRule ^/Login.* /v2/html/OneFS.html RewriteRule ^/cloudpool_eula.* /v2/html/cloudpool_eula.txt RewriteRule ^/OneFS$ /v2/html/OneFS.html # Reroute legacy /Status to base URI RewriteRule ^/Status* https://%{SERVER_ADDR}:8080/ [L,R] RewriteCond %{DOCUMENT_ROOT}%{REQUEST_FILENAME}.gz -f RewriteRule (.*\\.(html|js|css))$ $1.gz [L] # Rewrite all request past /OneFS to v3 RewriteRule ^/OneFS/(.*) /v3/$1 # For v3 requests, if the file exists on disk, serve it RewriteCond %{REQUEST_FILENAME} ^/v3 [NC] RewriteCond %{DOCUMENT_ROOT}%{REQUEST_FILENAME}.gz -f RewriteRule (.*)$ $1.gz [L] RewriteCond %{REQUEST_FILENAME} ^/v3 [NC] RewriteCond %{DOCUMENT_ROOT}%{REQUEST_FILENAME} -f RewriteRule ^ - [L] # For all request to v3 that do no exists (like 404) return the index file so # the app can handle them RewriteRule ^/v3 /v3/index.html.gz [L] # Stop requests for static files RewriteRule ^/(json|v2|vasa-catalog|httpd)/.* $0 [L] RewriteRule ^/namespace(.*) /namespace$1 [PT] RewriteRule ^/object(.*) - [R=503] RewriteRule ^/webhdfs(.*) - [R=503] RewriteRule ^/imagetransfer(.*) - [R=503] RewriteRule ^/jmx(.*) - [R=503] RewriteRule ^/authenticate /session/1/session [P] RewriteRule ^/objectstore(.*) /object$1 [PT] RewriteRule ^/favicon.ico /v2/images/favicon.ico [L] RewriteRule ^/MIBs/(.*) \"/usr/share/snmp/mibs/$1\" [L] RewriteRule ^/MIBs-DEFs/(.*) \"/usr/share/snmp/defs/$1\" [L] RewriteRule ^/platform(.*) /platform$1 [PT] RewriteRule ^/remote-service(.*) /remote-service$1 [PT] RewriteRule ^/session(.*) /session$1 [PT] RewriteRule ^/mod_ssl:error:HTTP-request https://%{SERVER_ADDR}:8080/ [L,R] </IfModule> <FilesMatch .*\\.html.gz> ForceType text/html </FilesMatch> <FilesMatch .*\\.css.gz> ForceType text/css </FilesMatch> <FilesMatch .*\\.js.gz> ForceType application/javascript </FilesMatch> <Directory \"/usr/share/snmp/mibs\"> Options Indexes MultiViews ForceType application/octet-stream AllowOverride None Require all granted </Directory> <Directory \"/usr/share/snmp/defs\"> Options Indexes MultiViews ForceType application/octet-stream AllowOverride None Require all granted </Directory> <Location /webhdfs> RemoveEncoding .gz .Z </Location> Header always add Strict-Transport-Security: \"max-age=31536000;\" # ================================================= # Platform API # ================================================= Alias /platform /usr/sbin/isi_papi_d <Location /platform> AuthType Isilon IsiAuthName \"platform\" IsiAuthTypeBasic Off IsiAuthTypeSessionCookie On IsiDisabledZoneAllow Off IsiMultiZoneAllow On IsiCsrfCheck On Require valid-user SetHandler fastcgi-script Options +ExecCGI ErrorDocument 400 /httpd/400PAPIText.html ErrorDocument 401 /httpd/401PAPIText.html ErrorDocument 403 /httpd/403PAPIText.html ErrorDocument 404 /httpd/404PAPIText.html ErrorDocument 405 /httpd/405PAPIText.html ErrorDocument 422 /httpd/422PAPIText.html ErrorDocument 423 /httpd/423PAPIText.html ErrorDocument 424 /httpd/424PAPIText.html ErrorDocument 500 /httpd/500PAPIText.html </Location> <Location /session/1/session> SetHandler session-service IsiAuthServices platform remote-service namespace ForceType text/plain ErrorDocument 401 /json/401.json </Location> <Location /session/1/saml/logout/slostatus> SetHandler saml-logout-slo ErrorDocument 401 /json/401.json </Location> <Location /session/1/saml/logout/session> SetHandler saml-logout-session ErrorDocument 401 /json/401.json </Location> # Authentication is not required to access these resources <Location /platform/*/cluster/identity> IsiAuthIgnore GET </Location> <Location /platform/*/cluster/identity/> IsiAuthIgnore GET </Location> <Location /platform/*/cluster/brand> IsiAuthIgnore GET </Location> <Location /platform/*/auth/users/*/change-password> IsiAuthIgnore PUT </Location> <Location /platform/*/auth/providers/saml-services/settings> IsiAuthIgnore GET </Location> <Location /platform/*/auth/providers/saml-services> IsiAuthIgnore GET </Location> <Location /platform/*/upgrade/cluster/mixed-mode> IsiAuthIgnore GET </Location> <Location /platform/*/cluster/version> IsiAuthIgnore GET </Location> # ================================================= # Object API # ================================================= Alias /namespace /usr/sbin/isi_object_d <Location /namespace> AuthType Isilon IsiAuthName \"namespace\" IsiAuthTypeBasic Off IsiAuthTypeSessionCookie On IsiDisabledZoneAllow Off IsiMultiZoneAllow On IsiCsrfCheck On Require valid-user SetHandler fastcgi-script Options +ExecCGI ErrorDocument 401 /json/401.json Header set Content-Security-Policy \"default-src 'none'\" </Location> # ================================================= # Remote-Service API # ================================================= Alias /remote-service /usr/sbin/isi_rsapi_d <Location /remote-service> AuthType Isilon IsiAuthName \"remote-service\" IsiAuthTypeBasic Off IsiAuthTypeSessionCookie On Require valid-user SetHandler fastcgi-script Options +ExecCGI ErrorDocument 401 /json/401.json </Location> # Session timeouts IsiAuthSessionTimeoutInactive 900 IsiAuthSessionTimeoutAbsolute 14400 Timeout 500 </VirtualHost>","title":"Apache Config"},{"location":"PowerScale%20Setup/","text":"PowerScale Setup PowerScale Setup Testing Test 1 - Generic Share Against Windows 11 Against Rocky Linux 9 Test 2 - Add FIPS Add FIPS Test Against Windows 11 Set Up Active Directory Test Against Rocky Linux 9 How Kerberos Works in This Scenario What is a Service Principal Debugging Testing Test 1 - Generic Share Created a system user grant on the local OS, set up a share, and gave that user privileges. Against Windows 11 I was able to access that without issue Default communication happened with SMBv2 Against Rocky Linux 9 Ran command sudo mount.cifs //10.10.25.80/ifs ~/share -o vers=2.0,username=grant,password='somepassword' which gave mount error(2): No such file or directory Received: I think the \"STATUS_FS_DRIVER_REQUIRED\" may be misleading because I have the CIFS module: [root@acas ~]# lsmod | grep cif cifs 2355200 0 cifs_arc4 16384 1 cifs rdma_cm 139264 1 cifs ib_core 450560 4 rdma_cm,cifs,iw_cm,ib_cm cifs_md4 16384 1 cifs dns_resolver 16384 1 cifs The problem was I was providing the share path not the share name - like a genius. sudo mount -t cifs //10.10.25.80/testshare /root/share -o username=grant,password='somepassword' works. Test 2 - Add FIPS Add FIPS Working from this procedure Check if FIPS is already active gcluster-1# isi security settings view FIPS Mode Enabled: No USB Ports Disabled: No Restricted shell Enabled: No Make sure nodes are healthy gcluster-1# isi status Cluster Name: gcluster Cluster Health: [ ATTN] Data Reduction: 1.00 : 1 Storage Efficiency: 0.31 : 1 Cluster Storage: HDD SSD Storage Size: 168.1G (234.9G Raw) 0 (0 Raw) VHS Size: 66.8G Used: 327.9M (< 1%) 0 (n/a) Avail: 167.8G (> 99%) 0 (n/a) Health Ext Throughput (bps) HDD Storage SSD Storage ID |IP Address |DASR |C/N| In Out Total| Used / Size |Used / Size ---+---------------+-----+---+-----+-----+-----+-----------------+----------------- 1|10.10.25.80 |-A-- | C |27.1k| 2.1M| 2.1M| 126M/56.0G(< 1%)|(No Storage SSDs) 2|10.10.25.81 |-A-- | C | 0|67.1k|67.1k| 109M/56.0G(< 1%)|(No Storage SSDs) 3|10.10.25.82 |-A-- | C | 0| 270k| 270k|93.1M/56.0G(< 1%)|(No Storage SSDs) ---+---------------+-----+---+-----+-----+-----+-----------------+----------------- Cluster Totals: |27.1k| 2.4M| 2.4M| 328M/ 168G(< 1%)|(No Storage SSDs) Health Fields: D = Down, A = Attention, S = Smartfailed, R = Read-Only External Network Fields: C = Connected, N = Not Connected Critical Events: Time LNN Event --------------- ---- ------------------------------------------------------- 07/27 12:04:21 1 One or more drives (location(s) Bay 7, Bay 8, Bay ... 07/27 12:30:35 3 Missing COMMITTED image in secure catalog 07/27 12:30:56 2 One or more drives (location(s) Bay 7, Bay 8, Bay ... 07/27 12:35:35 3 One or more drives (location(s) Bay 7, Bay 8, Bay ... Cluster Job Status: No running jobs. No paused or waiting jobs. No failed jobs. Recent job results: Time Job Event --------------- -------------------------- ------------------------------ 07/27 12:29:37 MultiScan[3] Succeeded 07/27 12:22:42 MultiScan[2] Succeeded 07/27 11:58:30 DomainTag[1] Succeeded Turn on FIPS gcluster-1# isi security settings modify --fips-mode-enabled=true gcluster-1# isi security settings view FIPS Mode Enabled: Yes USB Ports Disabled: No Restricted shell Enabled: No Test Against Windows 11 It nows fails: This tracks since it's clear from the NTLMSSP_NEGOTIATE flag that it is using NTLM which the docs say is disabled. Note: Updating the password hash also implicitly disables the NTLM support for SMB access that is used when shares are accessed through IP. The instructions also tell you to explicitly make the hash type SHA512 which also issues the warning regarding NTLM: gcluster-1# isi auth file modify System --password-hash-type=SHA512 NTLM support and authentication for all file protocols has been disabled for this provider due to change of password hash type. gcluster-1# isi auth local modify System --password-hash-type=SHA512 It also seems that without manual intervention the SSH key/cipher settings are more permissive than they should be: gcluster-1# isi ssh settings view Banner: /etc/motd CA Signature Algorithms: ecdsa-sha2-nistp256,ecdsa-sha2-nistp384,ecdsa-sha2-nistp521,ssh-ed25519,rsa-sha2-512,rsa-sha2-256,ssh-rsa Ciphers: aes256-ctr Host Key Algorithms: +ssh-dss,ssh-dss-cert-v01@openssh.com Ignore Rhosts: Yes Kex Algorithms: ecdh-sha2-nistp256,ecdh-sha2-nistp384,ecdh-sha2-nistp521,diffie-hellman-group14-sha256,diffie-hellman-group-exchange-sha256 Login Grace Time: 2m Log Level: INFO Macs: hmac-sha2-256 Max Auth Tries: 3 Max Sessions: - Max Startups: Permit Empty Passwords: No Permit Root Login: Yes Port: 22 Print Motd: Yes Pubkey Accepted Key Types: +ssh-dss,ssh-dss-cert-v01@openssh.com,ssh-rsa Strict Modes: No Subsystem: sftp /usr/local/libexec/sftp-server Syslog Facility: AUTH Tcp Keep Alive: No Auth Settings Template: any You have to follow this guide to fix them gcluster-1# isi ssh settings modify --kex-algorithms 'diffie-hellman-group16-sha512,diffie-hellman-group16-sha512,ecdh-sha2-nistp384' gcluster-1# isi ssh settings modify --ciphers 'aes256-ctr,aes256-gcm@openssh.com' gcluster-1# isi ssh settings modify --host-key-algorithms 'ecdsa-sha2-nistp384' gcluster-1# gcluster-1# isi_for_array 'yes | /usr/local/bin/ssh-keygen -t ecdsa -f /etc/ssh/ssh_host_ecdsa_key -b 384 -N \"\"' gcluster-3: Generating public/private ecdsa key pair. gcluster-3: /etc/ssh/ssh_host_ecdsa_key already exists. gcluster-3: Overwrite (y/n)? Your identification has been saved in /etc/ssh/ssh_host_ecdsa_key gcluster-3: Your public key has been saved in /etc/ssh/ssh_host_ecdsa_key.pub gcluster-3: The key fingerprint is: gcluster-3: SHA256:D6AMW5qyjg0jk61DRU2ed67zxOtqydWEU0xxzsnV+4Y root@gcluster-3 gcluster-3: The key's randomart image is: gcluster-3: +---[ECDSA 384]---+ gcluster-3: | o. oo.. ..| gcluster-3: | .... o= o .| gcluster-3: | o .o.. .o = .| gcluster-3: | O ...oo . . | gcluster-3: |. = o S.+ ..| gcluster-3: | * o+ . E o| gcluster-3: |O . .ooo. . | gcluster-3: |=* ++ . | gcluster-3: |oo. ..o+ | gcluster-3: +----[SHA256]-----+ gcluster-2: Generating public/private ecdsa key pair. gcluster-2: /etc/ssh/ssh_host_ecdsa_key already exists. gcluster-1: Generating public/private ecdsa key pair. gcluster-1: /etc/ssh/ssh_host_ecdsa_key already exists. gcluster-2: Overwrite (y/n)? Your identification has been saved in /etc/ssh/ssh_host_ecdsa_key gcluster-2: Your public key has been saved in /etc/ssh/ssh_host_ecdsa_key.pub gcluster-2: The key fingerprint is: gcluster-2: SHA256:mv24dik4vJVvGbHhOorynFlNa8N6Ol2kiRGwoMrWuCs root@gcluster-2 gcluster-2: The key's randomart image is: gcluster-2: +---[ECDSA 384]---+ gcluster-2: | . .. | gcluster-2: | . . .. | gcluster-2: |. . . | gcluster-2: |o o . + | gcluster-2: |.+ . oS= + | gcluster-2: |. . .Boo= | gcluster-2: | . .+oXo + | gcluster-2: |E o. ==+B== | gcluster-2: |.. o* =Oo*o | gcluster-2: +----[SHA256]-----+ gcluster-1: Overwrite (y/n)? Your identification has been saved in /etc/ssh/ssh_host_ecdsa_key gcluster-1: Your public key has been saved in /etc/ssh/ssh_host_ecdsa_key.pub gcluster-1: The key fingerprint is: gcluster-1: SHA256:ahrGrkc/YE1Ii4+YCxJgwFraGmiG2cb6M3yaY/AwMNA root@gcluster-1 gcluster-1: The key's randomart image is: gcluster-1: +---[ECDSA 384]---+ gcluster-1: |+. | gcluster-1: |ooE . | gcluster-1: |BB o o | gcluster-1: |@o* o . | gcluster-1: |+X o o S | gcluster-1: |@ ..= .. | gcluster-1: |oO o+oo | gcluster-1: |. O+o+o | gcluster-1: | .+Oo . | gcluster-1: +----[SHA256]-----+ gcluster-1# gcluster-1# isi ssh settings modify --pubkey-accepted-key-types 'ssh-rsa' gcluster-1# isi ssh settings modify --macs 'hmac-sha2-256,hmac-sha2-512,hmac-sha2-512-etm@openssh.com,hmac-sha2-256-etm@openssh.com' gcluster-1# isi ssh settings view Banner: /etc/motd CA Signature Algorithms: ecdsa-sha2-nistp256,ecdsa-sha2-nistp384,ecdsa-sha2-nistp521,ssh-ed25519,rsa-sha2-512,rsa-sha2-256,ssh-rsa Ciphers: aes256-ctr,aes256-gcm@openssh.com Host Key Algorithms: ecdsa-sha2-nistp384 Ignore Rhosts: Yes Kex Algorithms: diffie-hellman-group16-sha512,diffie-hellman-group16-sha512,ecdh-sha2-nistp384 Login Grace Time: 2m Log Level: INFO Macs: hmac-sha2-256,hmac-sha2-512,hmac-sha2-512-etm@openssh.com,hmac-sha2-256-etm@openssh.com Max Auth Tries: 3 Max Sessions: - Max Startups: Permit Empty Passwords: No Permit Root Login: Yes Port: 22 Print Motd: Yes Pubkey Accepted Key Types: ssh-rsa Strict Modes: No Subsystem: sftp /usr/local/libexec/sftp-server Syslog Facility: AUTH Tcp Keep Alive: No Auth Settings Template: any From the guide on having to disable FIPS mode: PowerScale clusters running OneFS 9.4.0.0 with FIPS mode enabled may upgrade to OneFS 9.5.0.0 or later. After upgrading to OneFS Release 9.5.0.0 or later and committing the upgrade, re-enable FIPS mode. Disabling FIPS mode before upgrading is not required. I noticed that turning on FIPS immediately broke the admin user for the web ui At this point I set up active directory I added domain admins to the admins roles for system and security for powerscale I added my grant domain user to the SMB share: At this point I swapped over to Linux. Set Up Active Directory I already had an active directory server to test against I was not able to join with isi auth ads create --name=win-6c2vli4n0lo.grant.lan --user=administrator --groupnet=groupnet0 DO NOT select the RFC2309 option Test Against Rocky Linux 9 Join the domain # Install required packages sudo dnf install -y realmd sssd oddjob oddjob-mkhomedir adcli samba-common # Discover and join the Active Directory domain sudo realm discover grant.lan sudo realm join -U administrator grant.lan # Allow domain users to log in sudo authselect select sssd --force # Enable home directory creation for domain users sudo systemctl enable oddjobd sudo systemctl start oddjobd # Verify the domain membership and users realm list Ensure domain join successful: [root@acas ~]# realm list grant.lan type: kerberos realm-name: GRANT.LAN domain-name: grant.lan configured: kerberos-member server-software: active-directory client-software: sssd required-package: oddjob required-package: oddjob-mkhomedir required-package: sssd required-package: adcli required-package: samba-common-tools login-formats: %U@grant.lan login-policy: allow-realm-logins Run the mount: [root@acas ~]# sudo mount -t cifs //10.10.25.80/testshare /mnt/testshare -o username=administrator,domain=grant.lan,password=somepassword mount error(13): Permission denied Refer to the mount.cifs(8) manual page (e.g. man mount.cifs) and kernel log messages (dmesg) Attempt to setting sec=krb5 returns bug-looking results: [root@acas ~]# sudo mount -t cifs //10.10.25.80/testshare /mnt/testshare -o username=administrator,domain=grant.lan,sec=krb5 mount error(0): Success Refer to the mount.cifs(8) manual page (e.g. man mount.cifs) and kernel log messages (dmesg) when you inspect dmseg you see: [86217.278254] CIFS: VFS: \\\\10.10.25.80 Send error in SessSetup = -13 [86228.682870] CIFS: Attempting to mount \\\\10.10.25.80\\testshare [86228.816520] CIFS: VFS: Verify user has a krb5 ticket and keyutils is installed [86228.816530] CIFS: VFS: \\\\10.10.25.80 Send error in SessSetup = -126 [86228.816564] CIFS: VFS: cifs_mount failed w/return code = -126 How Kerberos Works in This Scenario Overview: https://www.freecodecamp.org/news/how-does-kerberos-work-authentication-protocol/ Synopsis: the Active Directory user administrator@grant.lan logged into the Rocky 9 client ( acas.lan ) wants to mount a CIFS share on the Dell PowerScale ( gcluster-1 ) that requires Active Directory Kerberos authentication. TGT Acquisition with kinit: Before attempting to mount the CIFS share, the user administrator@grant.lan on the Rocky 9 client ( acas.lan ) must obtain a Ticket Granting Ticket (TGT) from the Key Distribution Center (KDC). To get the TGT, the user runs the kinit command and provides their password when prompted: bash-5.1$ kinit administrator@grant.lan Password for administrator@grant.lan: TGT Request to KDC: With the TGT in hand, the Rocky 9 client ( acas.lan ) can now initiate the request to mount the CIFS share on the Dell PowerScale ( gcluster-1 ) using the mount command. CIFS Share Mount: The user administrator@grant.lan runs the mount command with the required options to mount the CIFS share on the Dell PowerScale: bash-5.1$ sudo mount -t cifs //10.10.25.80/testshare /mnt/testshare -o username=administrator@grant.lan,domain=GRANT.LAN,sec=krb5 TGT Usage and Service Principal: The mount command uses the TGT acquired earlier to authenticate and obtain the Service Ticket from the KDC for accessing the CIFS service on gcluster-1 . The Service Ticket is generated for the service principal associated with the CIFS service on gcluster-1 . In this case, the service principal is CIFS/gcluster-1.grant.lan@GRANT.LAN . Service Ticket Response: The KDC ( dc.grant.lan ) validates the TGT and generates a Service Ticket for the CIFS service ( CIFS/gcluster-1.grant.lan ) on the Dell PowerScale. Mounting the CIFS Share: With the Service Ticket, the Rocky 9 client ( acas.lan ) successfully mounts the CIFS share on the Dell PowerScale ( gcluster-1 ) at the specified mount point. The user acquires a TGT using kinit , and then the mount command leverages the TGT to request and obtain the Service Ticket for the CIFS share from the KDC. The Service Ticket is then used for mounting the CIFS share on the Dell PowerScale, allowing the user to access the share securely without re-entering their password during the session. Mounting the CIFS Share: The Rocky 9 client ( acas.lan ) receives the Service Ticket and sends it, along with the request to mount the CIFS share, to the Dell PowerScale ( gcluster-1 ). The Dell PowerScale ( gcluster-1 ) decrypts the Service Ticket using the session key shared with the client and validates the user's identity and permissions to access the CIFS share. If everything checks out, the CIFS share is successfully mounted on the Rocky 9 client ( acas.lan ). Session Key for Secure Communication: The client ( acas.lan ) and the CIFS service on the Dell PowerScale ( gcluster-1 ) now have a shared session key for secure communication during the CIFS session. Throughout this process, the client, KDC, and CIFS service use symmetric encryption and shared secret keys to securely exchange credentials and generate tickets. Once the user is authenticated with a TGT, they can access the CIFS share without having to re-enter their password during the session. This provides a secure and seamless single sign-on (SSO) experience for the user. What is a Service Principal A service principal is a unique identity within a Kerberos-based authentication system that represents a specific network service or application. In a Kerberos authentication environment, each network service (e.g., web server, email server, file server) is assigned its own service principal, which is used to authenticate and authorize clients to access the service securely. Service principals are created and managed by the Key Distribution Center (KDC) in the Kerberos realm. The KDC issues a set of cryptographic keys to each service principal, which are used for secure communication between the client and the service. When a client wants to access a network service that requires Kerberos authentication, it requests a Ticket Granting Ticket (TGT) from the KDC by authenticating with its own principal (typically associated with a user). The TGT allows the client to request Service Tickets for specific service principals. The client then presents the Service Ticket to the service principal as proof of its identity, and the service principal validates the ticket and grants access to the requested service. Service principals are essential for securing communication in a Kerberos environment because they allow clients and services to establish trust and verify each other's identities. Each service principal has a unique name and is associated with a specific network service, ensuring that only authorized clients can access the corresponding service. For example, if you have a web server named \"example.com,\" it would have its own service principal called \"HTTP/example.com@REALM\" (where REALM is the Kerberos realm name). Clients authenticating to the web server would obtain Service Tickets for this specific service principal to gain access to the web server's resources securely. Debugging This command does not cause any traffic to the DC so it's not talking to it: bash-5.1$ kvno cifs/10.10.25.80 kvno: Server not found in Kerberos database while getting credentials for cifs/10.10.25.80@GRANT.LAN","title":"PowerScale Setup"},{"location":"PowerScale%20Setup/#powerscale-setup","text":"PowerScale Setup Testing Test 1 - Generic Share Against Windows 11 Against Rocky Linux 9 Test 2 - Add FIPS Add FIPS Test Against Windows 11 Set Up Active Directory Test Against Rocky Linux 9 How Kerberos Works in This Scenario What is a Service Principal Debugging","title":"PowerScale Setup"},{"location":"PowerScale%20Setup/#testing","text":"","title":"Testing"},{"location":"PowerScale%20Setup/#test-1-generic-share","text":"Created a system user grant on the local OS, set up a share, and gave that user privileges.","title":"Test 1 - Generic Share"},{"location":"PowerScale%20Setup/#against-windows-11","text":"I was able to access that without issue Default communication happened with SMBv2","title":"Against Windows 11"},{"location":"PowerScale%20Setup/#against-rocky-linux-9","text":"Ran command sudo mount.cifs //10.10.25.80/ifs ~/share -o vers=2.0,username=grant,password='somepassword' which gave mount error(2): No such file or directory Received: I think the \"STATUS_FS_DRIVER_REQUIRED\" may be misleading because I have the CIFS module: [root@acas ~]# lsmod | grep cif cifs 2355200 0 cifs_arc4 16384 1 cifs rdma_cm 139264 1 cifs ib_core 450560 4 rdma_cm,cifs,iw_cm,ib_cm cifs_md4 16384 1 cifs dns_resolver 16384 1 cifs The problem was I was providing the share path not the share name - like a genius. sudo mount -t cifs //10.10.25.80/testshare /root/share -o username=grant,password='somepassword' works.","title":"Against Rocky Linux 9"},{"location":"PowerScale%20Setup/#test-2-add-fips","text":"","title":"Test 2 - Add FIPS"},{"location":"PowerScale%20Setup/#add-fips","text":"Working from this procedure Check if FIPS is already active gcluster-1# isi security settings view FIPS Mode Enabled: No USB Ports Disabled: No Restricted shell Enabled: No Make sure nodes are healthy gcluster-1# isi status Cluster Name: gcluster Cluster Health: [ ATTN] Data Reduction: 1.00 : 1 Storage Efficiency: 0.31 : 1 Cluster Storage: HDD SSD Storage Size: 168.1G (234.9G Raw) 0 (0 Raw) VHS Size: 66.8G Used: 327.9M (< 1%) 0 (n/a) Avail: 167.8G (> 99%) 0 (n/a) Health Ext Throughput (bps) HDD Storage SSD Storage ID |IP Address |DASR |C/N| In Out Total| Used / Size |Used / Size ---+---------------+-----+---+-----+-----+-----+-----------------+----------------- 1|10.10.25.80 |-A-- | C |27.1k| 2.1M| 2.1M| 126M/56.0G(< 1%)|(No Storage SSDs) 2|10.10.25.81 |-A-- | C | 0|67.1k|67.1k| 109M/56.0G(< 1%)|(No Storage SSDs) 3|10.10.25.82 |-A-- | C | 0| 270k| 270k|93.1M/56.0G(< 1%)|(No Storage SSDs) ---+---------------+-----+---+-----+-----+-----+-----------------+----------------- Cluster Totals: |27.1k| 2.4M| 2.4M| 328M/ 168G(< 1%)|(No Storage SSDs) Health Fields: D = Down, A = Attention, S = Smartfailed, R = Read-Only External Network Fields: C = Connected, N = Not Connected Critical Events: Time LNN Event --------------- ---- ------------------------------------------------------- 07/27 12:04:21 1 One or more drives (location(s) Bay 7, Bay 8, Bay ... 07/27 12:30:35 3 Missing COMMITTED image in secure catalog 07/27 12:30:56 2 One or more drives (location(s) Bay 7, Bay 8, Bay ... 07/27 12:35:35 3 One or more drives (location(s) Bay 7, Bay 8, Bay ... Cluster Job Status: No running jobs. No paused or waiting jobs. No failed jobs. Recent job results: Time Job Event --------------- -------------------------- ------------------------------ 07/27 12:29:37 MultiScan[3] Succeeded 07/27 12:22:42 MultiScan[2] Succeeded 07/27 11:58:30 DomainTag[1] Succeeded Turn on FIPS gcluster-1# isi security settings modify --fips-mode-enabled=true gcluster-1# isi security settings view FIPS Mode Enabled: Yes USB Ports Disabled: No Restricted shell Enabled: No","title":"Add FIPS"},{"location":"PowerScale%20Setup/#test-against-windows-11","text":"It nows fails: This tracks since it's clear from the NTLMSSP_NEGOTIATE flag that it is using NTLM which the docs say is disabled. Note: Updating the password hash also implicitly disables the NTLM support for SMB access that is used when shares are accessed through IP. The instructions also tell you to explicitly make the hash type SHA512 which also issues the warning regarding NTLM: gcluster-1# isi auth file modify System --password-hash-type=SHA512 NTLM support and authentication for all file protocols has been disabled for this provider due to change of password hash type. gcluster-1# isi auth local modify System --password-hash-type=SHA512 It also seems that without manual intervention the SSH key/cipher settings are more permissive than they should be: gcluster-1# isi ssh settings view Banner: /etc/motd CA Signature Algorithms: ecdsa-sha2-nistp256,ecdsa-sha2-nistp384,ecdsa-sha2-nistp521,ssh-ed25519,rsa-sha2-512,rsa-sha2-256,ssh-rsa Ciphers: aes256-ctr Host Key Algorithms: +ssh-dss,ssh-dss-cert-v01@openssh.com Ignore Rhosts: Yes Kex Algorithms: ecdh-sha2-nistp256,ecdh-sha2-nistp384,ecdh-sha2-nistp521,diffie-hellman-group14-sha256,diffie-hellman-group-exchange-sha256 Login Grace Time: 2m Log Level: INFO Macs: hmac-sha2-256 Max Auth Tries: 3 Max Sessions: - Max Startups: Permit Empty Passwords: No Permit Root Login: Yes Port: 22 Print Motd: Yes Pubkey Accepted Key Types: +ssh-dss,ssh-dss-cert-v01@openssh.com,ssh-rsa Strict Modes: No Subsystem: sftp /usr/local/libexec/sftp-server Syslog Facility: AUTH Tcp Keep Alive: No Auth Settings Template: any You have to follow this guide to fix them gcluster-1# isi ssh settings modify --kex-algorithms 'diffie-hellman-group16-sha512,diffie-hellman-group16-sha512,ecdh-sha2-nistp384' gcluster-1# isi ssh settings modify --ciphers 'aes256-ctr,aes256-gcm@openssh.com' gcluster-1# isi ssh settings modify --host-key-algorithms 'ecdsa-sha2-nistp384' gcluster-1# gcluster-1# isi_for_array 'yes | /usr/local/bin/ssh-keygen -t ecdsa -f /etc/ssh/ssh_host_ecdsa_key -b 384 -N \"\"' gcluster-3: Generating public/private ecdsa key pair. gcluster-3: /etc/ssh/ssh_host_ecdsa_key already exists. gcluster-3: Overwrite (y/n)? Your identification has been saved in /etc/ssh/ssh_host_ecdsa_key gcluster-3: Your public key has been saved in /etc/ssh/ssh_host_ecdsa_key.pub gcluster-3: The key fingerprint is: gcluster-3: SHA256:D6AMW5qyjg0jk61DRU2ed67zxOtqydWEU0xxzsnV+4Y root@gcluster-3 gcluster-3: The key's randomart image is: gcluster-3: +---[ECDSA 384]---+ gcluster-3: | o. oo.. ..| gcluster-3: | .... o= o .| gcluster-3: | o .o.. .o = .| gcluster-3: | O ...oo . . | gcluster-3: |. = o S.+ ..| gcluster-3: | * o+ . E o| gcluster-3: |O . .ooo. . | gcluster-3: |=* ++ . | gcluster-3: |oo. ..o+ | gcluster-3: +----[SHA256]-----+ gcluster-2: Generating public/private ecdsa key pair. gcluster-2: /etc/ssh/ssh_host_ecdsa_key already exists. gcluster-1: Generating public/private ecdsa key pair. gcluster-1: /etc/ssh/ssh_host_ecdsa_key already exists. gcluster-2: Overwrite (y/n)? Your identification has been saved in /etc/ssh/ssh_host_ecdsa_key gcluster-2: Your public key has been saved in /etc/ssh/ssh_host_ecdsa_key.pub gcluster-2: The key fingerprint is: gcluster-2: SHA256:mv24dik4vJVvGbHhOorynFlNa8N6Ol2kiRGwoMrWuCs root@gcluster-2 gcluster-2: The key's randomart image is: gcluster-2: +---[ECDSA 384]---+ gcluster-2: | . .. | gcluster-2: | . . .. | gcluster-2: |. . . | gcluster-2: |o o . + | gcluster-2: |.+ . oS= + | gcluster-2: |. . .Boo= | gcluster-2: | . .+oXo + | gcluster-2: |E o. ==+B== | gcluster-2: |.. o* =Oo*o | gcluster-2: +----[SHA256]-----+ gcluster-1: Overwrite (y/n)? Your identification has been saved in /etc/ssh/ssh_host_ecdsa_key gcluster-1: Your public key has been saved in /etc/ssh/ssh_host_ecdsa_key.pub gcluster-1: The key fingerprint is: gcluster-1: SHA256:ahrGrkc/YE1Ii4+YCxJgwFraGmiG2cb6M3yaY/AwMNA root@gcluster-1 gcluster-1: The key's randomart image is: gcluster-1: +---[ECDSA 384]---+ gcluster-1: |+. | gcluster-1: |ooE . | gcluster-1: |BB o o | gcluster-1: |@o* o . | gcluster-1: |+X o o S | gcluster-1: |@ ..= .. | gcluster-1: |oO o+oo | gcluster-1: |. O+o+o | gcluster-1: | .+Oo . | gcluster-1: +----[SHA256]-----+ gcluster-1# gcluster-1# isi ssh settings modify --pubkey-accepted-key-types 'ssh-rsa' gcluster-1# isi ssh settings modify --macs 'hmac-sha2-256,hmac-sha2-512,hmac-sha2-512-etm@openssh.com,hmac-sha2-256-etm@openssh.com' gcluster-1# isi ssh settings view Banner: /etc/motd CA Signature Algorithms: ecdsa-sha2-nistp256,ecdsa-sha2-nistp384,ecdsa-sha2-nistp521,ssh-ed25519,rsa-sha2-512,rsa-sha2-256,ssh-rsa Ciphers: aes256-ctr,aes256-gcm@openssh.com Host Key Algorithms: ecdsa-sha2-nistp384 Ignore Rhosts: Yes Kex Algorithms: diffie-hellman-group16-sha512,diffie-hellman-group16-sha512,ecdh-sha2-nistp384 Login Grace Time: 2m Log Level: INFO Macs: hmac-sha2-256,hmac-sha2-512,hmac-sha2-512-etm@openssh.com,hmac-sha2-256-etm@openssh.com Max Auth Tries: 3 Max Sessions: - Max Startups: Permit Empty Passwords: No Permit Root Login: Yes Port: 22 Print Motd: Yes Pubkey Accepted Key Types: ssh-rsa Strict Modes: No Subsystem: sftp /usr/local/libexec/sftp-server Syslog Facility: AUTH Tcp Keep Alive: No Auth Settings Template: any From the guide on having to disable FIPS mode: PowerScale clusters running OneFS 9.4.0.0 with FIPS mode enabled may upgrade to OneFS 9.5.0.0 or later. After upgrading to OneFS Release 9.5.0.0 or later and committing the upgrade, re-enable FIPS mode. Disabling FIPS mode before upgrading is not required. I noticed that turning on FIPS immediately broke the admin user for the web ui At this point I set up active directory I added domain admins to the admins roles for system and security for powerscale I added my grant domain user to the SMB share: At this point I swapped over to Linux.","title":"Test Against Windows 11"},{"location":"PowerScale%20Setup/#set-up-active-directory","text":"I already had an active directory server to test against I was not able to join with isi auth ads create --name=win-6c2vli4n0lo.grant.lan --user=administrator --groupnet=groupnet0 DO NOT select the RFC2309 option","title":"Set Up Active Directory"},{"location":"PowerScale%20Setup/#test-against-rocky-linux-9","text":"Join the domain # Install required packages sudo dnf install -y realmd sssd oddjob oddjob-mkhomedir adcli samba-common # Discover and join the Active Directory domain sudo realm discover grant.lan sudo realm join -U administrator grant.lan # Allow domain users to log in sudo authselect select sssd --force # Enable home directory creation for domain users sudo systemctl enable oddjobd sudo systemctl start oddjobd # Verify the domain membership and users realm list Ensure domain join successful: [root@acas ~]# realm list grant.lan type: kerberos realm-name: GRANT.LAN domain-name: grant.lan configured: kerberos-member server-software: active-directory client-software: sssd required-package: oddjob required-package: oddjob-mkhomedir required-package: sssd required-package: adcli required-package: samba-common-tools login-formats: %U@grant.lan login-policy: allow-realm-logins Run the mount: [root@acas ~]# sudo mount -t cifs //10.10.25.80/testshare /mnt/testshare -o username=administrator,domain=grant.lan,password=somepassword mount error(13): Permission denied Refer to the mount.cifs(8) manual page (e.g. man mount.cifs) and kernel log messages (dmesg) Attempt to setting sec=krb5 returns bug-looking results: [root@acas ~]# sudo mount -t cifs //10.10.25.80/testshare /mnt/testshare -o username=administrator,domain=grant.lan,sec=krb5 mount error(0): Success Refer to the mount.cifs(8) manual page (e.g. man mount.cifs) and kernel log messages (dmesg) when you inspect dmseg you see: [86217.278254] CIFS: VFS: \\\\10.10.25.80 Send error in SessSetup = -13 [86228.682870] CIFS: Attempting to mount \\\\10.10.25.80\\testshare [86228.816520] CIFS: VFS: Verify user has a krb5 ticket and keyutils is installed [86228.816530] CIFS: VFS: \\\\10.10.25.80 Send error in SessSetup = -126 [86228.816564] CIFS: VFS: cifs_mount failed w/return code = -126","title":"Test Against Rocky Linux 9"},{"location":"PowerScale%20Setup/#how-kerberos-works-in-this-scenario","text":"Overview: https://www.freecodecamp.org/news/how-does-kerberos-work-authentication-protocol/ Synopsis: the Active Directory user administrator@grant.lan logged into the Rocky 9 client ( acas.lan ) wants to mount a CIFS share on the Dell PowerScale ( gcluster-1 ) that requires Active Directory Kerberos authentication. TGT Acquisition with kinit: Before attempting to mount the CIFS share, the user administrator@grant.lan on the Rocky 9 client ( acas.lan ) must obtain a Ticket Granting Ticket (TGT) from the Key Distribution Center (KDC). To get the TGT, the user runs the kinit command and provides their password when prompted: bash-5.1$ kinit administrator@grant.lan Password for administrator@grant.lan: TGT Request to KDC: With the TGT in hand, the Rocky 9 client ( acas.lan ) can now initiate the request to mount the CIFS share on the Dell PowerScale ( gcluster-1 ) using the mount command. CIFS Share Mount: The user administrator@grant.lan runs the mount command with the required options to mount the CIFS share on the Dell PowerScale: bash-5.1$ sudo mount -t cifs //10.10.25.80/testshare /mnt/testshare -o username=administrator@grant.lan,domain=GRANT.LAN,sec=krb5 TGT Usage and Service Principal: The mount command uses the TGT acquired earlier to authenticate and obtain the Service Ticket from the KDC for accessing the CIFS service on gcluster-1 . The Service Ticket is generated for the service principal associated with the CIFS service on gcluster-1 . In this case, the service principal is CIFS/gcluster-1.grant.lan@GRANT.LAN . Service Ticket Response: The KDC ( dc.grant.lan ) validates the TGT and generates a Service Ticket for the CIFS service ( CIFS/gcluster-1.grant.lan ) on the Dell PowerScale. Mounting the CIFS Share: With the Service Ticket, the Rocky 9 client ( acas.lan ) successfully mounts the CIFS share on the Dell PowerScale ( gcluster-1 ) at the specified mount point. The user acquires a TGT using kinit , and then the mount command leverages the TGT to request and obtain the Service Ticket for the CIFS share from the KDC. The Service Ticket is then used for mounting the CIFS share on the Dell PowerScale, allowing the user to access the share securely without re-entering their password during the session. Mounting the CIFS Share: The Rocky 9 client ( acas.lan ) receives the Service Ticket and sends it, along with the request to mount the CIFS share, to the Dell PowerScale ( gcluster-1 ). The Dell PowerScale ( gcluster-1 ) decrypts the Service Ticket using the session key shared with the client and validates the user's identity and permissions to access the CIFS share. If everything checks out, the CIFS share is successfully mounted on the Rocky 9 client ( acas.lan ). Session Key for Secure Communication: The client ( acas.lan ) and the CIFS service on the Dell PowerScale ( gcluster-1 ) now have a shared session key for secure communication during the CIFS session. Throughout this process, the client, KDC, and CIFS service use symmetric encryption and shared secret keys to securely exchange credentials and generate tickets. Once the user is authenticated with a TGT, they can access the CIFS share without having to re-enter their password during the session. This provides a secure and seamless single sign-on (SSO) experience for the user.","title":"How Kerberos Works in This Scenario"},{"location":"PowerScale%20Setup/#what-is-a-service-principal","text":"A service principal is a unique identity within a Kerberos-based authentication system that represents a specific network service or application. In a Kerberos authentication environment, each network service (e.g., web server, email server, file server) is assigned its own service principal, which is used to authenticate and authorize clients to access the service securely. Service principals are created and managed by the Key Distribution Center (KDC) in the Kerberos realm. The KDC issues a set of cryptographic keys to each service principal, which are used for secure communication between the client and the service. When a client wants to access a network service that requires Kerberos authentication, it requests a Ticket Granting Ticket (TGT) from the KDC by authenticating with its own principal (typically associated with a user). The TGT allows the client to request Service Tickets for specific service principals. The client then presents the Service Ticket to the service principal as proof of its identity, and the service principal validates the ticket and grants access to the requested service. Service principals are essential for securing communication in a Kerberos environment because they allow clients and services to establish trust and verify each other's identities. Each service principal has a unique name and is associated with a specific network service, ensuring that only authorized clients can access the corresponding service. For example, if you have a web server named \"example.com,\" it would have its own service principal called \"HTTP/example.com@REALM\" (where REALM is the Kerberos realm name). Clients authenticating to the web server would obtain Service Tickets for this specific service principal to gain access to the web server's resources securely.","title":"What is a Service Principal"},{"location":"PowerScale%20Setup/#debugging","text":"This command does not cause any traffic to the DC so it's not talking to it: bash-5.1$ kvno cifs/10.10.25.80 kvno: Server not found in Kerberos database while getting credentials for cifs/10.10.25.80@GRANT.LAN","title":"Debugging"},{"location":"Reset%20OS10%20Admin%20Password/","text":"Reset OS10 Admin Password Video: https://youtu.be/0VfJCa8s7yo Connect to the serial console port. The serial settings are 115,200 baud, 8 data bits, and no parity. Reboot or power up the system. Press ESC at the Grub prompt to view the boot menu. The OS10-A partition is selected by default. +-------------------------------------+ |*OS10-A | | OS10-B | | ONIE | +-------------------------------------+ Press e to open the OS10 GRUB editor. 1.Use the arrow keys to navigate to the end of the line that has set os_debug_args= and then add init=/bin/bash. +---------------------------------------------------------+ |setparams 'OS10-A' | | | | set os_debug_args=\"init=/bin/bash\" | | select_image A | | boot_os | | | +---------------------------------------------------------+ Press Alt + 0. The system boots to a root shell without a password. At the root prompt run passwd admin and set the password to whatever you want Run reboot -f to reboot the system When the system reboots log into the admin account with your new password. That password is temporary and is not permanently written to the system . Enter configuration mode with configure terminal and then run username admin password <YOURPASSWORD> role sysadmin to change the password Run write memory to make the changes permanent. Startup Config Location /config/etc/opt/dell/os10/db_init/startup.xml","title":"Reset OS10 Admin Password"},{"location":"Reset%20OS10%20Admin%20Password/#reset-os10-admin-password","text":"Video: https://youtu.be/0VfJCa8s7yo Connect to the serial console port. The serial settings are 115,200 baud, 8 data bits, and no parity. Reboot or power up the system. Press ESC at the Grub prompt to view the boot menu. The OS10-A partition is selected by default. +-------------------------------------+ |*OS10-A | | OS10-B | | ONIE | +-------------------------------------+ Press e to open the OS10 GRUB editor. 1.Use the arrow keys to navigate to the end of the line that has set os_debug_args= and then add init=/bin/bash. +---------------------------------------------------------+ |setparams 'OS10-A' | | | | set os_debug_args=\"init=/bin/bash\" | | select_image A | | boot_os | | | +---------------------------------------------------------+ Press Alt + 0. The system boots to a root shell without a password. At the root prompt run passwd admin and set the password to whatever you want Run reboot -f to reboot the system When the system reboots log into the admin account with your new password. That password is temporary and is not permanently written to the system . Enter configuration mode with configure terminal and then run username admin password <YOURPASSWORD> role sysadmin to change the password Run write memory to make the changes permanent.","title":"Reset OS10 Admin Password"},{"location":"Reset%20OS10%20Admin%20Password/#startup-config-location","text":"/config/etc/opt/dell/os10/db_init/startup.xml","title":"Startup Config Location"},{"location":"Reset%20OS10%20Admin%20Password/problems/","text":"Resetting Dell OS10 Password Working Instructions Connect to the serial console port. The serial settings are 115,200 baud, 8 data bits, and no parity. Reboot or power up the system. Press ESC at the Grub prompt to view the boot menu. The OS10-A partition is selected by default. +-------------------------------------+ |*OS10-A | | OS10-B | | ONIE | +-------------------------------------+ Press e to open the OS10 GRUB editor. 1.Use the arrow keys to navigate to the end of the line that has set os_debug_args= and then add init=/bin/bash. +---------------------------------------------------------+ |setparams 'OS10-A' | | | | set os_debug_args=\"init=/bin/bash\" | | select_image A | | boot_os | | | +---------------------------------------------------------+ Press Alt + 0. The system boots to a root shell without a password. At the root prompt run passwd admin and set the password to whatever you want Run reboot -f to reboot the system When the system reboots log into the admin account with your new password. That password is temporary and is not permanently written to the system . Enter configuration mode with configure terminal and then run username admin password <YOURPASSWORD> role sysadmin to change the password Run write memory to make the changes permanent. Problems with online documentation Based on official instructions here Instructions are for resetting the linuxadmin account. Majority of audience are network engineers who have little understanding of Linux. Even for me, a Linux engineer who also does network engineering, and has done extensive work with OS10 had no idea a linuxadmin account even existed much less what it was used for. Having worked extensively with customers in the field, I have never met one that actually knew that OS10 is just Debian Linux much less understood the relationship between the Linux command line and the OS10 shell. The instructions seem to assume that you understand the relationship between the OS10 shell, the Linux command line, and the Linux users. The vast majority of the users of OS10 will not understand these distinctions. In my case the customer was very confused as to what linuxadmin was. They had never used it and after resetting the password for linuxadmin tried logging into the admin account and was confused when it didn't work They were then further confused because they tried to log into OS10 B thinking it was some sort of synchronized backup. Since it was identical to OS10 A they thought changes made there would synchronize between the two. The instructions do not tell you that the changes made to the password are temporary and will not persist through reboot. As far as I can tell, we do not have any documentation describing how to reset the admin account for the OS10 command line which is what virtually all customers will want to do. Suggestions Ensure that instructions for resetting the admin account password are easily accessible online Clarify the purpose of the linuxadmin account If there are plans to expose the Linux nature of OS10 to the customer, provide documentation clearly articulating the relationship between Linux and OS10 from the perspective of what is relevant to someone with a network engineering background. I am happy to help with this.","title":"Resetting Dell OS10 Password"},{"location":"Reset%20OS10%20Admin%20Password/problems/#resetting-dell-os10-password","text":"","title":"Resetting Dell OS10 Password"},{"location":"Reset%20OS10%20Admin%20Password/problems/#working-instructions","text":"Connect to the serial console port. The serial settings are 115,200 baud, 8 data bits, and no parity. Reboot or power up the system. Press ESC at the Grub prompt to view the boot menu. The OS10-A partition is selected by default. +-------------------------------------+ |*OS10-A | | OS10-B | | ONIE | +-------------------------------------+ Press e to open the OS10 GRUB editor. 1.Use the arrow keys to navigate to the end of the line that has set os_debug_args= and then add init=/bin/bash. +---------------------------------------------------------+ |setparams 'OS10-A' | | | | set os_debug_args=\"init=/bin/bash\" | | select_image A | | boot_os | | | +---------------------------------------------------------+ Press Alt + 0. The system boots to a root shell without a password. At the root prompt run passwd admin and set the password to whatever you want Run reboot -f to reboot the system When the system reboots log into the admin account with your new password. That password is temporary and is not permanently written to the system . Enter configuration mode with configure terminal and then run username admin password <YOURPASSWORD> role sysadmin to change the password Run write memory to make the changes permanent.","title":"Working Instructions"},{"location":"Reset%20OS10%20Admin%20Password/problems/#problems-with-online-documentation","text":"Based on official instructions here Instructions are for resetting the linuxadmin account. Majority of audience are network engineers who have little understanding of Linux. Even for me, a Linux engineer who also does network engineering, and has done extensive work with OS10 had no idea a linuxadmin account even existed much less what it was used for. Having worked extensively with customers in the field, I have never met one that actually knew that OS10 is just Debian Linux much less understood the relationship between the Linux command line and the OS10 shell. The instructions seem to assume that you understand the relationship between the OS10 shell, the Linux command line, and the Linux users. The vast majority of the users of OS10 will not understand these distinctions. In my case the customer was very confused as to what linuxadmin was. They had never used it and after resetting the password for linuxadmin tried logging into the admin account and was confused when it didn't work They were then further confused because they tried to log into OS10 B thinking it was some sort of synchronized backup. Since it was identical to OS10 A they thought changes made there would synchronize between the two. The instructions do not tell you that the changes made to the password are temporary and will not persist through reboot. As far as I can tell, we do not have any documentation describing how to reset the admin account for the OS10 command line which is what virtually all customers will want to do.","title":"Problems with online documentation"},{"location":"Reset%20OS10%20Admin%20Password/problems/#suggestions","text":"Ensure that instructions for resetting the admin account password are easily accessible online Clarify the purpose of the linuxadmin account If there are plans to expose the Linux nature of OS10 to the customer, provide documentation clearly articulating the relationship between Linux and OS10 from the perspective of what is relevant to someone with a network engineering background. I am happy to help with this.","title":"Suggestions"},{"location":"Run%20VPN%20on%20OS10/","text":"Run VPN on OS10 My Configuration Dell 4112F-ON Dell EMC Networking OS10 Enterprise Copyright (c) 1999-2020 by Dell Inc. All Rights Reserved. OS Version: 10.5.0.4 Build Version: 10.5.0.4.638 Build Time: 2020-01-30T21:08:56+0000 System Type: S4112F-ON Architecture: x86_64 Up Time: 2 days 03:54:07 CentOS [root@centos ~]# cat /etc/*-release CentOS Linux release 7.6.1810 (Core) NAME=\"CentOS Linux\" VERSION=\"7 (Core)\" ID=\"centos\" ID_LIKE=\"rhel fedora\" VERSION_ID=\"7\" PRETTY_NAME=\"CentOS Linux 7 (Core)\" ANSI_COLOR=\"0;31\" CPE_NAME=\"cpe:/o:centos:centos:7\" HOME_URL=\"https://www.centos.org/\" BUG_REPORT_URL=\"https://bugs.centos.org/\" CENTOS_MANTISBT_PROJECT=\"CentOS-7\" CENTOS_MANTISBT_PROJECT_VERSION=\"7\" REDHAT_SUPPORT_PRODUCT=\"centos\" REDHAT_SUPPORT_PRODUCT_VERSION=\"7\" CentOS Linux release 7.6.1810 (Core) CentOS Linux release 7.6.1810 (Core) Testing Topology Switch Configuration Research Sources Helpful Book Basics of Network Processor Packet Processing How Network Processors Work NPU Interface Problem The one big gotcha with doing this is that when you drop to the command line in OS10 and do a ip a s , the interfaces you see that look like physical interfaces ex: 13: e101-001-0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc multiq master br32 state UP group default qlen 1000 link/ether 50:9a:4c:d6:0a:71 brd ff:ff:ff:ff:ff:ff 14: e101-002-0: <BROADCAST,MULTICAST> mtu 1500 qdisc noop master br1 state DOWN group default qlen 1000 link/ether 50:9a:4c:d6:0a:72 brd ff:ff:ff:ff:ff:ff 15: e101-003-0: <BROADCAST,MULTICAST> mtu 1500 qdisc noop master br1 state DOWN group default qlen 1000 link/ether 50:9a:4c:d6:0a:73 brd ff:ff:ff:ff:ff:ff 16: e101-004-0: <BROADCAST,MULTICAST> mtu 1500 qdisc noop master br1 state DOWN group default qlen 1000 link/ether 50:9a:4c:d6:0a:74 brd ff:ff:ff:ff:ff:ff are not actual physical interfaces. Under the hood the operating system is actually using tap interfaces. Inside the switch there are two processors - a regular x86 processor and a separate processor called the Network Processing Unit (NPU). The interfaces are connected to the NPU. Most traffic that comes in on the physical interfaces managed by the NPU does not flow up to the x86 chip. This means that if you do a tcpdump on one of the interfaces you see in ip a s you will see very little. In fact, the only traffic you will see is management traffic which is handled by the Linux kernel. This means if you want to set up a VPN you have to have a way to make sure all the traffic is visible to the Linux kernel. Fortunately, there is a way to make this happen. VLAN interfaces are virtual and subsequently are handled entirely by the Linux kernel. In fact, VLAN interfaces actually show up under the hood as bridge interfaces: 29: br32: <BROADCAST,MULTICAST,PROMISC,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default qlen 1000 link/ether 50:9a:4c:d6:0a:a1 brd ff:ff:ff:ff:ff:ff inet 192.168.32.1/24 brd 255.168.32.255 scope global br32 valid_lft forever preferred_lft forever inet6 fe80::529a:4cff:fed6:aa1/64 scope link valid_lft forever preferred_lft forever 41: br33: <BROADCAST,MULTICAST,PROMISC,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default qlen 1000 link/ether 50:9a:4c:d6:0a:a1 brd ff:ff:ff:ff:ff:ff inet 192.168.33.1/24 brd 255.168.33.255 scope global br33 valid_lft forever preferred_lft forever inet6 fe80::529a:4cff:fed6:aa1/64 scope link valid_lft forever preferred_lft forever These two correspond to interface vlans 32 and 33. By using VLAN interfaces you can tie the VPN to these interfaces and everything works just fine. You just place any associated physical interfaces as access VLANs or trunks with the appropriate allowed VLANs. Installing OpenVPN as a Server on the 4112F-ON Note: I ran a VPN server on my switch, but you could just as easily make the switch a point to point VPN gateway connecting to a PFSense instance such that anything that can reach the switch could participate in a multipoint network. On the 4112F-ON: Enter configuration mode from user mode by running en and then config <enter> Run ip name-server 192.168.1.1 to add a name server. Run write mem in enable mode to save your configuration changes. Run system bash Before continuing, make sure that the time is correct on the device. WARNING If you do not do this and you generate certificates, none of the encryption will work and you will have to recreate all of your certificates! Run sudo apt-get install -y openvpn vim . I installed vim because I don't hate myself. I used this script from git.io/vpn to install OpenVPN. Having done the entire thing manually before, I can tell you this saves a huge amount of time. To run the script run wget https://git.io/vpn -O openvpn-install.sh && chmod +x openvpn-install.sh && ./openvpn-install.sh 1.Fill in the options as needed. I did find some things you have to tweak with their script. Perform the below to clean things up. 1.Run vim /lib/systemd/system/openvpn@.service . Where it says --config /etc/openvpn/%i.conf , change that to --config /etc/openvpn/%i/%i.conf . For details on specifies work see this post . When you are done run systemctl daemon-reload to reload the systemd daemon. 2.If you used my version of the script then you do not need to do this. Otherwise you need to run vim /etc/openvpn/server/server.conf and you need to prepend /etc/openvpn/server/ on several of the paths or the service won't start. See my config below: local 192.168.32.1 port 1194 proto udp dev tun ca /etc/openvpn/server/ca.crt cert /etc/openvpn/server/server.crt key /etc/openvpn/server/server.key dh /etc/openvpn/server/dh.pem auth SHA512 tls-crypt /etc/openvpn/server/tc.key topology subnet server 10.8.0.0 255.255.255.0 ifconfig-pool-persist ipp.txt push \"redirect-gateway def1 bypass-dhcp\" push \"dhcp-option DNS 192.168.1.1\" keepalive 10 120 cipher AES-256-CBC user nobody group nogroup persist-key persist-tun status openvpn-status.log verb 3 crl-verify /etc/openvpn/server/crl.pem explicit-exit-notify You may want to add something like push route 192.168.1.0 255.255.255.0 to your server config. This allows the server to push routes to the client. For example, in my case the 192.168.1.0/24 network is behind my server, so I have to push a route so that the clients know how to get to it. Just keep in mind, that hosts on your distant network must have a route back to your VPN network. Run systemctl start openvpn@server to start the server. Rerun the script to add clients. Your output should look like the below. In my case I added one client to perform the test. ``` Looks like OpenVPN is already installed. What do you want to do? 1) Add a new user 2) Revoke an existing user 3) Remove OpenVPN 4) Exit Select an option: 1 Tell me a name for the client certificate. Client name: test-client Using SSL: openssl OpenSSL 1.1.0l 10 Sep 2019 Generating a RSA private key ........+++++ .......+++++ writing new private key to '/etc/openvpn/server/easy-rsa/pki/private/test-client.key.WONcIB6m1N' ----- Using configuration from ./safessl-easyrsa.cnf Check that the request matches the signature Signature ok The Subject's Distinguished Name is as follows commonName :ASN.1 12:'test-client' Certificate is to be certified until Mar 9 00:13:09 2030 GMT (3650 days) Write out database with 1 new entries Data Base Updated Client test-client added, configuration is available at: /root/test-client.ovpn ``` Copy the contents of your client config. In my case this was from /root/test-client.ovpn and it looked like: client dev tun proto udp remote <SERVER ADDRESS> 1194 resolv-retry infinite nobind persist-key persist-tun remote-cert-tls server auth SHA512 cipher AES-256-CBC ignore-unknown-option block-outside-dns block-outside-dns verb 3 <ca> -----BEGIN CERTIFICATE----- MIIDKzCCAhOgAwIBAgIJANmH49pJjiOUMA0GCSqGSIb3DQEBCwUAMBMxETAPBgNV BAMMCENoYW5nZU1lMB4XDTIwMDMxMTAwMTA1M1oXDTMwMDMwOTAwMTA1M1owEzER MA8GA1UEAwwIQ2hhbmdlTWUwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIB AQD5FZJN5STAXRX7ZBq8CVf7DntSQTgnVVqwntKJwggTPHgwn8uMUWRdaIpXZVN5 MYTGPCICDoxdlF/2KUgH9n/L1Rlmm9RW4beXMwFJUR8NIExf5vQy03gk6JpEO1DA Pu+x0/EhXGvGo/lAEpF4rk0ZPpNEkFM71bIqhKAMAe9M5c2ZrAxqplyTz/Zl4nRm YQSsqnx3ikN+SkxdnifIBlF3MzCHqCCV9QaOkrztXHs9XFhnWpyu+OLqyP5+ipOZ gYsTDA4otjv6D9MX+BoWCZ6zSzo/kMSkM7ByZt5jjyp1lQQaYnZe8LmRkB3vcBb4 lWlN8Gu3tvunXSlKJWp7Fh7VAgMBAAGjgYEwfzAdBgNVHQ4EFgQUSDkx6kENF55m RsJZip/xOrv2E2EwQwYDVR0jBDwwOoAUSDkx6kENF55mRsJZip/xOrv2E2GhF6QV MBMxETAPBgNVBAMMCENoYW5nZU1lggkA2Yfj2kmOI5QwDAYDVR0TBAUwAwEB/zAL BgNVHQ8EBAMCAQYwDQYJKoZIhvcNAQELBQADggEBADwKrP9NcTakAbQnd+x+lBzv co0I2XOJrsm6N1r8MKVjEq9Ti5quGtoDLNQDlORnKAaWVzSg6oAFNVrItVJU5GRe J+XI+t2pXqo/OBlVoXcwG52m2rXd9e5wjdmrYwpzijvj//FjjfIZysJJiLW8xSA9 t+3/BCCGqy6uBy2KNvuYMQHr2BdHU05haXtp/mrsalSTlvLFwJeUbHDrqCKoFlDj tXkzcF4sIOfF0dzQXdXT5qerZGOMsXBQ8ALFoHd/wvS5cJvI8nWywEg3w3vWCSO1 zLdcNmvIqYEYrLZBhtLlwBnjKuHSsXorfJsUcmdKsgwIw1KtMBF2bBMyd8twBn8= -----END CERTIFICATE----- </ca> <cert> -----BEGIN CERTIFICATE----- MIIDSTCCAjGgAwIBAgIRAODlLyd7mnQoRNC4oqxJm5AwDQYJKoZIhvcNAQELBQAw EzERMA8GA1UEAwwIQ2hhbmdlTWUwHhcNMjAwMzExMDAxMzA5WhcNMzAwMzA5MDAx MzA5WjAWMRQwEgYDVQQDDAt0ZXN0LWNsaWVudDCCASIwDQYJKoZIhvcNAQEBBQAD ggEPADCCAQoCggEBAOP2megEI8f/e0Xxi6n+EKQwaLZweYFTVg25vT2X6a2HHJfg 8tXznih0NxGJFyITmpl+lddBXEnm/ZqSH6HBGujyd8aWHZ1algvbpyzU0qNXRoAu AjknbkcQ4/m+28/1ocGukY2aKYjQXddp4HzquSQupza/3JcJ+5roWte1PzLZCC74 yfdzhdBwHHOfG4B7SfYOuT7eXQwisCrTFZmtK1FoONhwSlhqcEbMBaEjT9ZP7K7p WSmx82c7xyYhdD4JMZ79qiIm/pbeszu1SpUqd3682mVwmZZOCUWf3pRKwcwEyJnk YKS9ksKTh0F9B9VibfvNw2harR3471qwt6pbSXUCAwEAAaOBlDCBkTAJBgNVHRME AjAAMB0GA1UdDgQWBBTv4I3fmPShB7U6scRReENGsLkiQDBDBgNVHSMEPDA6gBRI OTHqQQ0XnmZGwlmKn/E6u/YTYaEXpBUwEzERMA8GA1UEAwwIQ2hhbmdlTWWCCQDZ h+PaSY4jlDATBgNVHSUEDDAKBggrBgEFBQcDAjALBgNVHQ8EBAMCB4AwDQYJKoZI hvcNAQELBQADggEBACKCvwckhCZ7w5j79gYvRhujm02z2Bah7aggZ9uoyYFw3EVi 1GmyU6aoa3ui2UKciWglm8R21TuhnPsUopbWNniHDlFqOOrVxFST11FD02Qfae8P 6YWhkbUoaS3IwF7NOPg56Q7VaU1P8+GI2fR5kjHrb9pBPTCFX+1gSpiA0TE3DHj4 zO7NFRq+hE17QqeE1+W7pq4uyZYQFpbC6n+VsCJWBXDm/8WR97uJpjWUjFCNPm71 PD5YN6cSa9iasBQVvBWbKkMaf+aFvtLHGteYrVUGkvpnw9DquYFxMnHpwegU4DQh PRL2TL8szw7751o2v2CHZ+zLJbDaq26thdoIh64= -----END CERTIFICATE----- </cert> <key> -----BEGIN PRIVATE KEY----- MIIEvgIBADANBgkqhkiG9w0BAQEFAASCBKgwggSkAgEAAoIBAQDj9pnoBCPH/3tF 8Yup/hCkMGi2cHmBU1YNub09l+mthxyX4PLV854odDcRiRciE5qZfpXXQVxJ5v2a kh+hwRro8nfGlh2dWpYL26cs1NKjV0aALgI5J25HEOP5vtvP9aHBrpGNmimI0F3X aeB86rkkLqc2v9yXCfua6FrXtT8y2Qgu+Mn3c4XQcBxznxuAe0n2Drk+3l0MIrAq 0xWZrStRaDjYcEpYanBGzAWhI0/WT+yu6VkpsfNnO8cmIXQ+CTGe/aoiJv6W3rM7 tUqVKnd+vNplcJmWTglFn96USsHMBMiZ5GCkvZLCk4dBfQfVYm37zcNoWq0d+O9a sLeqW0l1AgMBAAECggEBAKU7AscG6SB3b1R9BWxLeKhpZhyGXat9Sexc6muQhpF+ Ux1KsPiewc40ng2Zvii26OHEvLru5wOx57N3onHN08FwrZxFBmYdWJBzvzJhd+No yPLzZi0jBW2BMpy81/pd4cbOzzVBvkUqMjqGxW4Fe/hb0FuAqVTYqYPYUq/y8UHa atIehY3jNc46pSRmmFIDDdyh6K5lmFVZntpRKg9RzUibQxBkLZZwnRwFLf58wJbr Os9OT2QZsaSDIIK4mtL3xTVbT9ORC/ADY6XXO+Yb6IyLqD6WD5Yqh7wEpp/Gv4Ob BvlObULOEZnjeAK9FIPs9gFuimBjcJK5kX3an8yok6kCgYEA/cqQIMR8ORRdTBaj v2CK8RtQOJ2VPEpINcxPHK8vh38CrKNmCjETXqhkCwI1wOT/WKA4IUHBLfOhgC00 cHYn6k2JfossQGh8DvjyY+JtdmSamzeecQ4i13RcnSj5G+kY/iEQogTaSALpB1Uo cugU116HiHSvcz+FK3Ia4lAHnjcCgYEA5fJ+lg3lCCd1Cq4UpzGWLMWpo5VBX9Eu QhWRC2uIGkO4BAXVlkU/1TOvzonfoHLcyVUlLjE//p6djyVezkdVHTYYXQwrWIYE oinC4YnxV1Pcvii7WaBw9t3s5REYdgyvT0Wh7GIm+o6TMnfBTvVV/DMU6K9z59f+ wLXfMZaZH7MCgYBOXhdlVub5BTXOAgusU9ZznziFUvu7M0DbA+zF8b6ee3TK9GXU 7dSKXTsPPy50EwJaTpcmhdRuKRYMq2jO9V1b93dmkPkoJltwkCTg/RFKBsTK+0C8 rl3J5A+ZJAbQPIlQJ8uoDBGPPP7SGdS0rr+IxZLaaxWmY83uXXy5t3ayvwKBgQCM YMrovljI7pWkTHvtSfddI9qZNAAyB5jO3S2sJBx1tEu9oPYwg9whQymb1E3CPP0O qD1HgueHgLu9bNoA4klSyPh8rXY017Qyb346hCTi5B6JtIITiEAOZZM+kH43ay0H HwJoNc+H/Mxd7gAEPQAeM+0a1CnVKuaqLR2xvzeBwwKBgC+vydOv2Fqu58b+/cWi 52/stI11Y+xkdQ+/SP+cAucN05xVrfFzEbv90/Tintk2G+oCb5lWxM2uGIfSMCMA CUHg03a0oZdTTapUs+i0fahuhR/ojK5i4COTHM0jF3ryr1Gjo0RUgbJe/RlnRY5v bbOS07Ao554/jPNrXGzImnQz -----END PRIVATE KEY----- </key> <tls-crypt> -----BEGIN OpenVPN Static key V1----- 470a961d29e78b8f4884b46741587ecf 6008c6bb16acf2eae299f68df994133d 7fbe5dbacd187c21ac9e61bc2aab3de0 c88f39674dec40ef4844dddb80884ad4 652542876fdadd98ca95cf4e9f4ed6e8 2b2f6315aa77c0ae9fc5dca6df687622 82f629e230990b340b1b95f6f7ca18a4 185176cf29c04d5d0a9f9c19083fe3b6 24e55a25f5e5ccf2a48f33373d56792a 20f60074f9e6ef855e0b0ceca0a07300 294718d41af0a97da641053397fdc944 d21f5a9a702a118de21440fce772ab17 11a575acc9ce0097e2fdefc1233ea2e6 01e49032eaf2aa3e0898c3f5b334839f f8c69c80614a45cfb0ba7d43d3476e37 a22a4d43b0dbc96430b1115a6b1f6aac -----END OpenVPN Static key V1----- </tls-crypt> NOTE: The script automatically accounts for NAT. Notice in your client config that it sets the remote server as whatever your external address is. You may not want this behavior. If that is the case you will need to go in and edit the remote line with the IP address of your VPN server. On CentOS 7 Make sure everything is up to date. yum update -y && reboot . The reboot is important because if your kernel might update. If this happens you need to reboot to load the new kernel. Run yum install -y epel-release && yum update -y && yum install -y openvpn easy-rsa chrony && systemctl enable chronyd && chronyc makestep This is a long series of commands, but it installs openvpn and chrony. You need chrony to ensure your time is synched. WARNING : If the time is not synched between the server and your clients, the VPN will fail to connect! You should have copied your client config to your client already. If you haven't, do that now. To run the VPN, run openvpn <client_config_name>","title":"Run VPN on OS10"},{"location":"Run%20VPN%20on%20OS10/#run-vpn-on-os10","text":"","title":"Run VPN on OS10"},{"location":"Run%20VPN%20on%20OS10/#my-configuration","text":"","title":"My Configuration"},{"location":"Run%20VPN%20on%20OS10/#dell-4112f-on","text":"Dell EMC Networking OS10 Enterprise Copyright (c) 1999-2020 by Dell Inc. All Rights Reserved. OS Version: 10.5.0.4 Build Version: 10.5.0.4.638 Build Time: 2020-01-30T21:08:56+0000 System Type: S4112F-ON Architecture: x86_64 Up Time: 2 days 03:54:07","title":"Dell 4112F-ON"},{"location":"Run%20VPN%20on%20OS10/#centos","text":"[root@centos ~]# cat /etc/*-release CentOS Linux release 7.6.1810 (Core) NAME=\"CentOS Linux\" VERSION=\"7 (Core)\" ID=\"centos\" ID_LIKE=\"rhel fedora\" VERSION_ID=\"7\" PRETTY_NAME=\"CentOS Linux 7 (Core)\" ANSI_COLOR=\"0;31\" CPE_NAME=\"cpe:/o:centos:centos:7\" HOME_URL=\"https://www.centos.org/\" BUG_REPORT_URL=\"https://bugs.centos.org/\" CENTOS_MANTISBT_PROJECT=\"CentOS-7\" CENTOS_MANTISBT_PROJECT_VERSION=\"7\" REDHAT_SUPPORT_PRODUCT=\"centos\" REDHAT_SUPPORT_PRODUCT_VERSION=\"7\" CentOS Linux release 7.6.1810 (Core) CentOS Linux release 7.6.1810 (Core)","title":"CentOS"},{"location":"Run%20VPN%20on%20OS10/#testing-topology","text":"Switch Configuration","title":"Testing Topology"},{"location":"Run%20VPN%20on%20OS10/#research-sources","text":"Helpful Book Basics of Network Processor Packet Processing How Network Processors Work","title":"Research Sources"},{"location":"Run%20VPN%20on%20OS10/#npu-interface-problem","text":"The one big gotcha with doing this is that when you drop to the command line in OS10 and do a ip a s , the interfaces you see that look like physical interfaces ex: 13: e101-001-0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc multiq master br32 state UP group default qlen 1000 link/ether 50:9a:4c:d6:0a:71 brd ff:ff:ff:ff:ff:ff 14: e101-002-0: <BROADCAST,MULTICAST> mtu 1500 qdisc noop master br1 state DOWN group default qlen 1000 link/ether 50:9a:4c:d6:0a:72 brd ff:ff:ff:ff:ff:ff 15: e101-003-0: <BROADCAST,MULTICAST> mtu 1500 qdisc noop master br1 state DOWN group default qlen 1000 link/ether 50:9a:4c:d6:0a:73 brd ff:ff:ff:ff:ff:ff 16: e101-004-0: <BROADCAST,MULTICAST> mtu 1500 qdisc noop master br1 state DOWN group default qlen 1000 link/ether 50:9a:4c:d6:0a:74 brd ff:ff:ff:ff:ff:ff are not actual physical interfaces. Under the hood the operating system is actually using tap interfaces. Inside the switch there are two processors - a regular x86 processor and a separate processor called the Network Processing Unit (NPU). The interfaces are connected to the NPU. Most traffic that comes in on the physical interfaces managed by the NPU does not flow up to the x86 chip. This means that if you do a tcpdump on one of the interfaces you see in ip a s you will see very little. In fact, the only traffic you will see is management traffic which is handled by the Linux kernel. This means if you want to set up a VPN you have to have a way to make sure all the traffic is visible to the Linux kernel. Fortunately, there is a way to make this happen. VLAN interfaces are virtual and subsequently are handled entirely by the Linux kernel. In fact, VLAN interfaces actually show up under the hood as bridge interfaces: 29: br32: <BROADCAST,MULTICAST,PROMISC,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default qlen 1000 link/ether 50:9a:4c:d6:0a:a1 brd ff:ff:ff:ff:ff:ff inet 192.168.32.1/24 brd 255.168.32.255 scope global br32 valid_lft forever preferred_lft forever inet6 fe80::529a:4cff:fed6:aa1/64 scope link valid_lft forever preferred_lft forever 41: br33: <BROADCAST,MULTICAST,PROMISC,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default qlen 1000 link/ether 50:9a:4c:d6:0a:a1 brd ff:ff:ff:ff:ff:ff inet 192.168.33.1/24 brd 255.168.33.255 scope global br33 valid_lft forever preferred_lft forever inet6 fe80::529a:4cff:fed6:aa1/64 scope link valid_lft forever preferred_lft forever These two correspond to interface vlans 32 and 33. By using VLAN interfaces you can tie the VPN to these interfaces and everything works just fine. You just place any associated physical interfaces as access VLANs or trunks with the appropriate allowed VLANs.","title":"NPU Interface Problem"},{"location":"Run%20VPN%20on%20OS10/#installing-openvpn-as-a-server-on-the-4112f-on","text":"Note: I ran a VPN server on my switch, but you could just as easily make the switch a point to point VPN gateway connecting to a PFSense instance such that anything that can reach the switch could participate in a multipoint network. On the 4112F-ON: Enter configuration mode from user mode by running en and then config <enter> Run ip name-server 192.168.1.1 to add a name server. Run write mem in enable mode to save your configuration changes. Run system bash Before continuing, make sure that the time is correct on the device. WARNING If you do not do this and you generate certificates, none of the encryption will work and you will have to recreate all of your certificates! Run sudo apt-get install -y openvpn vim . I installed vim because I don't hate myself. I used this script from git.io/vpn to install OpenVPN. Having done the entire thing manually before, I can tell you this saves a huge amount of time. To run the script run wget https://git.io/vpn -O openvpn-install.sh && chmod +x openvpn-install.sh && ./openvpn-install.sh 1.Fill in the options as needed. I did find some things you have to tweak with their script. Perform the below to clean things up. 1.Run vim /lib/systemd/system/openvpn@.service . Where it says --config /etc/openvpn/%i.conf , change that to --config /etc/openvpn/%i/%i.conf . For details on specifies work see this post . When you are done run systemctl daemon-reload to reload the systemd daemon. 2.If you used my version of the script then you do not need to do this. Otherwise you need to run vim /etc/openvpn/server/server.conf and you need to prepend /etc/openvpn/server/ on several of the paths or the service won't start. See my config below: local 192.168.32.1 port 1194 proto udp dev tun ca /etc/openvpn/server/ca.crt cert /etc/openvpn/server/server.crt key /etc/openvpn/server/server.key dh /etc/openvpn/server/dh.pem auth SHA512 tls-crypt /etc/openvpn/server/tc.key topology subnet server 10.8.0.0 255.255.255.0 ifconfig-pool-persist ipp.txt push \"redirect-gateway def1 bypass-dhcp\" push \"dhcp-option DNS 192.168.1.1\" keepalive 10 120 cipher AES-256-CBC user nobody group nogroup persist-key persist-tun status openvpn-status.log verb 3 crl-verify /etc/openvpn/server/crl.pem explicit-exit-notify You may want to add something like push route 192.168.1.0 255.255.255.0 to your server config. This allows the server to push routes to the client. For example, in my case the 192.168.1.0/24 network is behind my server, so I have to push a route so that the clients know how to get to it. Just keep in mind, that hosts on your distant network must have a route back to your VPN network. Run systemctl start openvpn@server to start the server. Rerun the script to add clients. Your output should look like the below. In my case I added one client to perform the test. ``` Looks like OpenVPN is already installed. What do you want to do? 1) Add a new user 2) Revoke an existing user 3) Remove OpenVPN 4) Exit Select an option: 1 Tell me a name for the client certificate. Client name: test-client Using SSL: openssl OpenSSL 1.1.0l 10 Sep 2019 Generating a RSA private key ........+++++ .......+++++ writing new private key to '/etc/openvpn/server/easy-rsa/pki/private/test-client.key.WONcIB6m1N' ----- Using configuration from ./safessl-easyrsa.cnf Check that the request matches the signature Signature ok The Subject's Distinguished Name is as follows commonName :ASN.1 12:'test-client' Certificate is to be certified until Mar 9 00:13:09 2030 GMT (3650 days) Write out database with 1 new entries Data Base Updated Client test-client added, configuration is available at: /root/test-client.ovpn ``` Copy the contents of your client config. In my case this was from /root/test-client.ovpn and it looked like: client dev tun proto udp remote <SERVER ADDRESS> 1194 resolv-retry infinite nobind persist-key persist-tun remote-cert-tls server auth SHA512 cipher AES-256-CBC ignore-unknown-option block-outside-dns block-outside-dns verb 3 <ca> -----BEGIN CERTIFICATE----- MIIDKzCCAhOgAwIBAgIJANmH49pJjiOUMA0GCSqGSIb3DQEBCwUAMBMxETAPBgNV BAMMCENoYW5nZU1lMB4XDTIwMDMxMTAwMTA1M1oXDTMwMDMwOTAwMTA1M1owEzER MA8GA1UEAwwIQ2hhbmdlTWUwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIB AQD5FZJN5STAXRX7ZBq8CVf7DntSQTgnVVqwntKJwggTPHgwn8uMUWRdaIpXZVN5 MYTGPCICDoxdlF/2KUgH9n/L1Rlmm9RW4beXMwFJUR8NIExf5vQy03gk6JpEO1DA Pu+x0/EhXGvGo/lAEpF4rk0ZPpNEkFM71bIqhKAMAe9M5c2ZrAxqplyTz/Zl4nRm YQSsqnx3ikN+SkxdnifIBlF3MzCHqCCV9QaOkrztXHs9XFhnWpyu+OLqyP5+ipOZ gYsTDA4otjv6D9MX+BoWCZ6zSzo/kMSkM7ByZt5jjyp1lQQaYnZe8LmRkB3vcBb4 lWlN8Gu3tvunXSlKJWp7Fh7VAgMBAAGjgYEwfzAdBgNVHQ4EFgQUSDkx6kENF55m RsJZip/xOrv2E2EwQwYDVR0jBDwwOoAUSDkx6kENF55mRsJZip/xOrv2E2GhF6QV MBMxETAPBgNVBAMMCENoYW5nZU1lggkA2Yfj2kmOI5QwDAYDVR0TBAUwAwEB/zAL BgNVHQ8EBAMCAQYwDQYJKoZIhvcNAQELBQADggEBADwKrP9NcTakAbQnd+x+lBzv co0I2XOJrsm6N1r8MKVjEq9Ti5quGtoDLNQDlORnKAaWVzSg6oAFNVrItVJU5GRe J+XI+t2pXqo/OBlVoXcwG52m2rXd9e5wjdmrYwpzijvj//FjjfIZysJJiLW8xSA9 t+3/BCCGqy6uBy2KNvuYMQHr2BdHU05haXtp/mrsalSTlvLFwJeUbHDrqCKoFlDj tXkzcF4sIOfF0dzQXdXT5qerZGOMsXBQ8ALFoHd/wvS5cJvI8nWywEg3w3vWCSO1 zLdcNmvIqYEYrLZBhtLlwBnjKuHSsXorfJsUcmdKsgwIw1KtMBF2bBMyd8twBn8= -----END CERTIFICATE----- </ca> <cert> -----BEGIN CERTIFICATE----- MIIDSTCCAjGgAwIBAgIRAODlLyd7mnQoRNC4oqxJm5AwDQYJKoZIhvcNAQELBQAw EzERMA8GA1UEAwwIQ2hhbmdlTWUwHhcNMjAwMzExMDAxMzA5WhcNMzAwMzA5MDAx MzA5WjAWMRQwEgYDVQQDDAt0ZXN0LWNsaWVudDCCASIwDQYJKoZIhvcNAQEBBQAD ggEPADCCAQoCggEBAOP2megEI8f/e0Xxi6n+EKQwaLZweYFTVg25vT2X6a2HHJfg 8tXznih0NxGJFyITmpl+lddBXEnm/ZqSH6HBGujyd8aWHZ1algvbpyzU0qNXRoAu AjknbkcQ4/m+28/1ocGukY2aKYjQXddp4HzquSQupza/3JcJ+5roWte1PzLZCC74 yfdzhdBwHHOfG4B7SfYOuT7eXQwisCrTFZmtK1FoONhwSlhqcEbMBaEjT9ZP7K7p WSmx82c7xyYhdD4JMZ79qiIm/pbeszu1SpUqd3682mVwmZZOCUWf3pRKwcwEyJnk YKS9ksKTh0F9B9VibfvNw2harR3471qwt6pbSXUCAwEAAaOBlDCBkTAJBgNVHRME AjAAMB0GA1UdDgQWBBTv4I3fmPShB7U6scRReENGsLkiQDBDBgNVHSMEPDA6gBRI OTHqQQ0XnmZGwlmKn/E6u/YTYaEXpBUwEzERMA8GA1UEAwwIQ2hhbmdlTWWCCQDZ h+PaSY4jlDATBgNVHSUEDDAKBggrBgEFBQcDAjALBgNVHQ8EBAMCB4AwDQYJKoZI hvcNAQELBQADggEBACKCvwckhCZ7w5j79gYvRhujm02z2Bah7aggZ9uoyYFw3EVi 1GmyU6aoa3ui2UKciWglm8R21TuhnPsUopbWNniHDlFqOOrVxFST11FD02Qfae8P 6YWhkbUoaS3IwF7NOPg56Q7VaU1P8+GI2fR5kjHrb9pBPTCFX+1gSpiA0TE3DHj4 zO7NFRq+hE17QqeE1+W7pq4uyZYQFpbC6n+VsCJWBXDm/8WR97uJpjWUjFCNPm71 PD5YN6cSa9iasBQVvBWbKkMaf+aFvtLHGteYrVUGkvpnw9DquYFxMnHpwegU4DQh PRL2TL8szw7751o2v2CHZ+zLJbDaq26thdoIh64= -----END CERTIFICATE----- </cert> <key> -----BEGIN PRIVATE KEY----- MIIEvgIBADANBgkqhkiG9w0BAQEFAASCBKgwggSkAgEAAoIBAQDj9pnoBCPH/3tF 8Yup/hCkMGi2cHmBU1YNub09l+mthxyX4PLV854odDcRiRciE5qZfpXXQVxJ5v2a kh+hwRro8nfGlh2dWpYL26cs1NKjV0aALgI5J25HEOP5vtvP9aHBrpGNmimI0F3X aeB86rkkLqc2v9yXCfua6FrXtT8y2Qgu+Mn3c4XQcBxznxuAe0n2Drk+3l0MIrAq 0xWZrStRaDjYcEpYanBGzAWhI0/WT+yu6VkpsfNnO8cmIXQ+CTGe/aoiJv6W3rM7 tUqVKnd+vNplcJmWTglFn96USsHMBMiZ5GCkvZLCk4dBfQfVYm37zcNoWq0d+O9a sLeqW0l1AgMBAAECggEBAKU7AscG6SB3b1R9BWxLeKhpZhyGXat9Sexc6muQhpF+ Ux1KsPiewc40ng2Zvii26OHEvLru5wOx57N3onHN08FwrZxFBmYdWJBzvzJhd+No yPLzZi0jBW2BMpy81/pd4cbOzzVBvkUqMjqGxW4Fe/hb0FuAqVTYqYPYUq/y8UHa atIehY3jNc46pSRmmFIDDdyh6K5lmFVZntpRKg9RzUibQxBkLZZwnRwFLf58wJbr Os9OT2QZsaSDIIK4mtL3xTVbT9ORC/ADY6XXO+Yb6IyLqD6WD5Yqh7wEpp/Gv4Ob BvlObULOEZnjeAK9FIPs9gFuimBjcJK5kX3an8yok6kCgYEA/cqQIMR8ORRdTBaj v2CK8RtQOJ2VPEpINcxPHK8vh38CrKNmCjETXqhkCwI1wOT/WKA4IUHBLfOhgC00 cHYn6k2JfossQGh8DvjyY+JtdmSamzeecQ4i13RcnSj5G+kY/iEQogTaSALpB1Uo cugU116HiHSvcz+FK3Ia4lAHnjcCgYEA5fJ+lg3lCCd1Cq4UpzGWLMWpo5VBX9Eu QhWRC2uIGkO4BAXVlkU/1TOvzonfoHLcyVUlLjE//p6djyVezkdVHTYYXQwrWIYE oinC4YnxV1Pcvii7WaBw9t3s5REYdgyvT0Wh7GIm+o6TMnfBTvVV/DMU6K9z59f+ wLXfMZaZH7MCgYBOXhdlVub5BTXOAgusU9ZznziFUvu7M0DbA+zF8b6ee3TK9GXU 7dSKXTsPPy50EwJaTpcmhdRuKRYMq2jO9V1b93dmkPkoJltwkCTg/RFKBsTK+0C8 rl3J5A+ZJAbQPIlQJ8uoDBGPPP7SGdS0rr+IxZLaaxWmY83uXXy5t3ayvwKBgQCM YMrovljI7pWkTHvtSfddI9qZNAAyB5jO3S2sJBx1tEu9oPYwg9whQymb1E3CPP0O qD1HgueHgLu9bNoA4klSyPh8rXY017Qyb346hCTi5B6JtIITiEAOZZM+kH43ay0H HwJoNc+H/Mxd7gAEPQAeM+0a1CnVKuaqLR2xvzeBwwKBgC+vydOv2Fqu58b+/cWi 52/stI11Y+xkdQ+/SP+cAucN05xVrfFzEbv90/Tintk2G+oCb5lWxM2uGIfSMCMA CUHg03a0oZdTTapUs+i0fahuhR/ojK5i4COTHM0jF3ryr1Gjo0RUgbJe/RlnRY5v bbOS07Ao554/jPNrXGzImnQz -----END PRIVATE KEY----- </key> <tls-crypt> -----BEGIN OpenVPN Static key V1----- 470a961d29e78b8f4884b46741587ecf 6008c6bb16acf2eae299f68df994133d 7fbe5dbacd187c21ac9e61bc2aab3de0 c88f39674dec40ef4844dddb80884ad4 652542876fdadd98ca95cf4e9f4ed6e8 2b2f6315aa77c0ae9fc5dca6df687622 82f629e230990b340b1b95f6f7ca18a4 185176cf29c04d5d0a9f9c19083fe3b6 24e55a25f5e5ccf2a48f33373d56792a 20f60074f9e6ef855e0b0ceca0a07300 294718d41af0a97da641053397fdc944 d21f5a9a702a118de21440fce772ab17 11a575acc9ce0097e2fdefc1233ea2e6 01e49032eaf2aa3e0898c3f5b334839f f8c69c80614a45cfb0ba7d43d3476e37 a22a4d43b0dbc96430b1115a6b1f6aac -----END OpenVPN Static key V1----- </tls-crypt> NOTE: The script automatically accounts for NAT. Notice in your client config that it sets the remote server as whatever your external address is. You may not want this behavior. If that is the case you will need to go in and edit the remote line with the IP address of your VPN server.","title":"Installing OpenVPN as a Server on the 4112F-ON"},{"location":"Run%20VPN%20on%20OS10/#on-centos-7","text":"Make sure everything is up to date. yum update -y && reboot . The reboot is important because if your kernel might update. If this happens you need to reboot to load the new kernel. Run yum install -y epel-release && yum update -y && yum install -y openvpn easy-rsa chrony && systemctl enable chronyd && chronyc makestep This is a long series of commands, but it installs openvpn and chrony. You need chrony to ensure your time is synched. WARNING : If the time is not synched between the server and your clients, the VPN will fail to connect! You should have copied your client config to your client already. If you haven't, do that now. To run the VPN, run openvpn <client_config_name>","title":"On CentOS 7"},{"location":"Run%20VPN%20on%20OS10/README_wireguard_%28unfinished%29/","text":"Run VPN on OS10 My Configuration Dell 4112F-ON Dell EMC Networking OS10 Enterprise Copyright (c) 1999-2020 by Dell Inc. All Rights Reserved. OS Version: 10.5.0.4 Build Version: 10.5.0.4.638 Build Time: 2020-01-30T21:08:56+0000 System Type: S4112F-ON Architecture: x86_64 Up Time: 2 days 03:54:07 CentOS [root@centos ~]# cat /etc/*-release CentOS Linux release 7.6.1810 (Core) NAME=\"CentOS Linux\" VERSION=\"7 (Core)\" ID=\"centos\" ID_LIKE=\"rhel fedora\" VERSION_ID=\"7\" PRETTY_NAME=\"CentOS Linux 7 (Core)\" ANSI_COLOR=\"0;31\" CPE_NAME=\"cpe:/o:centos:centos:7\" HOME_URL=\"https://www.centos.org/\" BUG_REPORT_URL=\"https://bugs.centos.org/\" CENTOS_MANTISBT_PROJECT=\"CentOS-7\" CENTOS_MANTISBT_PROJECT_VERSION=\"7\" REDHAT_SUPPORT_PRODUCT=\"centos\" REDHAT_SUPPORT_PRODUCT_VERSION=\"7\" CentOS Linux release 7.6.1810 (Core) CentOS Linux release 7.6.1810 (Core) Physical Configuration Interface ethernet 1/1/12 on the 4112F-ON plugged directly into a server running ESXi. That interface was assigned as an uplink associated with a vswitch when then was tied to a portgroup running on VLAN 32. ethernet 1/1/12 was configured as follows: OS10(conf-if-eth1/1/12)# show configuration ! interface ethernet1/1/12 no shutdown switchport mode trunk switchport access vlan 1 switchport trunk allowed vlan 32 flowcontrol receive on interface vlan 32 was configured as follows: OS10(conf-if-vl-32)# show configuration ! interface vlan32 no shutdown ip address 192.168.32.1/24 The full switch config is here Interface ethernet 1/1/1 was configured as an access port in VLAN Research Sources Helpful Book Basics of Network Processor Packet Processing How Network Processors Work NPU Problem Explanation Installing WireGuard On the 4112F-ON: Enter configuration mode from user mode by running en and then config <enter> Run ip name-server 192.168.1.1 to add a name server. Run write mem in enable mode to save your configuration changes. Run system bash If you don't have a default route on your switch, you will need to add one with sudo ip route add 0.0.0.0/0 via 192.168.1.1 . Before installing WireGuard you will need the latest kernel headers. 1.The kernel headers on the system will work a bit differently than regular systems. I found the relevant kernel headers with apt search linux-headers | grep $(uname -r) 2.You should see an entry mentioning all - something like linux-headers-4.9.0-11-all . You want to install that package with sudo apt-get install -y <LINUX HEADER NAME> bc 3.At this point you will need to reboot. Do this by exiting the command line and in enable mode running reload 4.If you do not have a permanent default route set, when you log back in you will need to run system bash and then readd the default route with sudo ip route add 0.0.0.0/0 via 192.168.1.1 mkdir /opt/wireguard && cd /opt/wireguard Now you will either need to run all of the following as sudo or you will need to add a password to the root account with sudo passwd and then su - to become root. After you have done the above run: echo \"deb http://deb.debian.org/debian/ unstable main\" > /etc/apt/sources.list.d/unstable-wireguard.list printf 'Package: *\\nPin: release a=unstable\\nPin-Priority: 90\\n' > /etc/apt/preferences.d/limit-unstable apt update apt install wireguard Generate key pairs wg genkey | tee wg-private.key | wg pubkey > wg-public.key On CentOS 7 Make sure everything is up to date. yum update -y && reboot . The reboot is important because if your kernel might update. If this happens you need to reboot to load the new kernel. Wireguard requires kernel 3.10 or higher - I noticed if you haven't updated CentOS for a bit than your kernel might be to old and you'll get RTNETLINK answers: Operation not supported . Run: sudo yum install https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm sudo curl -o /etc/yum.repos.d/jdoss-wireguard-epel-7.repo https://copr.fedorainfracloud.org/coprs/jdoss/wireguard/repo/epel-7/jdoss-wireguard-epel-7.repo sudo yum install wireguard-dkms wireguard-tools mkdir /opt/wireguard && cd /opt/wireguard Generate key pairs wg genkey | tee wg-private.key | wg pubkey > wg-public.key ip link add wg0 type wireguard Set Wireguard interface IP ip address add dev wg0 10.0.0.2/24 . This IP address is the tunnel IP address. 1.If there are only two peers together you can do something like ip address add dev wg0 10.0.0.2 peer 10.0.0.1 wg set wg0 listen-port 51820 private-key /opt/wireguard/wg-private.key peer OQiSLUOd3YWCfEkHazJHuuaJVNc++8QOb2+sOOZl/2c= endpoint 192.168.32.1:8172 1.listen-port: the port this host will listen on 2.private-key: the private key for this host 3.peer: I thought this was a bit misleading - you want the public key of the other host to which you are connecting 4.endpoint: the other endpoint of the VPN and the port on which it is listening Strange Behavior OS10(config)# management route 192.168.1.0/24 192.168.1.1 % Error: Overlapping route for Management interface OS10(config)# ip route 0.0.0.0/0 192.168.1.1 interface ethernet 1/1/1 % Error: Network unreachable OS10(config)# ping 192.168.1.1 PING 192.168.1.1 (192.168.1.1) 56(84) bytes of data. 64 bytes from 192.168.1.1: icmp_seq=1 ttl=64 time=0.452 ms 64 bytes from 192.168.1.1: icmp_seq=2 ttl=64 time=0.388 ms ^C --- 192.168.1.1 ping statistics --- 2 packets transmitted, 2 received, 0% packet loss, time 1005ms rtt min/avg/max/mdev = 0.388/0.420/0.452/0.032 ms OS10(config)# do system bash admin@OS10:~$ su - Password: root@OS10:~# ip route 192.168.1.0/24 dev eth0 proto kernel scope link src 192.168.1.20 192.168.4.0/24 dev br32 proto none scope link root@OS10:~# ip route add 0.0.0.0/0 via 192.168.1.1 root@OS10:~# ip route default via 192.168.1.1 dev eth0 192.168.1.0/24 dev eth0 proto kernel scope link src 192.168.1.20 192.168.4.0/24 dev br32 proto none scope link root@OS10:~# ping google.com PING google.com (216.58.193.142) 56(84) bytes of data. 64 bytes from dfw25s34-in-f14.1e100.net (216.58.193.142): icmp_seq=1 ttl=55 time=25.4 ms 64 bytes from dfw25s34-in-f14.1e100.net (216.58.193.142): icmp_seq=2 ttl=55 time=22.9 ms 64 bytes from dfw25s34-in-f14.1e100.net (216.58.193.142): icmp_seq=3 ttl=55 time=22.1 ms 64 bytes from dfw25s34-in-f14.1e100.net (216.58.193.142): icmp_seq=4 ttl=55 time=22.2 ms ^C --- google.com ping statistics --- 4 packets transmitted, 4 received, 0% packet loss, time 3002ms rtt min/avg/max/mdev = 22.149/23.185/25.408/1.329 ms","title":"Run VPN on OS10"},{"location":"Run%20VPN%20on%20OS10/README_wireguard_%28unfinished%29/#run-vpn-on-os10","text":"","title":"Run VPN on OS10"},{"location":"Run%20VPN%20on%20OS10/README_wireguard_%28unfinished%29/#my-configuration","text":"","title":"My Configuration"},{"location":"Run%20VPN%20on%20OS10/README_wireguard_%28unfinished%29/#dell-4112f-on","text":"Dell EMC Networking OS10 Enterprise Copyright (c) 1999-2020 by Dell Inc. All Rights Reserved. OS Version: 10.5.0.4 Build Version: 10.5.0.4.638 Build Time: 2020-01-30T21:08:56+0000 System Type: S4112F-ON Architecture: x86_64 Up Time: 2 days 03:54:07","title":"Dell 4112F-ON"},{"location":"Run%20VPN%20on%20OS10/README_wireguard_%28unfinished%29/#centos","text":"[root@centos ~]# cat /etc/*-release CentOS Linux release 7.6.1810 (Core) NAME=\"CentOS Linux\" VERSION=\"7 (Core)\" ID=\"centos\" ID_LIKE=\"rhel fedora\" VERSION_ID=\"7\" PRETTY_NAME=\"CentOS Linux 7 (Core)\" ANSI_COLOR=\"0;31\" CPE_NAME=\"cpe:/o:centos:centos:7\" HOME_URL=\"https://www.centos.org/\" BUG_REPORT_URL=\"https://bugs.centos.org/\" CENTOS_MANTISBT_PROJECT=\"CentOS-7\" CENTOS_MANTISBT_PROJECT_VERSION=\"7\" REDHAT_SUPPORT_PRODUCT=\"centos\" REDHAT_SUPPORT_PRODUCT_VERSION=\"7\" CentOS Linux release 7.6.1810 (Core) CentOS Linux release 7.6.1810 (Core)","title":"CentOS"},{"location":"Run%20VPN%20on%20OS10/README_wireguard_%28unfinished%29/#physical-configuration","text":"Interface ethernet 1/1/12 on the 4112F-ON plugged directly into a server running ESXi. That interface was assigned as an uplink associated with a vswitch when then was tied to a portgroup running on VLAN 32. ethernet 1/1/12 was configured as follows: OS10(conf-if-eth1/1/12)# show configuration ! interface ethernet1/1/12 no shutdown switchport mode trunk switchport access vlan 1 switchport trunk allowed vlan 32 flowcontrol receive on interface vlan 32 was configured as follows: OS10(conf-if-vl-32)# show configuration ! interface vlan32 no shutdown ip address 192.168.32.1/24 The full switch config is here Interface ethernet 1/1/1 was configured as an access port in VLAN","title":"Physical Configuration"},{"location":"Run%20VPN%20on%20OS10/README_wireguard_%28unfinished%29/#research","text":"","title":"Research"},{"location":"Run%20VPN%20on%20OS10/README_wireguard_%28unfinished%29/#sources","text":"Helpful Book Basics of Network Processor Packet Processing How Network Processors Work","title":"Sources"},{"location":"Run%20VPN%20on%20OS10/README_wireguard_%28unfinished%29/#npu-problem","text":"Explanation","title":"NPU Problem"},{"location":"Run%20VPN%20on%20OS10/README_wireguard_%28unfinished%29/#installing-wireguard","text":"On the 4112F-ON: Enter configuration mode from user mode by running en and then config <enter> Run ip name-server 192.168.1.1 to add a name server. Run write mem in enable mode to save your configuration changes. Run system bash If you don't have a default route on your switch, you will need to add one with sudo ip route add 0.0.0.0/0 via 192.168.1.1 . Before installing WireGuard you will need the latest kernel headers. 1.The kernel headers on the system will work a bit differently than regular systems. I found the relevant kernel headers with apt search linux-headers | grep $(uname -r) 2.You should see an entry mentioning all - something like linux-headers-4.9.0-11-all . You want to install that package with sudo apt-get install -y <LINUX HEADER NAME> bc 3.At this point you will need to reboot. Do this by exiting the command line and in enable mode running reload 4.If you do not have a permanent default route set, when you log back in you will need to run system bash and then readd the default route with sudo ip route add 0.0.0.0/0 via 192.168.1.1 mkdir /opt/wireguard && cd /opt/wireguard Now you will either need to run all of the following as sudo or you will need to add a password to the root account with sudo passwd and then su - to become root. After you have done the above run: echo \"deb http://deb.debian.org/debian/ unstable main\" > /etc/apt/sources.list.d/unstable-wireguard.list printf 'Package: *\\nPin: release a=unstable\\nPin-Priority: 90\\n' > /etc/apt/preferences.d/limit-unstable apt update apt install wireguard Generate key pairs wg genkey | tee wg-private.key | wg pubkey > wg-public.key On CentOS 7 Make sure everything is up to date. yum update -y && reboot . The reboot is important because if your kernel might update. If this happens you need to reboot to load the new kernel. Wireguard requires kernel 3.10 or higher - I noticed if you haven't updated CentOS for a bit than your kernel might be to old and you'll get RTNETLINK answers: Operation not supported . Run: sudo yum install https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm sudo curl -o /etc/yum.repos.d/jdoss-wireguard-epel-7.repo https://copr.fedorainfracloud.org/coprs/jdoss/wireguard/repo/epel-7/jdoss-wireguard-epel-7.repo sudo yum install wireguard-dkms wireguard-tools mkdir /opt/wireguard && cd /opt/wireguard Generate key pairs wg genkey | tee wg-private.key | wg pubkey > wg-public.key ip link add wg0 type wireguard Set Wireguard interface IP ip address add dev wg0 10.0.0.2/24 . This IP address is the tunnel IP address. 1.If there are only two peers together you can do something like ip address add dev wg0 10.0.0.2 peer 10.0.0.1 wg set wg0 listen-port 51820 private-key /opt/wireguard/wg-private.key peer OQiSLUOd3YWCfEkHazJHuuaJVNc++8QOb2+sOOZl/2c= endpoint 192.168.32.1:8172 1.listen-port: the port this host will listen on 2.private-key: the private key for this host 3.peer: I thought this was a bit misleading - you want the public key of the other host to which you are connecting 4.endpoint: the other endpoint of the VPN and the port on which it is listening","title":"Installing WireGuard"},{"location":"Run%20VPN%20on%20OS10/README_wireguard_%28unfinished%29/#strange-behavior","text":"OS10(config)# management route 192.168.1.0/24 192.168.1.1 % Error: Overlapping route for Management interface OS10(config)# ip route 0.0.0.0/0 192.168.1.1 interface ethernet 1/1/1 % Error: Network unreachable OS10(config)# ping 192.168.1.1 PING 192.168.1.1 (192.168.1.1) 56(84) bytes of data. 64 bytes from 192.168.1.1: icmp_seq=1 ttl=64 time=0.452 ms 64 bytes from 192.168.1.1: icmp_seq=2 ttl=64 time=0.388 ms ^C --- 192.168.1.1 ping statistics --- 2 packets transmitted, 2 received, 0% packet loss, time 1005ms rtt min/avg/max/mdev = 0.388/0.420/0.452/0.032 ms OS10(config)# do system bash admin@OS10:~$ su - Password: root@OS10:~# ip route 192.168.1.0/24 dev eth0 proto kernel scope link src 192.168.1.20 192.168.4.0/24 dev br32 proto none scope link root@OS10:~# ip route add 0.0.0.0/0 via 192.168.1.1 root@OS10:~# ip route default via 192.168.1.1 dev eth0 192.168.1.0/24 dev eth0 proto kernel scope link src 192.168.1.20 192.168.4.0/24 dev br32 proto none scope link root@OS10:~# ping google.com PING google.com (216.58.193.142) 56(84) bytes of data. 64 bytes from dfw25s34-in-f14.1e100.net (216.58.193.142): icmp_seq=1 ttl=55 time=25.4 ms 64 bytes from dfw25s34-in-f14.1e100.net (216.58.193.142): icmp_seq=2 ttl=55 time=22.9 ms 64 bytes from dfw25s34-in-f14.1e100.net (216.58.193.142): icmp_seq=3 ttl=55 time=22.1 ms 64 bytes from dfw25s34-in-f14.1e100.net (216.58.193.142): icmp_seq=4 ttl=55 time=22.2 ms ^C --- google.com ping statistics --- 4 packets transmitted, 4 received, 0% packet loss, time 3002ms rtt min/avg/max/mdev = 22.149/23.185/25.408/1.329 ms","title":"Strange Behavior"},{"location":"Running%20DNS%20from%20OS10/","text":"Running DNS from OS10 Physical Configuration OS10# show version Dell EMC Networking OS10 Enterprise Copyright (c) 1999-2020 by Dell Inc. All Rights Reserved. OS Version: 10.5.1.3 Build Version: 10.5.1.3.190 Build Time: 2020-06-19T21:48:07+0000 System Type: S4112F-ON Architecture: x86_64 Up Time: 00:30:27 Software Configuration OS10(config)# configure terminal OS10(config)# ip name-server 192.168.1.6 OS10(config)# interface vlan 1 OS10(conf-if-vl-1)# ip address 192.168.1.26/24 OS10(conf-if-vl-1)# exit OS10(config)# ip route 0.0.0.0/0 192.168.1.6 interface vlan1 OS10(config)# end OS10# system bash sudo passwd root su - sudo apt-get update -y && sudo apt-get install dnsmasq vim dnsutils vim /etc/hosts Inside of /etc/hosts add some random entry. I added 192.168.1.26 testrouter.lan Run :wq! to save and exit vim Inside of my config /etc/resolv.conf had an erroneous search entry. I had to clear that out to ensure it was valid. Now run systemctl start dnsmasq Test on another host Warning! I have only tried this with VLAN interfaces which show up as a bridge with an IP under the hood. I'm not sure if you apply an IP address directly to an interface that it would still work. Would have to test it.","title":"Running DNS from OS10"},{"location":"Running%20DNS%20from%20OS10/#running-dns-from-os10","text":"","title":"Running DNS from OS10"},{"location":"Running%20DNS%20from%20OS10/#physical-configuration","text":"OS10# show version Dell EMC Networking OS10 Enterprise Copyright (c) 1999-2020 by Dell Inc. All Rights Reserved. OS Version: 10.5.1.3 Build Version: 10.5.1.3.190 Build Time: 2020-06-19T21:48:07+0000 System Type: S4112F-ON Architecture: x86_64 Up Time: 00:30:27","title":"Physical Configuration"},{"location":"Running%20DNS%20from%20OS10/#software-configuration","text":"OS10(config)# configure terminal OS10(config)# ip name-server 192.168.1.6 OS10(config)# interface vlan 1 OS10(conf-if-vl-1)# ip address 192.168.1.26/24 OS10(conf-if-vl-1)# exit OS10(config)# ip route 0.0.0.0/0 192.168.1.6 interface vlan1 OS10(config)# end OS10# system bash sudo passwd root su - sudo apt-get update -y && sudo apt-get install dnsmasq vim dnsutils vim /etc/hosts Inside of /etc/hosts add some random entry. I added 192.168.1.26 testrouter.lan Run :wq! to save and exit vim Inside of my config /etc/resolv.conf had an erroneous search entry. I had to clear that out to ensure it was valid. Now run systemctl start dnsmasq","title":"Software Configuration"},{"location":"Running%20DNS%20from%20OS10/#test-on-another-host","text":"","title":"Test on another host"},{"location":"Running%20DNS%20from%20OS10/#warning","text":"I have only tried this with VLAN interfaces which show up as a bridge with an IP under the hood. I'm not sure if you apply an IP address directly to an interface that it would still work. Would have to test it.","title":"Warning!"},{"location":"SONiC%20-%20Sample%20Datacenter%20Automation%20Architecture/","text":"SONiC - Sample Datacenter Automation Architecture Sample Datacenter Automation Architecture (PDF) Sample Datacenter Automation Architecture (Visio)","title":"SONiC - Sample Datacenter Automation Architecture"},{"location":"SONiC%20-%20Sample%20Datacenter%20Automation%20Architecture/#sonic-sample-datacenter-automation-architecture","text":"Sample Datacenter Automation Architecture (PDF) Sample Datacenter Automation Architecture (Visio)","title":"SONiC - Sample Datacenter Automation Architecture"},{"location":"Set%20Up%20RSPAN%20on%20OS10/","text":"Set Up RSPAN on OS10 Setup Source port for span is Switch 1, 1/1/9 Our goal is to move the traffic through switch 2, which is just an intermediary switch, to port 1/1/11 on switch 3 which will finally move it out port 1/1/12 where our laptop is listening. We will use VLAN 99 to transport our RSPAN traffic Explanation One of the things that's different about the Dell configuration is the use of ACLs in the configuration. The way OS10 sees RSPAN is on the source side you grab whatever source you want and then you push that to a special \"remote\" VLAN. This is indicated only on the source switch itself. Then, each other intermediate and destination switch uses an ACL to indicate what traffic should be captured from that VLAN and subsequently forwarded. Finally, on the destination a regular local monitor session is used to pull the traffic from the RSPAN VLAN and push it t o network sensors. Switch 1 Version OS10# show version Dell EMC Networking OS10 Enterprise Copyright (c) 1999-2020 by Dell Inc. All Rights Reserved. OS Version: 10.5.1.3 Build Version: 10.5.1.3.190 Build Time: 2020-06-19T21:48:07+0000 System Type: S4112F-ON Architecture: x86_64 Up Time: 18 weeks 3 days 11:10:52 Setup configure terminal interface vlan 99 description remote_span remote-span exit interface ethernet 1/1/13:1 no shutdown switchport mode trunk switchport trunk allowed vlan 99 exit monitor session 18 type rpm-source source interface ethernet 1/1/9 both destination remote-vlan 99 no shut Switch 2 Version Dell EMC Networking OS10 Enterprise Copyright (c) 1999-2020 by Dell Inc. All Rights Reserved. OS Version: 10.5.1.3 Build Version: 10.5.1.3.190 Build Time: 2020-06-19T21:48:07+0000 System Type: S4112F-ON Architecture: x86_64 Up Time: 00:48:32 Setup configure terminal interface vlan 99 exit interface range ethernet 1/1/11 switchport mode trunk switchport trunk allowed vlan 99 exit interface ethernet 1/1/12 mac access-group rspan in switchport mode trunk switchport trunk allowed vlan 99 exit monitor session 1 destination interface ethernet1/1/11 flow-based enable source interface ethernet1/1/12 no shut mac access-list rspan seq 10 permit any any capture session 1 vlan 99 Switch 3 Version OS10# show version Dell EMC Networking OS10 Enterprise Copyright (c) 1999-2020 by Dell Inc. All Rights Reserved. OS Version: 10.5.1.3 Build Version: 10.5.1.3.190 Build Time: 2020-06-19T21:48:07+0000 System Type: S4112F-ON Architecture: x86_64 Up Time: 03:00:15 Setup configure terminal interface vlan 99 no shut exit mac access-list rpan seq 10 permit any any capture session 1 vlan 99 interface ethernet 1/1/11 switchport mode trunk switchport trunk allowed vlan 99 mac access-group rpan in exit interface ethernet 1/1/12 no shut exit monitor session 1 destination interface ethernet1/1/12 flow-based enable source interface ethernet1/1/11 no shut Notice here that there is an access list which captures the traffic on the inbound trunk interface. This access list is required. Also notice that on our monitor session we have flow-based enable which is also a requirement. How RSPAN Works Under the Hood When you run commands in OS10 what you're really doing is calling into wrapper functions which ultimately do two things: configure the NPU and configure Debian. The commands you run are actually governed by a series of YANG files which exhaustively define what commands are acceptable and in what contexts. For example, the rpm-source command is defined in an enum here in /alt/opt/dell/os10/share/yang-models/dell-span.yang : typedef monitoring-types { type enumeration { enum \"local\" { value 1; description \"PM local session.\"; } enum \"rpm-source\" { value 2; description \"RPM source session.\"; } enum \"erpm-source\" { value 3; description \"ERPM source session.\"; } } description \"Monitor session types.\"; } These definitions are combined with XML files which ultimately define the exact syntax of each command. For example, /alt/opt/dell/os10/clisystem/command-tree/span.xml defines the syntax for SPAN commands (note how the yang_name key references ): <!-- monitor session <id> type [local|rpm-source|rpm-destination|erpm-source]--> <PARAM name=\"type\" help=\"Monitor session type\" optional=\"true\" ptype=\"SUBCOMMAND\" mode=\"subcommand\"> <PARAM name=\"spantype\" help=\"PM/RPM/ERPM sessions\" default=\"local\" ptype=\"SPANMODE\" yang_name=\"/sessions/session/monitortype=${spantype}\"> </PARAM> </PARAM> I didn't exhaustively reverse engineer this part but at some point those commands and their arguments are interpreted and pushed to a series of Python modules. You'll see below how VLANs are set up using brctl . That setup seems to be handled by /alt/opt/dell/os10/lib/python/dn_base_br_tool.py . Ex: def create_br(bname): \"\"\"Method to create a bridge. Args: bname (str): Name of the Bridge Returns: bool: The return value. True for success, False otherwise \"\"\" res = [] if run_command([BRCTL_CMD, 'addbr', bname], res) == 0: return True return False Remote SPAN VLAN gets setup as a bridge and a new dummy interface is created which is a member of that bridge: admin@OS10:/home/admin$ ip a s ..... 38: br99: <BROADCAST,MULTICAST,PROMISC,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default qlen 1000 link/ether 50:9a:4c:d6:0a:a1 brd ff:ff:ff:ff:ff:ff inet6 fe80::529a:4cff:fed6:aa1/64 scope link valid_lft forever preferred_lft forever 39: e101-013-1.99@e101-013-1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue master br99 state UP group default qlen 1000 link/ether 50:9a:4c:d6:0a:7d brd ff:ff:ff:ff:ff:ff root@OS10:~# brctl show br99 bridge name bridge id STP enabled interfaces br99 8000.509a4cd60aa1 no e101-013-1.99 Under the hood the NPU is instructed to push traffic coming in on port 1/1/9 to this bridge which will then forward it. If you really want to dig under the covers this is how that works. CPS (the userland tool which interacts with a database for configuring the switch) will receive the command. You can drop to command line with system bash and then go take a look at /alt/opt/dell/os10/bin/cps_config_mirror.py and actually see where the call to NAS is made. Note: The NAS (Network Abstraction Service) is responsible for abstracting the chipset (broadcom) API from the rest of the OS above. This is the definition for nas_mirror_add_source and nas_mirror_op: def nas_mirror_op(op, data_dict,commit=True): obj = cps_utils.CPSObject( module=\"base-mirror/entry\", data=data_dict) if commit: nas_common.get_cb_method(op)(obj) else: return obj def nas_mirror_add_source(obj,intf,direction): l = [\"intf\",\"0\",\"src\"] obj.add_embed_attr(l,nas_os_utils.if_nametoindex(intf)) l[2]=\"direction\" obj.add_embed_attr(l,direction_type[direction]) if commit is referring to a commit to the CPS database. The definition for get_cb_method is in /alt/opt/dell/os10/lib/python/nas_common_utils.py str_to_cps_cb = { \"create\": create, \"set\": set, \"delete\": delete, \"get\": get, \"rpc\": rpc, } def get_cb_method(op): return str_to_cps_cb[op] What's happening here is it's taking the CPS object which has our instructions which basically say \"make me an RSPAN\" and the argument (op) will be either create or set. What this is going to do is update the CPS database with our new configuration. This is the NAS' Northbound CPS APi being called here. on the southbound end it will call into the SAI which is the actual manufacturer's (Broadcom in our case) API. This process is described in detail here . The NDI represents this call to the SAI API. For remote mirrors that is defined here . You can explore it function by function but the TLDR version is it's going to create a struct with the info relevant to creating the monitor session and it's going to feed that info the SAI. That magic happens in this function: t_std_error ndi_mirror_create_session(ndi_mirror_entry_t * entry){ if(entry == NULL ){ NDI_MIRROR_LOG(ERR,0,\"NDI Mirror entry passed to create Mirror session is NULL\"); return STD_ERR(MIRROR,PARAM,0); } sai_status_t sai_ret; sai_attribute_t sai_mirror_attr_list[MAX_MIRROR_SAI_ATTR]; unsigned int ndi_mirror_attr_count = 0; if(!ndi_mirror_fill_common_attr(entry,sai_mirror_attr_list,ndi_mirror_attr_count)){ return STD_ERR(MIRROR,PARAM,0); } if(entry->mode == BASE_MIRROR_MODE_RSPAN || entry->mode == BASE_MIRROR_MODE_ERSPAN){ ndi_mirror_fill_rspan_attr(entry,sai_mirror_attr_list,ndi_mirror_attr_count); } if(entry->mode == BASE_MIRROR_MODE_ERSPAN){ ndi_mirror_fill_erspan_attr(entry,sai_mirror_attr_list,ndi_mirror_attr_count); } nas_ndi_db_t *ndi_db_ptr = ndi_db_ptr_get(entry->dst_port.npu_id); if ((sai_ret = ndi_mirror_api_get(ndi_db_ptr)->create_mirror_session((sai_object_id_t *) &entry->ndi_mirror_id,ndi_switch_id_get(),ndi_mirror_attr_count,sai_mirror_attr_list)) != SAI_STATUS_SUCCESS) { NDI_MIRROR_LOG(ERR,0,\"Failed to create a new Mirroring Session\"); return STD_ERR(MIRROR, FAIL, sai_ret); } NDI_MIRROR_LOG(INFO,3,\"Created new mirroring session with Id %\" PRIu64 \" \",entry->ndi_mirror_id); return STD_ERR_OK; } What's going on here is the function ndi_mirror_create_session is going to receive a pointer to a struct of type ndi_mirror_entry_t . That struct contains all the relevant info for creating the mirror, port ID, mode (RSPAN), VLAN, etc. That struct is going to be filled out by two functions - ndi_mirror_fill_common_attr and ndi_mirror_fill_rspan_attr . Finally it will be fed into the code at line 199 and this is where we lose track of it. This is where we drop off because below this is Broadcom's API and they don't publicly publish that stuff. At this point though you're calling into Broadcom's SDK which is going to program the NPU to behave a certain way.","title":"Set Up RSPAN on OS10"},{"location":"Set%20Up%20RSPAN%20on%20OS10/#set-up-rspan-on-os10","text":"","title":"Set Up RSPAN on OS10"},{"location":"Set%20Up%20RSPAN%20on%20OS10/#setup","text":"Source port for span is Switch 1, 1/1/9 Our goal is to move the traffic through switch 2, which is just an intermediary switch, to port 1/1/11 on switch 3 which will finally move it out port 1/1/12 where our laptop is listening. We will use VLAN 99 to transport our RSPAN traffic","title":"Setup"},{"location":"Set%20Up%20RSPAN%20on%20OS10/#explanation","text":"One of the things that's different about the Dell configuration is the use of ACLs in the configuration. The way OS10 sees RSPAN is on the source side you grab whatever source you want and then you push that to a special \"remote\" VLAN. This is indicated only on the source switch itself. Then, each other intermediate and destination switch uses an ACL to indicate what traffic should be captured from that VLAN and subsequently forwarded. Finally, on the destination a regular local monitor session is used to pull the traffic from the RSPAN VLAN and push it t o network sensors.","title":"Explanation"},{"location":"Set%20Up%20RSPAN%20on%20OS10/#switch-1","text":"","title":"Switch 1"},{"location":"Set%20Up%20RSPAN%20on%20OS10/#version","text":"OS10# show version Dell EMC Networking OS10 Enterprise Copyright (c) 1999-2020 by Dell Inc. All Rights Reserved. OS Version: 10.5.1.3 Build Version: 10.5.1.3.190 Build Time: 2020-06-19T21:48:07+0000 System Type: S4112F-ON Architecture: x86_64 Up Time: 18 weeks 3 days 11:10:52","title":"Version"},{"location":"Set%20Up%20RSPAN%20on%20OS10/#setup_1","text":"configure terminal interface vlan 99 description remote_span remote-span exit interface ethernet 1/1/13:1 no shutdown switchport mode trunk switchport trunk allowed vlan 99 exit monitor session 18 type rpm-source source interface ethernet 1/1/9 both destination remote-vlan 99 no shut","title":"Setup"},{"location":"Set%20Up%20RSPAN%20on%20OS10/#switch-2","text":"","title":"Switch 2"},{"location":"Set%20Up%20RSPAN%20on%20OS10/#version_1","text":"Dell EMC Networking OS10 Enterprise Copyright (c) 1999-2020 by Dell Inc. All Rights Reserved. OS Version: 10.5.1.3 Build Version: 10.5.1.3.190 Build Time: 2020-06-19T21:48:07+0000 System Type: S4112F-ON Architecture: x86_64 Up Time: 00:48:32","title":"Version"},{"location":"Set%20Up%20RSPAN%20on%20OS10/#setup_2","text":"configure terminal interface vlan 99 exit interface range ethernet 1/1/11 switchport mode trunk switchport trunk allowed vlan 99 exit interface ethernet 1/1/12 mac access-group rspan in switchport mode trunk switchport trunk allowed vlan 99 exit monitor session 1 destination interface ethernet1/1/11 flow-based enable source interface ethernet1/1/12 no shut mac access-list rspan seq 10 permit any any capture session 1 vlan 99","title":"Setup"},{"location":"Set%20Up%20RSPAN%20on%20OS10/#switch-3","text":"","title":"Switch 3"},{"location":"Set%20Up%20RSPAN%20on%20OS10/#version_2","text":"OS10# show version Dell EMC Networking OS10 Enterprise Copyright (c) 1999-2020 by Dell Inc. All Rights Reserved. OS Version: 10.5.1.3 Build Version: 10.5.1.3.190 Build Time: 2020-06-19T21:48:07+0000 System Type: S4112F-ON Architecture: x86_64 Up Time: 03:00:15","title":"Version"},{"location":"Set%20Up%20RSPAN%20on%20OS10/#setup_3","text":"configure terminal interface vlan 99 no shut exit mac access-list rpan seq 10 permit any any capture session 1 vlan 99 interface ethernet 1/1/11 switchport mode trunk switchport trunk allowed vlan 99 mac access-group rpan in exit interface ethernet 1/1/12 no shut exit monitor session 1 destination interface ethernet1/1/12 flow-based enable source interface ethernet1/1/11 no shut Notice here that there is an access list which captures the traffic on the inbound trunk interface. This access list is required. Also notice that on our monitor session we have flow-based enable which is also a requirement.","title":"Setup"},{"location":"Set%20Up%20RSPAN%20on%20OS10/#how-rspan-works-under-the-hood","text":"When you run commands in OS10 what you're really doing is calling into wrapper functions which ultimately do two things: configure the NPU and configure Debian. The commands you run are actually governed by a series of YANG files which exhaustively define what commands are acceptable and in what contexts. For example, the rpm-source command is defined in an enum here in /alt/opt/dell/os10/share/yang-models/dell-span.yang : typedef monitoring-types { type enumeration { enum \"local\" { value 1; description \"PM local session.\"; } enum \"rpm-source\" { value 2; description \"RPM source session.\"; } enum \"erpm-source\" { value 3; description \"ERPM source session.\"; } } description \"Monitor session types.\"; } These definitions are combined with XML files which ultimately define the exact syntax of each command. For example, /alt/opt/dell/os10/clisystem/command-tree/span.xml defines the syntax for SPAN commands (note how the yang_name key references ): <!-- monitor session <id> type [local|rpm-source|rpm-destination|erpm-source]--> <PARAM name=\"type\" help=\"Monitor session type\" optional=\"true\" ptype=\"SUBCOMMAND\" mode=\"subcommand\"> <PARAM name=\"spantype\" help=\"PM/RPM/ERPM sessions\" default=\"local\" ptype=\"SPANMODE\" yang_name=\"/sessions/session/monitortype=${spantype}\"> </PARAM> </PARAM> I didn't exhaustively reverse engineer this part but at some point those commands and their arguments are interpreted and pushed to a series of Python modules. You'll see below how VLANs are set up using brctl . That setup seems to be handled by /alt/opt/dell/os10/lib/python/dn_base_br_tool.py . Ex: def create_br(bname): \"\"\"Method to create a bridge. Args: bname (str): Name of the Bridge Returns: bool: The return value. True for success, False otherwise \"\"\" res = [] if run_command([BRCTL_CMD, 'addbr', bname], res) == 0: return True return False Remote SPAN VLAN gets setup as a bridge and a new dummy interface is created which is a member of that bridge: admin@OS10:/home/admin$ ip a s ..... 38: br99: <BROADCAST,MULTICAST,PROMISC,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default qlen 1000 link/ether 50:9a:4c:d6:0a:a1 brd ff:ff:ff:ff:ff:ff inet6 fe80::529a:4cff:fed6:aa1/64 scope link valid_lft forever preferred_lft forever 39: e101-013-1.99@e101-013-1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue master br99 state UP group default qlen 1000 link/ether 50:9a:4c:d6:0a:7d brd ff:ff:ff:ff:ff:ff root@OS10:~# brctl show br99 bridge name bridge id STP enabled interfaces br99 8000.509a4cd60aa1 no e101-013-1.99 Under the hood the NPU is instructed to push traffic coming in on port 1/1/9 to this bridge which will then forward it. If you really want to dig under the covers this is how that works. CPS (the userland tool which interacts with a database for configuring the switch) will receive the command. You can drop to command line with system bash and then go take a look at /alt/opt/dell/os10/bin/cps_config_mirror.py and actually see where the call to NAS is made. Note: The NAS (Network Abstraction Service) is responsible for abstracting the chipset (broadcom) API from the rest of the OS above. This is the definition for nas_mirror_add_source and nas_mirror_op: def nas_mirror_op(op, data_dict,commit=True): obj = cps_utils.CPSObject( module=\"base-mirror/entry\", data=data_dict) if commit: nas_common.get_cb_method(op)(obj) else: return obj def nas_mirror_add_source(obj,intf,direction): l = [\"intf\",\"0\",\"src\"] obj.add_embed_attr(l,nas_os_utils.if_nametoindex(intf)) l[2]=\"direction\" obj.add_embed_attr(l,direction_type[direction]) if commit is referring to a commit to the CPS database. The definition for get_cb_method is in /alt/opt/dell/os10/lib/python/nas_common_utils.py str_to_cps_cb = { \"create\": create, \"set\": set, \"delete\": delete, \"get\": get, \"rpc\": rpc, } def get_cb_method(op): return str_to_cps_cb[op] What's happening here is it's taking the CPS object which has our instructions which basically say \"make me an RSPAN\" and the argument (op) will be either create or set. What this is going to do is update the CPS database with our new configuration. This is the NAS' Northbound CPS APi being called here. on the southbound end it will call into the SAI which is the actual manufacturer's (Broadcom in our case) API. This process is described in detail here . The NDI represents this call to the SAI API. For remote mirrors that is defined here . You can explore it function by function but the TLDR version is it's going to create a struct with the info relevant to creating the monitor session and it's going to feed that info the SAI. That magic happens in this function: t_std_error ndi_mirror_create_session(ndi_mirror_entry_t * entry){ if(entry == NULL ){ NDI_MIRROR_LOG(ERR,0,\"NDI Mirror entry passed to create Mirror session is NULL\"); return STD_ERR(MIRROR,PARAM,0); } sai_status_t sai_ret; sai_attribute_t sai_mirror_attr_list[MAX_MIRROR_SAI_ATTR]; unsigned int ndi_mirror_attr_count = 0; if(!ndi_mirror_fill_common_attr(entry,sai_mirror_attr_list,ndi_mirror_attr_count)){ return STD_ERR(MIRROR,PARAM,0); } if(entry->mode == BASE_MIRROR_MODE_RSPAN || entry->mode == BASE_MIRROR_MODE_ERSPAN){ ndi_mirror_fill_rspan_attr(entry,sai_mirror_attr_list,ndi_mirror_attr_count); } if(entry->mode == BASE_MIRROR_MODE_ERSPAN){ ndi_mirror_fill_erspan_attr(entry,sai_mirror_attr_list,ndi_mirror_attr_count); } nas_ndi_db_t *ndi_db_ptr = ndi_db_ptr_get(entry->dst_port.npu_id); if ((sai_ret = ndi_mirror_api_get(ndi_db_ptr)->create_mirror_session((sai_object_id_t *) &entry->ndi_mirror_id,ndi_switch_id_get(),ndi_mirror_attr_count,sai_mirror_attr_list)) != SAI_STATUS_SUCCESS) { NDI_MIRROR_LOG(ERR,0,\"Failed to create a new Mirroring Session\"); return STD_ERR(MIRROR, FAIL, sai_ret); } NDI_MIRROR_LOG(INFO,3,\"Created new mirroring session with Id %\" PRIu64 \" \",entry->ndi_mirror_id); return STD_ERR_OK; } What's going on here is the function ndi_mirror_create_session is going to receive a pointer to a struct of type ndi_mirror_entry_t . That struct contains all the relevant info for creating the mirror, port ID, mode (RSPAN), VLAN, etc. That struct is going to be filled out by two functions - ndi_mirror_fill_common_attr and ndi_mirror_fill_rspan_attr . Finally it will be fed into the code at line 199 and this is where we lose track of it. This is where we drop off because below this is Broadcom's API and they don't publicly publish that stuff. At this point though you're calling into Broadcom's SDK which is going to program the NPU to behave a certain way.","title":"How RSPAN Works Under the Hood"},{"location":"Setting%20Up%20Breakout%20Cables/","text":"Setting Up Breakout Cables Helpful Links Dell OS10 Manual Platform Information WARNING If you are using QSFPs you must use Dell branded QSFPs! For breakout mode to work it requires physical modification of the QSFP. It is possible other vendors may make these modifications, but most of the time they do not. When I tested with Intel QSFPs it did not work. Physical Configuration OS Information OS10# show version Dell EMC Networking OS10-Enterprise Copyright (c) 1999-2019 by Dell Inc. All Rights Reserved. OS Version: 10.4.2.2 Build Version: 10.4.2.2.265 Build Time: 2019-01-14T15:15:14-0800 System Type: S4112F-ON Architecture: x86_64 Up Time: 00:13:36 Configure Management Port See Configure Management Interface on Dell OS10 Configure Breakout Port Automatically Run: OS10(config)# feature auto-breakout OS10(config)# do write memory You can then access the interfaces individually using the subport: OS10(config)# interface ethernet 1/1/13:1 Configure Breakout Port Manually Run: OS10(config)# interface breakout 1/1/13 map 10g-4x OS10(config)# do write memory","title":"Setting Up Breakout Cables"},{"location":"Setting%20Up%20Breakout%20Cables/#setting-up-breakout-cables","text":"","title":"Setting Up Breakout Cables"},{"location":"Setting%20Up%20Breakout%20Cables/#helpful-links","text":"Dell OS10 Manual","title":"Helpful Links"},{"location":"Setting%20Up%20Breakout%20Cables/#platform-information","text":"WARNING If you are using QSFPs you must use Dell branded QSFPs! For breakout mode to work it requires physical modification of the QSFP. It is possible other vendors may make these modifications, but most of the time they do not. When I tested with Intel QSFPs it did not work.","title":"Platform Information"},{"location":"Setting%20Up%20Breakout%20Cables/#physical-configuration","text":"","title":"Physical Configuration"},{"location":"Setting%20Up%20Breakout%20Cables/#os-information","text":"OS10# show version Dell EMC Networking OS10-Enterprise Copyright (c) 1999-2019 by Dell Inc. All Rights Reserved. OS Version: 10.4.2.2 Build Version: 10.4.2.2.265 Build Time: 2019-01-14T15:15:14-0800 System Type: S4112F-ON Architecture: x86_64 Up Time: 00:13:36","title":"OS Information"},{"location":"Setting%20Up%20Breakout%20Cables/#configure-management-port","text":"See Configure Management Interface on Dell OS10","title":"Configure Management Port"},{"location":"Setting%20Up%20Breakout%20Cables/#configure-breakout-port-automatically","text":"Run: OS10(config)# feature auto-breakout OS10(config)# do write memory You can then access the interfaces individually using the subport: OS10(config)# interface ethernet 1/1/13:1","title":"Configure Breakout Port Automatically"},{"location":"Setting%20Up%20Breakout%20Cables/#configure-breakout-port-manually","text":"Run: OS10(config)# interface breakout 1/1/13 map 10g-4x OS10(config)# do write memory","title":"Configure Breakout Port Manually"},{"location":"Setting%20Up%20SmartFabric%20Director/","text":"Setting Up SmartFabric Director Helpful Links SmartFabric Director Download Page Dell OS10 User's Guide Dell SmartFabric Director User's Guide My Configuration Dell 4112F-ON Dell EMC Networking OS10 Enterprise Copyright (c) 1999-2020 by Dell Inc. All Rights Reserved. OS Version: 10.5.1.0 Build Version: 10.5.1.0.124 Build Time: 2020-02-12T09:05:20+0000 System Type: S4112F-ON Architecture: x86_64 Up Time: 01:06:30 Dell OS10 Running in GNS3 I used the GNS3 VM and tied the switch with a virtual cloud into my actual infrastructure. Dell EMC Networking OS10 Enterprise Copyright (c) 1999-2019 by Dell Inc. All Rights Reserved. OS Version: 10.5.0.0 Build Version: 10.5.0.0.326 Build Time: 2019-08-07T00:12:30+0000 System Type: S5248F-VM Architecture: x86_64 Up Time: 00:32:14 Setup Download, extract, upload to vCenter (or ESXi) Fill in network settings. When it finishes importing, keep in mind that the username and password is: 1.Username: admin@sfd.local 2.Password: The password you set I had problems when I used a DNS name for NTP. See: BUG Go to the Fabric Designer online. 1.Enter your design in the Fabric Designer. The point of the Fabric Designer is to determine all of those materials you will need in order to build out your stack. 2.It will provide you a lot of useful info, for example a bill of materials: ![](images/bom.PNG) 3.It will also give you a logical view of your network ![](images/logical_view.PNG) 4.A view of your networking ![](images/network_view.PNG) 5.It will also allow you to download the configurations for each switch - either as a regular config or as a zero touch configuration. You can also save the overall design. 6. Problems Encountered On the 4112F-ON These were taken from the physical switch running 10.5.0.4.638 I have confirmed I am in full switch mode: OS10# show switch-operating-mode Switch-Operating-Mode : Full Switch Mode Page 10 step 6 of the manual does not work. There is no cert command. Page 11 step 1 of the manual does not work. There is no switch-port-profile command. With the VM See bug notes .","title":"Setting Up SmartFabric Director"},{"location":"Setting%20Up%20SmartFabric%20Director/#setting-up-smartfabric-director","text":"","title":"Setting Up SmartFabric Director"},{"location":"Setting%20Up%20SmartFabric%20Director/#helpful-links","text":"SmartFabric Director Download Page Dell OS10 User's Guide Dell SmartFabric Director User's Guide","title":"Helpful Links"},{"location":"Setting%20Up%20SmartFabric%20Director/#my-configuration","text":"","title":"My Configuration"},{"location":"Setting%20Up%20SmartFabric%20Director/#dell-4112f-on","text":"Dell EMC Networking OS10 Enterprise Copyright (c) 1999-2020 by Dell Inc. All Rights Reserved. OS Version: 10.5.1.0 Build Version: 10.5.1.0.124 Build Time: 2020-02-12T09:05:20+0000 System Type: S4112F-ON Architecture: x86_64 Up Time: 01:06:30","title":"Dell 4112F-ON"},{"location":"Setting%20Up%20SmartFabric%20Director/#dell-os10-running-in-gns3","text":"I used the GNS3 VM and tied the switch with a virtual cloud into my actual infrastructure. Dell EMC Networking OS10 Enterprise Copyright (c) 1999-2019 by Dell Inc. All Rights Reserved. OS Version: 10.5.0.0 Build Version: 10.5.0.0.326 Build Time: 2019-08-07T00:12:30+0000 System Type: S5248F-VM Architecture: x86_64 Up Time: 00:32:14","title":"Dell OS10 Running in GNS3"},{"location":"Setting%20Up%20SmartFabric%20Director/#setup","text":"Download, extract, upload to vCenter (or ESXi) Fill in network settings. When it finishes importing, keep in mind that the username and password is: 1.Username: admin@sfd.local 2.Password: The password you set I had problems when I used a DNS name for NTP. See: BUG Go to the Fabric Designer online. 1.Enter your design in the Fabric Designer. The point of the Fabric Designer is to determine all of those materials you will need in order to build out your stack. 2.It will provide you a lot of useful info, for example a bill of materials: ![](images/bom.PNG) 3.It will also give you a logical view of your network ![](images/logical_view.PNG) 4.A view of your networking ![](images/network_view.PNG) 5.It will also allow you to download the configurations for each switch - either as a regular config or as a zero touch configuration. You can also save the overall design. 6.","title":"Setup"},{"location":"Setting%20Up%20SmartFabric%20Director/#problems-encountered","text":"","title":"Problems Encountered"},{"location":"Setting%20Up%20SmartFabric%20Director/#on-the-4112f-on","text":"These were taken from the physical switch running 10.5.0.4.638 I have confirmed I am in full switch mode: OS10# show switch-operating-mode Switch-Operating-Mode : Full Switch Mode Page 10 step 6 of the manual does not work. There is no cert command. Page 11 step 1 of the manual does not work. There is no switch-port-profile command.","title":"On the 4112F-ON"},{"location":"Setting%20Up%20SmartFabric%20Director/#with-the-vm","text":"See bug notes .","title":"With the VM"},{"location":"Setting%20Up%20SmartFabric%20Director/BUG/","text":"Bug Write Up sfd_init.sh fails because it incorrectly checks ntp status. The failure condition can be replicated by using an ntp server of time.google.com on template import for the OVA. Version SFD SFD version is 1.1.0. Ubuntu admin@sfd.local@sfd:~$ cat /etc/*-release DISTRIB_ID=Ubuntu DISTRIB_RELEASE=16.04 DISTRIB_CODENAME=xenial DISTRIB_DESCRIPTION=\"Ubuntu 16.04.6 LTS\" NAME=\"Ubuntu\" VERSION=\"16.04.6 LTS (Xenial Xerus)\" ID=ubuntu ID_LIKE=debian PRETTY_NAME=\"Ubuntu 16.04.6 LTS\" VERSION_ID=\"16.04\" HOME_URL=\"http://www.ubuntu.com/\" SUPPORT_URL=\"http://help.ubuntu.com/\" BUG_REPORT_URL=\"http://bugs.launchpad.net/ubuntu/\" VERSION_CODENAME=xenial UBUNTU_CODENAME=xenial Suggested Patch Patch admin@sfd.local@sfd:~$ diff -c sfd_init.sh.original sfd_init.sh.patch *** sfd_init.sh.original 2020-02-04 21:58:22.327755522 +0000 --- sfd_init.sh.patch 2020-02-04 21:59:38.651751964 +0000 *************** *** 188,194 **** echo \"Checking if NTP is configured.\" until [[ $cnt -ge $max_tries ]] do ! ntpq -pn | awk '{print $2}' | xargs | grep -E -o \"([0-9]{1,3}[\\.]){3}[0-9]{1,3}|PPS|GPS|LOCAL|LOCL\" [[ \"$?\" -eq 0 ]] && echo \"NTP synchronized successfully.\" && return 0 cnt=$[$cnt+1] echo \"Waiting for NTP to get configured. Sleeping for 10 seconds...\" --- 188,194 ---- echo \"Checking if NTP is configured.\" until [[ $cnt -ge $max_tries ]] do ! ntpq -pn | awk '{print $1}' | sed -n '/^\\*/p' [[ \"$?\" -eq 0 ]] && echo \"NTP synchronized successfully.\" && return 0 cnt=$[$cnt+1] echo \"Waiting for NTP to get configured. Sleeping for 10 seconds...\" Explanation See below for a detailed explanation of my troubleshooting and additional information. Based on the regex looking for an IP I'm guessing the intent was to verify that the server is synched to a remote NTP server. However there are two problems. The first is that it is looking at the refid (column 2) which is not the server you are synched to, but the server that the server you are pointing at is synched to. Second, there is no guarentee that an IP address will be in this column (or PPS/GPS/LOCAL/LOCL). This was my case. Instead, I would check for the line that has the synchronized server (starts with *). Using ntpq -pn | awk '{print $1}' | sed -n '/^\\*/p' . You don't really need to check beyond that because it won't synchronize unless everything is working. Though if you want to take it a step further you could do: ntpq -pn | awk '{print $1}' | sed -n '/^\\*/p' | sed 's/\\*//g' | grep -E -o \"([0-9]{1,3}[\\.]){3}[0-9]{1,3}|PPS|GPS|LOCAL|LOCL\" . Detailed Explanation Page 17 step 1 of the manual doesn't work. There is no web server listening. It looks like on the first run the SFD service failed. There is no obvious log information output from the service. Suggest that sfd_init.sh print the log path to stdout on failure so that getting the service status will more obviously provide direction on where the problem is. # Dumped listening ports admin@sfd.local@sfd:~$ ss -ltn State Recv-Q Send-Q Local Address:Port Peer Address:Port LISTEN 0 128 127.0.0.1:39125 *:* LISTEN 0 128 *:22 *:* LISTEN 0 128 :::9100 :::* LISTEN 0 128 :::5555 :::* LISTEN 0 128 :::22 :::* # Checked service status after figuring out SFD ran a service admin@sfd.local@sfd:~$ systemctl status sfd \u25cf sfd.service - NFC Bootstrap Service Loaded: loaded (/etc/systemd/system/sfd.service; enabled; vendor preset: enabled) Active: failed (Result: exit-code) since Tue 2020-02-04 20:38:10 UTC; 18min ago Main PID: 1327 (code=exited, status=1/FAILURE) Entire system will hard fail if NTP cannot start. I realized this is the cause of the failure above. This should not be the default behavior. Should continue and issue a warning. Most customers working on servers would not know how to troubleshoot this. \u25cf ntp.service - LSB: Start NTP daemon Loaded: loaded (/etc/init.d/ntp; bad; vendor preset: enabled) Active: active (running) since Wed 2020-02-05 03:50:34 UTC; 10s ago Docs: man:systemd-sysv-generator(8) Process: 2235 ExecStop=/etc/init.d/ntp stop (code=exited, status=0/SUCCESS) Process: 2248 ExecStart=/etc/init.d/ntp start (code=exited, status=0/SUCCESS) Tasks: 2 Memory: 1.5M CPU: 12ms CGroup: /system.slice/ntp.service \u2514\u25002260 /usr/sbin/ntpd -p /var/run/ntpd.pid -g -u 112:117 Feb 05 03:50:34 sfd ntpd[2260]: Listen and drop on 1 v4wildcard 0.0.0.0:123 Feb 05 03:50:34 sfd ntpd[2260]: Listen normally on 2 lo 127.0.0.1:123 Feb 05 03:50:34 sfd ntpd[2260]: Listen normally on 3 ens192 192.168.1.31:123 Feb 05 03:50:34 sfd ntpd[2260]: Listen normally on 4 lo [::1]:123 Feb 05 03:50:34 sfd ntpd[2260]: Listening on routing socket on fd #21 for interface updates Feb 05 03:50:35 sfd ntpd[2260]: Soliciting pool server 216.239.35.12 Feb 05 03:50:36 sfd ntpd[2260]: Soliciting pool server 216.239.35.4 Feb 05 03:50:37 sfd ntpd[2260]: Soliciting pool server 216.239.35.0 Feb 05 03:50:38 sfd ntpd[2260]: Soliciting pool server 216.239.35.8 Feb 05 03:50:39 sfd ntpd[2260]: Soliciting pool server 2001:4860:4806:: -----RESTART NTP - END----- Checking if NTP service is running. NTP service is running. Checking if NTP is configured. Waiting for NTP to get configured. Sleeping for 10 seconds... ... SNIP ... Waiting for NTP to get configured. Sleeping for 10 seconds... NTP failed to get synced. NTP config failed # This failure seems to be erroneous. Checking the NTP service confirms it is working. admin@sfd.local@sfd:~$ systemctl status ntp \u25cf ntp.service - LSB: Start NTP daemon Loaded: loaded (/etc/init.d/ntp; bad; vendor preset: enabled) Active: active (running) since Wed 2020-02-05 03:50:34 UTC; 6h left Docs: man:systemd-sysv-generator(8) Process: 2235 ExecStop=/etc/init.d/ntp stop (code=exited, status=0/SUCCESS) Process: 2248 ExecStart=/etc/init.d/ntp start (code=exited, status=0/SUCCESS) Tasks: 2 Memory: 1.5M CPU: 81ms CGroup: /system.slice/ntp.service \u2514\u25002260 /usr/sbin/ntpd -p /var/run/ntpd.pid -g -u 112:117 admin@sfd.local@sfd:~$ ntpstat The program 'ntpstat' is currently not installed. You can install it by typing: sudo apt install ntpstat admin@sfd.local@sfd:~$ ntp -qn No command 'ntp' found, but there are 21 similar ones ntp: command not found admin@sfd.local@sfd:~$ ntpq -pn remote refid st t when poll reach delay offset jitter ============================================================================== time.google.com .POOL. 16 p - 64 0 0.000 0.000 0.000 +216.239.35.12 .GOOG. 1 u 54 64 375 64.340 -91.086 8.997 *216.239.35.4 .GOOG. 1 u 62 64 377 62.412 -83.944 8.385 +216.239.35.0 .GOOG. 1 u 42 64 377 63.484 -83.978 10.080 -216.239.35.8 .GOOG. 1 u 21 64 377 72.245 -73.087 15.674 Problem isolated to: max_tries=30 # check counter till 30. (total 300 seconds) echo \"Checking if NTP is configured.\" until [[ $cnt -ge $max_tries ]] do ntpq -pn | awk '{print $2}' | xargs | grep -E -o \"([0-9]{1,3}[\\.]){3}[0-9]{1,3}|PPS|GPS|LOCAL|LOCL\" [[ \"$?\" -eq 0 ]] && echo \"NTP synchronized successfully.\" && return 0 cnt=$[$cnt+1] echo \"Waiting for NTP to get configured. Sleeping for 10 seconds...\" sleep 10 done The offending line is ntpq -pn | awk '{print $2}' | xargs | grep -E -o \"([0-9]{1,3}[\\.]){3}[0-9]{1,3}|PPS|GPS|LOCAL|LOCL\" . ntpq -pn has output formatted like this: admin@sfd.local@sfd:~$ ntpq -pn remote refid st t when poll reach delay offset jitter ============================================================================== time.google.com .POOL. 16 p - 64 0 0.000 0.000 0.000 +216.239.35.12 .GOOG. 1 u 54 64 375 64.340 -91.086 8.997 *216.239.35.4 .GOOG. 1 u 62 64 377 62.412 -83.944 8.385 +216.239.35.0 .GOOG. 1 u 42 64 377 63.484 -83.978 10.080 -216.239.35.8 .GOOG. 1 u 21 64 377 72.245 -73.087 15.674 Based on the regex looking for an IP I'm guessing the intent was to verify that the server is synched to a remote NTP server. However there are two problems. The first is that it is looking at the refid which is not the server you are synched to, but the server that the server you are pointing at is synched to. Second, there is no guarentee that an IP address will be in this column (or PPS/GPS/LOCAL/LOCL). This was my case. See below for my ntpq -pn results. Instead, I would check for the line that has the synchronized server (starts with *). Using ntpq -pn | awk '{print $1}' | sed -n '/^\\*/p' . You don't really need to check beyond that because it won't synchronize unless everything is working. Though if you want to take it a step further you could do: ntpq -pn | awk '{print $1}' | sed -n '/^\\*/p' | sed 's/\\*//g' | grep -E -o \"([0-9]{1,3}[\\.]){3}[0-9]{1,3}|PPS|GPS|LOCAL|LOCL\" . My ntpq -pn with time.google.com as my server admin@sfd.local@sfd:~$ ntpq -pn remote refid st t when poll reach delay offset jitter ============================================================================== time.google.com .POOL. 16 p - 64 0 0.000 0.000 0.000 *216.239.35.12 .GOOG. 1 u 38 64 377 67.370 -0.549 23.161 +216.239.35.4 .GOOG. 1 u 29 64 377 75.466 -1.691 22.089 +216.239.35.0 .GOOG. 1 u 36 64 377 67.731 -1.434 24.201 +216.239.35.8 .GOOG. 1 u 38 64 377 76.705 -0.042 22.436","title":"Bug Write Up"},{"location":"Setting%20Up%20SmartFabric%20Director/BUG/#bug-write-up","text":"sfd_init.sh fails because it incorrectly checks ntp status. The failure condition can be replicated by using an ntp server of time.google.com on template import for the OVA.","title":"Bug Write Up"},{"location":"Setting%20Up%20SmartFabric%20Director/BUG/#version","text":"","title":"Version"},{"location":"Setting%20Up%20SmartFabric%20Director/BUG/#sfd","text":"SFD version is 1.1.0.","title":"SFD"},{"location":"Setting%20Up%20SmartFabric%20Director/BUG/#ubuntu","text":"admin@sfd.local@sfd:~$ cat /etc/*-release DISTRIB_ID=Ubuntu DISTRIB_RELEASE=16.04 DISTRIB_CODENAME=xenial DISTRIB_DESCRIPTION=\"Ubuntu 16.04.6 LTS\" NAME=\"Ubuntu\" VERSION=\"16.04.6 LTS (Xenial Xerus)\" ID=ubuntu ID_LIKE=debian PRETTY_NAME=\"Ubuntu 16.04.6 LTS\" VERSION_ID=\"16.04\" HOME_URL=\"http://www.ubuntu.com/\" SUPPORT_URL=\"http://help.ubuntu.com/\" BUG_REPORT_URL=\"http://bugs.launchpad.net/ubuntu/\" VERSION_CODENAME=xenial UBUNTU_CODENAME=xenial","title":"Ubuntu"},{"location":"Setting%20Up%20SmartFabric%20Director/BUG/#suggested-patch","text":"","title":"Suggested Patch"},{"location":"Setting%20Up%20SmartFabric%20Director/BUG/#patch","text":"admin@sfd.local@sfd:~$ diff -c sfd_init.sh.original sfd_init.sh.patch *** sfd_init.sh.original 2020-02-04 21:58:22.327755522 +0000 --- sfd_init.sh.patch 2020-02-04 21:59:38.651751964 +0000 *************** *** 188,194 **** echo \"Checking if NTP is configured.\" until [[ $cnt -ge $max_tries ]] do ! ntpq -pn | awk '{print $2}' | xargs | grep -E -o \"([0-9]{1,3}[\\.]){3}[0-9]{1,3}|PPS|GPS|LOCAL|LOCL\" [[ \"$?\" -eq 0 ]] && echo \"NTP synchronized successfully.\" && return 0 cnt=$[$cnt+1] echo \"Waiting for NTP to get configured. Sleeping for 10 seconds...\" --- 188,194 ---- echo \"Checking if NTP is configured.\" until [[ $cnt -ge $max_tries ]] do ! ntpq -pn | awk '{print $1}' | sed -n '/^\\*/p' [[ \"$?\" -eq 0 ]] && echo \"NTP synchronized successfully.\" && return 0 cnt=$[$cnt+1] echo \"Waiting for NTP to get configured. Sleeping for 10 seconds...\"","title":"Patch"},{"location":"Setting%20Up%20SmartFabric%20Director/BUG/#explanation","text":"See below for a detailed explanation of my troubleshooting and additional information. Based on the regex looking for an IP I'm guessing the intent was to verify that the server is synched to a remote NTP server. However there are two problems. The first is that it is looking at the refid (column 2) which is not the server you are synched to, but the server that the server you are pointing at is synched to. Second, there is no guarentee that an IP address will be in this column (or PPS/GPS/LOCAL/LOCL). This was my case. Instead, I would check for the line that has the synchronized server (starts with *). Using ntpq -pn | awk '{print $1}' | sed -n '/^\\*/p' . You don't really need to check beyond that because it won't synchronize unless everything is working. Though if you want to take it a step further you could do: ntpq -pn | awk '{print $1}' | sed -n '/^\\*/p' | sed 's/\\*//g' | grep -E -o \"([0-9]{1,3}[\\.]){3}[0-9]{1,3}|PPS|GPS|LOCAL|LOCL\" .","title":"Explanation"},{"location":"Setting%20Up%20SmartFabric%20Director/BUG/#detailed-explanation","text":"Page 17 step 1 of the manual doesn't work. There is no web server listening. It looks like on the first run the SFD service failed. There is no obvious log information output from the service. Suggest that sfd_init.sh print the log path to stdout on failure so that getting the service status will more obviously provide direction on where the problem is. # Dumped listening ports admin@sfd.local@sfd:~$ ss -ltn State Recv-Q Send-Q Local Address:Port Peer Address:Port LISTEN 0 128 127.0.0.1:39125 *:* LISTEN 0 128 *:22 *:* LISTEN 0 128 :::9100 :::* LISTEN 0 128 :::5555 :::* LISTEN 0 128 :::22 :::* # Checked service status after figuring out SFD ran a service admin@sfd.local@sfd:~$ systemctl status sfd \u25cf sfd.service - NFC Bootstrap Service Loaded: loaded (/etc/systemd/system/sfd.service; enabled; vendor preset: enabled) Active: failed (Result: exit-code) since Tue 2020-02-04 20:38:10 UTC; 18min ago Main PID: 1327 (code=exited, status=1/FAILURE) Entire system will hard fail if NTP cannot start. I realized this is the cause of the failure above. This should not be the default behavior. Should continue and issue a warning. Most customers working on servers would not know how to troubleshoot this. \u25cf ntp.service - LSB: Start NTP daemon Loaded: loaded (/etc/init.d/ntp; bad; vendor preset: enabled) Active: active (running) since Wed 2020-02-05 03:50:34 UTC; 10s ago Docs: man:systemd-sysv-generator(8) Process: 2235 ExecStop=/etc/init.d/ntp stop (code=exited, status=0/SUCCESS) Process: 2248 ExecStart=/etc/init.d/ntp start (code=exited, status=0/SUCCESS) Tasks: 2 Memory: 1.5M CPU: 12ms CGroup: /system.slice/ntp.service \u2514\u25002260 /usr/sbin/ntpd -p /var/run/ntpd.pid -g -u 112:117 Feb 05 03:50:34 sfd ntpd[2260]: Listen and drop on 1 v4wildcard 0.0.0.0:123 Feb 05 03:50:34 sfd ntpd[2260]: Listen normally on 2 lo 127.0.0.1:123 Feb 05 03:50:34 sfd ntpd[2260]: Listen normally on 3 ens192 192.168.1.31:123 Feb 05 03:50:34 sfd ntpd[2260]: Listen normally on 4 lo [::1]:123 Feb 05 03:50:34 sfd ntpd[2260]: Listening on routing socket on fd #21 for interface updates Feb 05 03:50:35 sfd ntpd[2260]: Soliciting pool server 216.239.35.12 Feb 05 03:50:36 sfd ntpd[2260]: Soliciting pool server 216.239.35.4 Feb 05 03:50:37 sfd ntpd[2260]: Soliciting pool server 216.239.35.0 Feb 05 03:50:38 sfd ntpd[2260]: Soliciting pool server 216.239.35.8 Feb 05 03:50:39 sfd ntpd[2260]: Soliciting pool server 2001:4860:4806:: -----RESTART NTP - END----- Checking if NTP service is running. NTP service is running. Checking if NTP is configured. Waiting for NTP to get configured. Sleeping for 10 seconds... ... SNIP ... Waiting for NTP to get configured. Sleeping for 10 seconds... NTP failed to get synced. NTP config failed # This failure seems to be erroneous. Checking the NTP service confirms it is working. admin@sfd.local@sfd:~$ systemctl status ntp \u25cf ntp.service - LSB: Start NTP daemon Loaded: loaded (/etc/init.d/ntp; bad; vendor preset: enabled) Active: active (running) since Wed 2020-02-05 03:50:34 UTC; 6h left Docs: man:systemd-sysv-generator(8) Process: 2235 ExecStop=/etc/init.d/ntp stop (code=exited, status=0/SUCCESS) Process: 2248 ExecStart=/etc/init.d/ntp start (code=exited, status=0/SUCCESS) Tasks: 2 Memory: 1.5M CPU: 81ms CGroup: /system.slice/ntp.service \u2514\u25002260 /usr/sbin/ntpd -p /var/run/ntpd.pid -g -u 112:117 admin@sfd.local@sfd:~$ ntpstat The program 'ntpstat' is currently not installed. You can install it by typing: sudo apt install ntpstat admin@sfd.local@sfd:~$ ntp -qn No command 'ntp' found, but there are 21 similar ones ntp: command not found admin@sfd.local@sfd:~$ ntpq -pn remote refid st t when poll reach delay offset jitter ============================================================================== time.google.com .POOL. 16 p - 64 0 0.000 0.000 0.000 +216.239.35.12 .GOOG. 1 u 54 64 375 64.340 -91.086 8.997 *216.239.35.4 .GOOG. 1 u 62 64 377 62.412 -83.944 8.385 +216.239.35.0 .GOOG. 1 u 42 64 377 63.484 -83.978 10.080 -216.239.35.8 .GOOG. 1 u 21 64 377 72.245 -73.087 15.674 Problem isolated to: max_tries=30 # check counter till 30. (total 300 seconds) echo \"Checking if NTP is configured.\" until [[ $cnt -ge $max_tries ]] do ntpq -pn | awk '{print $2}' | xargs | grep -E -o \"([0-9]{1,3}[\\.]){3}[0-9]{1,3}|PPS|GPS|LOCAL|LOCL\" [[ \"$?\" -eq 0 ]] && echo \"NTP synchronized successfully.\" && return 0 cnt=$[$cnt+1] echo \"Waiting for NTP to get configured. Sleeping for 10 seconds...\" sleep 10 done The offending line is ntpq -pn | awk '{print $2}' | xargs | grep -E -o \"([0-9]{1,3}[\\.]){3}[0-9]{1,3}|PPS|GPS|LOCAL|LOCL\" . ntpq -pn has output formatted like this: admin@sfd.local@sfd:~$ ntpq -pn remote refid st t when poll reach delay offset jitter ============================================================================== time.google.com .POOL. 16 p - 64 0 0.000 0.000 0.000 +216.239.35.12 .GOOG. 1 u 54 64 375 64.340 -91.086 8.997 *216.239.35.4 .GOOG. 1 u 62 64 377 62.412 -83.944 8.385 +216.239.35.0 .GOOG. 1 u 42 64 377 63.484 -83.978 10.080 -216.239.35.8 .GOOG. 1 u 21 64 377 72.245 -73.087 15.674 Based on the regex looking for an IP I'm guessing the intent was to verify that the server is synched to a remote NTP server. However there are two problems. The first is that it is looking at the refid which is not the server you are synched to, but the server that the server you are pointing at is synched to. Second, there is no guarentee that an IP address will be in this column (or PPS/GPS/LOCAL/LOCL). This was my case. See below for my ntpq -pn results. Instead, I would check for the line that has the synchronized server (starts with *). Using ntpq -pn | awk '{print $1}' | sed -n '/^\\*/p' . You don't really need to check beyond that because it won't synchronize unless everything is working. Though if you want to take it a step further you could do: ntpq -pn | awk '{print $1}' | sed -n '/^\\*/p' | sed 's/\\*//g' | grep -E -o \"([0-9]{1,3}[\\.]){3}[0-9]{1,3}|PPS|GPS|LOCAL|LOCL\" .","title":"Detailed Explanation"},{"location":"Setting%20Up%20SmartFabric%20Director/BUG/#my-ntpq-pn-with-timegooglecom-as-my-server","text":"admin@sfd.local@sfd:~$ ntpq -pn remote refid st t when poll reach delay offset jitter ============================================================================== time.google.com .POOL. 16 p - 64 0 0.000 0.000 0.000 *216.239.35.12 .GOOG. 1 u 38 64 377 67.370 -0.549 23.161 +216.239.35.4 .GOOG. 1 u 29 64 377 75.466 -1.691 22.089 +216.239.35.0 .GOOG. 1 u 36 64 377 67.731 -1.434 24.201 +216.239.35.8 .GOOG. 1 u 38 64 377 76.705 -0.042 22.436","title":"My ntpq -pn with time.google.com as my server"},{"location":"Setting%20Up%20iDRAC%20Telemetry%20with%20Splunk/","text":"Setting Up iDRAC Telemetry with Splunk Helpful Links Dell API Docs: https://developer.dell.com/apis/2978/versions/5.xx/docs/0WhatsNew.md Redfish Telemetry Whitepaper: https://www.dmtf.org/sites/default/files/standards/documents/DSP2051_1.0.0.pdf Description of the AMQP Messaging Protocol: https://www.ionos.com/digitalguide/websites/web-development/advanced-message-queuing-protocol-amqp/ Setting Up Splunk for the First Time: https://docs.splunk.com/Documentation/Splunk/8.2.4/Installation/StartSplunkforthefirsttime Integrate iDRAC Telemetry Data Into Splunk: Link to PDF My Test Environment RHEL NAME=\"Red Hat Enterprise Linux\" VERSION=\"8.5 (Ootpa)\" ID=\"rhel\" ID_LIKE=\"fedora\" VERSION_ID=\"8.5\" PLATFORM_ID=\"platform:el8\" PRETTY_NAME=\"Red Hat Enterprise Linux 8.5 (Ootpa)\" ANSI_COLOR=\"0;31\" CPE_NAME=\"cpe:/o:redhat:enterprise_linux:8::baseos\" HOME_URL=\"https://www.redhat.com/\" DOCUMENTATION_URL=\"https://access.redhat.com/documentation/red_hat_enterprise_linux/8/\" BUG_REPORT_URL=\"https://bugzilla.redhat.com/\" REDHAT_BUGZILLA_PRODUCT=\"Red Hat Enterprise Linux 8\" REDHAT_BUGZILLA_PRODUCT_VERSION=8.5 REDHAT_SUPPORT_PRODUCT=\"Red Hat Enterprise Linux\" REDHAT_SUPPORT_PRODUCT_VERSION=\"8.5\" Red Hat Enterprise Linux release 8.5 (Ootpa) Red Hat Enterprise Linux release 8.5 (Ootpa) Installation Setup Splunk Download trial of Splunk Follow Splunk installation instructions By default it will install to /opt/splunk. Run /opt/splunk/bin/splunk start (I suggest you do this in tmux or another terminal emulator) Run firewall-cmd --permanent --zone public --add-port=8000/tcp && firewall-cmd --reload Make splunk start on boot with /opt/splunk/bin/splunk enable boot-start Using Syslog Following the instructions here Install podman with dnf install -y podman Follow the instructions here 1.NOTE: When adding the HTTP input in Splunk it failed out because the token weren't enabled. I had to manually edit /opt/splunk/etc/apps/splunk_httpinput/default/inputs.conf and set disabled to 0 then do a systemctl restart splunk Run systemctl stop rsyslog && systemctl disable rsyslog Using ActiveMQ and splunkpump dnf install -y podman mkdir -p mkdir -p /opt/activemq/data && /opt/activemq/conf Run the following to generate default configs: bash podman run --user root --rm -ti -p 61616:61616 -p 8161:8161 -v /opt/activemq/conf:/mnt/conf:z -v /opt/activemq/data:/mnt/data:z rmohr/activemq /bin/sh chown activemq:activemq /mnt/conf chown activemq:activemq /mnt/data cp -a /opt/activemq/conf/* /mnt/conf/ cp -a /opt/activemq/data/* /mnt/data/ exit podman run -p 61616:61616 -p 8161:8161 -v /opt/activemq/conf:/opt/activemq/conf -v /opt/activemq/data:/opt/activemq/data rmohr/activemq Configure the iDRAC Download this script which will enable telemetry reports. Run EnableOrDisableAllTelemetryReports.py -ip $target -u $user -p $password 1.This enables telemetry on the target server Using ActiveMQ and splunkpump Using Syslog Next you will need to enable Redfish alerting which will publish the events to Splunk. Download this script Run the following command SubscriptionManagementREDFISH.py -ip $target -u $user -p $password -c y -D https://$splunkserver/services/collector/raw -E Alert -V Event 1. $target is the ip address or DNS name of the iDRAC 2. $user/$password are the username and password for iDRAC 3. $splunkserver is the IP address or DNS name of your Splunk HTTP event collector instance On the command line (racadm) 1.SSH to the iDRAC 2.Run ``` racadm set idrac.telemetry.RsyslogServer1 \"<splunk_ip/fqdn>\" racadm set idrac.telemetry.RsyslogServer1port \"514\" racadm testrsyslogconnection ```","title":"Setting Up iDRAC Telemetry with Splunk"},{"location":"Setting%20Up%20iDRAC%20Telemetry%20with%20Splunk/#setting-up-idrac-telemetry-with-splunk","text":"","title":"Setting Up iDRAC Telemetry with Splunk"},{"location":"Setting%20Up%20iDRAC%20Telemetry%20with%20Splunk/#helpful-links","text":"Dell API Docs: https://developer.dell.com/apis/2978/versions/5.xx/docs/0WhatsNew.md Redfish Telemetry Whitepaper: https://www.dmtf.org/sites/default/files/standards/documents/DSP2051_1.0.0.pdf Description of the AMQP Messaging Protocol: https://www.ionos.com/digitalguide/websites/web-development/advanced-message-queuing-protocol-amqp/ Setting Up Splunk for the First Time: https://docs.splunk.com/Documentation/Splunk/8.2.4/Installation/StartSplunkforthefirsttime Integrate iDRAC Telemetry Data Into Splunk: Link to PDF","title":"Helpful Links"},{"location":"Setting%20Up%20iDRAC%20Telemetry%20with%20Splunk/#my-test-environment","text":"","title":"My Test Environment"},{"location":"Setting%20Up%20iDRAC%20Telemetry%20with%20Splunk/#rhel","text":"NAME=\"Red Hat Enterprise Linux\" VERSION=\"8.5 (Ootpa)\" ID=\"rhel\" ID_LIKE=\"fedora\" VERSION_ID=\"8.5\" PLATFORM_ID=\"platform:el8\" PRETTY_NAME=\"Red Hat Enterprise Linux 8.5 (Ootpa)\" ANSI_COLOR=\"0;31\" CPE_NAME=\"cpe:/o:redhat:enterprise_linux:8::baseos\" HOME_URL=\"https://www.redhat.com/\" DOCUMENTATION_URL=\"https://access.redhat.com/documentation/red_hat_enterprise_linux/8/\" BUG_REPORT_URL=\"https://bugzilla.redhat.com/\" REDHAT_BUGZILLA_PRODUCT=\"Red Hat Enterprise Linux 8\" REDHAT_BUGZILLA_PRODUCT_VERSION=8.5 REDHAT_SUPPORT_PRODUCT=\"Red Hat Enterprise Linux\" REDHAT_SUPPORT_PRODUCT_VERSION=\"8.5\" Red Hat Enterprise Linux release 8.5 (Ootpa) Red Hat Enterprise Linux release 8.5 (Ootpa)","title":"RHEL"},{"location":"Setting%20Up%20iDRAC%20Telemetry%20with%20Splunk/#installation","text":"","title":"Installation"},{"location":"Setting%20Up%20iDRAC%20Telemetry%20with%20Splunk/#setup-splunk","text":"Download trial of Splunk Follow Splunk installation instructions By default it will install to /opt/splunk. Run /opt/splunk/bin/splunk start (I suggest you do this in tmux or another terminal emulator) Run firewall-cmd --permanent --zone public --add-port=8000/tcp && firewall-cmd --reload Make splunk start on boot with /opt/splunk/bin/splunk enable boot-start","title":"Setup Splunk"},{"location":"Setting%20Up%20iDRAC%20Telemetry%20with%20Splunk/#using-syslog","text":"Following the instructions here Install podman with dnf install -y podman Follow the instructions here 1.NOTE: When adding the HTTP input in Splunk it failed out because the token weren't enabled. I had to manually edit /opt/splunk/etc/apps/splunk_httpinput/default/inputs.conf and set disabled to 0 then do a systemctl restart splunk Run systemctl stop rsyslog && systemctl disable rsyslog","title":"Using Syslog"},{"location":"Setting%20Up%20iDRAC%20Telemetry%20with%20Splunk/#using-activemq-and-splunkpump","text":"dnf install -y podman mkdir -p mkdir -p /opt/activemq/data && /opt/activemq/conf Run the following to generate default configs: bash podman run --user root --rm -ti -p 61616:61616 -p 8161:8161 -v /opt/activemq/conf:/mnt/conf:z -v /opt/activemq/data:/mnt/data:z rmohr/activemq /bin/sh chown activemq:activemq /mnt/conf chown activemq:activemq /mnt/data cp -a /opt/activemq/conf/* /mnt/conf/ cp -a /opt/activemq/data/* /mnt/data/ exit podman run -p 61616:61616 -p 8161:8161 -v /opt/activemq/conf:/opt/activemq/conf -v /opt/activemq/data:/opt/activemq/data rmohr/activemq","title":"Using ActiveMQ and splunkpump"},{"location":"Setting%20Up%20iDRAC%20Telemetry%20with%20Splunk/#configure-the-idrac","text":"Download this script which will enable telemetry reports. Run EnableOrDisableAllTelemetryReports.py -ip $target -u $user -p $password 1.This enables telemetry on the target server","title":"Configure the iDRAC"},{"location":"Setting%20Up%20iDRAC%20Telemetry%20with%20Splunk/#using-activemq-and-splunkpump_1","text":"","title":"Using ActiveMQ and splunkpump"},{"location":"Setting%20Up%20iDRAC%20Telemetry%20with%20Splunk/#using-syslog_1","text":"Next you will need to enable Redfish alerting which will publish the events to Splunk. Download this script Run the following command SubscriptionManagementREDFISH.py -ip $target -u $user -p $password -c y -D https://$splunkserver/services/collector/raw -E Alert -V Event 1. $target is the ip address or DNS name of the iDRAC 2. $user/$password are the username and password for iDRAC 3. $splunkserver is the IP address or DNS name of your Splunk HTTP event collector instance On the command line (racadm) 1.SSH to the iDRAC 2.Run ``` racadm set idrac.telemetry.RsyslogServer1 \"<splunk_ip/fqdn>\" racadm set idrac.telemetry.RsyslogServer1port \"514\" racadm testrsyslogconnection ```","title":"Using Syslog"},{"location":"Setup%20IDPA/","text":"Setup IDPA Version DP4400 Current Documentation Manual Setup Guide Notes IDPA DP4400 model is a hyperconverged, 2U system that a user can install and configure onsite. The DP4400 includes a virtual edition of Avamar server (AVE) as the Backup Server node, a virtual edition of Data Domain system (DDVE) as the Protection Storage node, Cloud Disaster Recovery, IDPA System Manager as a centralized system management, an Appliance Configuration Manager(ACM) for simplified configuration and upgrades, Search, Reporting and Analytics, and a compute node that hosts the virtual components and the software. Components Appliance administration The ACM provides a web-based interface for configuring, monitoring, and upgrading the appliance.The ACM dashboard displays a summary of the configuration of the individual components. It also enables the administrators to monitor the appliance, modify configuration details such as expanding the Data Domain disk capacity, change the common password for the appliance, change LDAP settings, update customer information, and change the values in the General Settings panel. The ACM dashboard enables you to upgrade the system and its components. It also displays the health information of the Appliance Server and VMware components. Backup administration The IDPA uses Avamar Virtual Edition (AVE) servers fo-r the DP5xxx and DP4xxx models and a physical Avamar for DP8xxxx to perform backup operations, with the data being stored in a Data Domain system. Generally, when using the Avamar Administrator Management Console, all Avamar servers look and behave the same. The main differences among the Avamar server configurations are the number of nodes and disk drives that are reported in the Server Monitor console. You can also add an Avamar NDMP Accelerator (one NDMP Accelerator node is supported in DP4400 and DP5800) to enable backup and recovery of NAS systems. For more information about the configuration details, see Table 3. Configuration options for each model on page 9. The Avamar NDMP Accelerator uses the network data management protocol (NDMP) to enable backup and recovery of network attached storage (NAS) systems. The accelerator performs NDMP processing and then sends the data directly to the Data Domain Server (Data Domain Virtual Edition Storage). Instructions Network Setup Set up 12 continuous IP addresses in DNS plus idrac. They must be in the same subnet. idrac can be separate TODO Register the 13 IP addresses in DNS with forward and reverse lookup entries for each address. Ensure that the router for the 13 IP addresses can be pinged. 1.Cannot use _ in the hostname 2.Addresses should cover the following: My Configuration 192.168.2.64 acm.lan 192.168.2.65 idpa-esxi.lan 192.168.2.66 idpa-ddve-backup-1.lan 192.168.2.67 idpa-ddve-backup-2.lan 192.168.2.68 idpa-ave-server.lan 192.168.2.69 idpa-proxy.lan 192.168.2.70 idpa-system-manager.lan 192.168.2.71 idpa-application-server.lan 192.168.2.72 datastore-server-host.lan 192.168.2.73 index-master-node.lan 192.168.2.74 cdra.lan Configuration On the switch: switch(config)# interface range ethernet 1/1/1 switch(config)# interface range ethernet 1/1/1-1/1/5 switch(conf-range-eth1/1/1-1/1/5)# switchport mode access switch(conf-range-eth1/1/1-1/1/5)# switchport access vlan 2 On a jump box set an IP of 192.168.100.98 Verify you can ping 192.168.100.100 Browse to: https://192.168.100.100:8543/ Log in with default creds: 1.User root, password Idpa_1234 Follow the prompts and set up the networking. After you finish setting up the networking the UI will wait while the network settings are applied. Afterwards, it usually autoreconnects with updated IP information. Otherwise you can browse to `https:// :8543 Provide the information in the following screens: Proxy does image level backups. The system will backup the local VMs. Adding Backups Do not use change block tracking with vCenter Static vs dynamic - dynamic is dependent on it being in a folder. If the VMs are in a sub folder and you select the folder itself, then when you import the folder, the folder will be purple. That means dynamic, if someone creates another VM it will automatically add it. You can do rule based filtering and assign it to a policy. Recursive protection - Subfolders","title":"Setup IDPA"},{"location":"Setup%20IDPA/#setup-idpa","text":"","title":"Setup IDPA"},{"location":"Setup%20IDPA/#version","text":"DP4400","title":"Version"},{"location":"Setup%20IDPA/#current-documentation","text":"Manual Setup Guide","title":"Current Documentation"},{"location":"Setup%20IDPA/#notes","text":"IDPA DP4400 model is a hyperconverged, 2U system that a user can install and configure onsite. The DP4400 includes a virtual edition of Avamar server (AVE) as the Backup Server node, a virtual edition of Data Domain system (DDVE) as the Protection Storage node, Cloud Disaster Recovery, IDPA System Manager as a centralized system management, an Appliance Configuration Manager(ACM) for simplified configuration and upgrades, Search, Reporting and Analytics, and a compute node that hosts the virtual components and the software.","title":"Notes"},{"location":"Setup%20IDPA/#components","text":"","title":"Components"},{"location":"Setup%20IDPA/#appliance-administration","text":"The ACM provides a web-based interface for configuring, monitoring, and upgrading the appliance.The ACM dashboard displays a summary of the configuration of the individual components. It also enables the administrators to monitor the appliance, modify configuration details such as expanding the Data Domain disk capacity, change the common password for the appliance, change LDAP settings, update customer information, and change the values in the General Settings panel. The ACM dashboard enables you to upgrade the system and its components. It also displays the health information of the Appliance Server and VMware components.","title":"Appliance administration"},{"location":"Setup%20IDPA/#backup-administration","text":"The IDPA uses Avamar Virtual Edition (AVE) servers fo-r the DP5xxx and DP4xxx models and a physical Avamar for DP8xxxx to perform backup operations, with the data being stored in a Data Domain system. Generally, when using the Avamar Administrator Management Console, all Avamar servers look and behave the same. The main differences among the Avamar server configurations are the number of nodes and disk drives that are reported in the Server Monitor console. You can also add an Avamar NDMP Accelerator (one NDMP Accelerator node is supported in DP4400 and DP5800) to enable backup and recovery of NAS systems. For more information about the configuration details, see Table 3. Configuration options for each model on page 9. The Avamar NDMP Accelerator uses the network data management protocol (NDMP) to enable backup and recovery of network attached storage (NAS) systems. The accelerator performs NDMP processing and then sends the data directly to the Data Domain Server (Data Domain Virtual Edition Storage).","title":"Backup administration"},{"location":"Setup%20IDPA/#instructions","text":"","title":"Instructions"},{"location":"Setup%20IDPA/#network-setup","text":"Set up 12 continuous IP addresses in DNS plus idrac. They must be in the same subnet. idrac can be separate TODO Register the 13 IP addresses in DNS with forward and reverse lookup entries for each address. Ensure that the router for the 13 IP addresses can be pinged. 1.Cannot use _ in the hostname 2.Addresses should cover the following:","title":"Network Setup"},{"location":"Setup%20IDPA/#my-configuration","text":"192.168.2.64 acm.lan 192.168.2.65 idpa-esxi.lan 192.168.2.66 idpa-ddve-backup-1.lan 192.168.2.67 idpa-ddve-backup-2.lan 192.168.2.68 idpa-ave-server.lan 192.168.2.69 idpa-proxy.lan 192.168.2.70 idpa-system-manager.lan 192.168.2.71 idpa-application-server.lan 192.168.2.72 datastore-server-host.lan 192.168.2.73 index-master-node.lan 192.168.2.74 cdra.lan","title":"My Configuration"},{"location":"Setup%20IDPA/#configuration","text":"On the switch: switch(config)# interface range ethernet 1/1/1 switch(config)# interface range ethernet 1/1/1-1/1/5 switch(conf-range-eth1/1/1-1/1/5)# switchport mode access switch(conf-range-eth1/1/1-1/1/5)# switchport access vlan 2 On a jump box set an IP of 192.168.100.98 Verify you can ping 192.168.100.100 Browse to: https://192.168.100.100:8543/ Log in with default creds: 1.User root, password Idpa_1234 Follow the prompts and set up the networking. After you finish setting up the networking the UI will wait while the network settings are applied. Afterwards, it usually autoreconnects with updated IP information. Otherwise you can browse to `https:// :8543 Provide the information in the following screens: Proxy does image level backups. The system will backup the local VMs.","title":"Configuration"},{"location":"Setup%20IDPA/#adding-backups","text":"Do not use change block tracking with vCenter Static vs dynamic - dynamic is dependent on it being in a folder. If the VMs are in a sub folder and you select the folder itself, then when you import the folder, the folder will be purple. That means dynamic, if someone creates another VM it will automatically add it. You can do rule based filtering and assign it to a policy. Recursive protection - Subfolders","title":"Adding Backups"},{"location":"Site%20to%20Site%20VPN%20with%20PFSense%20and%20CentOS%208/","text":"Site to Site VPN with PFSense and CentOS 8 On PFSense Go to openvpn server creation Select UDP on IPv4 only with tun Use a Peer to Peer (Shared Key) For the shared key automatically generate it No other special settings required. After you create the server, save it, and then go back in and copy the shared key. Open port 1194 UDP on the firewall. On CentOS 8 Make sure everything is up to date. yum update -y && reboot . The reboot is important because if your kernel might update. If this happens you need to reboot to load the new kernel. Run yum install -y epel-release && yum update -y && yum install -y openvpn easy-rsa chrony && systemctl enable chronyd && chronyc makestep This is a long series of commands, but it installs openvpn and chrony. You need chrony to ensure your time is synched. WARNING : If the time is not synched between the server and your clients, the VPN will fail to connect! systemctl stop firewalld - otherwise you'll have to allow everything going to and from the networks on a case by case basis. Run sysctl -w net.ipv4.ip_forward=1 && echo 1 > /proc/sys/net/ipv4/ip_forward Use the following config file: dev ovpnc3 verb 6 dev-type tun dev-node /dev/tun3 writepid /var/run/openvpn_client3.pid user nobody group nobody script-security 3 keepalive 10 60 ping-timer-rem persist-tun persist-key proto udp4 cipher AES-128-CBC auth SHA256 local lport 0 management /etc/openvpn/client3.sock unix remote 1194 udp4 ifconfig route compress resolv-retry infinite secret /etc/openvpn/client/secret In my scenario the 192.168.2.0/24 was the remote site network and 192.168.1.1 was the local network.","title":"Site to Site VPN with PFSense and CentOS 8"},{"location":"Site%20to%20Site%20VPN%20with%20PFSense%20and%20CentOS%208/#site-to-site-vpn-with-pfsense-and-centos-8","text":"","title":"Site to Site VPN with PFSense and CentOS 8"},{"location":"Site%20to%20Site%20VPN%20with%20PFSense%20and%20CentOS%208/#on-pfsense","text":"Go to openvpn server creation Select UDP on IPv4 only with tun Use a Peer to Peer (Shared Key) For the shared key automatically generate it No other special settings required. After you create the server, save it, and then go back in and copy the shared key. Open port 1194 UDP on the firewall.","title":"On PFSense"},{"location":"Site%20to%20Site%20VPN%20with%20PFSense%20and%20CentOS%208/#on-centos-8","text":"Make sure everything is up to date. yum update -y && reboot . The reboot is important because if your kernel might update. If this happens you need to reboot to load the new kernel. Run yum install -y epel-release && yum update -y && yum install -y openvpn easy-rsa chrony && systemctl enable chronyd && chronyc makestep This is a long series of commands, but it installs openvpn and chrony. You need chrony to ensure your time is synched. WARNING : If the time is not synched between the server and your clients, the VPN will fail to connect! systemctl stop firewalld - otherwise you'll have to allow everything going to and from the networks on a case by case basis. Run sysctl -w net.ipv4.ip_forward=1 && echo 1 > /proc/sys/net/ipv4/ip_forward Use the following config file: dev ovpnc3 verb 6 dev-type tun","title":"On CentOS 8"},{"location":"Site%20to%20Site%20VPN%20with%20PFSense%20and%20CentOS%208/#dev-node-devtun3","text":"writepid /var/run/openvpn_client3.pid","title":"dev-node /dev/tun3"},{"location":"Site%20to%20Site%20VPN%20with%20PFSense%20and%20CentOS%208/#user-nobody","text":"","title":"user nobody"},{"location":"Site%20to%20Site%20VPN%20with%20PFSense%20and%20CentOS%208/#group-nobody","text":"script-security 3 keepalive 10 60 ping-timer-rem persist-tun persist-key proto udp4 cipher AES-128-CBC auth SHA256 local lport 0 management /etc/openvpn/client3.sock unix remote 1194 udp4 ifconfig route compress resolv-retry infinite secret /etc/openvpn/client/secret In my scenario the 192.168.2.0/24 was the remote site network and 192.168.1.1 was the local network.","title":"group nobody"},{"location":"Switch%20Directly%20to%20Client%20Test/","text":"Switch Directly to Client Test The purpose of this test is to see if a 10Gb/s switch with a 10Gb/s SFP will autonegotiate when plugged directly to a client. OS 10 Version OS10# show version Dell EMC Networking OS10 Enterprise Copyright (c) 1999-2019 by Dell Inc. All Rights Reserved. OS Version: 10.5.0.2 Build Version: 10.5.0.2.468 Build Time: 2019-10-19T00:29:00+0000 System Type: S4112F-ON Architecture: x86_64 Up Time: 00:03:39 Setup I first plugged a Dell Precision 7730 with Intel I219-LM network card into Ethernet 1/1/8. I later plugged in an ESXi instance with an Intel x710 to Ethernet 1/1/7. SFP Used Results I had to set the interface speed for it to pick up, but once the interface speed was set it worked. OS10# show ip interface brief Interface Name IP-Address OK Method Status Protocol ========================================================================================= Ethernet 1/1/1 unassigned NO unset up down Ethernet 1/1/2 unassigned NO unset up down Ethernet 1/1/3 unassigned NO unset up down Ethernet 1/1/4 unassigned NO unset up down Ethernet 1/1/5 unassigned NO unset up down Ethernet 1/1/6 unassigned NO unset up down Ethernet 1/1/7 unassigned NO unset up down Ethernet 1/1/8 unassigned NO unset up down Ethernet 1/1/9 unassigned NO unset up down Ethernet 1/1/10 unassigned NO unset up down Ethernet 1/1/11 unassigned NO unset up down Ethernet 1/1/12 unassigned NO unset up down Ethernet 1/1/13 unassigned NO unset up down Ethernet 1/1/14 unassigned NO unset up down Ethernet 1/1/15 unassigned NO unset up down Management 1/1/1 192.168.1.20/24 YES manual up up Vlan 1 unassigned NO unset up down Port-channel 1 unassigned NO unset up down OS10# configure terminal OS10(config)# interface ethernet 1/1/8 OS10(conf-if-eth1/1/8)# speed 10 1000 10000 OS10(conf-if-eth1/1/8)# speed 1000 OS10(conf-if-eth1/1/8)# show ip interface brief Interface Name IP-Address OK Method Status Protocol ========================================================================================= Ethernet 1/1/1 unassigned NO unset up down Ethernet 1/1/2 unassigned NO unset up down Ethernet 1/1/3 unassigned NO unset up down Ethernet 1/1/4 unassigned NO unset up down Ethernet 1/1/5 unassigned NO unset up down Ethernet 1/1/6 unassigned NO unset up down Ethernet 1/1/7 unassigned NO unset up down Ethernet 1/1/8 unassigned YES unset up up Ethernet 1/1/9 unassigned NO unset up down Ethernet 1/1/10 unassigned NO unset up down Ethernet 1/1/11 unassigned NO unset up down Ethernet 1/1/12 unassigned NO unset up down Ethernet 1/1/13 unassigned NO unset up down Ethernet 1/1/14 unassigned NO unset up down Ethernet 1/1/15 unassigned NO unset up down Management 1/1/1 192.168.1.20/24 YES manual up up Vlan 1 unassigned YES unset up up Port-channel 1 unassigned NO unset up down On the client side the network showed Unidentified both before and after the change in the speed configuration. After I brought the interface up I connect Ethernet 1/1/7 to the Intel x710 with ESXi hosting the card. I connected it to a RHEL 8 VM and confirmed ping worked between the 7730 and the ESXi instance. [root@rhel8 ~]# ping 10.0.0.1 PING 10.0.0.1 (10.0.0.1) 56(84) bytes of data. 64 bytes from 10.0.0.1: icmp_seq=1 ttl=128 time=0.953 ms 64 bytes from 10.0.0.1: icmp_seq=2 ttl=128 time=0.775 ms 64 bytes from 10.0.0.1: icmp_seq=3 ttl=128 time=0.571 ms 64 bytes from 10.0.0.1: icmp_seq=4 ttl=128 time=0.613 ms ^C --- 10.0.0.1 ping statistics --- 4 packets transmitted, 4 received, 0% packet loss, time 114ms rtt min/avg/max/mdev = 0.571/0.728/0.953/0.150 ms","title":"Switch Directly to Client Test"},{"location":"Switch%20Directly%20to%20Client%20Test/#switch-directly-to-client-test","text":"The purpose of this test is to see if a 10Gb/s switch with a 10Gb/s SFP will autonegotiate when plugged directly to a client.","title":"Switch Directly to Client Test"},{"location":"Switch%20Directly%20to%20Client%20Test/#os-10-version","text":"OS10# show version Dell EMC Networking OS10 Enterprise Copyright (c) 1999-2019 by Dell Inc. All Rights Reserved. OS Version: 10.5.0.2 Build Version: 10.5.0.2.468 Build Time: 2019-10-19T00:29:00+0000 System Type: S4112F-ON Architecture: x86_64 Up Time: 00:03:39","title":"OS 10 Version"},{"location":"Switch%20Directly%20to%20Client%20Test/#setup","text":"I first plugged a Dell Precision 7730 with Intel I219-LM network card into Ethernet 1/1/8. I later plugged in an ESXi instance with an Intel x710 to Ethernet 1/1/7.","title":"Setup"},{"location":"Switch%20Directly%20to%20Client%20Test/#sfp-used","text":"","title":"SFP Used"},{"location":"Switch%20Directly%20to%20Client%20Test/#results","text":"I had to set the interface speed for it to pick up, but once the interface speed was set it worked. OS10# show ip interface brief Interface Name IP-Address OK Method Status Protocol ========================================================================================= Ethernet 1/1/1 unassigned NO unset up down Ethernet 1/1/2 unassigned NO unset up down Ethernet 1/1/3 unassigned NO unset up down Ethernet 1/1/4 unassigned NO unset up down Ethernet 1/1/5 unassigned NO unset up down Ethernet 1/1/6 unassigned NO unset up down Ethernet 1/1/7 unassigned NO unset up down Ethernet 1/1/8 unassigned NO unset up down Ethernet 1/1/9 unassigned NO unset up down Ethernet 1/1/10 unassigned NO unset up down Ethernet 1/1/11 unassigned NO unset up down Ethernet 1/1/12 unassigned NO unset up down Ethernet 1/1/13 unassigned NO unset up down Ethernet 1/1/14 unassigned NO unset up down Ethernet 1/1/15 unassigned NO unset up down Management 1/1/1 192.168.1.20/24 YES manual up up Vlan 1 unassigned NO unset up down Port-channel 1 unassigned NO unset up down OS10# configure terminal OS10(config)# interface ethernet 1/1/8 OS10(conf-if-eth1/1/8)# speed 10 1000 10000 OS10(conf-if-eth1/1/8)# speed 1000 OS10(conf-if-eth1/1/8)# show ip interface brief Interface Name IP-Address OK Method Status Protocol ========================================================================================= Ethernet 1/1/1 unassigned NO unset up down Ethernet 1/1/2 unassigned NO unset up down Ethernet 1/1/3 unassigned NO unset up down Ethernet 1/1/4 unassigned NO unset up down Ethernet 1/1/5 unassigned NO unset up down Ethernet 1/1/6 unassigned NO unset up down Ethernet 1/1/7 unassigned NO unset up down Ethernet 1/1/8 unassigned YES unset up up Ethernet 1/1/9 unassigned NO unset up down Ethernet 1/1/10 unassigned NO unset up down Ethernet 1/1/11 unassigned NO unset up down Ethernet 1/1/12 unassigned NO unset up down Ethernet 1/1/13 unassigned NO unset up down Ethernet 1/1/14 unassigned NO unset up down Ethernet 1/1/15 unassigned NO unset up down Management 1/1/1 192.168.1.20/24 YES manual up up Vlan 1 unassigned YES unset up up Port-channel 1 unassigned NO unset up down On the client side the network showed Unidentified both before and after the change in the speed configuration. After I brought the interface up I connect Ethernet 1/1/7 to the Intel x710 with ESXi hosting the card. I connected it to a RHEL 8 VM and confirmed ping worked between the 7730 and the ESXi instance. [root@rhel8 ~]# ping 10.0.0.1 PING 10.0.0.1 (10.0.0.1) 56(84) bytes of data. 64 bytes from 10.0.0.1: icmp_seq=1 ttl=128 time=0.953 ms 64 bytes from 10.0.0.1: icmp_seq=2 ttl=128 time=0.775 ms 64 bytes from 10.0.0.1: icmp_seq=3 ttl=128 time=0.571 ms 64 bytes from 10.0.0.1: icmp_seq=4 ttl=128 time=0.613 ms ^C --- 10.0.0.1 ping statistics --- 4 packets transmitted, 4 received, 0% packet loss, time 114ms rtt min/avg/max/mdev = 0.571/0.728/0.953/0.150 ms","title":"Results"},{"location":"Testing%20Bare%20Metal%20Orchestrator/","text":"Testing Bare Metal Orchestrator Testing Bare Metal Orchestrator Supported Operating Systems License Requirements Architecture BMO Features and Functions Archecture with HA Setup Create a User for Global Admin Deploy a Server Questions Problems Bug Report Supported Operating Systems https://infohub.delltechnologies.com/l/bare-metal-orchestrator-1-2-command-line-interface-user-s-guide-1/validated-hypervisors-and-operating-systems-7 License Requirements https://infohub.delltechnologies.com/l/bare-metal-orchestrator-1-2-command-line-interface-user-s-guide-1/license-and-firmware-requirements-1 Architecture What it does https://infohub.delltechnologies.com/l/bare-metal-orchestrator-1-2-command-line-interface-user-s-guide-1/bare-metal-orchestrator-introduction-7 User interfaces\u2014 Bare Metal Orchestrator provides a web-based User Interface (UI), a Command Line Interface (CLI) client, and an Application Programming Interface (API) to perform remote infrastructure management tasks. All requests and actions from these interfaces reach the global controller. Global Controller\u2014The Global Controller (GC) is a fully contained management cluster that is deployed at the central office. The Global Controller can manage sites and servers that are associated with it. It constitutes core components and site components. For more information, see Bare Metal Orchestrator components . BMO Features and Functions https://infohub.delltechnologies.com/l/bare-metal-orchestrator-1-2-command-line-interface-user-s-guide-1/bare-metal-orchestrator-infrastructure-management-features-and-functions-7 Archecture with HA GlusterFS provides distributed file storage for the Global Controller and the two redundant HA nodes in the control plane cluster. The distributed storage volumes replicate the Bare Metal Orchestrator cluster data when using PersistentVolumeClaim (PVC). Distributed storage can be deployed locally in the three-node control plane cluster or externally. For external storage deployments, the VMs hosting the storage volumes must be reachable by the HA cluster. A minimum of three storage nodes are required. Note: The remote site uses local-path as the storage class. Setup Note: I'm going to try the single node first Log in with installer / Dell1234 Configure networking (just fill in the blanks) with vim /etc/network/interfaces Update DNS vim /etc/resolv.conf Update the hosts file with <address you used above> localregistry.io Reboot cd mw-ova-ansible Edit inventory/my-cluster/hosts.ini to have the following (replace the IP with your global IP address) ; ansible-playbook -v site.yml -i inventory/sample/hosts.ini [global_controller] 192.168.1.67 [ha] [loadbalancer] [gluster_nodes] 192.168.1.67 [secondary_ip] ; cp1 - secondary IP address. set this for single node and HA cluster. (OPTIONAL) ; cp2 - secondary IP address. set for HA cluster. (OPTIONAL) ; cp3 - secondary IP address. set for HA cluster. (OPTIONAL) [node] ; host1 ; host2 [node-remove] ; host1 [hosts] 192.168.1.67 ansible_python_interpreter=/usr/bin/python3 ; 192.168.dd152.160 ansible_python_interpreter=/usr/bin/python3 Edit inventory/my-cluster/group_vars/all.yml and change gluster_volume_type to \"none\" Run lsblk and make sure that you see: Note: on mine the swap drive wasd SDA5 and I choose to ignore it. We'll see if I'm punished. sda \u2014\u2014sda1 \u2014\u2014sda2 \u2014\u2014sda3 sdb \u2014\u2014sdb1 ``` 9. Update singlenode-site.yaml with your info 10. Run sudo ansible-playbook ssh-copy-heketi.yaml -i inventory/my-cluster/hosts.ini sudo ansible-playbook setup.yaml -i inventory/my-cluster/hosts.ini ### Create a User for Global Admin 1. On BMO create a YAML file with the below name: Admin email: admin@dell.com country: USA city: Denver organization: Dell orgUnit: BDC province: Co roles: - global-admin 2. After you have created the file run `bmo create user -f <username>.yaml > <config>.yaml` 3. Download the output file and browse to `https://BMO_ADDRESS` 4. Use the config file to login ## Deploy a Server TODO ## Questions - I see the API proxy - how does billing / licensing work if you are offline? ## Problems - A lot o the documentation links are broken. Ex: - https://infohub.delltechnologies.com/l/bare-metal-orchestrator-1-2-command-line-interface-user-s-guide-1/managed-device-discovery-overview-1#GUID-C1AEBE7E-0569-4E04-83CB-5CD77FFA6615 - The mMetadata link for servers: https://infohub.delltechnologies.com/l/bare-metal-orchestrator-1-2-command-line-interface-user-s-guide-1/create-a-server-or-multiple-servers-and-update-configurations-3#GUID-12309190-709C-4436-8937-D367B5978993 - Error when adding server. When I was in sites I just used the name. - I had to look under inventory, actions, discover device to realize that the global site name was actually gc - This link is broken: https://infohub.delltechnologies.com/l/bare-metal-orchestrator-1-2-command-line-interface-user-s-guide-1/create-a-server-or-multiple-servers-and-update-configurations-3#GUID-AF06D60B-3E6D-46A3-9722-B3D2E0D44BCB on https://infohub.delltechnologies.com/l/bare-metal-orchestrator-1-2-command-line-interface-user-s-guide-1/create-a-server-or-multiple-servers-and-update-configurations-3 ![](images/2022-10-12-14-31-59.png) dell@bmo-manager-1:/tmp$ bmo create server -f r7525_perc.yaml Failed to create server in the 'metalweaver' namespace, server :dell-perc-r7525, reason: site does not exist: global dell@bmo-manager-1:/tmp$ cat r7525_perc.yaml apiVersion: mw.dell.com/v1 kind: Server metadata: name: dell-perc-r7525 labels: profile: baseline-profile site: global spec: # Add fields here bmcEndPoint: \"https://192.168.1.46\" userName: \"root\" password: \"password\" - It would be helpful if the help were contextual. Ex: `bmo create --help` returns the command help fur create rather than the generic BMO command - It would be helpful to have a way to pretty print the output of something like `bmo create hardwareprofile -f <FILE>` - Error I received on import was dell@bmo-manager-1:~$ bmo create hardwareprofile -f hw_pf_bios.yaml Failed to create hardware profile in the 'metalweaver' namespace, reason: HardwareProfile.mw.dell.com \"r7525-profile\" is invalid: [spec.server.bios.attributes.numLock: Unsupported value: \"true\": supported values: \"On\", \"Off\", spec.server.bios.attributes.setBootOrderEn: Invalid value: \"AHCI\": spec.server.bios.attributes.setBootOrderEn in body should match '^$|^((\\w+|*).(\\w+|*).(\\w+|*)(-(\\w+|*)) ((\\w+|*)) (\\:((\\w+|*).(\\w+|*).(\\w+|*)(-(\\w+|*)) ((\\w+|*)) )) )(\\,(\\w+|*).(\\w+|*).(\\w+|*)(-(\\w+|*)) ((\\w+|*)) (\\:((\\w+|*).(\\w+|*).(\\w+|*)(-(\\w+|*)) ((\\w+|*)) )) )*$', spec.server.bios.attributes.serialPortAddress: Unsupported value: \"Com\": supported values: \"Com1\", \"Com2\", spec.server.bios.attributes.conTermType: Unsupported value: \"Vt\": supported values: \"Vt100Vt220\", \"Ansi\", spec.server.bios.attributes.tpmSecurity: Unsupported value: \"true\": supported values: \"On\", \"Off\", \"OnPbm\", \"OnNoPbm\", spec.server.bios.attributes.usbManagedPort: Unsupported value: \"true\": supported values: \"On\", \"Off\", spec.server.bios.attributes.pxeDev1Interface: Invalid value: \"NIC\": spec.server.bios.attributes.pxeDev1Interface in body should match '^NIC.[A-Za-z]+.[0-9]+-[0-9]+-[0-9]+'] ``` but in my file I had NumLock: On which is confusing because I did properly have numlock set to On. The real problem was that it wanted numLock instead of NumLock Bug Report See Bug Report","title":"Testing Bare Metal Orchestrator"},{"location":"Testing%20Bare%20Metal%20Orchestrator/#testing-bare-metal-orchestrator","text":"Testing Bare Metal Orchestrator Supported Operating Systems License Requirements Architecture BMO Features and Functions Archecture with HA Setup Create a User for Global Admin Deploy a Server Questions Problems Bug Report","title":"Testing Bare Metal Orchestrator"},{"location":"Testing%20Bare%20Metal%20Orchestrator/#supported-operating-systems","text":"https://infohub.delltechnologies.com/l/bare-metal-orchestrator-1-2-command-line-interface-user-s-guide-1/validated-hypervisors-and-operating-systems-7","title":"Supported Operating Systems"},{"location":"Testing%20Bare%20Metal%20Orchestrator/#license-requirements","text":"https://infohub.delltechnologies.com/l/bare-metal-orchestrator-1-2-command-line-interface-user-s-guide-1/license-and-firmware-requirements-1","title":"License Requirements"},{"location":"Testing%20Bare%20Metal%20Orchestrator/#architecture","text":"What it does https://infohub.delltechnologies.com/l/bare-metal-orchestrator-1-2-command-line-interface-user-s-guide-1/bare-metal-orchestrator-introduction-7 User interfaces\u2014 Bare Metal Orchestrator provides a web-based User Interface (UI), a Command Line Interface (CLI) client, and an Application Programming Interface (API) to perform remote infrastructure management tasks. All requests and actions from these interfaces reach the global controller. Global Controller\u2014The Global Controller (GC) is a fully contained management cluster that is deployed at the central office. The Global Controller can manage sites and servers that are associated with it. It constitutes core components and site components. For more information, see Bare Metal Orchestrator components .","title":"Architecture"},{"location":"Testing%20Bare%20Metal%20Orchestrator/#bmo-features-and-functions","text":"https://infohub.delltechnologies.com/l/bare-metal-orchestrator-1-2-command-line-interface-user-s-guide-1/bare-metal-orchestrator-infrastructure-management-features-and-functions-7","title":"BMO Features and Functions"},{"location":"Testing%20Bare%20Metal%20Orchestrator/#archecture-with-ha","text":"GlusterFS provides distributed file storage for the Global Controller and the two redundant HA nodes in the control plane cluster. The distributed storage volumes replicate the Bare Metal Orchestrator cluster data when using PersistentVolumeClaim (PVC). Distributed storage can be deployed locally in the three-node control plane cluster or externally. For external storage deployments, the VMs hosting the storage volumes must be reachable by the HA cluster. A minimum of three storage nodes are required. Note: The remote site uses local-path as the storage class.","title":"Archecture with HA"},{"location":"Testing%20Bare%20Metal%20Orchestrator/#setup","text":"Note: I'm going to try the single node first Log in with installer / Dell1234 Configure networking (just fill in the blanks) with vim /etc/network/interfaces Update DNS vim /etc/resolv.conf Update the hosts file with <address you used above> localregistry.io Reboot cd mw-ova-ansible Edit inventory/my-cluster/hosts.ini to have the following (replace the IP with your global IP address) ; ansible-playbook -v site.yml -i inventory/sample/hosts.ini [global_controller] 192.168.1.67 [ha] [loadbalancer] [gluster_nodes] 192.168.1.67 [secondary_ip] ; cp1 - secondary IP address. set this for single node and HA cluster. (OPTIONAL) ; cp2 - secondary IP address. set for HA cluster. (OPTIONAL) ; cp3 - secondary IP address. set for HA cluster. (OPTIONAL) [node] ; host1 ; host2 [node-remove] ; host1 [hosts] 192.168.1.67 ansible_python_interpreter=/usr/bin/python3 ; 192.168.dd152.160 ansible_python_interpreter=/usr/bin/python3 Edit inventory/my-cluster/group_vars/all.yml and change gluster_volume_type to \"none\" Run lsblk and make sure that you see: Note: on mine the swap drive wasd SDA5 and I choose to ignore it. We'll see if I'm punished. sda \u2014\u2014sda1 \u2014\u2014sda2 \u2014\u2014sda3 sdb \u2014\u2014sdb1 ``` 9. Update singlenode-site.yaml with your info 10. Run sudo ansible-playbook ssh-copy-heketi.yaml -i inventory/my-cluster/hosts.ini sudo ansible-playbook setup.yaml -i inventory/my-cluster/hosts.ini ### Create a User for Global Admin 1. On BMO create a YAML file with the below name: Admin email: admin@dell.com country: USA city: Denver organization: Dell orgUnit: BDC province: Co roles: - global-admin 2. After you have created the file run `bmo create user -f <username>.yaml > <config>.yaml` 3. Download the output file and browse to `https://BMO_ADDRESS` 4. Use the config file to login ## Deploy a Server TODO ## Questions - I see the API proxy - how does billing / licensing work if you are offline? ## Problems - A lot o the documentation links are broken. Ex: - https://infohub.delltechnologies.com/l/bare-metal-orchestrator-1-2-command-line-interface-user-s-guide-1/managed-device-discovery-overview-1#GUID-C1AEBE7E-0569-4E04-83CB-5CD77FFA6615 - The mMetadata link for servers: https://infohub.delltechnologies.com/l/bare-metal-orchestrator-1-2-command-line-interface-user-s-guide-1/create-a-server-or-multiple-servers-and-update-configurations-3#GUID-12309190-709C-4436-8937-D367B5978993 - Error when adding server. When I was in sites I just used the name. - I had to look under inventory, actions, discover device to realize that the global site name was actually gc - This link is broken: https://infohub.delltechnologies.com/l/bare-metal-orchestrator-1-2-command-line-interface-user-s-guide-1/create-a-server-or-multiple-servers-and-update-configurations-3#GUID-AF06D60B-3E6D-46A3-9722-B3D2E0D44BCB on https://infohub.delltechnologies.com/l/bare-metal-orchestrator-1-2-command-line-interface-user-s-guide-1/create-a-server-or-multiple-servers-and-update-configurations-3 ![](images/2022-10-12-14-31-59.png) dell@bmo-manager-1:/tmp$ bmo create server -f r7525_perc.yaml Failed to create server in the 'metalweaver' namespace, server :dell-perc-r7525, reason: site does not exist: global dell@bmo-manager-1:/tmp$ cat r7525_perc.yaml apiVersion: mw.dell.com/v1 kind: Server metadata: name: dell-perc-r7525 labels: profile: baseline-profile site: global spec: # Add fields here bmcEndPoint: \"https://192.168.1.46\" userName: \"root\" password: \"password\" - It would be helpful if the help were contextual. Ex: `bmo create --help` returns the command help fur create rather than the generic BMO command - It would be helpful to have a way to pretty print the output of something like `bmo create hardwareprofile -f <FILE>` - Error I received on import was dell@bmo-manager-1:~$ bmo create hardwareprofile -f hw_pf_bios.yaml Failed to create hardware profile in the 'metalweaver' namespace, reason: HardwareProfile.mw.dell.com \"r7525-profile\" is invalid: [spec.server.bios.attributes.numLock: Unsupported value: \"true\": supported values: \"On\", \"Off\", spec.server.bios.attributes.setBootOrderEn: Invalid value: \"AHCI\": spec.server.bios.attributes.setBootOrderEn in body should match '^$|^((\\w+|*).(\\w+|*).(\\w+|*)(-(\\w+|*)) ((\\w+|*)) (\\:((\\w+|*).(\\w+|*).(\\w+|*)(-(\\w+|*)) ((\\w+|*)) )) )(\\,(\\w+|*).(\\w+|*).(\\w+|*)(-(\\w+|*)) ((\\w+|*)) (\\:((\\w+|*).(\\w+|*).(\\w+|*)(-(\\w+|*)) ((\\w+|*)) )) )*$', spec.server.bios.attributes.serialPortAddress: Unsupported value: \"Com\": supported values: \"Com1\", \"Com2\", spec.server.bios.attributes.conTermType: Unsupported value: \"Vt\": supported values: \"Vt100Vt220\", \"Ansi\", spec.server.bios.attributes.tpmSecurity: Unsupported value: \"true\": supported values: \"On\", \"Off\", \"OnPbm\", \"OnNoPbm\", spec.server.bios.attributes.usbManagedPort: Unsupported value: \"true\": supported values: \"On\", \"Off\", spec.server.bios.attributes.pxeDev1Interface: Invalid value: \"NIC\": spec.server.bios.attributes.pxeDev1Interface in body should match '^NIC.[A-Za-z]+.[0-9]+-[0-9]+-[0-9]+'] ``` but in my file I had NumLock: On which is confusing because I did properly have numlock set to On. The real problem was that it wanted numLock instead of NumLock","title":"Setup"},{"location":"Testing%20Bare%20Metal%20Orchestrator/#bug-report","text":"See Bug Report","title":"Bug Report"},{"location":"Testing%20Bare%20Metal%20Orchestrator/bug_report/","text":"Bug Report Bug Report BMO Version Problem Description Command Output Config file BMO Version dell@bmo-manager-1:~$ !b bmo version COMPONENT NAME VERSION mw-release v1.3.1 hpe-redfish-sku-pack v1.3.1 mw-api-proxy v1.3.1 mw-api-svc v1.3.1 mw-cli v1.3.1 mw-common v1.3.1 mw-dhcp-server v1.3.1 mw-discovery-manager v1.3.1 mw-event-router v1.3.1 mw-gui v1.3.1 mw-hardware-controller v1.3.1 mw-install v1.3.1 mw-iso-builder v1.3.1 mw-pxe-service v1.3.1 mw-server-controller v1.3.1 mw-site-controller v1.3.1 mw-site-manager v1.3.1 mw-sku-sdk v1.3.1 mw-stack-deployer v1.3.1 mw-stack-sku-pack v1.3.1 mw-stack-sku-sdk v1.3.1 mw-switch-controller v1.3.1 mw-tenant-controller v1.3.1 nso-sku-pack v1.3.1 redfish-sku-pack v1.3.1 samples v1.3.1 supermicro-redfish-sku-pack v1.3.1 switch-sku-pack v1.3.1 wr-stack-sku-pack v1.3.1 mw-s3-global v1.3.1 mw-s3 v1.3.1 reloader v1.3.1 redis v1.3.1 mw-nginx v1.3.1 mw-internal-nginx v1.3.1 mw-opensearch v1.3.1 mw-opensearch-dashboard v1.3.1 mw-fluentd v1.3.1 mw-tftp-server v1.3.1 mw-dhcp-relay v1.3.1 storage-version-migration-initializer v0.0.5 storage-version-migration-trigger v0.0.5 storage-version-migration-migrator v0.0.5 Problem Description When importing hardware profiles error output errenously reports incorrect fields See below namespace, reason: HardwareProfile.mw.dell.com \"r7525-profile\" is invalid: [spec.server.bios.attributes.numLock: Unsupported value: \"true\": supported values: \"On\", \"Off\" but if you check the configuration numLock is correctly set to On and not true . This error repeats for several other fields - ex: tpmSecurity Running the import multiple times yields different error messages instead of printing the entire error message It is unclear how to take the values from a server configuration exported from the iDRAC or OME and import it into BMO. In my case, I attempted to export from iDRAC, clean with find/replace regex ^[ \\t]+<Attribute Name=\"(.+)\">([A-Za-z]+).* -> $1: $2 but while most of these fields directly match, capitalization does not seem to work. I am unsure what else is not working because I can't properly tell which fields aren't being accepted. Command Output dell@bmo-manager-1:~$ bmo create hardwareprofile -f hw_pf_bios.yaml Failed to create hardware profile in the 'metalweaver' namespace, reason: HardwareProfile.mw.dell.com \"r7525-profile\" is invalid: [spec.server.bios.attributes.numLock: Unsupported value: \"true\": supported values: \"On\", \"Off\", spec.server.bios.attributes.setBootOrderEn: Invalid value: \"AHCI\": spec.server.bios.attributes.setBootOrderEn in body should match '^$|^((\\w+|\\*)\\.(\\w+|\\*)\\.(\\w+|\\*)(\\-(\\w+|\\*))*((\\w+|\\*))*(\\:((\\w+|\\*)\\.(\\w+|\\*)\\.(\\w+|\\*)(\\-(\\w+|\\*))*((\\w+|\\*))*))*)(\\,(\\w+|\\*)\\.(\\w+|\\*)\\.(\\w+|\\*)(\\-(\\w+|\\*))*((\\w+|\\*))*(\\:((\\w+|\\*)\\.(\\w+|\\*)\\.(\\w+|\\*)(\\-(\\w+|\\*))*((\\w+|\\*))*))*)*$', spec.server.bios.attributes.serialPortAddress: Unsupported value: \"Com\": supported values: \"Com1\", \"Com2\", spec.server.bios.attributes.conTermType: Unsupported value: \"Vt\": supported values: \"Vt100Vt220\", \"Ansi\", spec.server.bios.attributes.tpmSecurity: Unsupported value: \"true\": supported values: \"On\", \"Off\", \"OnPbm\", \"OnNoPbm\", spec.server.bios.attributes.usbManagedPort: Unsupported value: \"true\": supported values: \"On\", \"Off\", spec.server.bios.attributes.pxeDev1Interface: Invalid value: \"NIC\": spec.server.bios.attributes.pxeDev1Interface in body should match '^NIC\\.[A-Za-z]+\\.[0-9]+\\-[0-9]+\\-[0-9]+'] dell@bmo-manager-1:~$ fg vim hw_pf_bios.yaml [1]+ Stopped vim hw_pf_bios.yaml dell@bmo-manager-1:~$ !b bmo create hardwareprofile -f hw_pf_bios.yaml Failed to create hardware profile in the 'metalweaver' namespace, reason: HardwareProfile.mw.dell.com \"r7525-profile\" is invalid: [spec.server.bios.attributes.numLock: Unsupported value: \"true\": supported values: \"On\", \"Off\", spec.server.bios.attributes.setBootOrderEn: Invalid value: \"AHCI\": spec.server.bios.attributes.setBootOrderEn in body should match '^$|^((\\w+|\\*)\\.(\\w+|\\*)\\.(\\w+|\\*)(\\-(\\w+|\\*))*((\\w+|\\*))*(\\:((\\w+|\\*)\\.(\\w+|\\*)\\.(\\w+|\\*)(\\-(\\w+|\\*))*((\\w+|\\*))*))*)(\\,(\\w+|\\*)\\.(\\w+|\\*)\\.(\\w+|\\*)(\\-(\\w+|\\*))*((\\w+|\\*))*(\\:((\\w+|\\*)\\.(\\w+|\\*)\\.(\\w+|\\*)(\\-(\\w+|\\*))*((\\w+|\\*))*))*)*$', spec.server.bios.attributes.serialPortAddress: Unsupported value: \"Com\": supported values: \"Com1\", \"Com2\", spec.server.bios.attributes.conTermType: Unsupported value: \"Vt\": supported values: \"Vt100Vt220\", \"Ansi\", spec.server.bios.attributes.tpmSecurity: Unsupported value: \"true\": supported values: \"On\", \"Off\", \"OnPbm\", \"OnNoPbm\", spec.server.bios.attributes.usbManagedPort: Unsupported value: \"true\": supported values: \"On\", \"Off\", spec.server.bios.attributes.pxeDev1Interface: Invalid value: \"NIC\": spec.server.bios.attributes.pxeDev1Interface in body should match '^NIC\\.[A-Za-z]+\\.[0-9]+\\-[0-9]+\\-[0-9]+'] dell@bmo-manager-1:~$ !b bmo create hardwareprofile -f hw_pf_bios.yaml Failed to create hardware profile in the 'metalweaver' namespace, reason: HardwareProfile.mw.dell.com \"r7525-profile\" is invalid: [spec.server.bios.attributes.numLock: Unsupported value: \"true\": supported values: \"On\", \"Off\", spec.server.bios.attributes.setBootOrderEn: Invalid value: \"AHCI\": spec.server.bios.attributes.setBootOrderEn in body should match '^$|^((\\w+|\\*)\\.(\\w+|\\*)\\.(\\w+|\\*)(\\-(\\w+|\\*))*((\\w+|\\*))*(\\:((\\w+|\\*)\\.(\\w+|\\*)\\.(\\w+|\\*)(\\-(\\w+|\\*))*((\\w+|\\*))*))*)(\\,(\\w+|\\*)\\.(\\w+|\\*)\\.(\\w+|\\*)(\\-(\\w+|\\*))*((\\w+|\\*))*(\\:((\\w+|\\*)\\.(\\w+|\\*)\\.(\\w+|\\*)(\\-(\\w+|\\*))*((\\w+|\\*))*))*)*$', spec.server.bios.attributes.serialPortAddress: Unsupported value: \"Com\": supported values: \"Com1\", \"Com2\", spec.server.bios.attributes.conTermType: Unsupported value: \"Vt\": supported values: \"Vt100Vt220\", \"Ansi\", spec.server.bios.attributes.tpmSecurity: Unsupported value: \"true\": supported values: \"On\", \"Off\", \"OnPbm\", \"OnNoPbm\", spec.server.bios.attributes.usbManagedPort: Unsupported value: \"true\": supported values: \"On\", \"Off\", spec.server.bios.attributes.pxeDev1Interface: Invalid value: \"NIC\": spec.server.bios.attributes.pxeDev1Interface in body should match '^NIC\\.[A-Za-z]+\\.[0-9]+\\-[0-9]+\\-[0-9]+'] dell@bmo-manager-1:~$ !b bmo create hardwareprofile -f hw_pf_bios.yaml Failed to create hardware profile in the 'metalweaver' namespace, reason: HardwareProfile.mw.dell.com \"r7525-profile\" is invalid: [spec.server.bios.attributes.setBootOrderEn: Invalid value: \"AHCI\": spec.server.bios.attributes.setBootOrderEn in body should match '^$|^((\\w+|\\*)\\.(\\w+|\\*)\\.(\\w+|\\*)(\\-(\\w+|\\*))*((\\w+|\\*))*(\\:((\\w+|\\*)\\.(\\w+|\\*)\\.(\\w+|\\*)(\\-(\\w+|\\*))*((\\w+|\\*))*))*)(\\,(\\w+|\\*)\\.(\\w+|\\*)\\.(\\w+|\\*)(\\-(\\w+|\\*))*((\\w+|\\*))*(\\:((\\w+|\\*)\\.(\\w+|\\*)\\.(\\w+|\\*)(\\-(\\w+|\\*))*((\\w+|\\*))*))*)*$', spec.server.bios.attributes.serialPortAddress: Unsupported value: \"Com\": supported values: \"Com1\", \"Com2\", spec.server.bios.attributes.conTermType: Unsupported value: \"Vt\": supported values: \"Vt100Vt220\", \"Ansi\", spec.server.bios.attributes.tpmSecurity: Unsupported value: \"true\": supported values: \"On\", \"Off\", \"OnPbm\", \"OnNoPbm\", spec.server.bios.attributes.usbManagedPort: Unsupported value: \"true\": supported values: \"On\", \"Off\", spec.server.bios.attributes.pxeDev1Interface: Invalid value: \"NIC\": spec.server.bios.attributes.pxeDev1Interface in body should match '^NIC\\.[A-Za-z]+\\.[0-9]+\\-[0-9]+\\-[0-9]+', spec.server.bios.attributes.numLock: Unsupported value: \"true\": supported values: \"On\", \"Off\"] dell@bmo-manager-1:~$ !b bmo create hardwareprofile -f hw_pf_bios.yaml Failed to create hardware profile in the 'metalweaver' namespace, reason: HardwareProfile.mw.dell.com \"r7525-profile\" is invalid: [spec.server.bios.attributes.numLock: Unsupported value: \"true\": supported values: \"On\", \"Off\", spec.server.bios.attributes.setBootOrderEn: Invalid value: \"AHCI\": spec.server.bios.attributes.setBootOrderEn in body should match '^$|^((\\w+|\\*)\\.(\\w+|\\*)\\.(\\w+|\\*)(\\-(\\w+|\\*))*((\\w+|\\*))*(\\:((\\w+|\\*)\\.(\\w+|\\*)\\.(\\w+|\\*)(\\-(\\w+|\\*))*((\\w+|\\*))*))*)(\\,(\\w+|\\*)\\.(\\w+|\\*)\\.(\\w+|\\*)(\\-(\\w+|\\*))*((\\w+|\\*))*(\\:((\\w+|\\*)\\.(\\w+|\\*)\\.(\\w+|\\*)(\\-(\\w+|\\*))*((\\w+|\\*))*))*)*$', spec.server.bios.attributes.serialPortAddress: Unsupported value: \"Com\": supported values: \"Com1\", \"Com2\", spec.server.bios.attributes.conTermType: Unsupported value: \"Vt\": supported values: \"Vt100Vt220\", \"Ansi\", spec.server.bios.attributes.tpmSecurity: Unsupported value: \"true\": supported values: \"On\", \"Off\", \"OnPbm\", \"OnNoPbm\", spec.server.bios.attributes.usbManagedPort: Unsupported value: \"true\": supported values: \"On\", \"Off\", spec.server.bios.attributes.pxeDev1Interface: Invalid value: \"NIC\": spec.server.bios.attributes.pxeDev1Interface in body should match '^NIC\\.[A-Za-z]+\\.[0-9]+\\-[0-9]+\\-[0-9]+'] dell@bmo-manager-1:~$ !b bmo create hardwareprofile -f hw_pf_bios.yaml Failed to create hardware profile in the 'metalweaver' namespace, reason: HardwareProfile.mw.dell.com \"r7525-profile\" is invalid: [spec.server.bios.attributes.tpmSecurity: Unsupported value: \"true\": supported values: \"On\", \"Off\", \"OnPbm\", \"OnNoPbm\", spec.server.bios.attributes.usbManagedPort: Unsupported value: \"true\": supported values: \"On\", \"Off\", spec.server.bios.attributes.pxeDev1Interface: Invalid value: \"NIC\": spec.server.bios.attributes.pxeDev1Interface in body should match '^NIC\\.[A-Za-z]+\\.[0-9]+\\-[0-9]+\\-[0-9]+', spec.server.bios.attributes.numLock: Unsupported value: \"true\": supported values: \"On\", \"Off\", spec.server.bios.attributes.setBootOrderEn: Invalid value: \"AHCI\": spec.server.bios.attributes.setBootOrderEn in body should match '^$|^((\\w+|\\*)\\.(\\w+|\\*)\\.(\\w+|\\*)(\\-(\\w+|\\*))*((\\w+|\\*))*(\\:((\\w+|\\*)\\.(\\w+|\\*)\\.(\\w+|\\*)(\\-(\\w+|\\*))*((\\w+|\\*))*))*)(\\,(\\w+|\\*)\\.(\\w+|\\*)\\.(\\w+|\\*)(\\-(\\w+|\\*))*((\\w+|\\*))*(\\:((\\w+|\\*)\\.(\\w+|\\*)\\.(\\w+|\\*)(\\-(\\w+|\\*))*((\\w+|\\*))*))*)*$', spec.server.bios.attributes.serialPortAddress: Unsupported value: \"Com\": supported values: \"Com1\", \"Com2\", spec.server.bios.attributes.conTermType: Unsupported value: \"Vt\": supported values: \"Vt100Vt220\", \"Ansi\"] Config file apiVersion: mw.dell.com/v1 kind: HardwareProfile metadata: name: r7525-profile labels: site: gc spec: # Add fields here apply: false preview: true server: bios: attributes: LogicalProc: Enabled ProcVirtualization: Enabled KernelDmaProtection: Disabled L1StreamHwPrefetcher: Enabled L2StreamHwPrefetcher: Enabled MadtCoreEnumeration: Linear CcxAsNumaDomain: Enabled TransparentSme: Disabled ProcConfigTdp: Maximum ProcX2Apic: Enabled ProcCcds: All CcdCores: All ControlledTurbo: Disabled OptimizerMode: Auto EmbSata: AhciMode SecurityFreezeLock: Enabled WriteCache: Disabled BiosNvmeDriver: DellQualifiedDrives BootMode: Uefi BootSeqRetry: Enabled GenericUsbBoot: Disabled HddPlaceholder: Disabled SysPrepClean: None SetBootOrderEn: AHCI pxeDev1Interface: NIC pxeDev1Protocol: IPv pxeDev1VlanEnDis: Disabled usbPorts: AllOn usbManagedPort: On IntegratedRaid: Enabled IntegratedNetwork1: Enabled EmbNic1Nic2: Enabled EmbVideo: Enabled PciePreferredIoBus: Disabled PcieEnhancedPreferredIo: Disabled SriovGlobalEnable: Disabled OsWatchdogTimer: Disabled DellAutoDiscovery: PlatformDefault SerialComm: OnNoConRedir serialPortAddress: Com ConTermType: Vt RedirAfterBoot: Enabled SysProfile: PerfOptimized PasswordStatus: Unlocked tpmSecurity: On Tpm2Hierarchy: Enabled PwrButton: Enabled AcPwrRcvry: Last AcPwrRcvryDelay: Immediate UefiVariableAccess: Standard SecureBoot: Disabled SecureBootPolicy: Standard SecureBootMode: DeployedMode TpmPpiBypassProvision: Disabled TpmPpiBypassClear: Disabled Tpm2Algorithm: SHA RedundantOsLocation: None MemTest: Disabled DramRefreshDelay: Minimum MemoryInterleaving: Auto CorrEccSmi: Enabled OppSrefEn: Disabled CECriticalSEL: Disabled PPROnUCE: Enabled numLock: On ErrPrompt: Enabled ForceInt10: Disabled DellWyseP25BIOSAccess: Enabled selectors: profileName: r7525","title":"Bug Report"},{"location":"Testing%20Bare%20Metal%20Orchestrator/bug_report/#bug-report","text":"Bug Report BMO Version Problem Description Command Output Config file","title":"Bug Report"},{"location":"Testing%20Bare%20Metal%20Orchestrator/bug_report/#bmo-version","text":"dell@bmo-manager-1:~$ !b bmo version COMPONENT NAME VERSION mw-release v1.3.1 hpe-redfish-sku-pack v1.3.1 mw-api-proxy v1.3.1 mw-api-svc v1.3.1 mw-cli v1.3.1 mw-common v1.3.1 mw-dhcp-server v1.3.1 mw-discovery-manager v1.3.1 mw-event-router v1.3.1 mw-gui v1.3.1 mw-hardware-controller v1.3.1 mw-install v1.3.1 mw-iso-builder v1.3.1 mw-pxe-service v1.3.1 mw-server-controller v1.3.1 mw-site-controller v1.3.1 mw-site-manager v1.3.1 mw-sku-sdk v1.3.1 mw-stack-deployer v1.3.1 mw-stack-sku-pack v1.3.1 mw-stack-sku-sdk v1.3.1 mw-switch-controller v1.3.1 mw-tenant-controller v1.3.1 nso-sku-pack v1.3.1 redfish-sku-pack v1.3.1 samples v1.3.1 supermicro-redfish-sku-pack v1.3.1 switch-sku-pack v1.3.1 wr-stack-sku-pack v1.3.1 mw-s3-global v1.3.1 mw-s3 v1.3.1 reloader v1.3.1 redis v1.3.1 mw-nginx v1.3.1 mw-internal-nginx v1.3.1 mw-opensearch v1.3.1 mw-opensearch-dashboard v1.3.1 mw-fluentd v1.3.1 mw-tftp-server v1.3.1 mw-dhcp-relay v1.3.1 storage-version-migration-initializer v0.0.5 storage-version-migration-trigger v0.0.5 storage-version-migration-migrator v0.0.5","title":"BMO Version"},{"location":"Testing%20Bare%20Metal%20Orchestrator/bug_report/#problem-description","text":"When importing hardware profiles error output errenously reports incorrect fields See below namespace, reason: HardwareProfile.mw.dell.com \"r7525-profile\" is invalid: [spec.server.bios.attributes.numLock: Unsupported value: \"true\": supported values: \"On\", \"Off\" but if you check the configuration numLock is correctly set to On and not true . This error repeats for several other fields - ex: tpmSecurity Running the import multiple times yields different error messages instead of printing the entire error message It is unclear how to take the values from a server configuration exported from the iDRAC or OME and import it into BMO. In my case, I attempted to export from iDRAC, clean with find/replace regex ^[ \\t]+<Attribute Name=\"(.+)\">([A-Za-z]+).* -> $1: $2 but while most of these fields directly match, capitalization does not seem to work. I am unsure what else is not working because I can't properly tell which fields aren't being accepted.","title":"Problem Description"},{"location":"Testing%20Bare%20Metal%20Orchestrator/bug_report/#command-output","text":"dell@bmo-manager-1:~$ bmo create hardwareprofile -f hw_pf_bios.yaml Failed to create hardware profile in the 'metalweaver' namespace, reason: HardwareProfile.mw.dell.com \"r7525-profile\" is invalid: [spec.server.bios.attributes.numLock: Unsupported value: \"true\": supported values: \"On\", \"Off\", spec.server.bios.attributes.setBootOrderEn: Invalid value: \"AHCI\": spec.server.bios.attributes.setBootOrderEn in body should match '^$|^((\\w+|\\*)\\.(\\w+|\\*)\\.(\\w+|\\*)(\\-(\\w+|\\*))*((\\w+|\\*))*(\\:((\\w+|\\*)\\.(\\w+|\\*)\\.(\\w+|\\*)(\\-(\\w+|\\*))*((\\w+|\\*))*))*)(\\,(\\w+|\\*)\\.(\\w+|\\*)\\.(\\w+|\\*)(\\-(\\w+|\\*))*((\\w+|\\*))*(\\:((\\w+|\\*)\\.(\\w+|\\*)\\.(\\w+|\\*)(\\-(\\w+|\\*))*((\\w+|\\*))*))*)*$', spec.server.bios.attributes.serialPortAddress: Unsupported value: \"Com\": supported values: \"Com1\", \"Com2\", spec.server.bios.attributes.conTermType: Unsupported value: \"Vt\": supported values: \"Vt100Vt220\", \"Ansi\", spec.server.bios.attributes.tpmSecurity: Unsupported value: \"true\": supported values: \"On\", \"Off\", \"OnPbm\", \"OnNoPbm\", spec.server.bios.attributes.usbManagedPort: Unsupported value: \"true\": supported values: \"On\", \"Off\", spec.server.bios.attributes.pxeDev1Interface: Invalid value: \"NIC\": spec.server.bios.attributes.pxeDev1Interface in body should match '^NIC\\.[A-Za-z]+\\.[0-9]+\\-[0-9]+\\-[0-9]+'] dell@bmo-manager-1:~$ fg vim hw_pf_bios.yaml [1]+ Stopped vim hw_pf_bios.yaml dell@bmo-manager-1:~$ !b bmo create hardwareprofile -f hw_pf_bios.yaml Failed to create hardware profile in the 'metalweaver' namespace, reason: HardwareProfile.mw.dell.com \"r7525-profile\" is invalid: [spec.server.bios.attributes.numLock: Unsupported value: \"true\": supported values: \"On\", \"Off\", spec.server.bios.attributes.setBootOrderEn: Invalid value: \"AHCI\": spec.server.bios.attributes.setBootOrderEn in body should match '^$|^((\\w+|\\*)\\.(\\w+|\\*)\\.(\\w+|\\*)(\\-(\\w+|\\*))*((\\w+|\\*))*(\\:((\\w+|\\*)\\.(\\w+|\\*)\\.(\\w+|\\*)(\\-(\\w+|\\*))*((\\w+|\\*))*))*)(\\,(\\w+|\\*)\\.(\\w+|\\*)\\.(\\w+|\\*)(\\-(\\w+|\\*))*((\\w+|\\*))*(\\:((\\w+|\\*)\\.(\\w+|\\*)\\.(\\w+|\\*)(\\-(\\w+|\\*))*((\\w+|\\*))*))*)*$', spec.server.bios.attributes.serialPortAddress: Unsupported value: \"Com\": supported values: \"Com1\", \"Com2\", spec.server.bios.attributes.conTermType: Unsupported value: \"Vt\": supported values: \"Vt100Vt220\", \"Ansi\", spec.server.bios.attributes.tpmSecurity: Unsupported value: \"true\": supported values: \"On\", \"Off\", \"OnPbm\", \"OnNoPbm\", spec.server.bios.attributes.usbManagedPort: Unsupported value: \"true\": supported values: \"On\", \"Off\", spec.server.bios.attributes.pxeDev1Interface: Invalid value: \"NIC\": spec.server.bios.attributes.pxeDev1Interface in body should match '^NIC\\.[A-Za-z]+\\.[0-9]+\\-[0-9]+\\-[0-9]+'] dell@bmo-manager-1:~$ !b bmo create hardwareprofile -f hw_pf_bios.yaml Failed to create hardware profile in the 'metalweaver' namespace, reason: HardwareProfile.mw.dell.com \"r7525-profile\" is invalid: [spec.server.bios.attributes.numLock: Unsupported value: \"true\": supported values: \"On\", \"Off\", spec.server.bios.attributes.setBootOrderEn: Invalid value: \"AHCI\": spec.server.bios.attributes.setBootOrderEn in body should match '^$|^((\\w+|\\*)\\.(\\w+|\\*)\\.(\\w+|\\*)(\\-(\\w+|\\*))*((\\w+|\\*))*(\\:((\\w+|\\*)\\.(\\w+|\\*)\\.(\\w+|\\*)(\\-(\\w+|\\*))*((\\w+|\\*))*))*)(\\,(\\w+|\\*)\\.(\\w+|\\*)\\.(\\w+|\\*)(\\-(\\w+|\\*))*((\\w+|\\*))*(\\:((\\w+|\\*)\\.(\\w+|\\*)\\.(\\w+|\\*)(\\-(\\w+|\\*))*((\\w+|\\*))*))*)*$', spec.server.bios.attributes.serialPortAddress: Unsupported value: \"Com\": supported values: \"Com1\", \"Com2\", spec.server.bios.attributes.conTermType: Unsupported value: \"Vt\": supported values: \"Vt100Vt220\", \"Ansi\", spec.server.bios.attributes.tpmSecurity: Unsupported value: \"true\": supported values: \"On\", \"Off\", \"OnPbm\", \"OnNoPbm\", spec.server.bios.attributes.usbManagedPort: Unsupported value: \"true\": supported values: \"On\", \"Off\", spec.server.bios.attributes.pxeDev1Interface: Invalid value: \"NIC\": spec.server.bios.attributes.pxeDev1Interface in body should match '^NIC\\.[A-Za-z]+\\.[0-9]+\\-[0-9]+\\-[0-9]+'] dell@bmo-manager-1:~$ !b bmo create hardwareprofile -f hw_pf_bios.yaml Failed to create hardware profile in the 'metalweaver' namespace, reason: HardwareProfile.mw.dell.com \"r7525-profile\" is invalid: [spec.server.bios.attributes.setBootOrderEn: Invalid value: \"AHCI\": spec.server.bios.attributes.setBootOrderEn in body should match '^$|^((\\w+|\\*)\\.(\\w+|\\*)\\.(\\w+|\\*)(\\-(\\w+|\\*))*((\\w+|\\*))*(\\:((\\w+|\\*)\\.(\\w+|\\*)\\.(\\w+|\\*)(\\-(\\w+|\\*))*((\\w+|\\*))*))*)(\\,(\\w+|\\*)\\.(\\w+|\\*)\\.(\\w+|\\*)(\\-(\\w+|\\*))*((\\w+|\\*))*(\\:((\\w+|\\*)\\.(\\w+|\\*)\\.(\\w+|\\*)(\\-(\\w+|\\*))*((\\w+|\\*))*))*)*$', spec.server.bios.attributes.serialPortAddress: Unsupported value: \"Com\": supported values: \"Com1\", \"Com2\", spec.server.bios.attributes.conTermType: Unsupported value: \"Vt\": supported values: \"Vt100Vt220\", \"Ansi\", spec.server.bios.attributes.tpmSecurity: Unsupported value: \"true\": supported values: \"On\", \"Off\", \"OnPbm\", \"OnNoPbm\", spec.server.bios.attributes.usbManagedPort: Unsupported value: \"true\": supported values: \"On\", \"Off\", spec.server.bios.attributes.pxeDev1Interface: Invalid value: \"NIC\": spec.server.bios.attributes.pxeDev1Interface in body should match '^NIC\\.[A-Za-z]+\\.[0-9]+\\-[0-9]+\\-[0-9]+', spec.server.bios.attributes.numLock: Unsupported value: \"true\": supported values: \"On\", \"Off\"] dell@bmo-manager-1:~$ !b bmo create hardwareprofile -f hw_pf_bios.yaml Failed to create hardware profile in the 'metalweaver' namespace, reason: HardwareProfile.mw.dell.com \"r7525-profile\" is invalid: [spec.server.bios.attributes.numLock: Unsupported value: \"true\": supported values: \"On\", \"Off\", spec.server.bios.attributes.setBootOrderEn: Invalid value: \"AHCI\": spec.server.bios.attributes.setBootOrderEn in body should match '^$|^((\\w+|\\*)\\.(\\w+|\\*)\\.(\\w+|\\*)(\\-(\\w+|\\*))*((\\w+|\\*))*(\\:((\\w+|\\*)\\.(\\w+|\\*)\\.(\\w+|\\*)(\\-(\\w+|\\*))*((\\w+|\\*))*))*)(\\,(\\w+|\\*)\\.(\\w+|\\*)\\.(\\w+|\\*)(\\-(\\w+|\\*))*((\\w+|\\*))*(\\:((\\w+|\\*)\\.(\\w+|\\*)\\.(\\w+|\\*)(\\-(\\w+|\\*))*((\\w+|\\*))*))*)*$', spec.server.bios.attributes.serialPortAddress: Unsupported value: \"Com\": supported values: \"Com1\", \"Com2\", spec.server.bios.attributes.conTermType: Unsupported value: \"Vt\": supported values: \"Vt100Vt220\", \"Ansi\", spec.server.bios.attributes.tpmSecurity: Unsupported value: \"true\": supported values: \"On\", \"Off\", \"OnPbm\", \"OnNoPbm\", spec.server.bios.attributes.usbManagedPort: Unsupported value: \"true\": supported values: \"On\", \"Off\", spec.server.bios.attributes.pxeDev1Interface: Invalid value: \"NIC\": spec.server.bios.attributes.pxeDev1Interface in body should match '^NIC\\.[A-Za-z]+\\.[0-9]+\\-[0-9]+\\-[0-9]+'] dell@bmo-manager-1:~$ !b bmo create hardwareprofile -f hw_pf_bios.yaml Failed to create hardware profile in the 'metalweaver' namespace, reason: HardwareProfile.mw.dell.com \"r7525-profile\" is invalid: [spec.server.bios.attributes.tpmSecurity: Unsupported value: \"true\": supported values: \"On\", \"Off\", \"OnPbm\", \"OnNoPbm\", spec.server.bios.attributes.usbManagedPort: Unsupported value: \"true\": supported values: \"On\", \"Off\", spec.server.bios.attributes.pxeDev1Interface: Invalid value: \"NIC\": spec.server.bios.attributes.pxeDev1Interface in body should match '^NIC\\.[A-Za-z]+\\.[0-9]+\\-[0-9]+\\-[0-9]+', spec.server.bios.attributes.numLock: Unsupported value: \"true\": supported values: \"On\", \"Off\", spec.server.bios.attributes.setBootOrderEn: Invalid value: \"AHCI\": spec.server.bios.attributes.setBootOrderEn in body should match '^$|^((\\w+|\\*)\\.(\\w+|\\*)\\.(\\w+|\\*)(\\-(\\w+|\\*))*((\\w+|\\*))*(\\:((\\w+|\\*)\\.(\\w+|\\*)\\.(\\w+|\\*)(\\-(\\w+|\\*))*((\\w+|\\*))*))*)(\\,(\\w+|\\*)\\.(\\w+|\\*)\\.(\\w+|\\*)(\\-(\\w+|\\*))*((\\w+|\\*))*(\\:((\\w+|\\*)\\.(\\w+|\\*)\\.(\\w+|\\*)(\\-(\\w+|\\*))*((\\w+|\\*))*))*)*$', spec.server.bios.attributes.serialPortAddress: Unsupported value: \"Com\": supported values: \"Com1\", \"Com2\", spec.server.bios.attributes.conTermType: Unsupported value: \"Vt\": supported values: \"Vt100Vt220\", \"Ansi\"]","title":"Command Output"},{"location":"Testing%20Bare%20Metal%20Orchestrator/bug_report/#config-file","text":"apiVersion: mw.dell.com/v1 kind: HardwareProfile metadata: name: r7525-profile labels: site: gc spec: # Add fields here apply: false preview: true server: bios: attributes: LogicalProc: Enabled ProcVirtualization: Enabled KernelDmaProtection: Disabled L1StreamHwPrefetcher: Enabled L2StreamHwPrefetcher: Enabled MadtCoreEnumeration: Linear CcxAsNumaDomain: Enabled TransparentSme: Disabled ProcConfigTdp: Maximum ProcX2Apic: Enabled ProcCcds: All CcdCores: All ControlledTurbo: Disabled OptimizerMode: Auto EmbSata: AhciMode SecurityFreezeLock: Enabled WriteCache: Disabled BiosNvmeDriver: DellQualifiedDrives BootMode: Uefi BootSeqRetry: Enabled GenericUsbBoot: Disabled HddPlaceholder: Disabled SysPrepClean: None SetBootOrderEn: AHCI pxeDev1Interface: NIC pxeDev1Protocol: IPv pxeDev1VlanEnDis: Disabled usbPorts: AllOn usbManagedPort: On IntegratedRaid: Enabled IntegratedNetwork1: Enabled EmbNic1Nic2: Enabled EmbVideo: Enabled PciePreferredIoBus: Disabled PcieEnhancedPreferredIo: Disabled SriovGlobalEnable: Disabled OsWatchdogTimer: Disabled DellAutoDiscovery: PlatformDefault SerialComm: OnNoConRedir serialPortAddress: Com ConTermType: Vt RedirAfterBoot: Enabled SysProfile: PerfOptimized PasswordStatus: Unlocked tpmSecurity: On Tpm2Hierarchy: Enabled PwrButton: Enabled AcPwrRcvry: Last AcPwrRcvryDelay: Immediate UefiVariableAccess: Standard SecureBoot: Disabled SecureBootPolicy: Standard SecureBootMode: DeployedMode TpmPpiBypassProvision: Disabled TpmPpiBypassClear: Disabled Tpm2Algorithm: SHA RedundantOsLocation: None MemTest: Disabled DramRefreshDelay: Minimum MemoryInterleaving: Auto CorrEccSmi: Enabled OppSrefEn: Disabled CECriticalSEL: Disabled PPROnUCE: Enabled numLock: On ErrPrompt: Enabled ForceInt10: Disabled DellWyseP25BIOSAccess: Enabled selectors: profileName: r7525","title":"Config file"},{"location":"Testing%20Intel%20x520%20on%20RHEL%206/","text":"Testing Intel x520 on RHEL 6 RHEL Release [root@r440 ~]# cat /etc/*-release LSB_VERSION=base-4.0-amd64:base-4.0-noarch:core-4.0-amd64:core-4.0-noarch:graphics-4.0-amd64:graphics-4.0-noarch:printing-4.0-amd64:printing-4.0-noarch Red Hat Enterprise Linux Server release 6.10 (Santiago) Red Hat Enterprise Linux Server release 6.10 (Santiago) Server/Card Model # Server Dell R440 # Card Model [root@r440 ~]# lspci | grep Network 3b:00.0 Ethernet controller: Intel Corporation 82599ES 10-Gigabit SFI/SFP+ Network Connection (rev 01) 3b:00.1 Ethernet controller: Intel Corporation 82599ES 10-Gigabit SFI/SFP+ Network Connection (rev 01) Note: As shown in the product documentation is the name of the controller for the Intel x520. Server Inventory I have attached the server inventory so that it can be used for comparison. Note: Because I did not use an x520 that came with the box or flash it with idrac compatible drivers it does not appear under network devices, but it does appear under PCI devices as expected. SFPs Used Testing Basic Connectivity On initial install of RHEL 6.10 both the 1Gb/s ethernet SFP and the 10Gb/s fiber SFP were detected without issue and pulled DHCP addresses as expected: 4: p2p1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP qlen 1000 link/ether 90:e2:ba:8b:88:14 brd ff:ff:ff:ff:ff:ff inet 192.168.1.214/24 brd 192.168.1.255 scope global p2p1 inet6 2601:152:4100:212f:92e2:baff:fe8b:8814/64 scope global dynamic valid_lft 86393sec preferred_lft 14393sec inet6 fe80::92e2:baff:fe8b:8814/64 scope link valid_lft forever preferred_lft forever 5: p2p2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP qlen 1000 link/ether 90:e2:ba:8b:88:15 brd ff:ff:ff:ff:ff:ff inet 192.168.1.242/24 brd 192.168.1.255 scope global p2p2 inet6 2601:152:4100:212f:92e2:baff:fe8b:8815/64 scope global dynamic valid_lft 86393sec preferred_lft 14393sec inet6 fe80::92e2:baff:fe8b:8815/64 scope link valid_lft forever preferred_lft forever They both operated without modification at the correct speed: [root@r440 ~]# ethtool p2p1 | grep -i speed && ethtool p2p2 | grep -i speed Speed: 10000Mb/s Speed: 1000Mb/s My initial firmware version was: [root@r440 ~]# ethtool -i p2p1 driver: ixgbe version: 4.2.1-k firmware-version: 0x61c10001 bus-info: 0000:3b:00.0 supports-statistics: yes supports-test: yes supports-eeprom-access: yes supports-register-dump: yes supports-priv-flags: no Lights operated as expected: I went to Dell's support website and pulled the latest driver for the R440 / x520 which was released 30 November 2018. When I ran the script I was given additional version info: Update Package 18.08.200 (BLD_311) Copyright (c) 2003 Dell, Inc. All Rights Reserved. Release Title: Intel NIC Family Version 18.8.0 Firmware for I350, I354, X520, X540, and X550 adapters, 18.8.9, A00 Release Date: October 05, 2018 However this got me This Update Package is not compatible with your system configuration. . I didn't investigate why. I took the card out of another box and added it to this one so I wasn't overly surprised. I decided to pull directly from Intel. The latest driver I could find was 5.5.5 available here . Detailed Description Overview This is the most current release of the ixgbe driver for Linux*, which supports kernel versions 2.6.18 up through 4.20. It also has been tested on the following distributions: RHEL* 6.10 RHEL 7.6 SLES* 12SP4 SLES 15 Ubuntu* 18.04 Changes in this release: Added support for 4.20 kernel version Added support for SLES 12SP4 Added support for RHEL 7.6 I ran: make install rmmod ixgbe && insmod /root/Downloads/ixgbe-5.5.5/src/ixgbe.ko to load the driver. I did not see any problems with a speed drop: [root@r440 ~]# ethtool p2p1 | grep -i speed && ethtool p2p2 | grep -i speed Speed: 10000Mb/s Speed: 1000Mb/s Version confirmation: [root@r440 ~]# ethtool -i p2p1 driver: ixgbe version: 5.5.5 firmware-version: 0x61c10001, 255.65535.255 bus-info: 0000:3b:00.0 supports-statistics: yes supports-test: yes supports-eeprom-access: yes supports-register-dump: yes supports-priv-flags: yes I confirmed the lights continued to work as expected.","title":"Testing Intel x520 on RHEL 6"},{"location":"Testing%20Intel%20x520%20on%20RHEL%206/#testing-intel-x520-on-rhel-6","text":"","title":"Testing Intel x520 on RHEL 6"},{"location":"Testing%20Intel%20x520%20on%20RHEL%206/#rhel-release","text":"[root@r440 ~]# cat /etc/*-release LSB_VERSION=base-4.0-amd64:base-4.0-noarch:core-4.0-amd64:core-4.0-noarch:graphics-4.0-amd64:graphics-4.0-noarch:printing-4.0-amd64:printing-4.0-noarch Red Hat Enterprise Linux Server release 6.10 (Santiago) Red Hat Enterprise Linux Server release 6.10 (Santiago)","title":"RHEL Release"},{"location":"Testing%20Intel%20x520%20on%20RHEL%206/#servercard-model","text":"# Server Dell R440 # Card Model [root@r440 ~]# lspci | grep Network 3b:00.0 Ethernet controller: Intel Corporation 82599ES 10-Gigabit SFI/SFP+ Network Connection (rev 01) 3b:00.1 Ethernet controller: Intel Corporation 82599ES 10-Gigabit SFI/SFP+ Network Connection (rev 01) Note: As shown in the product documentation is the name of the controller for the Intel x520.","title":"Server/Card Model"},{"location":"Testing%20Intel%20x520%20on%20RHEL%206/#server-inventory","text":"I have attached the server inventory so that it can be used for comparison. Note: Because I did not use an x520 that came with the box or flash it with idrac compatible drivers it does not appear under network devices, but it does appear under PCI devices as expected.","title":"Server Inventory"},{"location":"Testing%20Intel%20x520%20on%20RHEL%206/#sfps-used","text":"","title":"SFPs Used"},{"location":"Testing%20Intel%20x520%20on%20RHEL%206/#testing","text":"","title":"Testing"},{"location":"Testing%20Intel%20x520%20on%20RHEL%206/#basic-connectivity","text":"On initial install of RHEL 6.10 both the 1Gb/s ethernet SFP and the 10Gb/s fiber SFP were detected without issue and pulled DHCP addresses as expected: 4: p2p1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP qlen 1000 link/ether 90:e2:ba:8b:88:14 brd ff:ff:ff:ff:ff:ff inet 192.168.1.214/24 brd 192.168.1.255 scope global p2p1 inet6 2601:152:4100:212f:92e2:baff:fe8b:8814/64 scope global dynamic valid_lft 86393sec preferred_lft 14393sec inet6 fe80::92e2:baff:fe8b:8814/64 scope link valid_lft forever preferred_lft forever 5: p2p2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP qlen 1000 link/ether 90:e2:ba:8b:88:15 brd ff:ff:ff:ff:ff:ff inet 192.168.1.242/24 brd 192.168.1.255 scope global p2p2 inet6 2601:152:4100:212f:92e2:baff:fe8b:8815/64 scope global dynamic valid_lft 86393sec preferred_lft 14393sec inet6 fe80::92e2:baff:fe8b:8815/64 scope link valid_lft forever preferred_lft forever They both operated without modification at the correct speed: [root@r440 ~]# ethtool p2p1 | grep -i speed && ethtool p2p2 | grep -i speed Speed: 10000Mb/s Speed: 1000Mb/s My initial firmware version was: [root@r440 ~]# ethtool -i p2p1 driver: ixgbe version: 4.2.1-k firmware-version: 0x61c10001 bus-info: 0000:3b:00.0 supports-statistics: yes supports-test: yes supports-eeprom-access: yes supports-register-dump: yes supports-priv-flags: no Lights operated as expected: I went to Dell's support website and pulled the latest driver for the R440 / x520 which was released 30 November 2018. When I ran the script I was given additional version info: Update Package 18.08.200 (BLD_311) Copyright (c) 2003 Dell, Inc. All Rights Reserved. Release Title: Intel NIC Family Version 18.8.0 Firmware for I350, I354, X520, X540, and X550 adapters, 18.8.9, A00 Release Date: October 05, 2018 However this got me This Update Package is not compatible with your system configuration. . I didn't investigate why. I took the card out of another box and added it to this one so I wasn't overly surprised. I decided to pull directly from Intel. The latest driver I could find was 5.5.5 available here . Detailed Description Overview This is the most current release of the ixgbe driver for Linux*, which supports kernel versions 2.6.18 up through 4.20. It also has been tested on the following distributions: RHEL* 6.10 RHEL 7.6 SLES* 12SP4 SLES 15 Ubuntu* 18.04 Changes in this release: Added support for 4.20 kernel version Added support for SLES 12SP4 Added support for RHEL 7.6 I ran: make install rmmod ixgbe && insmod /root/Downloads/ixgbe-5.5.5/src/ixgbe.ko to load the driver. I did not see any problems with a speed drop: [root@r440 ~]# ethtool p2p1 | grep -i speed && ethtool p2p2 | grep -i speed Speed: 10000Mb/s Speed: 1000Mb/s Version confirmation: [root@r440 ~]# ethtool -i p2p1 driver: ixgbe version: 5.5.5 firmware-version: 0x61c10001, 255.65535.255 bus-info: 0000:3b:00.0 supports-statistics: yes supports-test: yes supports-eeprom-access: yes supports-register-dump: yes supports-priv-flags: yes I confirmed the lights continued to work as expected.","title":"Basic Connectivity"},{"location":"Understanding%20Memory/","text":"Understanding Memory NOTE: Probably best to just read the Reddit post at the bottom. Understanding Memory What is a RAM channel? Analogy What is a DIMM (beyond the obvious)? What is a memory rank? What is DRAM? What is a chip select? Back to Memory Ranks Performance of multiple rank modules What is ECC Memory? What is Registered Memory? Why does the buffer allow for more total addressable memory? R-DIMMs LR-DIMMs What is a data line? Why does the data buffer matter? RAM and it's relation to CPU Speed More on Channels Best Memory for Different Circumstances DRAM Channel vs Rank vs Bank What is a RAM channel? From RAM Channels Guide: The What, and The How To be clear, these memory channels are actual wires that exist on/in the motherboard. Though RAM kits may call their arrangements \"channels,\" the actual number of channels and the number of RAM sticks are independent of each other; any mention of channel count on a RAM kit\u2019s product/specification page is just an informal, technically-incorrect way of referring to how many sticks of RAM there are in the kit. In addition, the number of RAM slots on a motherboard is independent of the number of memory channels. A channel needs only one stick to be used, and any more than that doesn\u2019t necessarily stop things from working. In addition, CPUs also support a certain maximum amount of memory channels. You don\u2019t really need to worry about this, as every CPU will handle the amount of memory channels available on their supporting motherboards. There are only two notable exceptions: Intel\u2019s i5-7640X and i7-7740X, which are both LGA 2066 CPUs, and very odd purchases anyway. Analogy Imagine a manufacturer of products: Let\u2019s say this manufacturer (your CPU), with potentially many factories (cores) in need of materials, makes orders for materials from only one supplier (memory channel). Even if the supplier has a whole lot of materials (capacity / stored data), and may run multiple warehouses (RAM sticks) of their own, it has a limited capacity for making shipments, and so can\u2019t handle multiple shipments to the manufacturer at once. There may be multiple shipments ready to go, but they can\u2019t actually start shipping until the current shipment is done. A single-channel supplier warehouse attempting to serve a quad-factory manufacturer with one truck The problem is, this manufacturer can often use materials faster than their supplier can ship them, and the delay from waiting on the supplier\u2019s logistics system for consecutive orders can slow things down. Especially when this manufacturer\u2019s factories are being heavily loaded with orders of their own from vendors and customers (your other components) while relying on materials orders, the supplier can pose a problem. So, the manufacturer contracts with a second supplier in addition to the first. Now, the manufacturer does something efficient: They alternate orders between the two suppliers. This way, the manufacturer can have two simultaneous shipments coming their way, and they suddenly find that waiting on consecutive orders to be shipped is now significantly less of an issue, since their effective capacity for getting shipments has been doubled. This same idea can extend even further across more suppliers. Really, how much the number of suppliers the manufacturer uses actually matters all depends on: how quickly materials are being used, how many factories they have (since each might come in need of materials at any given moment), how busy the manufacturer or specific factories are, and how quickly the suppliers themselves can send shipments to the manufacturer. Most of the time, this isn\u2019t a big deal, but when things line up well or poorly, the number of suppliers (i.e. memory channels) can make a notable difference. What is a DIMM (beyond the obvious)? From What is LR-DIMM , LRDIMM Memory ? ( Load-Reduce DIMM) DIMM stands for Dual Inline Memory Module. It is the RAM memory we found in our desktop computer. It consists of a few black chips (IC) on a small PCB. It stores our file and data temporally when we turn on our computer. \"Dual Inline\" refers to pins on both side of the module. We generally call them \"gold fingers\". What is a memory rank? What is DRAM? From Dynamic random-access memory and MOSFET Dynamic random-access memory (DRAM) is a type of random access semiconductor memory that stores each bit of data in a memory cell consisting of a tiny capacitor and a transistor, both typically based on metal-oxide-semiconductor (MOS) technology. After this I Googled metal\u2013oxide\u2013semiconductor field-effect transistor and basically the gist of what I read is that it's just the type of transistor used to store the data. The way they fabricate them is by oxidizing silicon. Beyond that I hit the \"I believe button\". What is a chip select? From Chip select Chip select (CS) or slave select (SS) is the name of a control line in digital electronics used to select one (or a set) of integrated circuits (commonly called \"chips\") out of several connected to the same computer bus, usually utilizing the three-state logic. When an engineer needs to connect several devices to the same set of input wires (e.g., a computer bus), but retain the ability to send and receive data or commands to each device independently of the others on the bus, they can use a chip select. The chip select is a command pin on many integrated circuits which connects the I/O pins on the device to the internal circuitry of that device. Back to Memory Ranks From Memory rank A memory rank is a set of DRAM chips connected to the same chip select, which are therefore accessed simultaneously. In practice all DRAM chips share all of the other command and control signals, and only the chip select pins for each rank are separate (the data pins are shared across ranks). On a DDR, DDR2, or DDR3 memory module, each rank has a 64-bit-wide data bus (72 bits wide on DIMMs that support ECC). The number of physical DRAMs depends on their individual widths. For example, a rank of \u00d78 (8-bit wide) DRAMs would consist of eight physical chips (nine if ECC is supported), but a rank of \u00d74 (4-bit wide) DRAMs would consist of 16 physical chips (18, if ECC is supported). Multiple ranks can coexist on a single DIMM, and modern DIMMs can consist of one rank (single rank), two ranks (dual rank), four ranks (quad rank), or eight ranks (octal rank). Increasing the number of ranks per DIMM is mainly intended to increase the memory density per channel. Too many ranks in the channel can cause excessive loading and decrease the speed of the channel. Also some memory controllers have a maximum supported number of ranks. DRAM load on the command/address (CA) bus can be reduced by using registered memory. Performance of multiple rank modules From Memory rank There are several effects to consider regarding memory performance in multi-rank configurations: Multi-rank modules allow several open DRAM pages (row) in each rank (typically eight pages per rank). This increases the possibility of getting a hit on an already open row address. The performance gain that can be achieved is highly dependent on the application and the memory controller's ability to take advantage of open pages. Multi-rank modules have higher loading on the data bus (and on unbuffered DIMMs the CA bus as well). Therefore if more than dual rank DIMMS are connected in one channel, the speed might be reduced. Subject to some limitations, ranks can be accessed independently, although not simultaneously as the data lines are still shared between ranks on a channel. For example, the controller can send write data to one rank while it awaits read data previously selected from another rank. While the write data is consumed from the data bus, the other rank could perform read-related operations such as the activation of a row or internal transfer of the data to the output drivers. Once the CA bus is free from noise from the previous read, the DRAM can drive out the read data. Controlling interleaved accesses like so is done by the memory controller. There is a small performance reduction for multi-rank systems as they require some pipeline stalls between accessing different ranks. For two ranks on a single DIMM it might not even be required, but this parameter is often programmed independently of the rank location in the system (if on the same DIMM or different DIMMs). Nevertheless, this pipeline stall is negligible compared to the aforementioned effects. Grant Note : A pipeline still is when execution stops because some hazard has to be resolved. A hazard in computer architecture is when there would be a conflict due to concurrent execution for whatever reason. Basically you're waiting in limbo for something else to complete before it is safe for you to continue. What is ECC Memory? From Computer Memory Issues Standard memory, also called non-parity memory, uses 8 bits to store an 8-bit byte. ECC memory (Error Correcting Code memory), sometimes called parity memory, uses 9 bits to store an 8-bit byte. The extra bit provided by ECC memory is used to store error detection and correction information. A non-parity memory module can neither detect nor correct errors. An ECC memory module can detect all multi-bit errors, correct all single-bit errors, and correct some multi-bit errors. Memory errors are so rare that most desktop systems use non-parity memory, which is less expensive and faster than ECC memory. In fact, most desktop chipsets do not support ECC memory. If you install ECC memory in such a system, it may not recognize the memory at all, but more likely it will simply treat the ECC memory as non-parity memory, ignoring the extra bit. ECC memory is occasionally used in desktop systems, but is much more common in servers and other large, critical systems. Because ECC modules contain additional memory chips, in the ratio of 9:8, they typically cost 10% to 15% more than similar non-parity modules. Also, because the circuitry on ECC modules that calculates and stores ECC values imposes some overhead, ECC modules are marginally slower than similar non-parity modules. What is Registered Memory? From Computer Memory Issues Unbuffered memory modules allow the memory controller to interact directly with the memory chips on the module. Registered memory (also called buffered memory) modules place an additional layer of circuitry between the memory controller and the memory chips. Registered memory is necessary in some environments, because all memory controllers have limitations on how many devices (individual memory chips) they can control, which in turn limits the maximum capacity of the memory modules they can use. When a memory controller interacts with an unbuffered memory module, it controls every memory chip directly. When a memory controller interacts with a registered memory module, it works only with the buffer circuitry; the actual memory chips are invisible to it. The sole advantage of registered memory is that it permits a system to use higher-capacity memory modules. (The highest capacity modules at any given time are often available only in registered form.) The disadvantages of registered memory are that it is considerably more expensive than unbuffered memory and noticeably slower because of the additional overhead incurred from using a buffer. Why does the buffer allow for more total addressable memory? From Registered Memory Before this can make sense you have to read from Parasitic capacitance Parasitic capacitance, or stray capacitance is an unavoidable and usually unwanted capacitance that exists between the parts of an electronic component or circuit simply because of their proximity to each other. When two electrical conductors at different voltages are close together, the electric field between them causes electric charge to be stored on them; this effect is parasitic capacitance. All actual circuit elements such as inductors, diodes, and transistors have internal capacitance, which can cause their behavior to depart from that of 'ideal' circuit elements. Additionally, there is always non-zero capacitance between any two conductors; this can be significant at higher frequencies with closely spaced conductors, such as wires or printed circuit board traces. Parasitic capacitance is a significant problem in high frequency circuits and is often the factor limiting the operating frequency and bandwidth of electronic components and circuits. More on Parasitic capacitance Wait a minute! You just cannot keep adding memory into your computer without any penalty. At 1333Mhz (1.3GHz), noise gets involved. It generally is something called the signal integrity or signal reflection issue. At a point, the accumulated noise in the system would render the system not operable. At high frequency, there is also something called \"loading factor\". Each memory chip (IC) has input capacitance that tends to suppress the high frequency signal. Generally, each chip has about 3 to 5 pf of input capacitance. The more chips on the module, the more accumulated capacitance will weaken the signal to an in-operable state. R-DIMMs Registered (Buffered) DIMMs (R-DIMMs) insert a buffer between the command/address bus pins on the DIMM and the memory chips proper. A high-density DIMM might have 36 memory chips (assuming four ranks and ECC), each of which must receive the memory address, and their combined input capacitance limits the speed at which the memory bus can operate. By amplifying the signal on the DIMM, this allows more chips to be connected to the memory bus. The cost is one additional clock cycle of memory latency required for the address to traverse the additional buffer. Early registered RAM modules were physically incompatible with unregistered RAM modules, but SDRAM DIMMs are interchangeable, and some motherboards support both types. LR-DIMMs Load Reduced DIMMs (LR-DIMMs) modules are similar, but add a buffer to the data lines as well. As a result, LRDIMM memory provides large overall maximum memory capacities, while avoiding the performance and power consumption problems of FB-DIMM memory. What is a data line? From How Memory Works The processor specifies which memory cell it wants to use by giving a binary address. Some simple Boolean logic, address decoding, then converts the address into a row and column select and the memory still works and its efficient. Notice that at the moment we are looking at a single memory chip and this arrangement can only store a single bit. If you want to store a byte you need eight such chips, one for each bit, and eight data input and eight data output lines. The eight data lines are grouped together into a data input bus and a data output \"bus\" \u2013 a bus is just a group of wires. Early computers really did have separate buses for input and output but today\u2019s machines have a single unified data bus that can be used for either input or output. If you want more storage than a bank of eight chips can provide then you have to add another bank of eight chips and some additional address decoding logic to select the correct bank. The address lines that come from the processor are generally referred to as an address bus and now we have the fundamental architecture of a simple but modern computer. Why does the data buffer matter? From What is LR-DIMM, LRDIMM Memory? (Load-Reduce DIMM) and Basics of LRDIMM You just cannot keep adding memory into your computer without any penalty. At 1333Mhz (1.3GHz), noise gets involved. It generally is something called the signal integrity or signal reflection issue. At a point, the accumulated noise in the system would render the system not operable. Figure 1: LRDIMM conceptual drawing featuring one memory buffer on the front side of the memory module and multiple ranks of DRAM mounted on both front and back sides of the memory module. The memory buffer re-drives all of the data, command, address and clock signals from the host memory controller and provides them to the multiple ranks of DRAM. The memory buffer isolates the DRAM from the host, reducing the electrical load on all interfaces. Reducing the electrical loading in this manner allows a system to operate at a higher speed for a given memory capacity, or to support a higher memory capacity at a given speed. On existing RDIMM technology, the data bus connects directly to the multiple ranks of DRAM, increasing electrical load and limiting system speed as the desired server capacity increases. Figure 2 shows the system-level block diagram for RDIMM and LRDIMM. To maximize module and system-level memory capacity and overcome limitations on the number of chip selects per channel, the LRDIMM supports Rank Multiplication, where multiple physical DRAM ranks appear to the host controller as a single logical rank of a larger size. This is done by utilizing additional row address bits during the Activate command as sub-rank select bits. One of the primary advantages of the LRDIMM is the ability to dramatically increase total system memory capacity without sacrificing speed. By electrically isolating the DRAM from the data bus, additional ranks of DRAM can be added to each DIMM while maintaining signal integrity, and additional DIMMs can be installed on each system memory channel. LRDIMM capacities up to 32GB are possible today with 4Rx4 modules using 4 Gb, DDP (dual-die package) DRAM. Since each LRDIMM presents a single electrical load to the host, more DIMMs can be installed per channel as well. RAM and it's relation to CPU Speed From Understanding CPU Limitations with Memory There are more in the weeds details, but it really is as simple as the clock speed of the CPU's memory controller has to be as good as or better than the target RAM or the RAM will downclock to meet the CPU's speed. More on Channels See this Reddit thread Ever since SDRAM first hit the shelves, main memory inside PC's has been a 64 bit wide interface. That means that 64 bits are sent in each transaction. DDR just meant that two lots of 64 bit transactions are sent in one clock. Obviously you see that DDR would increase the available memory bandwidth (speed), because twice as much data is being sent per clock. Well there is another way to send more data per clock, that is to send more than 64 bits in one go. You could either, widen the memory bus, to say 128 bits (GPU's use 384/512bit interfaces, but it's more complicated than that in truth) or you could add another interface. So when we go from single 'channel' to dual channel, what we are doing is adding another 64 bit wide interface (and another 2 when going from dual to quad). Now, usually you have 4 slots on your mobo for RAM. So you either have 4 independent slots connected to their own memory channel, or you have 2 pairs of slots, each pair connected to it's own memory channel. Lets take one pair. This pair has 240 pins (in desktop DDR3) coming off the CPU, connecting to one slot, then the next. This memory bus has the signalling to talk to either slot, a read request is sent out and each chip on that bus determines if it is indeed the chip that contains the requested data (to put it simply) and once it has found it, sends the data back. It can take a little while to do all this so many read and write requests can end up queued and/or in the pipeline. You add your second channel and you interleave your data across it, so bit one is the first bit on channel 1, bit 2 is the first bit on channel 2, bit 3 the second on channel 1 etc (likely to be done in chunks larger than single bits though) In the exact same way that RAID can give you much faster hard disk access (but certain access patterns not so much) adding extra memory channels allows you to read extra data. Double your memory channels, double the data you can read per second. Of course in reality nothing ever scales perfectly but you get the idea. The flip side of having these extra memory channels is that you need to populate them all, or you're wasting them. So a quad channel system will need 4 memory modules, if it has any less than that it is only using a number of memory channels equal to the number of modules you're using. This was a pretty good explanation, until you got to the part about bit interleaving. That is definitely not how it works. I know you corrected yourself and said that it probably works at a chunkier granularity, but I only noticed that after I'd already written all of this, so sorry. I'm going to get a little technical, so ignore this unless you want lots of details. DRAM systems are logically organized into (going from largest to smallest sub-division): channels, ranks, banks, rows, and columns. A channel is the physical connection between the memory controller (which is on the CPU die these days), and the DIMMs (the boards with DRAM chips on them). It's the wires, the aforementioned 64-bit data interface, plus all the control signals. A channel may have one or more DIMMs on it (for DDR3 at least, but DDR4 will require only a single DIMM per channel), and when there is a memory request (be it read or write) all of the data for that request comes from a single channel. There is no bit-interleaving across channels. All of the CPUs you and I deal with on a daily basis use 64 byte cache lines, so DRAM is accessed at the granularity of 64 byte blocks. So when there is a DRAM read, all 64 bytes come from a single channel. Furthermore, all 64 bytes come from a single rank within that channel. A rank is a collection of DRAM chips on a DIMM that work together to service a DRAM transaction. In the desktop systems we interact with every day, a rank is typically comprised of 8 DRAM chips. A DIMM may have more than 1 rank on it (the DIMMs you are familiar with probably have 8 DRAM chips on each side, so 2 ranks). This means that when your CPU sends a DRAM read request along a channel, 8 DRAM chips on the DIMM work together to fulfill that request. Remember that the data bus width is 64 bits, so each of these 8 DRAM chips is only responsible for transmitting 8 bits each per transfer. They transmit at the exact same time, in lock step with each other, and so from the CPU's perspective it seems like a single 64-bit data bus. After one DRAM transaction is completed on that channel, the next transaction might come from a different rank on the same channel (perhaps the other set of 8 DRAM chips on the same DIMM, or a rank on another DIMM on the same channel), or it could be the same rank again. Here is the important part: separate channels can work 100% independently, because they have absolutely 0 resources they have to manage and share between them, but separate ranks on the same channel must serialize their actions, because they all share the same data and control buses, and they must take turns using these resources. Depending on the access pattern you expect to be most common, you can layout data across your DRAM channels/ranks/banks/rows in various ways. One high performance strategy is to layout consecutive cache lines across channels (so address A is in channel 1, and address A+64 is in channel 2). This lets you fetch A+64 while you're still working on A. There are other data layout strategies for other expected access patterns. What I'm saying is that it is common for cache lines to be striped (interleaved) across channels, but not individual bits. If anyone wants, I can go into more detail about how banks, rows, and columns work, but I think channels and ranks are all that is needed to answer the OP's question. One last thing, about GPU memory buses. In graphics card specs, they always say that the GPU has a \"384-bit bus\", but this is misleading. It does have 384 total bits of GDDR5 data bus, but it is not logically organized as a single bus (like the 8 DRAM chips of a rank work together to logically form a 64-bit bus in DDR3). Instead, a GPU with a \"384-bit bus\" will actually have 12 independent 32-bit GDDR5 data buses that all act independently from one another. Source: Ph.D. computer architecture student who's studied DRAM organization for the last 4 years. Best Memory for Different Circumstances https://www.microway.com/hpc-tech-tips/ddr4-rdimm-lrdimm-performance-comparison/ DRAM Channel vs Rank vs Bank https://electronics.stackexchange.com/a/454717/279031","title":"Understanding Memory"},{"location":"Understanding%20Memory/#understanding-memory","text":"NOTE: Probably best to just read the Reddit post at the bottom. Understanding Memory What is a RAM channel? Analogy What is a DIMM (beyond the obvious)? What is a memory rank? What is DRAM? What is a chip select? Back to Memory Ranks Performance of multiple rank modules What is ECC Memory? What is Registered Memory? Why does the buffer allow for more total addressable memory? R-DIMMs LR-DIMMs What is a data line? Why does the data buffer matter? RAM and it's relation to CPU Speed More on Channels Best Memory for Different Circumstances DRAM Channel vs Rank vs Bank","title":"Understanding Memory"},{"location":"Understanding%20Memory/#what-is-a-ram-channel","text":"From RAM Channels Guide: The What, and The How To be clear, these memory channels are actual wires that exist on/in the motherboard. Though RAM kits may call their arrangements \"channels,\" the actual number of channels and the number of RAM sticks are independent of each other; any mention of channel count on a RAM kit\u2019s product/specification page is just an informal, technically-incorrect way of referring to how many sticks of RAM there are in the kit. In addition, the number of RAM slots on a motherboard is independent of the number of memory channels. A channel needs only one stick to be used, and any more than that doesn\u2019t necessarily stop things from working. In addition, CPUs also support a certain maximum amount of memory channels. You don\u2019t really need to worry about this, as every CPU will handle the amount of memory channels available on their supporting motherboards. There are only two notable exceptions: Intel\u2019s i5-7640X and i7-7740X, which are both LGA 2066 CPUs, and very odd purchases anyway.","title":"What is a RAM channel?"},{"location":"Understanding%20Memory/#analogy","text":"Imagine a manufacturer of products: Let\u2019s say this manufacturer (your CPU), with potentially many factories (cores) in need of materials, makes orders for materials from only one supplier (memory channel). Even if the supplier has a whole lot of materials (capacity / stored data), and may run multiple warehouses (RAM sticks) of their own, it has a limited capacity for making shipments, and so can\u2019t handle multiple shipments to the manufacturer at once. There may be multiple shipments ready to go, but they can\u2019t actually start shipping until the current shipment is done. A single-channel supplier warehouse attempting to serve a quad-factory manufacturer with one truck The problem is, this manufacturer can often use materials faster than their supplier can ship them, and the delay from waiting on the supplier\u2019s logistics system for consecutive orders can slow things down. Especially when this manufacturer\u2019s factories are being heavily loaded with orders of their own from vendors and customers (your other components) while relying on materials orders, the supplier can pose a problem. So, the manufacturer contracts with a second supplier in addition to the first. Now, the manufacturer does something efficient: They alternate orders between the two suppliers. This way, the manufacturer can have two simultaneous shipments coming their way, and they suddenly find that waiting on consecutive orders to be shipped is now significantly less of an issue, since their effective capacity for getting shipments has been doubled. This same idea can extend even further across more suppliers. Really, how much the number of suppliers the manufacturer uses actually matters all depends on: how quickly materials are being used, how many factories they have (since each might come in need of materials at any given moment), how busy the manufacturer or specific factories are, and how quickly the suppliers themselves can send shipments to the manufacturer. Most of the time, this isn\u2019t a big deal, but when things line up well or poorly, the number of suppliers (i.e. memory channels) can make a notable difference.","title":"Analogy"},{"location":"Understanding%20Memory/#what-is-a-dimm-beyond-the-obvious","text":"From What is LR-DIMM , LRDIMM Memory ? ( Load-Reduce DIMM) DIMM stands for Dual Inline Memory Module. It is the RAM memory we found in our desktop computer. It consists of a few black chips (IC) on a small PCB. It stores our file and data temporally when we turn on our computer. \"Dual Inline\" refers to pins on both side of the module. We generally call them \"gold fingers\".","title":"What is a DIMM (beyond the obvious)?"},{"location":"Understanding%20Memory/#what-is-a-memory-rank","text":"","title":"What is a memory rank?"},{"location":"Understanding%20Memory/#what-is-dram","text":"From Dynamic random-access memory and MOSFET Dynamic random-access memory (DRAM) is a type of random access semiconductor memory that stores each bit of data in a memory cell consisting of a tiny capacitor and a transistor, both typically based on metal-oxide-semiconductor (MOS) technology. After this I Googled metal\u2013oxide\u2013semiconductor field-effect transistor and basically the gist of what I read is that it's just the type of transistor used to store the data. The way they fabricate them is by oxidizing silicon. Beyond that I hit the \"I believe button\".","title":"What is DRAM?"},{"location":"Understanding%20Memory/#what-is-a-chip-select","text":"From Chip select Chip select (CS) or slave select (SS) is the name of a control line in digital electronics used to select one (or a set) of integrated circuits (commonly called \"chips\") out of several connected to the same computer bus, usually utilizing the three-state logic. When an engineer needs to connect several devices to the same set of input wires (e.g., a computer bus), but retain the ability to send and receive data or commands to each device independently of the others on the bus, they can use a chip select. The chip select is a command pin on many integrated circuits which connects the I/O pins on the device to the internal circuitry of that device.","title":"What is a chip select?"},{"location":"Understanding%20Memory/#back-to-memory-ranks","text":"From Memory rank A memory rank is a set of DRAM chips connected to the same chip select, which are therefore accessed simultaneously. In practice all DRAM chips share all of the other command and control signals, and only the chip select pins for each rank are separate (the data pins are shared across ranks). On a DDR, DDR2, or DDR3 memory module, each rank has a 64-bit-wide data bus (72 bits wide on DIMMs that support ECC). The number of physical DRAMs depends on their individual widths. For example, a rank of \u00d78 (8-bit wide) DRAMs would consist of eight physical chips (nine if ECC is supported), but a rank of \u00d74 (4-bit wide) DRAMs would consist of 16 physical chips (18, if ECC is supported). Multiple ranks can coexist on a single DIMM, and modern DIMMs can consist of one rank (single rank), two ranks (dual rank), four ranks (quad rank), or eight ranks (octal rank). Increasing the number of ranks per DIMM is mainly intended to increase the memory density per channel. Too many ranks in the channel can cause excessive loading and decrease the speed of the channel. Also some memory controllers have a maximum supported number of ranks. DRAM load on the command/address (CA) bus can be reduced by using registered memory.","title":"Back to Memory Ranks"},{"location":"Understanding%20Memory/#performance-of-multiple-rank-modules","text":"From Memory rank There are several effects to consider regarding memory performance in multi-rank configurations: Multi-rank modules allow several open DRAM pages (row) in each rank (typically eight pages per rank). This increases the possibility of getting a hit on an already open row address. The performance gain that can be achieved is highly dependent on the application and the memory controller's ability to take advantage of open pages. Multi-rank modules have higher loading on the data bus (and on unbuffered DIMMs the CA bus as well). Therefore if more than dual rank DIMMS are connected in one channel, the speed might be reduced. Subject to some limitations, ranks can be accessed independently, although not simultaneously as the data lines are still shared between ranks on a channel. For example, the controller can send write data to one rank while it awaits read data previously selected from another rank. While the write data is consumed from the data bus, the other rank could perform read-related operations such as the activation of a row or internal transfer of the data to the output drivers. Once the CA bus is free from noise from the previous read, the DRAM can drive out the read data. Controlling interleaved accesses like so is done by the memory controller. There is a small performance reduction for multi-rank systems as they require some pipeline stalls between accessing different ranks. For two ranks on a single DIMM it might not even be required, but this parameter is often programmed independently of the rank location in the system (if on the same DIMM or different DIMMs). Nevertheless, this pipeline stall is negligible compared to the aforementioned effects. Grant Note : A pipeline still is when execution stops because some hazard has to be resolved. A hazard in computer architecture is when there would be a conflict due to concurrent execution for whatever reason. Basically you're waiting in limbo for something else to complete before it is safe for you to continue.","title":"Performance of multiple rank modules"},{"location":"Understanding%20Memory/#what-is-ecc-memory","text":"From Computer Memory Issues Standard memory, also called non-parity memory, uses 8 bits to store an 8-bit byte. ECC memory (Error Correcting Code memory), sometimes called parity memory, uses 9 bits to store an 8-bit byte. The extra bit provided by ECC memory is used to store error detection and correction information. A non-parity memory module can neither detect nor correct errors. An ECC memory module can detect all multi-bit errors, correct all single-bit errors, and correct some multi-bit errors. Memory errors are so rare that most desktop systems use non-parity memory, which is less expensive and faster than ECC memory. In fact, most desktop chipsets do not support ECC memory. If you install ECC memory in such a system, it may not recognize the memory at all, but more likely it will simply treat the ECC memory as non-parity memory, ignoring the extra bit. ECC memory is occasionally used in desktop systems, but is much more common in servers and other large, critical systems. Because ECC modules contain additional memory chips, in the ratio of 9:8, they typically cost 10% to 15% more than similar non-parity modules. Also, because the circuitry on ECC modules that calculates and stores ECC values imposes some overhead, ECC modules are marginally slower than similar non-parity modules.","title":"What is ECC Memory?"},{"location":"Understanding%20Memory/#what-is-registered-memory","text":"From Computer Memory Issues Unbuffered memory modules allow the memory controller to interact directly with the memory chips on the module. Registered memory (also called buffered memory) modules place an additional layer of circuitry between the memory controller and the memory chips. Registered memory is necessary in some environments, because all memory controllers have limitations on how many devices (individual memory chips) they can control, which in turn limits the maximum capacity of the memory modules they can use. When a memory controller interacts with an unbuffered memory module, it controls every memory chip directly. When a memory controller interacts with a registered memory module, it works only with the buffer circuitry; the actual memory chips are invisible to it. The sole advantage of registered memory is that it permits a system to use higher-capacity memory modules. (The highest capacity modules at any given time are often available only in registered form.) The disadvantages of registered memory are that it is considerably more expensive than unbuffered memory and noticeably slower because of the additional overhead incurred from using a buffer.","title":"What is Registered Memory?"},{"location":"Understanding%20Memory/#why-does-the-buffer-allow-for-more-total-addressable-memory","text":"From Registered Memory Before this can make sense you have to read from Parasitic capacitance Parasitic capacitance, or stray capacitance is an unavoidable and usually unwanted capacitance that exists between the parts of an electronic component or circuit simply because of their proximity to each other. When two electrical conductors at different voltages are close together, the electric field between them causes electric charge to be stored on them; this effect is parasitic capacitance. All actual circuit elements such as inductors, diodes, and transistors have internal capacitance, which can cause their behavior to depart from that of 'ideal' circuit elements. Additionally, there is always non-zero capacitance between any two conductors; this can be significant at higher frequencies with closely spaced conductors, such as wires or printed circuit board traces. Parasitic capacitance is a significant problem in high frequency circuits and is often the factor limiting the operating frequency and bandwidth of electronic components and circuits. More on Parasitic capacitance Wait a minute! You just cannot keep adding memory into your computer without any penalty. At 1333Mhz (1.3GHz), noise gets involved. It generally is something called the signal integrity or signal reflection issue. At a point, the accumulated noise in the system would render the system not operable. At high frequency, there is also something called \"loading factor\". Each memory chip (IC) has input capacitance that tends to suppress the high frequency signal. Generally, each chip has about 3 to 5 pf of input capacitance. The more chips on the module, the more accumulated capacitance will weaken the signal to an in-operable state.","title":"Why does the buffer allow for more total addressable memory?"},{"location":"Understanding%20Memory/#r-dimms","text":"Registered (Buffered) DIMMs (R-DIMMs) insert a buffer between the command/address bus pins on the DIMM and the memory chips proper. A high-density DIMM might have 36 memory chips (assuming four ranks and ECC), each of which must receive the memory address, and their combined input capacitance limits the speed at which the memory bus can operate. By amplifying the signal on the DIMM, this allows more chips to be connected to the memory bus. The cost is one additional clock cycle of memory latency required for the address to traverse the additional buffer. Early registered RAM modules were physically incompatible with unregistered RAM modules, but SDRAM DIMMs are interchangeable, and some motherboards support both types.","title":"R-DIMMs"},{"location":"Understanding%20Memory/#lr-dimms","text":"Load Reduced DIMMs (LR-DIMMs) modules are similar, but add a buffer to the data lines as well. As a result, LRDIMM memory provides large overall maximum memory capacities, while avoiding the performance and power consumption problems of FB-DIMM memory.","title":"LR-DIMMs"},{"location":"Understanding%20Memory/#what-is-a-data-line","text":"From How Memory Works The processor specifies which memory cell it wants to use by giving a binary address. Some simple Boolean logic, address decoding, then converts the address into a row and column select and the memory still works and its efficient. Notice that at the moment we are looking at a single memory chip and this arrangement can only store a single bit. If you want to store a byte you need eight such chips, one for each bit, and eight data input and eight data output lines. The eight data lines are grouped together into a data input bus and a data output \"bus\" \u2013 a bus is just a group of wires. Early computers really did have separate buses for input and output but today\u2019s machines have a single unified data bus that can be used for either input or output. If you want more storage than a bank of eight chips can provide then you have to add another bank of eight chips and some additional address decoding logic to select the correct bank. The address lines that come from the processor are generally referred to as an address bus and now we have the fundamental architecture of a simple but modern computer.","title":"What is a data line?"},{"location":"Understanding%20Memory/#why-does-the-data-buffer-matter","text":"From What is LR-DIMM, LRDIMM Memory? (Load-Reduce DIMM) and Basics of LRDIMM You just cannot keep adding memory into your computer without any penalty. At 1333Mhz (1.3GHz), noise gets involved. It generally is something called the signal integrity or signal reflection issue. At a point, the accumulated noise in the system would render the system not operable. Figure 1: LRDIMM conceptual drawing featuring one memory buffer on the front side of the memory module and multiple ranks of DRAM mounted on both front and back sides of the memory module. The memory buffer re-drives all of the data, command, address and clock signals from the host memory controller and provides them to the multiple ranks of DRAM. The memory buffer isolates the DRAM from the host, reducing the electrical load on all interfaces. Reducing the electrical loading in this manner allows a system to operate at a higher speed for a given memory capacity, or to support a higher memory capacity at a given speed. On existing RDIMM technology, the data bus connects directly to the multiple ranks of DRAM, increasing electrical load and limiting system speed as the desired server capacity increases. Figure 2 shows the system-level block diagram for RDIMM and LRDIMM. To maximize module and system-level memory capacity and overcome limitations on the number of chip selects per channel, the LRDIMM supports Rank Multiplication, where multiple physical DRAM ranks appear to the host controller as a single logical rank of a larger size. This is done by utilizing additional row address bits during the Activate command as sub-rank select bits. One of the primary advantages of the LRDIMM is the ability to dramatically increase total system memory capacity without sacrificing speed. By electrically isolating the DRAM from the data bus, additional ranks of DRAM can be added to each DIMM while maintaining signal integrity, and additional DIMMs can be installed on each system memory channel. LRDIMM capacities up to 32GB are possible today with 4Rx4 modules using 4 Gb, DDP (dual-die package) DRAM. Since each LRDIMM presents a single electrical load to the host, more DIMMs can be installed per channel as well.","title":"Why does the data buffer matter?"},{"location":"Understanding%20Memory/#ram-and-its-relation-to-cpu-speed","text":"From Understanding CPU Limitations with Memory There are more in the weeds details, but it really is as simple as the clock speed of the CPU's memory controller has to be as good as or better than the target RAM or the RAM will downclock to meet the CPU's speed.","title":"RAM and it's relation to CPU Speed"},{"location":"Understanding%20Memory/#more-on-channels","text":"See this Reddit thread Ever since SDRAM first hit the shelves, main memory inside PC's has been a 64 bit wide interface. That means that 64 bits are sent in each transaction. DDR just meant that two lots of 64 bit transactions are sent in one clock. Obviously you see that DDR would increase the available memory bandwidth (speed), because twice as much data is being sent per clock. Well there is another way to send more data per clock, that is to send more than 64 bits in one go. You could either, widen the memory bus, to say 128 bits (GPU's use 384/512bit interfaces, but it's more complicated than that in truth) or you could add another interface. So when we go from single 'channel' to dual channel, what we are doing is adding another 64 bit wide interface (and another 2 when going from dual to quad). Now, usually you have 4 slots on your mobo for RAM. So you either have 4 independent slots connected to their own memory channel, or you have 2 pairs of slots, each pair connected to it's own memory channel. Lets take one pair. This pair has 240 pins (in desktop DDR3) coming off the CPU, connecting to one slot, then the next. This memory bus has the signalling to talk to either slot, a read request is sent out and each chip on that bus determines if it is indeed the chip that contains the requested data (to put it simply) and once it has found it, sends the data back. It can take a little while to do all this so many read and write requests can end up queued and/or in the pipeline. You add your second channel and you interleave your data across it, so bit one is the first bit on channel 1, bit 2 is the first bit on channel 2, bit 3 the second on channel 1 etc (likely to be done in chunks larger than single bits though) In the exact same way that RAID can give you much faster hard disk access (but certain access patterns not so much) adding extra memory channels allows you to read extra data. Double your memory channels, double the data you can read per second. Of course in reality nothing ever scales perfectly but you get the idea. The flip side of having these extra memory channels is that you need to populate them all, or you're wasting them. So a quad channel system will need 4 memory modules, if it has any less than that it is only using a number of memory channels equal to the number of modules you're using. This was a pretty good explanation, until you got to the part about bit interleaving. That is definitely not how it works. I know you corrected yourself and said that it probably works at a chunkier granularity, but I only noticed that after I'd already written all of this, so sorry. I'm going to get a little technical, so ignore this unless you want lots of details. DRAM systems are logically organized into (going from largest to smallest sub-division): channels, ranks, banks, rows, and columns. A channel is the physical connection between the memory controller (which is on the CPU die these days), and the DIMMs (the boards with DRAM chips on them). It's the wires, the aforementioned 64-bit data interface, plus all the control signals. A channel may have one or more DIMMs on it (for DDR3 at least, but DDR4 will require only a single DIMM per channel), and when there is a memory request (be it read or write) all of the data for that request comes from a single channel. There is no bit-interleaving across channels. All of the CPUs you and I deal with on a daily basis use 64 byte cache lines, so DRAM is accessed at the granularity of 64 byte blocks. So when there is a DRAM read, all 64 bytes come from a single channel. Furthermore, all 64 bytes come from a single rank within that channel. A rank is a collection of DRAM chips on a DIMM that work together to service a DRAM transaction. In the desktop systems we interact with every day, a rank is typically comprised of 8 DRAM chips. A DIMM may have more than 1 rank on it (the DIMMs you are familiar with probably have 8 DRAM chips on each side, so 2 ranks). This means that when your CPU sends a DRAM read request along a channel, 8 DRAM chips on the DIMM work together to fulfill that request. Remember that the data bus width is 64 bits, so each of these 8 DRAM chips is only responsible for transmitting 8 bits each per transfer. They transmit at the exact same time, in lock step with each other, and so from the CPU's perspective it seems like a single 64-bit data bus. After one DRAM transaction is completed on that channel, the next transaction might come from a different rank on the same channel (perhaps the other set of 8 DRAM chips on the same DIMM, or a rank on another DIMM on the same channel), or it could be the same rank again. Here is the important part: separate channels can work 100% independently, because they have absolutely 0 resources they have to manage and share between them, but separate ranks on the same channel must serialize their actions, because they all share the same data and control buses, and they must take turns using these resources. Depending on the access pattern you expect to be most common, you can layout data across your DRAM channels/ranks/banks/rows in various ways. One high performance strategy is to layout consecutive cache lines across channels (so address A is in channel 1, and address A+64 is in channel 2). This lets you fetch A+64 while you're still working on A. There are other data layout strategies for other expected access patterns. What I'm saying is that it is common for cache lines to be striped (interleaved) across channels, but not individual bits. If anyone wants, I can go into more detail about how banks, rows, and columns work, but I think channels and ranks are all that is needed to answer the OP's question. One last thing, about GPU memory buses. In graphics card specs, they always say that the GPU has a \"384-bit bus\", but this is misleading. It does have 384 total bits of GDDR5 data bus, but it is not logically organized as a single bus (like the 8 DRAM chips of a rank work together to logically form a 64-bit bus in DDR3). Instead, a GPU with a \"384-bit bus\" will actually have 12 independent 32-bit GDDR5 data buses that all act independently from one another. Source: Ph.D. computer architecture student who's studied DRAM organization for the last 4 years.","title":"More on Channels"},{"location":"Understanding%20Memory/#best-memory-for-different-circumstances","text":"https://www.microway.com/hpc-tech-tips/ddr4-rdimm-lrdimm-performance-comparison/","title":"Best Memory for Different Circumstances"},{"location":"Understanding%20Memory/#dram-channel-vs-rank-vs-bank","text":"https://electronics.stackexchange.com/a/454717/279031","title":"DRAM Channel vs Rank vs Bank"},{"location":"Use%20OS10%20as%20Aggregator/","text":"Use OS10 as Aggregator In this test case the goal is to create a simple load balancer using a reverse LAG port. The idea is to have one input port which is then mirrored to a logical LAG port and at the other end of the LAG port is a number of security sensors. Helpful Links ONIE Network Install Process Overview My Configuration OS 10 Version OS10(config)# do show version Dell EMC Networking OS10 Enterprise Copyright (c) 1999-2020 by Dell Inc. All Rights Reserved. OS Version: 10.5.1.0 Build Version: 10.5.1.0.124 Build Time: 2020-02-12T09:05:20+0000 System Type: S4112F-ON Architecture: x86_64 Up Time: 00:05:09 Configure Device for LAG Physical Configuration 2 10Gb/s fiber SFPs one each connected to two separate VMs. These will serve as our traffic generators. A third 10Gb/s SFP is plugged into a third VM and this will serve as our receiver. Configuration Configuration Test Scenario I played a PCAP made up entirely of HTTP from port 1 and PCAP made entirely of DNS from port 2 and then listened on the VM attached to port 3. Confirmed, all traffic showed up on port 3 as expected.","title":"Use OS10 as Aggregator"},{"location":"Use%20OS10%20as%20Aggregator/#use-os10-as-aggregator","text":"In this test case the goal is to create a simple load balancer using a reverse LAG port. The idea is to have one input port which is then mirrored to a logical LAG port and at the other end of the LAG port is a number of security sensors.","title":"Use OS10 as Aggregator"},{"location":"Use%20OS10%20as%20Aggregator/#helpful-links","text":"ONIE Network Install Process Overview","title":"Helpful Links"},{"location":"Use%20OS10%20as%20Aggregator/#my-configuration","text":"","title":"My Configuration"},{"location":"Use%20OS10%20as%20Aggregator/#os-10-version","text":"OS10(config)# do show version Dell EMC Networking OS10 Enterprise Copyright (c) 1999-2020 by Dell Inc. All Rights Reserved. OS Version: 10.5.1.0 Build Version: 10.5.1.0.124 Build Time: 2020-02-12T09:05:20+0000 System Type: S4112F-ON Architecture: x86_64 Up Time: 00:05:09","title":"OS 10 Version"},{"location":"Use%20OS10%20as%20Aggregator/#configure-device-for-lag","text":"","title":"Configure Device for LAG"},{"location":"Use%20OS10%20as%20Aggregator/#physical-configuration","text":"2 10Gb/s fiber SFPs one each connected to two separate VMs. These will serve as our traffic generators. A third 10Gb/s SFP is plugged into a third VM and this will serve as our receiver.","title":"Physical Configuration"},{"location":"Use%20OS10%20as%20Aggregator/#configuration","text":"Configuration","title":"Configuration"},{"location":"Use%20OS10%20as%20Aggregator/#test-scenario","text":"I played a PCAP made up entirely of HTTP from port 1 and PCAP made entirely of DNS from port 2 and then listened on the VM attached to port 3. Confirmed, all traffic showed up on port 3 as expected.","title":"Test Scenario"},{"location":"Using%20FIO/","text":"Using FIO Understanding FIO Test Output Overview Prerequisites Installing Python 3 Installing FIO Using the Script Script Arguments Running the Script Selecting a Device Example Usage Interpreting Results Warnings and Recommendations Understanding Output Initial Setup Information Main Performance Metrics Latency Statistics Latency Percentiles Bandwidth and IOPS Analysis Additional Statistics IO Depths Disk Stats IO Depth Context in Real-World Scenarios Choosing the Right iodepth Overview The script uses FIO (Flexible I/O Tester) to perform various I/O operations on a specified storage device. Prerequisites Before using the script, ensure you have Python 3 and FIO installed on your system. Installing Python 3 Linux: Use your native Python instance macOS (using Homebrew): brew install python Installing FIO Ubuntu : sudo apt-get install fio RHEL : sudo dnf install fio macOS (using Homebrew): brew install fio Using the Script Script Arguments -e, --ioengine : I/O engine for the test (default: 'libaio'). -b, --block_size : Block size for I/O operations (default: '4k'). -t, --io_type : Type of I/O operation (default: 'rw'). -s, --size : Size of the test data (default: '2G'). -d, --iodepth : I/O depth for operations (default: 64). -j, --numjobs : Number of jobs to spawn (default: Number of CPU cores). device : The target device for testing (required). Running the Script Download the Script : Save the provided Python script to your system. Make the Script Executable (Linux/macOS): chmod +x fio_test.py Run the Script : Use the command line to navigate to the script's directory and run it. Replace [arguments] with desired options: python3 fio_test.py [arguments] /dev/sda Replace /dev/sda with the appropriate device identifier for your system. Selecting a Device Use commands like lsblk , df , or fdisk -l to identify the correct storage device. Be extremely cautious with device selection to avoid accidental data loss. Example Usage Run a test with the default settings on device /dev/sda : python3 fio_test.py /dev/sda Run a test with a block size of 1M, in read mode, on device /dev/nvme0n1 : python3 fio_test.py -b 1M -t read /dev/nvme0n1 Interpreting Results The script will output various metrics such as IOPS (Input/Output Operations Per Second), bandwidth (BW), latency, and more. These metrics provide insight into the performance characteristics of the tested storage device. See Understanding Output Warnings and Recommendations Data Loss : Direct I/O operations can overwrite data. Ensure you have backups and use a non-critical device for testing. Permissions : Running the script might require appropriate permissions, especially when accessing raw devices. Long-Running Tests : Some tests, especially with large sizes or high I/O depths, may take a significant amount of time to complete. Understanding Output FIO (Flexible I/O Tester) provides comprehensive details about I/O performance. Here's a breakdown of the output from a typical run: Initial Setup Information test: (g=0): rw=write, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=1 rw : Read/Write mode (here, it's set to write). bs : Block size for read (R), write (W), and trim (T) operations. ioengine : I/O engine used (libaio, in this case). iodepth : Number of I/O operations to keep in flight. Main Performance Metrics write: IOPS=53.4k, BW=208MiB/s (219MB/s)(2048MiB/9826msec); 0 zone resets IOPS (Input/Output Operations Per Second) : Number of read/write operations per second. BW (Bandwidth) : The rate of data transfer, shown in MiB/s (Mebibytes per second) and MB/s (Megabytes per second). 2048MiB/9826msec : Total data transferred and the total time taken. Latency Statistics slat (nsec) : Submission latency (time from submission to the device until it's queued). clat (usec) : Completion latency (time from submission until completion). lat (usec) : Total latency (submission to completion). min, max, avg, stdev : Minimum, maximum, average, and standard deviation of latencies. Latency Percentiles clat percentiles (nsec) : Shows various percentiles for completion latency. Percentiles indicate the latency under which a certain percentage of operations completed. Bandwidth and IOPS Analysis bw (KiB/s) : Bandwidth in Kibibytes per second. iops : Input/output operations per second. min, max, avg, stdev, samples : Statistics on IOPS. Additional Statistics cpu : CPU usage statistics, including user ( usr ) and system ( sys ) percentages. ctx : Number of context switches. majf/minf : Major and minor faults. IO Depths Shows the percentage of I/Os that were at each depth. 1=100.0% : All I/Os had a depth of 1 (synchronous I/O). Disk Stats Disk stats (read/write) : Shows the read and write statistics for the disk. sdb: ios=50/515233, merge=0/0, ticks=3/8010, in_queue=8014, util=99.13% ios : Number of read and write I/Os. merge : Number of merged read and write requests. ticks : Duration of read and write requests. in_queue : Time the device had I/O requests queued. util : Utilization percentage of the device. IO Depth iodepth refers to the number of I/O operations (I/O requests) that FIO will keep in the queue (or \"in flight\") before waiting for them to complete. Essentially, it's about how many I/O requests are outstanding at any given moment. Impact on I/O Performance Testing : Low iodepth (e.g., 1) : This setting means FIO will wait for an I/O operation to complete before issuing the next one. This simulates a more sequential and synchronous I/O pattern, which is typical for workloads where each operation depends on the completion of the previous one. High iodepth : A higher value, such as 32 or 64, allows FIO to issue multiple I/O requests simultaneously. This setting is used to simulate asynchronous I/O, where multiple operations are happening at the same time. It's more representative of high-performance environments, like databases or servers handling multiple parallel requests. Context in Real-World Scenarios SSD vs HDD : Solid State Drives (SSDs) often benefit from higher iodepth values because they can handle a large number of simultaneous operations efficiently. In contrast, traditional Hard Disk Drives (HDDs) might not show the same level of performance improvement with high iodepth due to their mechanical nature. Testing Different Workloads : By varying the iodepth , you can simulate different types of workloads. For instance, a web server handling multiple concurrent requests might be best simulated with a higher iodepth , while an application that does heavy but sequential file writing might be more accurately tested with a lower iodepth . Choosing the Right iodepth Depends on Your Goals : The right iodepth depends on what aspect of I/O performance you're trying to measure or what kind of workload you want to simulate. Trial and Error : Often, finding the optimal iodepth for a specific scenario involves some experimentation. You might need to run tests at different iodepth levels to see how your storage device or system performs under varying load conditions.","title":"Using FIO"},{"location":"Using%20FIO/#using-fio","text":"Understanding FIO Test Output Overview Prerequisites Installing Python 3 Installing FIO Using the Script Script Arguments Running the Script Selecting a Device Example Usage Interpreting Results Warnings and Recommendations Understanding Output Initial Setup Information Main Performance Metrics Latency Statistics Latency Percentiles Bandwidth and IOPS Analysis Additional Statistics IO Depths Disk Stats IO Depth Context in Real-World Scenarios Choosing the Right iodepth","title":"Using FIO"},{"location":"Using%20FIO/#overview","text":"The script uses FIO (Flexible I/O Tester) to perform various I/O operations on a specified storage device.","title":"Overview"},{"location":"Using%20FIO/#prerequisites","text":"Before using the script, ensure you have Python 3 and FIO installed on your system.","title":"Prerequisites"},{"location":"Using%20FIO/#installing-python-3","text":"Linux: Use your native Python instance macOS (using Homebrew): brew install python","title":"Installing Python 3"},{"location":"Using%20FIO/#installing-fio","text":"Ubuntu : sudo apt-get install fio RHEL : sudo dnf install fio macOS (using Homebrew): brew install fio","title":"Installing FIO"},{"location":"Using%20FIO/#using-the-script","text":"","title":"Using the Script"},{"location":"Using%20FIO/#script-arguments","text":"-e, --ioengine : I/O engine for the test (default: 'libaio'). -b, --block_size : Block size for I/O operations (default: '4k'). -t, --io_type : Type of I/O operation (default: 'rw'). -s, --size : Size of the test data (default: '2G'). -d, --iodepth : I/O depth for operations (default: 64). -j, --numjobs : Number of jobs to spawn (default: Number of CPU cores). device : The target device for testing (required).","title":"Script Arguments"},{"location":"Using%20FIO/#running-the-script","text":"Download the Script : Save the provided Python script to your system. Make the Script Executable (Linux/macOS): chmod +x fio_test.py Run the Script : Use the command line to navigate to the script's directory and run it. Replace [arguments] with desired options: python3 fio_test.py [arguments] /dev/sda Replace /dev/sda with the appropriate device identifier for your system.","title":"Running the Script"},{"location":"Using%20FIO/#selecting-a-device","text":"Use commands like lsblk , df , or fdisk -l to identify the correct storage device. Be extremely cautious with device selection to avoid accidental data loss.","title":"Selecting a Device"},{"location":"Using%20FIO/#example-usage","text":"Run a test with the default settings on device /dev/sda : python3 fio_test.py /dev/sda Run a test with a block size of 1M, in read mode, on device /dev/nvme0n1 : python3 fio_test.py -b 1M -t read /dev/nvme0n1","title":"Example Usage"},{"location":"Using%20FIO/#interpreting-results","text":"The script will output various metrics such as IOPS (Input/Output Operations Per Second), bandwidth (BW), latency, and more. These metrics provide insight into the performance characteristics of the tested storage device. See Understanding Output","title":"Interpreting Results"},{"location":"Using%20FIO/#warnings-and-recommendations","text":"Data Loss : Direct I/O operations can overwrite data. Ensure you have backups and use a non-critical device for testing. Permissions : Running the script might require appropriate permissions, especially when accessing raw devices. Long-Running Tests : Some tests, especially with large sizes or high I/O depths, may take a significant amount of time to complete.","title":"Warnings and Recommendations"},{"location":"Using%20FIO/#understanding-output","text":"FIO (Flexible I/O Tester) provides comprehensive details about I/O performance. Here's a breakdown of the output from a typical run:","title":"Understanding Output"},{"location":"Using%20FIO/#initial-setup-information","text":"test: (g=0): rw=write, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=1 rw : Read/Write mode (here, it's set to write). bs : Block size for read (R), write (W), and trim (T) operations. ioengine : I/O engine used (libaio, in this case). iodepth : Number of I/O operations to keep in flight.","title":"Initial Setup Information"},{"location":"Using%20FIO/#main-performance-metrics","text":"write: IOPS=53.4k, BW=208MiB/s (219MB/s)(2048MiB/9826msec); 0 zone resets IOPS (Input/Output Operations Per Second) : Number of read/write operations per second. BW (Bandwidth) : The rate of data transfer, shown in MiB/s (Mebibytes per second) and MB/s (Megabytes per second). 2048MiB/9826msec : Total data transferred and the total time taken.","title":"Main Performance Metrics"},{"location":"Using%20FIO/#latency-statistics","text":"slat (nsec) : Submission latency (time from submission to the device until it's queued). clat (usec) : Completion latency (time from submission until completion). lat (usec) : Total latency (submission to completion). min, max, avg, stdev : Minimum, maximum, average, and standard deviation of latencies.","title":"Latency Statistics"},{"location":"Using%20FIO/#latency-percentiles","text":"clat percentiles (nsec) : Shows various percentiles for completion latency. Percentiles indicate the latency under which a certain percentage of operations completed.","title":"Latency Percentiles"},{"location":"Using%20FIO/#bandwidth-and-iops-analysis","text":"bw (KiB/s) : Bandwidth in Kibibytes per second. iops : Input/output operations per second. min, max, avg, stdev, samples : Statistics on IOPS.","title":"Bandwidth and IOPS Analysis"},{"location":"Using%20FIO/#additional-statistics","text":"cpu : CPU usage statistics, including user ( usr ) and system ( sys ) percentages. ctx : Number of context switches. majf/minf : Major and minor faults.","title":"Additional Statistics"},{"location":"Using%20FIO/#io-depths","text":"Shows the percentage of I/Os that were at each depth. 1=100.0% : All I/Os had a depth of 1 (synchronous I/O).","title":"IO Depths"},{"location":"Using%20FIO/#disk-stats","text":"Disk stats (read/write) : Shows the read and write statistics for the disk. sdb: ios=50/515233, merge=0/0, ticks=3/8010, in_queue=8014, util=99.13% ios : Number of read and write I/Os. merge : Number of merged read and write requests. ticks : Duration of read and write requests. in_queue : Time the device had I/O requests queued. util : Utilization percentage of the device.","title":"Disk Stats"},{"location":"Using%20FIO/#io-depth","text":"iodepth refers to the number of I/O operations (I/O requests) that FIO will keep in the queue (or \"in flight\") before waiting for them to complete. Essentially, it's about how many I/O requests are outstanding at any given moment. Impact on I/O Performance Testing : Low iodepth (e.g., 1) : This setting means FIO will wait for an I/O operation to complete before issuing the next one. This simulates a more sequential and synchronous I/O pattern, which is typical for workloads where each operation depends on the completion of the previous one. High iodepth : A higher value, such as 32 or 64, allows FIO to issue multiple I/O requests simultaneously. This setting is used to simulate asynchronous I/O, where multiple operations are happening at the same time. It's more representative of high-performance environments, like databases or servers handling multiple parallel requests.","title":"IO Depth"},{"location":"Using%20FIO/#context-in-real-world-scenarios","text":"SSD vs HDD : Solid State Drives (SSDs) often benefit from higher iodepth values because they can handle a large number of simultaneous operations efficiently. In contrast, traditional Hard Disk Drives (HDDs) might not show the same level of performance improvement with high iodepth due to their mechanical nature. Testing Different Workloads : By varying the iodepth , you can simulate different types of workloads. For instance, a web server handling multiple concurrent requests might be best simulated with a higher iodepth , while an application that does heavy but sequential file writing might be more accurately tested with a lower iodepth .","title":"Context in Real-World Scenarios"},{"location":"Using%20FIO/#choosing-the-right-iodepth","text":"Depends on Your Goals : The right iodepth depends on what aspect of I/O performance you're trying to measure or what kind of workload you want to simulate. Trial and Error : Often, finding the optimal iodepth for a specific scenario involves some experimentation. You might need to run tests at different iodepth levels to see how your storage device or system performs under varying load conditions.","title":"Choosing the Right iodepth"},{"location":"Using%20the%20iDRAC%20Service%20Module/","text":"Using the iDRAC Service Module Objective Establish direct, inter-chassis, communication between a host operating system and the iDRAC. My Setup Hardware Dell R840 Note : This will work with any Dell server with iDRAC support. Operating System NAME=\"Red Hat Enterprise Linux\" VERSION=\"8.4 (Ootpa)\" ID=\"rhel\" ID_LIKE=\"fedora\" VERSION_ID=\"8.4\" PLATFORM_ID=\"platform:el8\" PRETTY_NAME=\"Red Hat Enterprise Linux 8.4 (Ootpa)\" ANSI_COLOR=\"0;31\" CPE_NAME=\"cpe:/o:redhat:enterprise_linux:8.4:GA\" HOME_URL=\"https://www.redhat.com/\" DOCUMENTATION_URL=\"https://access.redhat.com/documentation/red_hat_enterprise_linux/8/\" BUG_REPORT_URL=\"https://bugzilla.redhat.com/\" REDHAT_BUGZILLA_PRODUCT=\"Red Hat Enterprise Linux 8\" REDHAT_BUGZILLA_PRODUCT_VERSION=8.4 REDHAT_SUPPORT_PRODUCT=\"Red Hat Enterprise Linux\" REDHAT_SUPPORT_PRODUCT_VERSION=\"8.4\" Red Hat Enterprise Linux release 8.4 (Ootpa) Red Hat Enterprise Linux release 8.4 (Ootpa) Installation Go to your server's support page. In my case this is for the R840 and select the appropriate operating system. Click \"Show All\" on the driver list to see all the different drivers Look for the entry which says \"iDRAC Service Module\" and download it. The Linux entry is here Move the file to your server, extract, and run setup.sh Establishing Direct Communication The way the iDRAC service module works is that it adds a driver which then adds a virtual NIC called iDRAC: [root@r8402 OM-SrvAdmin-Dell-Web-LX-10.1.0.0-4561.RHEL8.x86_64_A00]# ip a s 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2: eno145: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc mq state DOWN group default qlen 1000 link/ether e4:43:4b:9f:44:00 brd ff:ff:ff:ff:ff:ff 3: eno99: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP group default qlen 1000 link/ether e4:43:4b:9f:44:20 brd ff:ff:ff:ff:ff:ff inet 192.168.1.89/24 brd 192.168.1.255 scope global noprefixroute eno99 valid_lft forever preferred_lft forever inet6 fe80::e643:4bff:fe9f:4420/64 scope link noprefixroute valid_lft forever preferred_lft forever 4: eno146: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc mq state DOWN group default qlen 1000 link/ether e4:43:4b:9f:44:02 brd ff:ff:ff:ff:ff:ff 5: eno100: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc mq state DOWN group default qlen 1000 link/ether e4:43:4b:9f:44:21 brd ff:ff:ff:ff:ff:ff 6: idrac: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UNKNOWN group default qlen 1000 link/ether 70:b5:e8:e2:a0:d3 brd ff:ff:ff:ff:ff:ff inet 169.254.0.2/16 brd 169.254.255.255 scope global idrac valid_lft forever preferred_lft forever 7: virbr0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default qlen 1000 link/ether 52:54:00:b5:c5:1b brd ff:ff:ff:ff:ff:ff inet 192.168.122.1/24 brd 192.168.122.255 scope global virbr0 valid_lft forever preferred_lft forever 8: virbr0-nic: <BROADCAST,MULTICAST> mtu 1500 qdisc fq_codel master virbr0 state DOWN group default qlen 1000 link/ether 52:54:00:b5:c5:1b brd ff:ff:ff:ff:ff:ff The inter-chassis network uses the IPv4 link local subnet 169.254.0.0/16. In general, the server should be 169.254.0.2 and the iDRAC itself is 169.254.0.1. You can connect to 169.254.0.1 as you would any iDRAC address:","title":"Using the iDRAC Service Module"},{"location":"Using%20the%20iDRAC%20Service%20Module/#using-the-idrac-service-module","text":"","title":"Using the iDRAC Service Module"},{"location":"Using%20the%20iDRAC%20Service%20Module/#objective","text":"Establish direct, inter-chassis, communication between a host operating system and the iDRAC.","title":"Objective"},{"location":"Using%20the%20iDRAC%20Service%20Module/#my-setup","text":"","title":"My Setup"},{"location":"Using%20the%20iDRAC%20Service%20Module/#hardware","text":"Dell R840 Note : This will work with any Dell server with iDRAC support.","title":"Hardware"},{"location":"Using%20the%20iDRAC%20Service%20Module/#operating-system","text":"NAME=\"Red Hat Enterprise Linux\" VERSION=\"8.4 (Ootpa)\" ID=\"rhel\" ID_LIKE=\"fedora\" VERSION_ID=\"8.4\" PLATFORM_ID=\"platform:el8\" PRETTY_NAME=\"Red Hat Enterprise Linux 8.4 (Ootpa)\" ANSI_COLOR=\"0;31\" CPE_NAME=\"cpe:/o:redhat:enterprise_linux:8.4:GA\" HOME_URL=\"https://www.redhat.com/\" DOCUMENTATION_URL=\"https://access.redhat.com/documentation/red_hat_enterprise_linux/8/\" BUG_REPORT_URL=\"https://bugzilla.redhat.com/\" REDHAT_BUGZILLA_PRODUCT=\"Red Hat Enterprise Linux 8\" REDHAT_BUGZILLA_PRODUCT_VERSION=8.4 REDHAT_SUPPORT_PRODUCT=\"Red Hat Enterprise Linux\" REDHAT_SUPPORT_PRODUCT_VERSION=\"8.4\" Red Hat Enterprise Linux release 8.4 (Ootpa) Red Hat Enterprise Linux release 8.4 (Ootpa)","title":"Operating System"},{"location":"Using%20the%20iDRAC%20Service%20Module/#installation","text":"Go to your server's support page. In my case this is for the R840 and select the appropriate operating system. Click \"Show All\" on the driver list to see all the different drivers Look for the entry which says \"iDRAC Service Module\" and download it. The Linux entry is here Move the file to your server, extract, and run setup.sh","title":"Installation"},{"location":"Using%20the%20iDRAC%20Service%20Module/#establishing-direct-communication","text":"The way the iDRAC service module works is that it adds a driver which then adds a virtual NIC called iDRAC: [root@r8402 OM-SrvAdmin-Dell-Web-LX-10.1.0.0-4561.RHEL8.x86_64_A00]# ip a s 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2: eno145: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc mq state DOWN group default qlen 1000 link/ether e4:43:4b:9f:44:00 brd ff:ff:ff:ff:ff:ff 3: eno99: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP group default qlen 1000 link/ether e4:43:4b:9f:44:20 brd ff:ff:ff:ff:ff:ff inet 192.168.1.89/24 brd 192.168.1.255 scope global noprefixroute eno99 valid_lft forever preferred_lft forever inet6 fe80::e643:4bff:fe9f:4420/64 scope link noprefixroute valid_lft forever preferred_lft forever 4: eno146: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc mq state DOWN group default qlen 1000 link/ether e4:43:4b:9f:44:02 brd ff:ff:ff:ff:ff:ff 5: eno100: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc mq state DOWN group default qlen 1000 link/ether e4:43:4b:9f:44:21 brd ff:ff:ff:ff:ff:ff 6: idrac: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UNKNOWN group default qlen 1000 link/ether 70:b5:e8:e2:a0:d3 brd ff:ff:ff:ff:ff:ff inet 169.254.0.2/16 brd 169.254.255.255 scope global idrac valid_lft forever preferred_lft forever 7: virbr0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default qlen 1000 link/ether 52:54:00:b5:c5:1b brd ff:ff:ff:ff:ff:ff inet 192.168.122.1/24 brd 192.168.122.255 scope global virbr0 valid_lft forever preferred_lft forever 8: virbr0-nic: <BROADCAST,MULTICAST> mtu 1500 qdisc fq_codel master virbr0 state DOWN group default qlen 1000 link/ether 52:54:00:b5:c5:1b brd ff:ff:ff:ff:ff:ff The inter-chassis network uses the IPv4 link local subnet 169.254.0.0/16. In general, the server should be 169.254.0.2 and the iDRAC itself is 169.254.0.1. You can connect to 169.254.0.1 as you would any iDRAC address:","title":"Establishing Direct Communication"},{"location":"VEP%20Testing/","text":"VEP Testing Unscrew the tiny panel on the back. Underneath is microUSB. Plug in. Bring up device manager. Look at ports. After the automatic driver installation a port should appear there with notation Silicon Labs CP210x USB to UART Bridge (COMX) . This is what you want to connect to Open putty set to serial with speed 115200 and COM You will need to download ESXi from the Dell website using your serial number. This is very important as the ESXi installation has unique drivers injected into it for your convienience. You could load vanilla ESXi and load the drivers yourself. Burn ESXi to a flash drive using Rufus and plug it into the side of the VEP. Power cycle the VEP and hit delete when prompted to open the firmware screen. Set the flash drive as the highest priority for boot and then save and exit. Install ESXi normally via the serial console.","title":"VEP Testing"},{"location":"VEP%20Testing/#vep-testing","text":"Unscrew the tiny panel on the back. Underneath is microUSB. Plug in. Bring up device manager. Look at ports. After the automatic driver installation a port should appear there with notation Silicon Labs CP210x USB to UART Bridge (COMX) . This is what you want to connect to Open putty set to serial with speed 115200 and COM You will need to download ESXi from the Dell website using your serial number. This is very important as the ESXi installation has unique drivers injected into it for your convienience. You could load vanilla ESXi and load the drivers yourself. Burn ESXi to a flash drive using Rufus and plug it into the side of the VEP. Power cycle the VEP and hit delete when prompted to open the firmware screen. Set the flash drive as the highest priority for boot and then save and exit. Install ESXi normally via the serial console.","title":"VEP Testing"},{"location":"VMWare/Automate%20ESXi%20Installation/","text":"Automating ESXi Installation This Ansible playbook runs on CentOS. It will create a server which allows you to install ESXi automatically via PXE boot and Kickstart. For more information on ESXi's Kickstart capabilities see this link Prerequisites Install CentOS You can download and install CentOS here The installation services used by this program are very lightweight so you can install on just about anything. A 12GB hard drive, 2GB RAM, and a single processor are enough for what we are doing. Go ahead and install CentOS on whatever you like, the only requirement is that it must be reachable at layer 2 by the ESXi hosts you want to install. Install Ansible and git Before continuing you will need to install Ansible on your host by running yum install -y ansible git . Clone the Repo I typically use opt for optional programs. You may install wherever you like, but in this guide I will use opt. Clone the repo with: git clone https://github.com/grantcurell/esxi-autoinstall.git Move into the esxi-autoinstall directory with cd /opt/esxi-autoinstall Configure the Inventory File Ansible is controlled by what is called an inventory file. This inventory file contains all the configuration settings which will be used by Ansible. In our case, we will need to populate this with the settings we wish ESXi to have after installation. The inventory file is called inventory.yml You will need to fill in the following values: dns dhcp_start dhcp_end gateway netmask domain root_password server_ip iso_esxi_pth iso_esxi_checksum Finally you will also need to fill in the host information. These are the hosts on which you would like to install ESXi. Boot Drives Most of the values are self explanatory however, boot_drive and data_drives may be a bit confusing. These options allow you to tell Ansible where to install ESXi and on which disks to configure datastores. The official documentation on how ESXi names drives is here The disks are in the order in which ESXi detects them. To get the order you may have to manually install ESXi on a system. You can make an educated guess from the BIOS menu or if you have something like iDrac you can look at the order of the disks on the RAID controller (assuming you have one). Running the Code Once you are finished editing the inventory.yml file, cd to the root of the esxi-autoinstall directory and run make . This will run the makefile in the directory. You can see what it is doing by examining the file called Makefile Advanced You may want the installer to do some crazier things. The kickstart script used to configure the hosts can be modified to run arbitrary esxcli commands. You're on your own if you want to go this route, but you can find the kickstart file with the esxcli commands in roles/profiles/templates/ks.cfg.j2 .","title":"Automating ESXi Installation"},{"location":"VMWare/Automate%20ESXi%20Installation/#automating-esxi-installation","text":"This Ansible playbook runs on CentOS. It will create a server which allows you to install ESXi automatically via PXE boot and Kickstart. For more information on ESXi's Kickstart capabilities see this link","title":"Automating ESXi Installation"},{"location":"VMWare/Automate%20ESXi%20Installation/#prerequisites","text":"","title":"Prerequisites"},{"location":"VMWare/Automate%20ESXi%20Installation/#install-centos","text":"You can download and install CentOS here The installation services used by this program are very lightweight so you can install on just about anything. A 12GB hard drive, 2GB RAM, and a single processor are enough for what we are doing. Go ahead and install CentOS on whatever you like, the only requirement is that it must be reachable at layer 2 by the ESXi hosts you want to install.","title":"Install CentOS"},{"location":"VMWare/Automate%20ESXi%20Installation/#install-ansible-and-git","text":"Before continuing you will need to install Ansible on your host by running yum install -y ansible git .","title":"Install Ansible and git"},{"location":"VMWare/Automate%20ESXi%20Installation/#clone-the-repo","text":"I typically use opt for optional programs. You may install wherever you like, but in this guide I will use opt. Clone the repo with: git clone https://github.com/grantcurell/esxi-autoinstall.git Move into the esxi-autoinstall directory with cd /opt/esxi-autoinstall","title":"Clone the Repo"},{"location":"VMWare/Automate%20ESXi%20Installation/#configure-the-inventory-file","text":"Ansible is controlled by what is called an inventory file. This inventory file contains all the configuration settings which will be used by Ansible. In our case, we will need to populate this with the settings we wish ESXi to have after installation. The inventory file is called inventory.yml You will need to fill in the following values: dns dhcp_start dhcp_end gateway netmask domain root_password server_ip iso_esxi_pth iso_esxi_checksum Finally you will also need to fill in the host information. These are the hosts on which you would like to install ESXi.","title":"Configure the Inventory File"},{"location":"VMWare/Automate%20ESXi%20Installation/#boot-drives","text":"Most of the values are self explanatory however, boot_drive and data_drives may be a bit confusing. These options allow you to tell Ansible where to install ESXi and on which disks to configure datastores. The official documentation on how ESXi names drives is here The disks are in the order in which ESXi detects them. To get the order you may have to manually install ESXi on a system. You can make an educated guess from the BIOS menu or if you have something like iDrac you can look at the order of the disks on the RAID controller (assuming you have one).","title":"Boot Drives"},{"location":"VMWare/Automate%20ESXi%20Installation/#running-the-code","text":"Once you are finished editing the inventory.yml file, cd to the root of the esxi-autoinstall directory and run make . This will run the makefile in the directory. You can see what it is doing by examining the file called Makefile","title":"Running the Code"},{"location":"VMWare/Automate%20ESXi%20Installation/#advanced","text":"You may want the installer to do some crazier things. The kickstart script used to configure the hosts can be modified to run arbitrary esxcli commands. You're on your own if you want to go this route, but you can find the kickstart file with the esxcli commands in roles/profiles/templates/ks.cfg.j2 .","title":"Advanced"},{"location":"VMWare/Automate%20ESXi%20Installation/LICENSE/","text":"GNU GENERAL PUBLIC LICENSE Version 3, 29 June 2007 Copyright (C) 2007 Free Software Foundation, Inc. http://fsf.org/ Everyone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed. Preamble The GNU General Public License is a free, copyleft license for software and other kinds of works. The licenses for most software and other practical works are designed to take away your freedom to share and change the works. By contrast, the GNU General Public License is intended to guarantee your freedom to share and change all versions of a program--to make sure it remains free software for all its users. We, the Free Software Foundation, use the GNU General Public License for most of our software; it applies also to any other work released this way by its authors. You can apply it to your programs, too. When we speak of free software, we are referring to freedom, not price. Our General Public Licenses are designed to make sure that you have the freedom to distribute copies of free software (and charge for them if you wish), that you receive source code or can get it if you want it, that you can change the software or use pieces of it in new free programs, and that you know you can do these things. To protect your rights, we need to prevent others from denying you these rights or asking you to surrender the rights. Therefore, you have certain responsibilities if you distribute copies of the software, or if you modify it: responsibilities to respect the freedom of others. For example, if you distribute copies of such a program, whether gratis or for a fee, you must pass on to the recipients the same freedoms that you received. You must make sure that they, too, receive or can get the source code. And you must show them these terms so they know their rights. Developers that use the GNU GPL protect your rights with two steps: (1) assert copyright on the software, and (2) offer you this License giving you legal permission to copy, distribute and/or modify it. For the developers' and authors' protection, the GPL clearly explains that there is no warranty for this free software. For both users' and authors' sake, the GPL requires that modified versions be marked as changed, so that their problems will not be attributed erroneously to authors of previous versions. Some devices are designed to deny users access to install or run modified versions of the software inside them, although the manufacturer can do so. This is fundamentally incompatible with the aim of protecting users' freedom to change the software. The systematic pattern of such abuse occurs in the area of products for individuals to use, which is precisely where it is most unacceptable. Therefore, we have designed this version of the GPL to prohibit the practice for those products. If such problems arise substantially in other domains, we stand ready to extend this provision to those domains in future versions of the GPL, as needed to protect the freedom of users. Finally, every program is threatened constantly by software patents. States should not allow patents to restrict development and use of software on general-purpose computers, but in those that do, we wish to avoid the special danger that patents applied to a free program could make it effectively proprietary. To prevent this, the GPL assures that patents cannot be used to render the program non-free. The precise terms and conditions for copying, distribution and modification follow. TERMS AND CONDITIONS Definitions. \"This License\" refers to version 3 of the GNU General Public License. \"Copyright\" also means copyright-like laws that apply to other kinds of works, such as semiconductor masks. \"The Program\" refers to any copyrightable work licensed under this License. Each licensee is addressed as \"you\". \"Licensees\" and \"recipients\" may be individuals or organizations. To \"modify\" a work means to copy from or adapt all or part of the work in a fashion requiring copyright permission, other than the making of an exact copy. The resulting work is called a \"modified version\" of the earlier work or a work \"based on\" the earlier work. A \"covered work\" means either the unmodified Program or a work based on the Program. To \"propagate\" a work means to do anything with it that, without permission, would make you directly or secondarily liable for infringement under applicable copyright law, except executing it on a computer or modifying a private copy. Propagation includes copying, distribution (with or without modification), making available to the public, and in some countries other activities as well. To \"convey\" a work means any kind of propagation that enables other parties to make or receive copies. Mere interaction with a user through a computer network, with no transfer of a copy, is not conveying. An interactive user interface displays \"Appropriate Legal Notices\" to the extent that it includes a convenient and prominently visible feature that (1) displays an appropriate copyright notice, and (2) tells the user that there is no warranty for the work (except to the extent that warranties are provided), that licensees may convey the work under this License, and how to view a copy of this License. If the interface presents a list of user commands or options, such as a menu, a prominent item in the list meets this criterion. Source Code. The \"source code\" for a work means the preferred form of the work for making modifications to it. \"Object code\" means any non-source form of a work. A \"Standard Interface\" means an interface that either is an official standard defined by a recognized standards body, or, in the case of interfaces specified for a particular programming language, one that is widely used among developers working in that language. The \"System Libraries\" of an executable work include anything, other than the work as a whole, that (a) is included in the normal form of packaging a Major Component, but which is not part of that Major Component, and (b) serves only to enable use of the work with that Major Component, or to implement a Standard Interface for which an implementation is available to the public in source code form. A \"Major Component\", in this context, means a major essential component (kernel, window system, and so on) of the specific operating system (if any) on which the executable work runs, or a compiler used to produce the work, or an object code interpreter used to run it. The \"Corresponding Source\" for a work in object code form means all the source code needed to generate, install, and (for an executable work) run the object code and to modify the work, including scripts to control those activities. However, it does not include the work's System Libraries, or general-purpose tools or generally available free programs which are used unmodified in performing those activities but which are not part of the work. For example, Corresponding Source includes interface definition files associated with source files for the work, and the source code for shared libraries and dynamically linked subprograms that the work is specifically designed to require, such as by intimate data communication or control flow between those subprograms and other parts of the work. The Corresponding Source need not include anything that users can regenerate automatically from other parts of the Corresponding Source. The Corresponding Source for a work in source code form is that same work. Basic Permissions. All rights granted under this License are granted for the term of copyright on the Program, and are irrevocable provided the stated conditions are met. This License explicitly affirms your unlimited permission to run the unmodified Program. The output from running a covered work is covered by this License only if the output, given its content, constitutes a covered work. This License acknowledges your rights of fair use or other equivalent, as provided by copyright law. You may make, run and propagate covered works that you do not convey, without conditions so long as your license otherwise remains in force. You may convey covered works to others for the sole purpose of having them make modifications exclusively for you, or provide you with facilities for running those works, provided that you comply with the terms of this License in conveying all material for which you do not control copyright. Those thus making or running the covered works for you must do so exclusively on your behalf, under your direction and control, on terms that prohibit them from making any copies of your copyrighted material outside their relationship with you. Conveying under any other circumstances is permitted solely under the conditions stated below. Sublicensing is not allowed; section 10 makes it unnecessary. Protecting Users' Legal Rights From Anti-Circumvention Law. No covered work shall be deemed part of an effective technological measure under any applicable law fulfilling obligations under article 11 of the WIPO copyright treaty adopted on 20 December 1996, or similar laws prohibiting or restricting circumvention of such measures. When you convey a covered work, you waive any legal power to forbid circumvention of technological measures to the extent such circumvention is effected by exercising rights under this License with respect to the covered work, and you disclaim any intention to limit operation or modification of the work as a means of enforcing, against the work's users, your or third parties' legal rights to forbid circumvention of technological measures. Conveying Verbatim Copies. You may convey verbatim copies of the Program's source code as you receive it, in any medium, provided that you conspicuously and appropriately publish on each copy an appropriate copyright notice; keep intact all notices stating that this License and any non-permissive terms added in accord with section 7 apply to the code; keep intact all notices of the absence of any warranty; and give all recipients a copy of this License along with the Program. You may charge any price or no price for each copy that you convey, and you may offer support or warranty protection for a fee. Conveying Modified Source Versions. You may convey a work based on the Program, or the modifications to produce it from the Program, in the form of source code under the terms of section 4, provided that you also meet all of these conditions: a) The work must carry prominent notices stating that you modified it, and giving a relevant date. b) The work must carry prominent notices stating that it is released under this License and any conditions added under section 7. This requirement modifies the requirement in section 4 to \"keep intact all notices\". c) You must license the entire work, as a whole, under this License to anyone who comes into possession of a copy. This License will therefore apply, along with any applicable section 7 additional terms, to the whole of the work, and all its parts, regardless of how they are packaged. This License gives no permission to license the work in any other way, but it does not invalidate such permission if you have separately received it. d) If the work has interactive user interfaces, each must display Appropriate Legal Notices; however, if the Program has interactive interfaces that do not display Appropriate Legal Notices, your work need not make them do so. A compilation of a covered work with other separate and independent works, which are not by their nature extensions of the covered work, and which are not combined with it such as to form a larger program, in or on a volume of a storage or distribution medium, is called an \"aggregate\" if the compilation and its resulting copyright are not used to limit the access or legal rights of the compilation's users beyond what the individual works permit. Inclusion of a covered work in an aggregate does not cause this License to apply to the other parts of the aggregate. Conveying Non-Source Forms. You may convey a covered work in object code form under the terms of sections 4 and 5, provided that you also convey the machine-readable Corresponding Source under the terms of this License, in one of these ways: a) Convey the object code in, or embodied in, a physical product (including a physical distribution medium), accompanied by the Corresponding Source fixed on a durable physical medium customarily used for software interchange. b) Convey the object code in, or embodied in, a physical product (including a physical distribution medium), accompanied by a written offer, valid for at least three years and valid for as long as you offer spare parts or customer support for that product model, to give anyone who possesses the object code either (1) a copy of the Corresponding Source for all the software in the product that is covered by this License, on a durable physical medium customarily used for software interchange, for a price no more than your reasonable cost of physically performing this conveying of source, or (2) access to copy the Corresponding Source from a network server at no charge. c) Convey individual copies of the object code with a copy of the written offer to provide the Corresponding Source. This alternative is allowed only occasionally and noncommercially, and only if you received the object code with such an offer, in accord with subsection 6b. d) Convey the object code by offering access from a designated place (gratis or for a charge), and offer equivalent access to the Corresponding Source in the same way through the same place at no further charge. You need not require recipients to copy the Corresponding Source along with the object code. If the place to copy the object code is a network server, the Corresponding Source may be on a different server (operated by you or a third party) that supports equivalent copying facilities, provided you maintain clear directions next to the object code saying where to find the Corresponding Source. Regardless of what server hosts the Corresponding Source, you remain obligated to ensure that it is available for as long as needed to satisfy these requirements. e) Convey the object code using peer-to-peer transmission, provided you inform other peers where the object code and Corresponding Source of the work are being offered to the general public at no charge under subsection 6d. A separable portion of the object code, whose source code is excluded from the Corresponding Source as a System Library, need not be included in conveying the object code work. A \"User Product\" is either (1) a \"consumer product\", which means any tangible personal property which is normally used for personal, family, or household purposes, or (2) anything designed or sold for incorporation into a dwelling. In determining whether a product is a consumer product, doubtful cases shall be resolved in favor of coverage. For a particular product received by a particular user, \"normally used\" refers to a typical or common use of that class of product, regardless of the status of the particular user or of the way in which the particular user actually uses, or expects or is expected to use, the product. A product is a consumer product regardless of whether the product has substantial commercial, industrial or non-consumer uses, unless such uses represent the only significant mode of use of the product. \"Installation Information\" for a User Product means any methods, procedures, authorization keys, or other information required to install and execute modified versions of a covered work in that User Product from a modified version of its Corresponding Source. The information must suffice to ensure that the continued functioning of the modified object code is in no case prevented or interfered with solely because modification has been made. If you convey an object code work under this section in, or with, or specifically for use in, a User Product, and the conveying occurs as part of a transaction in which the right of possession and use of the User Product is transferred to the recipient in perpetuity or for a fixed term (regardless of how the transaction is characterized), the Corresponding Source conveyed under this section must be accompanied by the Installation Information. But this requirement does not apply if neither you nor any third party retains the ability to install modified object code on the User Product (for example, the work has been installed in ROM). The requirement to provide Installation Information does not include a requirement to continue to provide support service, warranty, or updates for a work that has been modified or installed by the recipient, or for the User Product in which it has been modified or installed. Access to a network may be denied when the modification itself materially and adversely affects the operation of the network or violates the rules and protocols for communication across the network. Corresponding Source conveyed, and Installation Information provided, in accord with this section must be in a format that is publicly documented (and with an implementation available to the public in source code form), and must require no special password or key for unpacking, reading or copying. Additional Terms. \"Additional permissions\" are terms that supplement the terms of this License by making exceptions from one or more of its conditions. Additional permissions that are applicable to the entire Program shall be treated as though they were included in this License, to the extent that they are valid under applicable law. If additional permissions apply only to part of the Program, that part may be used separately under those permissions, but the entire Program remains governed by this License without regard to the additional permissions. When you convey a copy of a covered work, you may at your option remove any additional permissions from that copy, or from any part of it. (Additional permissions may be written to require their own removal in certain cases when you modify the work.) You may place additional permissions on material, added by you to a covered work, for which you have or can give appropriate copyright permission. Notwithstanding any other provision of this License, for material you add to a covered work, you may (if authorized by the copyright holders of that material) supplement the terms of this License with terms: a) Disclaiming warranty or limiting liability differently from the terms of sections 15 and 16 of this License; or b) Requiring preservation of specified reasonable legal notices or author attributions in that material or in the Appropriate Legal Notices displayed by works containing it; or c) Prohibiting misrepresentation of the origin of that material, or requiring that modified versions of such material be marked in reasonable ways as different from the original version; or d) Limiting the use for publicity purposes of names of licensors or authors of the material; or e) Declining to grant rights under trademark law for use of some trade names, trademarks, or service marks; or f) Requiring indemnification of licensors and authors of that material by anyone who conveys the material (or modified versions of it) with contractual assumptions of liability to the recipient, for any liability that these contractual assumptions directly impose on those licensors and authors. All other non-permissive additional terms are considered \"further restrictions\" within the meaning of section 10. If the Program as you received it, or any part of it, contains a notice stating that it is governed by this License along with a term that is a further restriction, you may remove that term. If a license document contains a further restriction but permits relicensing or conveying under this License, you may add to a covered work material governed by the terms of that license document, provided that the further restriction does not survive such relicensing or conveying. If you add terms to a covered work in accord with this section, you must place, in the relevant source files, a statement of the additional terms that apply to those files, or a notice indicating where to find the applicable terms. Additional terms, permissive or non-permissive, may be stated in the form of a separately written license, or stated as exceptions; the above requirements apply either way. Termination. You may not propagate or modify a covered work except as expressly provided under this License. Any attempt otherwise to propagate or modify it is void, and will automatically terminate your rights under this License (including any patent licenses granted under the third paragraph of section 11). However, if you cease all violation of this License, then your license from a particular copyright holder is reinstated (a) provisionally, unless and until the copyright holder explicitly and finally terminates your license, and (b) permanently, if the copyright holder fails to notify you of the violation by some reasonable means prior to 60 days after the cessation. Moreover, your license from a particular copyright holder is reinstated permanently if the copyright holder notifies you of the violation by some reasonable means, this is the first time you have received notice of violation of this License (for any work) from that copyright holder, and you cure the violation prior to 30 days after your receipt of the notice. Termination of your rights under this section does not terminate the licenses of parties who have received copies or rights from you under this License. If your rights have been terminated and not permanently reinstated, you do not qualify to receive new licenses for the same material under section 10. Acceptance Not Required for Having Copies. You are not required to accept this License in order to receive or run a copy of the Program. Ancillary propagation of a covered work occurring solely as a consequence of using peer-to-peer transmission to receive a copy likewise does not require acceptance. However, nothing other than this License grants you permission to propagate or modify any covered work. These actions infringe copyright if you do not accept this License. Therefore, by modifying or propagating a covered work, you indicate your acceptance of this License to do so. Automatic Licensing of Downstream Recipients. Each time you convey a covered work, the recipient automatically receives a license from the original licensors, to run, modify and propagate that work, subject to this License. You are not responsible for enforcing compliance by third parties with this License. An \"entity transaction\" is a transaction transferring control of an organization, or substantially all assets of one, or subdividing an organization, or merging organizations. If propagation of a covered work results from an entity transaction, each party to that transaction who receives a copy of the work also receives whatever licenses to the work the party's predecessor in interest had or could give under the previous paragraph, plus a right to possession of the Corresponding Source of the work from the predecessor in interest, if the predecessor has it or can get it with reasonable efforts. You may not impose any further restrictions on the exercise of the rights granted or affirmed under this License. For example, you may not impose a license fee, royalty, or other charge for exercise of rights granted under this License, and you may not initiate litigation (including a cross-claim or counterclaim in a lawsuit) alleging that any patent claim is infringed by making, using, selling, offering for sale, or importing the Program or any portion of it. Patents. A \"contributor\" is a copyright holder who authorizes use under this License of the Program or a work on which the Program is based. The work thus licensed is called the contributor's \"contributor version\". A contributor's \"essential patent claims\" are all patent claims owned or controlled by the contributor, whether already acquired or hereafter acquired, that would be infringed by some manner, permitted by this License, of making, using, or selling its contributor version, but do not include claims that would be infringed only as a consequence of further modification of the contributor version. For purposes of this definition, \"control\" includes the right to grant patent sublicenses in a manner consistent with the requirements of this License. Each contributor grants you a non-exclusive, worldwide, royalty-free patent license under the contributor's essential patent claims, to make, use, sell, offer for sale, import and otherwise run, modify and propagate the contents of its contributor version. In the following three paragraphs, a \"patent license\" is any express agreement or commitment, however denominated, not to enforce a patent (such as an express permission to practice a patent or covenant not to sue for patent infringement). To \"grant\" such a patent license to a party means to make such an agreement or commitment not to enforce a patent against the party. If you convey a covered work, knowingly relying on a patent license, and the Corresponding Source of the work is not available for anyone to copy, free of charge and under the terms of this License, through a publicly available network server or other readily accessible means, then you must either (1) cause the Corresponding Source to be so available, or (2) arrange to deprive yourself of the benefit of the patent license for this particular work, or (3) arrange, in a manner consistent with the requirements of this License, to extend the patent license to downstream recipients. \"Knowingly relying\" means you have actual knowledge that, but for the patent license, your conveying the covered work in a country, or your recipient's use of the covered work in a country, would infringe one or more identifiable patents in that country that you have reason to believe are valid. If, pursuant to or in connection with a single transaction or arrangement, you convey, or propagate by procuring conveyance of, a covered work, and grant a patent license to some of the parties receiving the covered work authorizing them to use, propagate, modify or convey a specific copy of the covered work, then the patent license you grant is automatically extended to all recipients of the covered work and works based on it. A patent license is \"discriminatory\" if it does not include within the scope of its coverage, prohibits the exercise of, or is conditioned on the non-exercise of one or more of the rights that are specifically granted under this License. You may not convey a covered work if you are a party to an arrangement with a third party that is in the business of distributing software, under which you make payment to the third party based on the extent of your activity of conveying the work, and under which the third party grants, to any of the parties who would receive the covered work from you, a discriminatory patent license (a) in connection with copies of the covered work conveyed by you (or copies made from those copies), or (b) primarily for and in connection with specific products or compilations that contain the covered work, unless you entered into that arrangement, or that patent license was granted, prior to 28 March 2007. Nothing in this License shall be construed as excluding or limiting any implied license or other defenses to infringement that may otherwise be available to you under applicable patent law. No Surrender of Others' Freedom. If conditions are imposed on you (whether by court order, agreement or otherwise) that contradict the conditions of this License, they do not excuse you from the conditions of this License. If you cannot convey a covered work so as to satisfy simultaneously your obligations under this License and any other pertinent obligations, then as a consequence you may not convey it at all. For example, if you agree to terms that obligate you to collect a royalty for further conveying from those to whom you convey the Program, the only way you could satisfy both those terms and this License would be to refrain entirely from conveying the Program. Use with the GNU Affero General Public License. Notwithstanding any other provision of this License, you have permission to link or combine any covered work with a work licensed under version 3 of the GNU Affero General Public License into a single combined work, and to convey the resulting work. The terms of this License will continue to apply to the part which is the covered work, but the special requirements of the GNU Affero General Public License, section 13, concerning interaction through a network will apply to the combination as such. Revised Versions of this License. The Free Software Foundation may publish revised and/or new versions of the GNU General Public License from time to time. Such new versions will be similar in spirit to the present version, but may differ in detail to address new problems or concerns. Each version is given a distinguishing version number. If the Program specifies that a certain numbered version of the GNU General Public License \"or any later version\" applies to it, you have the option of following the terms and conditions either of that numbered version or of any later version published by the Free Software Foundation. If the Program does not specify a version number of the GNU General Public License, you may choose any version ever published by the Free Software Foundation. If the Program specifies that a proxy can decide which future versions of the GNU General Public License can be used, that proxy's public statement of acceptance of a version permanently authorizes you to choose that version for the Program. Later license versions may give you additional or different permissions. However, no additional obligations are imposed on any author or copyright holder as a result of your choosing to follow a later version. Disclaimer of Warranty. THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY APPLICABLE LAW. EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM \"AS IS\" WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM IS WITH YOU. SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF ALL NECESSARY SERVICING, REPAIR OR CORRECTION. Limitation of Liability. IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS), EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Interpretation of Sections 15 and 16. If the disclaimer of warranty and limitation of liability provided above cannot be given local legal effect according to their terms, reviewing courts shall apply local law that most closely approximates an absolute waiver of all civil liability in connection with the Program, unless a warranty or assumption of liability accompanies a copy of the Program in return for a fee. END OF TERMS AND CONDITIONS How to Apply These Terms to Your New Programs If you develop a new program, and you want it to be of the greatest possible use to the public, the best way to achieve this is to make it free software which everyone can redistribute and change under these terms. To do so, attach the following notices to the program. It is safest to attach them to the start of each source file to most effectively state the exclusion of warranty; and each file should have at least the \"copyright\" line and a pointer to where the full notice is found. <one line to give the program's name and a brief idea of what it does.> Copyright (C) <year> <name of author> This program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details. You should have received a copy of the GNU General Public License along with this program. If not, see <http://www.gnu.org/licenses/>. Also add information on how to contact you by electronic and paper mail. If the program does terminal interaction, make it output a short notice like this when it starts in an interactive mode: {project} Copyright (C) {year} {fullname} This program comes with ABSOLUTELY NO WARRANTY; for details type `show w'. This is free software, and you are welcome to redistribute it under certain conditions; type `show c' for details. The hypothetical commands show w' and show c' should show the appropriate parts of the General Public License. Of course, your program's commands might be different; for a GUI interface, you would use an \"about box\". You should also get your employer (if you work as a programmer) or school, if any, to sign a \"copyright disclaimer\" for the program, if necessary. For more information on this, and how to apply and follow the GNU GPL, see http://www.gnu.org/licenses/ . The GNU General Public License does not permit incorporating your program into proprietary programs. If your program is a subroutine library, you may consider it more useful to permit linking proprietary applications with the library. If this is what you want to do, use the GNU Lesser General Public License instead of this License. But first, please read http://www.gnu.org/philosophy/why-not-lgpl.html .","title":"LICENSE"},{"location":"VMWare/Change%20User%20on%20VxRail%20Plugin/","text":"Change User on VxRail Plugin Background The VxRail Manager VM must have the user credentials for both vCenter and the various ESXi nodes stored in its credential database, a container called lockbox, in order to work properly. These service accounts exist on vCenter and the local ESXi nodes so that VxRail manager can communicate with their APIs. The management account for vCenter is set during setup and for the ESXi nodes defaults to management_account_esxi_<serial_number> . Problem If the credentials vCenter uses for its VxRail Manager account do not match the account on the VxRail manager itself or the account the ESXi instances have do not match what is on VxRail manager all API calls from the VxRail plugin in vCenter will fail. You will see the following in the VxRail manager /var/log/mystic/web.log 2022-08-31T16:58:37.056+0000 ERROR [myScheduler-5] com.emc.mystic.manager.commons.vmware.service.VMwareServiceImpl VMwareServiceImpl.getVimOperator:60 - Invalid login to VMware com.vmware.vim25.InvalidLoginFaultMsg: Cannot complete login due to an incorrect user name or password. at jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:?] at jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) ~[?:?] at jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:?] at java.lang.reflect.Constructor.newInstance(Constructor.java:490) ~[?:?] at com.sun.xml.ws.fault.SOAPFaultBuilder.createException(SOAPFaultBuilder.java:117) ~[jaxws-rt-2.3.3-b01.jar:2.3.3-b01] at com.sun.xml.ws.client.sei.StubHandler.readResponse(StubHandler.java:223) ~[jaxws-rt-2.3.3-b01.jar:2.3.3-b01] at com.sun.xml.ws.db.DatabindingImpl.deserializeResponse(DatabindingImpl.java:176) ~[jaxws-rt-2.3.3-b01.jar:2.3.3-b01] at com.sun.xml.ws.db.DatabindingImpl.deserializeResponse(DatabindingImpl.java:263) ~[jaxws-rt-2.3.3-b01.jar:2.3.3-b01] at com.sun.xml.ws.client.sei.SyncMethodHandler.invoke(SyncMethodHandler.java:89) ~[jaxws-rt-2.3.3-b01.jar:2.3.3-b01] at com.sun.xml.ws.client.sei.SyncMethodHandler.invoke(SyncMethodHandler.java:62) ~[jaxws-rt-2.3.3-b01.jar:2.3.3-b01] at com.sun.xml.ws.client.sei.SEIStub.invoke(SEIStub.java:131) ~[jaxws-rt-2.3.3-b01.jar:2.3.3-b01] at com.sun.proxy.$Proxy580.login(Unknown Source) ~[?:?] at com.emc.mystic.manager.commons.vmware.ServiceConnection.connect(ServiceConnection.java:140) ~[mystic.manager.commons.vmware-7.0.320.jar:?] at com.emc.mystic.manager.commons.vmware.ServiceUtil.clientConnect(ServiceUtil.java:82) ~[mystic.manager.commons.vmware-7.0.320.jar:?] at com.emc.mystic.manager.commons.vmware.VimServiceFactory.getServiceUtil(VimServiceFactory.java:51) ~[mystic.manager.commons.vmware-7.0.320.jar:?] at com.emc.mystic.manager.commons.vmware.VMwareClient.<init>(VMwareClient.java:108) ~[mystic.manager.commons.vmware-7.0.320.jar:?] at com.emc.mystic.manager.commons.vmware.service.VMwareServiceImpl.init(VMwareServiceImpl.java:174) ~[mystic.manager.commons.vmware-7.0.320.jar:?] at com.emc.mystic.manager.commons.vmware.service.VMwareServiceImpl.getNormalOperator(VMwareServiceImpl.java:79) ~[mystic.manager.commons.vmware-7.0.320.jar:?] at com.emc.mystic.manager.commons.vmware.service.VMwareServiceImpl.getVimOperator(VMwareServiceImpl.java:56) [mystic.manager.commons.vmware-7.0.320.jar:?] at com.emc.mystic.manager.commons.vmware.service.VMwareServiceImpl.getOperator(VMwareServiceImpl.java:46) [mystic.manager.commons.vmware-7.0.320.jar:?] at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:?] at jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:?] at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:?] at java.lang.reflect.Method.invoke(Method.java:566) ~[?:?] at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:344) [spring-aop-5.2.4.RELEASE.jar:5.2.4.RELEASE] at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:198) [spring-aop-5.2.4.RELEASE.jar:5.2.4.RELEASE] at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163) [spring-aop-5.2.4.RELEASE.jar:5.2.4.RELEASE] at org.springframework.aop.aspectj.MethodInvocationProceedingJoinPoint.proceed(MethodInvocationProceedingJoinPoint.java:88) [spring-aop-5.2.4.RELEASE.jar:5.2.4.RELEASE] at com.emc.mystic.manager.commons.emc.aspect.VMwareConnectAspect.checkVCPasswordAround(VMwareConnectAspect.java:68) [mystic.manager.commons.emc-7.0.320.jar:?] at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:?] at jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:?] at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:?] at java.lang.reflect.Method.invoke(Method.java:566) ~[?:?] at org.springframework.aop.aspectj.AbstractAspectJAdvice.invokeAdviceMethodWithGivenArgs(AbstractAspectJAdvice.java:644) [spring-aop-5.2.4.RELEASE.jar:5.2.4.RELEASE] at org.springframework.aop.aspectj.AbstractAspectJAdvice.invokeAdviceMethod(AbstractAspectJAdvice.java:633) [spring-aop-5.2.4.RELEASE.jar:5.2.4.RELEASE] at org.springframework.aop.aspectj.AspectJAroundAdvice.invoke(AspectJAroundAdvice.java:70) [spring-aop-5.2.4.RELEASE.jar:5.2.4.RELEASE] at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186) [spring-aop-5.2.4.RELEASE.jar:5.2.4.RELEASE] at org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:95) [spring-aop-5.2.4.RELEASE.jar:5.2.4.RELEASE] at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186) [spring-aop-5.2.4.RELEASE.jar:5.2.4.RELEASE] at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:212) [spring-aop-5.2.4.RELEASE.jar:5.2.4.RELEASE] at com.sun.proxy.$Proxy112.getOperator(Unknown Source) [?:?] at com.emc.mystic.manager.web.service.AuthorizationServiceImpl.keepVcConnectionAlive(AuthorizationServiceImpl.java:226) [classes/:?] at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:?] at jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:?] at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:?] at java.lang.reflect.Method.invoke(Method.java:566) ~[?:?] at org.springframework.scheduling.support.ScheduledMethodRunnable.run(ScheduledMethodRunnable.java:84) [spring-context-5.2.4.RELEASE.jar:5.2.4.RELEASE] at org.springframework.scheduling.support.DelegatingErrorHandlingRunnable.run(DelegatingErrorHandlingRunnable.java:54) [spring-context-5.2.4.RELEASE.jar:5.2.4.RELEASE] at java.util.concurrent.Executors$RunnableAdapter.c You will also see irratic behavior in the VxRail Manager plugin - most notably a number of Internal 500 errors. For example, this is what it looks like when trying to shut down a system with mismatched user accounts. Technical Details Lockbox VxRail manager uses a container called lockbox to store its credentials. You can see this container on the VxRail Manager with the following: vcluster202-vxm:~ # docker ps -a | grep -i lockbox | grep -v Exited 36d4e40e348b microservice/lockbox:1.8.10 \"/bin/bash entrypoin\u2026\" 29 minutes ago Up 29 minutes (healthy) 5000/tcp func_lockbox.1.pyeqo9oy4tvurnj1lackegz3y When VxRail Manager wants to retrieve credentials it uses the Python program lockbox_app/model/lockbox.py in lockbox and calls the following functions: def get_credentials(self, credential_names, show_password): return self.get_credentials_operation(credential_names, show_password) def get_credentials_operation(self, credential_names, show_password): res = self.check_lockbox_c_instance() if not res: raise Exception('No lockbox_c_instance in the memory') credential_name_arr = credential_names.split(',') credentials = list() for name in credential_name_arr: one_result = dict() one_result['credential_name'] = name item_value = LockboxCInterface.retrieve_item(self._lockbox_c_instance, name) if not item_value: msg = 'Error occurs when getting credential for {}'.format(name) logger.error(msg) raise Exception(msg) # To be compatible with python2 and python3 if isinstance(item_value, bytes): item_value = item_value.decode() if item_value == NON_EXIST: logger.info(\"credential for {} doesn't exist in the lockbox.\".format(name)) is_found = False else: is_found = True value_json = json.loads(item_value) if not show_password: value_json.pop('password', None) one_result.update(value_json) one_result['is_found'] = is_found credentials.append(one_result) return credentials Nginx To get to the lockbox app, the request must traverse Nginx. All API requests going to the VxRail manager first go through Nginx which acts as a load balancer. You can see these requests by following the Nginx container log. For example, here are the log entries associated with the shutdown shown above coming from the vCenter to VxRail: 192.168.1.20 - - 2022-08-31T17:20:40+00:00 \"HEAD /rest/vxm/v1/plugin HTTP/1.1\" 200 0 \"https://vcluster202-vcsa.demo.local/ui/vxrail/cluster/shutdown?locale=en_US\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36\" \"192.168.1.2\" 192.168.1.20 - - 2022-08-31T17:20:40+00:00 \"GET index.html HTTP/1.1\" 200 416 \"https://vcluster202-vcsa.demo.local/ui/vxrail/cluster/shutdown?locale=en_US\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36\" \"192.168.1.2\" 192.168.1.20 - - 2022-08-31T17:20:41+00:00 \"GET physical-view-app/runtime.7317203350b8216532b5.js HTTP/1.1\" 200 2863 \"https://vcluster202-vcsa.demo.local/ui/vxrail/remote/~192.168.1.19~443/cluster/shutdown?locale=en_US&__vxrail_epi=true&__vxrail_rdt=micro-apps\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36\" \"192.168.1.2\" 192.168.1.20 - - 2022-08-31T17:20:41+00:00 \"GET physical-view-app/styles.357deb6e147990d87b3d.css HTTP/1.1\" 200 3933 \"https://vcluster202-vcsa.demo.local/ui/vxrail/remote/~192.168.1.19~443/cluster/shutdown?locale=en_US&__vxrail_epi=true&__vxrail_rdt=micro-apps\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36\" \"192.168.1.2\" 192.168.1.20 - - 2022-08-31T17:20:41+00:00 \"GET physical-view-app/polyfills.8983567f117ff1baceed.js HTTP/1.1\" 200 219470 \"https://vcluster202-vcsa.demo.local/ui/vxrail/remote/~192.168.1.19~443/cluster/shutdown?locale=en_US&__vxrail_epi=true&__vxrail_rdt=micro-apps\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36\" \"192.168.1.2\" 192.168.1.20 - - 2022-08-31T17:20:41+00:00 \"GET physical-view-app/main.eb370d8d3ec93e2b77cf.js HTTP/1.1\" 200 862664 \"https://vcluster202-vcsa.demo.local/ui/vxrail/remote/~192.168.1.19~443/cluster/shutdown?locale=en_US&__vxrail_epi=true&__vxrail_rdt=micro-apps\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36\" \"192.168.1.2\" 192.168.1.20 - - 2022-08-31T17:20:41+00:00 \"GET physical-view-app/i18n-en-US-json.08405cc615e1a50f9a94.js HTTP/1.1\" 200 29884 \"https://vcluster202-vcsa.demo.local/ui/vxrail/remote/~192.168.1.19~443/cluster/shutdown?locale=en_US&__vxrail_epi=true&__vxrail_rdt=micro-apps\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36\" \"192.168.1.2\" 192.168.1.20 - - 2022-08-31T17:20:41+00:00 \"GET physical-view-app/17.0060c2f2429a52f4d47c.js HTTP/1.1\" 200 30805 \"https://vcluster202-vcsa.demo.local/ui/vxrail/remote/~192.168.1.19~443/cluster/shutdown?locale=en_US&__vxrail_epi=true&__vxrail_rdt=micro-apps\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36\" \"192.168.1.2\" 192.168.1.20 - - 2022-08-31T17:20:41+00:00 \"GET physical-view-app/theme-light.css HTTP/1.1\" 200 541510 \"https://vcluster202-vcsa.demo.local/ui/vxrail/remote/~192.168.1.19~443/cluster/shutdown?locale=en_US&__vxrail_epi=true&__vxrail_rdt=micro-apps\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36\" \"192.168.1.2\" 192.168.1.20 - - 2022-08-31T17:20:41+00:00 \"GET physical-view-app/1.b79e49bbc243cdf5c8e9.js HTTP/1.1\" 200 2822273 \"https://vcluster202-vcsa.demo.local/ui/vxrail/remote/~192.168.1.19~443/cluster/shutdown?locale=en_US&__vxrail_epi=true&__vxrail_rdt=micro-apps\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36\" \"192.168.1.2\" 192.168.1.20 - - 2022-08-31T17:20:52+00:00 \"POST /rest/vxm/v1/cluster/shutdown HTTP/1.1\" 202 95 \"https://vcluster202-vcsa.demo.local/ui/vxrail/remote/~192.168.1.19~443/cluster/shutdown?locale=en_US&__vxrail_epi=true&__vxrail_rdt=micro-apps\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36\" \"192.168.1.2\" 192.168.1.20 - - 2022-08-31T17:20:59+00:00 \"GET /rest/vxm/v1/requests/1cb807b9-749e-4a77-942f-67370c76a230 HTTP/1.1\" 200 511 \"https://vcluster202-vcsa.demo.local/ui/vxrail/remote/~192.168.1.19~443/cluster/shutdown?locale=en_US&__vxrail_epi=true&__vxrail_rdt=micro-apps\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36\" \"192.168.1.2\" 192.168.1.20 - - 2022-08-31T17:21:22+00:00 \"GET /rest/vxm/v1/requests/1cb807b9-749e-4a77-942f-67370c76a230 HTTP/1.1\" 200 535 \"https://vcluster202-vcsa.demo.local/ui/vxrail/remote/~192.168.1.19~443/cluster/shutdown?locale=en_US&__vxrail_epi=true&__vxrail_rdt=micro-apps\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36\" \"192.168.1.2\" 192.168.1.20 - - 2022-08-31T17:21:27+00:00 \"GET /rest/vxm/v1/requests/1cb807b9-749e-4a77-942f-67370c76a230 HTTP/1.1\" 200 538 \"https://vcluster202-vcsa.demo.local/ui/vxrail/remote/~192.168.1.19~443/cluster/shutdown?locale=en_US&__vxrail_epi=true&__vxrail_rdt=micro-apps\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36\" \"192.168.1.2\" 192.168.1.20 - - 2022-08-31T17:21:37+00:00 \"POST /rest/vxm/v1/cluster/shutdown HTTP/1.1\" 202 90 \"https://vcluster202-vcsa.demo.local/ui/vxrail/remote/~192.168.1.19~443/cluster/shutdown?locale=en_US&__vxrail_epi=true&__vxrail_rdt=micro-apps\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36\" \"192.168.1.2\" 192.168.1.20 - - 2022-08-31T17:21:41+00:00 \"POST /rest/vxm/v1/cluster/shutdown HTTP/1.1\" 202 94 \"https://vcluster202-vcsa.demo.local/ui/vxrail/remote/~192.168.1.19~443/cluster/shutdown?locale=en_US&__vxrail_epi=true&__vxrail_rdt=micro-apps\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36\" \"192.168.1.2\" 192.168.1.20 - - 2022-08-31T17:21:47+00:00 \"GET /rest/vxm/v1/requests/a1ee234c-bb54-408e-bbf3-8d5b038bf138 HTTP/1.1\" 200 512 \"https://vcluster202-vcsa.demo.local/ui/vxrail/remote/~192.168.1.19~443/cluster/shutdown?locale=en_US&__vxrail_epi=true&__vxrail_rdt=micro-apps\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36\" \"192.168.1.2\" 192.168.1.20 - - 2022-08-31T17:21:51+00:00 \"GET /rest/vxm/v1/requests/1cb807b9-749e-4a77-942f-67370c76a230 HTTP/1.1\" 200 535 \"https://vcluster202-vcsa.demo.local/ui/vxrail/remote/~192.168.1.19~443/cluster/shutdown?locale=en_US&__vxrail_epi=true&__vxrail_rdt=micro-apps\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36\" \"192.168.1.2\" 192.168.1.20 - - 2022-08-31T17:22:00+00:00 \"GET /rest/vxm/v1/requests/1cb807b9-749e-4a77-942f-67370c76a230 HTTP/1.1\" 200 538 \"https://vcluster202-vcsa.demo.local/ui/vxrail/remote/~192.168.1.19~443/cluster/shutdown?locale=en_US&__vxrail_epi=true&__vxrail_rdt=micro-apps\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36\" \"192.168.1.2\" 192.168.1.20 - - 2022-08-31T17:22:09+00:00 \"GET /rest/vxm/v1/requests/1cb807b9-749e-4a77-942f-67370c76a230 HTTP/1.1\" 200 516 \"https://vcluster202-vcsa.demo.local/ui/vxrail/remote/~192.168.1.19~443/cluster/shutdown?locale=en_US&__vxrail_epi=true&__vxrail_rdt=micro-apps\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36\" \"192.168.1.2\" 192.168.1.20 - - 2022-08-31T17:22:13+00:00 \"GET /rest/vxm/v1/requests/a1ee234c-bb54-408e-bbf3-8d5b038bf138 HTTP/1.1\" 200 512 \"https://vcluster202-vcsa.demo.local/ui/vxrail/remote/~192.168.1.19~443/cluster/shutdown?locale=en_US&__vxrail_epi=true&__vxrail_rdt=micro-apps\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36\" \"192.168.1.2\" 192.168.1.20 - - 2022-08-31T17:22:18+00:00 \"POST /rest/vxm/v1/cluster/shutdown HTTP/1.1\" 202 95 \"https://vcluster202-vcsa.demo.local/ui/vxrail/remote/~192.168.1.19~443/cluster/shutdown?locale=en_US&__vxrail_epi=true&__vxrail_rdt=micro-apps\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36\" \"192.168.1.2\" 192.168.1.20 - - 2022-08-31T17:22:25+00:00 \"GET /rest/vxm/v1/requests/c0691b5d-3ada-4000-a603-73229580e6d9 HTTP/1.1\" 200 513 \"https://vcluster202-vcsa.demo.local/ui/vxrail/remote/~192.168.1.19~443/cluster/shutdown?locale=en_US&__vxrail_epi=true&__vxrail_rdt=micro-apps\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36\" \"192.168.1.2\" 192.168.1.20 - - 2022-08-31T17:22:57+00:00 \"GET /rest/vxm/v1/requests/c0691b5d-3ada-4000-a603-73229580e6d9 HTTP/1.1\" 200 536 \"https://vcluster202-vcsa.demo.local/ui/vxrail/remote/~192.168.1.19~443/cluster/shutdown?locale=en_US&__vxrail_epi=true&__vxrail_rdt=micro-apps\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36\" \"192.168.1.2\" 192.168.1.20 - - 2022-08-31T17:23:08+00:00 \"GET /rest/vxm/v1/requests/a1ee234c-bb54-408e-bbf3-8d5b038bf138 HTTP/1.1\" 200 534 \"https://vcluster202-vcsa.demo.local/ui/vxrail/remote/~192.168.1.19~443/cluster/shutdown?locale=en_US&__vxrail_epi=true&__vxrail_rdt=micro-apps\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36\" \"192.168.1.2\" 192.168.1.20 - - 2022-08-31T17:23:19+00:00 \"GET /rest/vxm/v1/requests/a1ee234c-bb54-408e-bbf3-8d5b038bf138 HTTP/1.1\" 200 539 \"https://vcluster202-vcsa.demo.local/ui/vxrail/remote/~192.168.1.19~443/cluster/shutdown?locale=en_US&__vxrail_epi=true&__vxrail_rdt=micro-apps\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36\" \"192.168.1.2\" 192.168.1.20 - - 2022-08-31T17:23:27+00:00 \"GET /rest/vxm/v1/requests/a1ee234c-bb54-408e-bbf3-8d5b038bf138 HTTP/1.1\" 200 544 \"https://vcluster202-vcsa.demo.local/ui/vxrail/remote/~192.168.1.19~443/cluster/shutdown?locale=en_US&__vxrail_epi=true&__vxrail_rdt=micro-apps\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36\" \"192.168.1.2\" 192.168.1.20 - - 2022-08-31T17:23:30+00:00 \"GET /rest/vxm/v1/requests/c0691b5d-3ada-4000-a603-73229580e6d9 HTTP/1.1\" 200 541 \"https://vcluster202-vcsa.demo.local/ui/vxrail/remote/~192.168.1.19~443/cluster/shutdown?locale=en_US&__vxrail_epi=true&__vxrail_rdt=micro-apps\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36\" \"192.168.1.2\" 192.168.1.20 - - 2022-08-31T17:23:39+00:00 \"GET /rest/vxm/v1/requests/c0691b5d-3ada-4000-a603-73229580e6d9 HTTP/1.1\" 200 541 \"https://vcluster202-vcsa.demo.local/ui/vxrail/remote/~192.168.1.19~443/cluster/shutdown?locale=en_US&__vxrail_epi=true&__vxrail_rdt=micro-apps\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36\" \"192.168.1.2\" 192.168.1.20 - - 2022-08-31T17:23:50+00:00 \"GET /rest/vxm/v1/requests/c0691b5d-3ada-4000-a603-73229580e6d9 HTTP/1.1\" 200 541 \"https://vcluster202-vcsa.demo.local/ui/vxrail/remote/~192.168.1.19~443/cluster/shutdown?locale=en_US&__vxrail_epi=true&__vxrail_rdt=micro-apps\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36\" \"192.168.1.2\" 192.168.1.20 - - 2022-08-31T17:23:53+00:00 \"GET /rest/vxm/v1/requests/a1ee234c-bb54-408e-bbf3-8d5b038bf138 HTTP/1.1\" 200 535 \"https://vcluster202-vcsa.demo.local/ui/vxrail/remote/~192.168.1.19~443/cluster/shutdown?locale=en_US&__vxrail_epi=true&__vxrail_rdt=micro-apps\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36\" \"192.168.1.2\" 192.168.1.20 - - 2022-08-31T17:24:00+00:00 \"POST /rest/vxm/v1/cluster/shutdown HTTP/1.1\" 202 90 \"https://vcluster202-vcsa.demo.local/ui/vxrail/remote/~192.168.1.19~443/cluster/shutdown?locale=en_US&__vxrail_epi=true&__vxrail_rdt=micro-apps\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36\" \"192.168.1.2\" 192.168.1.20 - - 2022-08-31T17:24:08+00:00 \"GET /rest/vxm/v1/requests/c0691b5d-3ada-4000-a603-73229580e6d9 HTTP/1.1\" 200 539 \"https://vcluster202-vcsa.demo.local/ui/vxrail/remote/~192.168.1.19~443/cluster/shutdown?locale=en_US&__vxrail_epi=true&__vxrail_rdt=micro-apps\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36\" \"192.168.1.2\" 192.168.1.20 - - 2022-08-31T17:24:12+00:00 \"GET /rest/vxm/v1/requests/a1ee234c-bb54-408e-bbf3-8d5b038bf138 HTTP/1.1\" 200 535 \"https://vcluster202-vcsa.demo.local/ui/vxrail/remote/~192.168.1.19~443/cluster/shutdown?locale=en_US&__vxrail_epi=true&__vxrail_rdt=micro-apps\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36\" \"192.168.1.2\" 192.168.1.20 - - 2022-08-31T17:24:22+00:00 \"POST /rest/vxm/v1/cluster/shutdown HTTP/1.1\" 202 95 \"https://vcluster202-vcsa.demo.local/ui/vxrail/remote/~192.168.1.19~443/cluster/shutdown?locale=en_US&__vxrail_epi=true&__vxrail_rdt=micro-apps\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36\" \"192.168.1.2\" 192.168.1.20 - - 2022-08-31T17:24:29+00:00 \"GET /rest/vxm/v1/requests/3c17bfe3-19ae-494f-b2e5-62f4ff2b347c HTTP/1.1\" 200 516 \"https://vcluster202-vcsa.demo.local/ui/vxrail/remote/~192.168.1.19~443/cluster/shutdown?locale=en_US&__vxrail_epi=true&__vxrail_rdt=micro-apps\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36\" \"192.168.1.2\" 192.168.1.20 - - 2022-08-31T17:24:38+00:00 \"GET /rest/vxm/v1/requests/3c17bfe3-19ae-494f-b2e5-62f4ff2b347c HTTP/1.1\" 200 511 \"https://vcluster202-vcsa.demo.local/ui/vxrail/remote/~192.168.1.19~443/cluster/shutdown?locale=en_US&__vxrail_epi=true&__vxrail_rdt=micro-apps\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36\" \"192.168.1.2\" 192.168.1.20 - - 2022-08-31T17:24:42+00:00 \"GET /rest/vxm/v1/requests/3c17bfe3-19ae-494f-b2e5-62f4ff2b347c HTTP/1.1\" 200 516 \"https://vcluster202-vcsa.demo.local/ui/vxrail/remote/~192.168.1.19~443/cluster/shutdown?locale=en_US&__vxrail_epi=true&__vxrail_rdt=micro-apps\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36\" \"192.168.1.2\" 192.168.1.20 - - 2022-08-31T17:24:55+00:00 \"GET /rest/vxm/v1/requests/3c17bfe3-19ae-494f-b2e5-62f4ff2b347c HTTP/1.1\" 200 533 \"https://vcluster202-vcsa.demo.local/ui/vxrail/remote/~192.168.1.19~443/cluster/shutdown?locale=en_US&__vxrail_epi=true&__vxrail_rdt=micro-apps\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36\" \"192.168.1.2\" 192.168.1.20 - - 2022-08-31T17:25:07+00:00 \"GET /rest/vxm/v1/requests/3c17bfe3-19ae-494f-b2e5-62f4ff2b347c HTTP/1.1\" 200 541 \"https://vcluster202-vcsa.demo.local/ui/vxrail/remote/~192.168.1.19~443/cluster/shutdown?locale=en_US&__vxrail_epi=true&__vxrail_rdt=micro-apps\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36\" \"192.168.1.2\" 192.168.1.20 - - 2022-08-31T17:25:09+00:00 \"GET /rest/vxm/v1/requests/c0691b5d-3ada-4000-a603-73229580e6d9 HTTP/1.1\" 200 517 \"https://vcluster202-vcsa.demo.local/ui/vxrail/remote/~192.168.1.19~443/cluster/shutdown?locale=en_US&__vxrail_epi=true&__vxrail_rdt=micro-apps\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36\" \"192.168.1.2\" 192.168.1.20 - - 2022-08-31T17:25:12+00:00 \"GET /rest/vxm/v1/requests/a1ee234c-bb54-408e-bbf3-8d5b038bf138 HTTP/1.1\" 200 516 \"https://vcluster202-vcsa.demo.local/ui/vxrail/remote/~192.168.1.19~443/cluster/shutdown?locale=en_US&__vxrail_epi=true&__vxrail_rdt=micro-apps\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36\" \"192.168.1.2\" 192.168.1.20 - - 2022-08-31T17:25:23+00:00 \"GET /rest/vxm/v1/requests/3c17bfe3-19ae-494f-b2e5-62f4ff2b347c HTTP/1.1\" 200 540 \"https://vcluster202-vcsa.demo.local/ui/vxrail/remote/~192.168.1.19~443/cluster/shutdown?locale=en_US&__vxrail_epi=true&__vxrail_rdt=micro-apps\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36\" \"192.168.1.2\" 192.168.1.20 - - 2022-08-31T17:25:32+00:00 \"GET /rest/vxm/v1/requests/3c17bfe3-19ae-494f-b2e5-62f4ff2b347c HTTP/1.1\" 200 532 \"https://vcluster202-vcsa.demo.local/ui/vxrail/remote/~192.168.1.19~443/cluster/shutdown?locale=en_US&__vxrail_epi=true&__vxrail_rdt=micro-apps\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36\" \"192.168.1.2\" 192.168.1.20 - - 2022-08-31T17:25:55+00:00 \"GET /rest/vxm/v1/requests/3c17bfe3-19ae-494f-b2e5-62f4ff2b347c HTTP/1.1\" 200 511 \"https://vcluster202-vcsa.demo.local/ui/vxrail/remote/~192.168.1.19~443/cluster/shutdown?locale=en_US&__vxrail_epi=true&__vxrail_rdt=micro-apps\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36\" \"192.168.1.2\" Resolution You must make sure that the accounts match. vCenter Account for VxRail Manager Checking What Credentials are in Use The below will pull the credentials from the VxRail Manager keystore. You run this command on the command line in VxRail Manager. curl --unix-socket /var/lib/vxrail/nginx/socket/nginx.sock -X GET 'http://localhost/rest/vxm/internal/lockbox/v1/credentials?lockbox_name=SYSTEM&credential_names=management_account_vc'|jq vcluster202-vxm:~ # curl --unix-socket /var/lib/vxrail/nginx/socket/nginx.sock -X GET 'http://localhost/rest/vxm/internal/lockbox/v1/credentials?lockbox_name=SYSTEM&credential_names=management_account_vc'|jq % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 162 100 162 0 0 8526 0 --:--:-- --:--:-- --:--:-- 8526 { \"items\": [ { \"connection_type\": \"INVALID\", // Note: This is here because I deliberately broke the system. \"credential_name\": \"management_account_vc\", \"is_found\": true, \"password\": \"UEBzc3cwcmQxMjM0\", \"username\": \"management@localos\" } ] } Notice in the above that the password is base 64 encoded. You have to decode it with the following: vcluster202-vxm:~ # echo -n \"UEBzc3cwcmQxMjM0\" | base64 -d P@ssw0rd1234 In the above JSON output, credential_name refers to the name of the credential inside of the VxRail Manager lockbox program . The field username refers to the local account on vCenter. You will be able to see this account on vCenter by running cat /etc/passwd which shows all local users on the system. Note : Only accounts in the localos domain will show in /etc/passwd . Changing the Credentials To change the credentials in the VxRail Manager lockbox program use the following code on the VxRail Manager curl -X PUT --unix-socket /var/lib/vxrail/nginx/socket/nginx.sock -H \"accept: application/json\" -H \"Content-Type: application/json\" -d '{\"lockbox_name\":\"SYSTEM\",\"credentials\":[{\"credential_name\":\"management_account_vc\",\"username\":\"VC_management_account_user_name\",\"password\":\"encoded_password\"}]}' 'http://localhost/rest/vxm/internal/lockbox/v1/credentials' In the above where it says VC_management_account_user_name you will replace that with whatever is in username in your JSON output. In my case that is management@localos and you must replace encoded_password with YOUR base64 encoded password. In my case, I changed the password on vCenter to This.is.a.great.password1 which is why the system isn't currently working. So we have to base64 encode that and update in lockbox. vcluster202-vxm:~ # echo -n \"This.is.a.great.password1\" | base64 VGhpcy5pcy5hLmdyZWF0LnBhc3N3b3JkMQ== Next we have to update it with: curl -X PUT --unix-socket /var/lib/vxrail/nginx/socket/nginx.sock -H \"accept: application/json\" -H \"Content-Type: application/json\" -d '{\"lockbox_name\":\"SYSTEM\",\"credentials\":[{\"credential_name\":\"management_account_vc\",\"username\":\"management@localos\",\"password\":\"VGhpcy5pcy5hLmdyZWF0LnBhc3N3b3JkMQ==\"}]}' 'http://localhost/rest/vxm/internal/lockbox/v1/credentials' Now we need to restart the lockbox service. You can do this by rebooting the VxRail Manager or you can simply restart the lockbox container with docker service scale func_lockbox=0 && docker service scale func_lockbox=1 . Setting the Credentials on vCenter On the vCenter side, use passwd <vCenter account name> to reset the password on vCenter. After you change it, restart vCenter to force the change. ESXi Credentials The same problem exists with the VxRail Manager and the individual ESXi hosts. Repeat the same process as above for the management account for each ESXi host. For example: curl --unix-socket /var/lib/vxrail/nginx/socket/nginx.sock -X GET 'http://localhost/rest/vxm/internal/lockbox/v1/credentials?lockbox_name=SYSTEM&credential_names=management_account_esxi__<host-SN>'| jq where host-SN is the service tag for the server. This will return an output like: { \"items\": [ { \"credential_name\": \"management_account_esxi__V015001\", \"is_found\": true, \"password\": \"BASE64_ENCODED_PASSWORD\", \"username\": \"management\" } ] } As before you will have to update the password with: curl -X PUT --unix-socket /var/lib/vxrail/nginx/socket/nginx.sock -H \"accept: application/json\" -H \"Content-Type: application/json\" -d '{\"lockbox_name\":\"SYSTEM\",\"credentials\":[{\"credential_name\":\"management_account_esxi__<host-SN>\",\"username\":\"ESXi_management_account_user_name\",\"password\":\"encoded_password\"}]}' 'http://localhost/rest/vxm/internal/lockbox/v1/credentials' where <host-sn> is your service tag, ESXi_management_account_user_name is the name of your ESXi management account from above, and encoded password is your password base64 encoded with echo -n \"PASSWORD\" | base64 . Ensure that the password on ESXi matches by browsing to Manage->Security & Users->Users on the ESXi management interface: Click Edit User and then change the password to match whatever you used above.","title":"Change User on VxRail Plugin"},{"location":"VMWare/Change%20User%20on%20VxRail%20Plugin/#change-user-on-vxrail-plugin","text":"","title":"Change User on VxRail Plugin"},{"location":"VMWare/Change%20User%20on%20VxRail%20Plugin/#background","text":"The VxRail Manager VM must have the user credentials for both vCenter and the various ESXi nodes stored in its credential database, a container called lockbox, in order to work properly. These service accounts exist on vCenter and the local ESXi nodes so that VxRail manager can communicate with their APIs. The management account for vCenter is set during setup and for the ESXi nodes defaults to management_account_esxi_<serial_number> .","title":"Background"},{"location":"VMWare/Change%20User%20on%20VxRail%20Plugin/#problem","text":"If the credentials vCenter uses for its VxRail Manager account do not match the account on the VxRail manager itself or the account the ESXi instances have do not match what is on VxRail manager all API calls from the VxRail plugin in vCenter will fail. You will see the following in the VxRail manager /var/log/mystic/web.log 2022-08-31T16:58:37.056+0000 ERROR [myScheduler-5] com.emc.mystic.manager.commons.vmware.service.VMwareServiceImpl VMwareServiceImpl.getVimOperator:60 - Invalid login to VMware com.vmware.vim25.InvalidLoginFaultMsg: Cannot complete login due to an incorrect user name or password. at jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:?] at jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) ~[?:?] at jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:?] at java.lang.reflect.Constructor.newInstance(Constructor.java:490) ~[?:?] at com.sun.xml.ws.fault.SOAPFaultBuilder.createException(SOAPFaultBuilder.java:117) ~[jaxws-rt-2.3.3-b01.jar:2.3.3-b01] at com.sun.xml.ws.client.sei.StubHandler.readResponse(StubHandler.java:223) ~[jaxws-rt-2.3.3-b01.jar:2.3.3-b01] at com.sun.xml.ws.db.DatabindingImpl.deserializeResponse(DatabindingImpl.java:176) ~[jaxws-rt-2.3.3-b01.jar:2.3.3-b01] at com.sun.xml.ws.db.DatabindingImpl.deserializeResponse(DatabindingImpl.java:263) ~[jaxws-rt-2.3.3-b01.jar:2.3.3-b01] at com.sun.xml.ws.client.sei.SyncMethodHandler.invoke(SyncMethodHandler.java:89) ~[jaxws-rt-2.3.3-b01.jar:2.3.3-b01] at com.sun.xml.ws.client.sei.SyncMethodHandler.invoke(SyncMethodHandler.java:62) ~[jaxws-rt-2.3.3-b01.jar:2.3.3-b01] at com.sun.xml.ws.client.sei.SEIStub.invoke(SEIStub.java:131) ~[jaxws-rt-2.3.3-b01.jar:2.3.3-b01] at com.sun.proxy.$Proxy580.login(Unknown Source) ~[?:?] at com.emc.mystic.manager.commons.vmware.ServiceConnection.connect(ServiceConnection.java:140) ~[mystic.manager.commons.vmware-7.0.320.jar:?] at com.emc.mystic.manager.commons.vmware.ServiceUtil.clientConnect(ServiceUtil.java:82) ~[mystic.manager.commons.vmware-7.0.320.jar:?] at com.emc.mystic.manager.commons.vmware.VimServiceFactory.getServiceUtil(VimServiceFactory.java:51) ~[mystic.manager.commons.vmware-7.0.320.jar:?] at com.emc.mystic.manager.commons.vmware.VMwareClient.<init>(VMwareClient.java:108) ~[mystic.manager.commons.vmware-7.0.320.jar:?] at com.emc.mystic.manager.commons.vmware.service.VMwareServiceImpl.init(VMwareServiceImpl.java:174) ~[mystic.manager.commons.vmware-7.0.320.jar:?] at com.emc.mystic.manager.commons.vmware.service.VMwareServiceImpl.getNormalOperator(VMwareServiceImpl.java:79) ~[mystic.manager.commons.vmware-7.0.320.jar:?] at com.emc.mystic.manager.commons.vmware.service.VMwareServiceImpl.getVimOperator(VMwareServiceImpl.java:56) [mystic.manager.commons.vmware-7.0.320.jar:?] at com.emc.mystic.manager.commons.vmware.service.VMwareServiceImpl.getOperator(VMwareServiceImpl.java:46) [mystic.manager.commons.vmware-7.0.320.jar:?] at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:?] at jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:?] at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:?] at java.lang.reflect.Method.invoke(Method.java:566) ~[?:?] at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:344) [spring-aop-5.2.4.RELEASE.jar:5.2.4.RELEASE] at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:198) [spring-aop-5.2.4.RELEASE.jar:5.2.4.RELEASE] at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163) [spring-aop-5.2.4.RELEASE.jar:5.2.4.RELEASE] at org.springframework.aop.aspectj.MethodInvocationProceedingJoinPoint.proceed(MethodInvocationProceedingJoinPoint.java:88) [spring-aop-5.2.4.RELEASE.jar:5.2.4.RELEASE] at com.emc.mystic.manager.commons.emc.aspect.VMwareConnectAspect.checkVCPasswordAround(VMwareConnectAspect.java:68) [mystic.manager.commons.emc-7.0.320.jar:?] at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:?] at jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:?] at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:?] at java.lang.reflect.Method.invoke(Method.java:566) ~[?:?] at org.springframework.aop.aspectj.AbstractAspectJAdvice.invokeAdviceMethodWithGivenArgs(AbstractAspectJAdvice.java:644) [spring-aop-5.2.4.RELEASE.jar:5.2.4.RELEASE] at org.springframework.aop.aspectj.AbstractAspectJAdvice.invokeAdviceMethod(AbstractAspectJAdvice.java:633) [spring-aop-5.2.4.RELEASE.jar:5.2.4.RELEASE] at org.springframework.aop.aspectj.AspectJAroundAdvice.invoke(AspectJAroundAdvice.java:70) [spring-aop-5.2.4.RELEASE.jar:5.2.4.RELEASE] at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186) [spring-aop-5.2.4.RELEASE.jar:5.2.4.RELEASE] at org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:95) [spring-aop-5.2.4.RELEASE.jar:5.2.4.RELEASE] at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186) [spring-aop-5.2.4.RELEASE.jar:5.2.4.RELEASE] at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:212) [spring-aop-5.2.4.RELEASE.jar:5.2.4.RELEASE] at com.sun.proxy.$Proxy112.getOperator(Unknown Source) [?:?] at com.emc.mystic.manager.web.service.AuthorizationServiceImpl.keepVcConnectionAlive(AuthorizationServiceImpl.java:226) [classes/:?] at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:?] at jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:?] at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:?] at java.lang.reflect.Method.invoke(Method.java:566) ~[?:?] at org.springframework.scheduling.support.ScheduledMethodRunnable.run(ScheduledMethodRunnable.java:84) [spring-context-5.2.4.RELEASE.jar:5.2.4.RELEASE] at org.springframework.scheduling.support.DelegatingErrorHandlingRunnable.run(DelegatingErrorHandlingRunnable.java:54) [spring-context-5.2.4.RELEASE.jar:5.2.4.RELEASE] at java.util.concurrent.Executors$RunnableAdapter.c You will also see irratic behavior in the VxRail Manager plugin - most notably a number of Internal 500 errors. For example, this is what it looks like when trying to shut down a system with mismatched user accounts.","title":"Problem"},{"location":"VMWare/Change%20User%20on%20VxRail%20Plugin/#technical-details","text":"","title":"Technical Details"},{"location":"VMWare/Change%20User%20on%20VxRail%20Plugin/#lockbox","text":"VxRail manager uses a container called lockbox to store its credentials. You can see this container on the VxRail Manager with the following: vcluster202-vxm:~ # docker ps -a | grep -i lockbox | grep -v Exited 36d4e40e348b microservice/lockbox:1.8.10 \"/bin/bash entrypoin\u2026\" 29 minutes ago Up 29 minutes (healthy) 5000/tcp func_lockbox.1.pyeqo9oy4tvurnj1lackegz3y When VxRail Manager wants to retrieve credentials it uses the Python program lockbox_app/model/lockbox.py in lockbox and calls the following functions: def get_credentials(self, credential_names, show_password): return self.get_credentials_operation(credential_names, show_password) def get_credentials_operation(self, credential_names, show_password): res = self.check_lockbox_c_instance() if not res: raise Exception('No lockbox_c_instance in the memory') credential_name_arr = credential_names.split(',') credentials = list() for name in credential_name_arr: one_result = dict() one_result['credential_name'] = name item_value = LockboxCInterface.retrieve_item(self._lockbox_c_instance, name) if not item_value: msg = 'Error occurs when getting credential for {}'.format(name) logger.error(msg) raise Exception(msg) # To be compatible with python2 and python3 if isinstance(item_value, bytes): item_value = item_value.decode() if item_value == NON_EXIST: logger.info(\"credential for {} doesn't exist in the lockbox.\".format(name)) is_found = False else: is_found = True value_json = json.loads(item_value) if not show_password: value_json.pop('password', None) one_result.update(value_json) one_result['is_found'] = is_found credentials.append(one_result) return credentials","title":"Lockbox"},{"location":"VMWare/Change%20User%20on%20VxRail%20Plugin/#nginx","text":"To get to the lockbox app, the request must traverse Nginx. All API requests going to the VxRail manager first go through Nginx which acts as a load balancer. You can see these requests by following the Nginx container log. For example, here are the log entries associated with the shutdown shown above coming from the vCenter to VxRail: 192.168.1.20 - - 2022-08-31T17:20:40+00:00 \"HEAD /rest/vxm/v1/plugin HTTP/1.1\" 200 0 \"https://vcluster202-vcsa.demo.local/ui/vxrail/cluster/shutdown?locale=en_US\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36\" \"192.168.1.2\" 192.168.1.20 - - 2022-08-31T17:20:40+00:00 \"GET index.html HTTP/1.1\" 200 416 \"https://vcluster202-vcsa.demo.local/ui/vxrail/cluster/shutdown?locale=en_US\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36\" \"192.168.1.2\" 192.168.1.20 - - 2022-08-31T17:20:41+00:00 \"GET physical-view-app/runtime.7317203350b8216532b5.js HTTP/1.1\" 200 2863 \"https://vcluster202-vcsa.demo.local/ui/vxrail/remote/~192.168.1.19~443/cluster/shutdown?locale=en_US&__vxrail_epi=true&__vxrail_rdt=micro-apps\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36\" \"192.168.1.2\" 192.168.1.20 - - 2022-08-31T17:20:41+00:00 \"GET physical-view-app/styles.357deb6e147990d87b3d.css HTTP/1.1\" 200 3933 \"https://vcluster202-vcsa.demo.local/ui/vxrail/remote/~192.168.1.19~443/cluster/shutdown?locale=en_US&__vxrail_epi=true&__vxrail_rdt=micro-apps\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36\" \"192.168.1.2\" 192.168.1.20 - - 2022-08-31T17:20:41+00:00 \"GET physical-view-app/polyfills.8983567f117ff1baceed.js HTTP/1.1\" 200 219470 \"https://vcluster202-vcsa.demo.local/ui/vxrail/remote/~192.168.1.19~443/cluster/shutdown?locale=en_US&__vxrail_epi=true&__vxrail_rdt=micro-apps\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36\" \"192.168.1.2\" 192.168.1.20 - - 2022-08-31T17:20:41+00:00 \"GET physical-view-app/main.eb370d8d3ec93e2b77cf.js HTTP/1.1\" 200 862664 \"https://vcluster202-vcsa.demo.local/ui/vxrail/remote/~192.168.1.19~443/cluster/shutdown?locale=en_US&__vxrail_epi=true&__vxrail_rdt=micro-apps\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36\" \"192.168.1.2\" 192.168.1.20 - - 2022-08-31T17:20:41+00:00 \"GET physical-view-app/i18n-en-US-json.08405cc615e1a50f9a94.js HTTP/1.1\" 200 29884 \"https://vcluster202-vcsa.demo.local/ui/vxrail/remote/~192.168.1.19~443/cluster/shutdown?locale=en_US&__vxrail_epi=true&__vxrail_rdt=micro-apps\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36\" \"192.168.1.2\" 192.168.1.20 - - 2022-08-31T17:20:41+00:00 \"GET physical-view-app/17.0060c2f2429a52f4d47c.js HTTP/1.1\" 200 30805 \"https://vcluster202-vcsa.demo.local/ui/vxrail/remote/~192.168.1.19~443/cluster/shutdown?locale=en_US&__vxrail_epi=true&__vxrail_rdt=micro-apps\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36\" \"192.168.1.2\" 192.168.1.20 - - 2022-08-31T17:20:41+00:00 \"GET physical-view-app/theme-light.css HTTP/1.1\" 200 541510 \"https://vcluster202-vcsa.demo.local/ui/vxrail/remote/~192.168.1.19~443/cluster/shutdown?locale=en_US&__vxrail_epi=true&__vxrail_rdt=micro-apps\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36\" \"192.168.1.2\" 192.168.1.20 - - 2022-08-31T17:20:41+00:00 \"GET physical-view-app/1.b79e49bbc243cdf5c8e9.js HTTP/1.1\" 200 2822273 \"https://vcluster202-vcsa.demo.local/ui/vxrail/remote/~192.168.1.19~443/cluster/shutdown?locale=en_US&__vxrail_epi=true&__vxrail_rdt=micro-apps\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36\" \"192.168.1.2\" 192.168.1.20 - - 2022-08-31T17:20:52+00:00 \"POST /rest/vxm/v1/cluster/shutdown HTTP/1.1\" 202 95 \"https://vcluster202-vcsa.demo.local/ui/vxrail/remote/~192.168.1.19~443/cluster/shutdown?locale=en_US&__vxrail_epi=true&__vxrail_rdt=micro-apps\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36\" \"192.168.1.2\" 192.168.1.20 - - 2022-08-31T17:20:59+00:00 \"GET /rest/vxm/v1/requests/1cb807b9-749e-4a77-942f-67370c76a230 HTTP/1.1\" 200 511 \"https://vcluster202-vcsa.demo.local/ui/vxrail/remote/~192.168.1.19~443/cluster/shutdown?locale=en_US&__vxrail_epi=true&__vxrail_rdt=micro-apps\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36\" \"192.168.1.2\" 192.168.1.20 - - 2022-08-31T17:21:22+00:00 \"GET /rest/vxm/v1/requests/1cb807b9-749e-4a77-942f-67370c76a230 HTTP/1.1\" 200 535 \"https://vcluster202-vcsa.demo.local/ui/vxrail/remote/~192.168.1.19~443/cluster/shutdown?locale=en_US&__vxrail_epi=true&__vxrail_rdt=micro-apps\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36\" \"192.168.1.2\" 192.168.1.20 - - 2022-08-31T17:21:27+00:00 \"GET /rest/vxm/v1/requests/1cb807b9-749e-4a77-942f-67370c76a230 HTTP/1.1\" 200 538 \"https://vcluster202-vcsa.demo.local/ui/vxrail/remote/~192.168.1.19~443/cluster/shutdown?locale=en_US&__vxrail_epi=true&__vxrail_rdt=micro-apps\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36\" \"192.168.1.2\" 192.168.1.20 - - 2022-08-31T17:21:37+00:00 \"POST /rest/vxm/v1/cluster/shutdown HTTP/1.1\" 202 90 \"https://vcluster202-vcsa.demo.local/ui/vxrail/remote/~192.168.1.19~443/cluster/shutdown?locale=en_US&__vxrail_epi=true&__vxrail_rdt=micro-apps\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36\" \"192.168.1.2\" 192.168.1.20 - - 2022-08-31T17:21:41+00:00 \"POST /rest/vxm/v1/cluster/shutdown HTTP/1.1\" 202 94 \"https://vcluster202-vcsa.demo.local/ui/vxrail/remote/~192.168.1.19~443/cluster/shutdown?locale=en_US&__vxrail_epi=true&__vxrail_rdt=micro-apps\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36\" \"192.168.1.2\" 192.168.1.20 - - 2022-08-31T17:21:47+00:00 \"GET /rest/vxm/v1/requests/a1ee234c-bb54-408e-bbf3-8d5b038bf138 HTTP/1.1\" 200 512 \"https://vcluster202-vcsa.demo.local/ui/vxrail/remote/~192.168.1.19~443/cluster/shutdown?locale=en_US&__vxrail_epi=true&__vxrail_rdt=micro-apps\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36\" \"192.168.1.2\" 192.168.1.20 - - 2022-08-31T17:21:51+00:00 \"GET /rest/vxm/v1/requests/1cb807b9-749e-4a77-942f-67370c76a230 HTTP/1.1\" 200 535 \"https://vcluster202-vcsa.demo.local/ui/vxrail/remote/~192.168.1.19~443/cluster/shutdown?locale=en_US&__vxrail_epi=true&__vxrail_rdt=micro-apps\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36\" \"192.168.1.2\" 192.168.1.20 - - 2022-08-31T17:22:00+00:00 \"GET /rest/vxm/v1/requests/1cb807b9-749e-4a77-942f-67370c76a230 HTTP/1.1\" 200 538 \"https://vcluster202-vcsa.demo.local/ui/vxrail/remote/~192.168.1.19~443/cluster/shutdown?locale=en_US&__vxrail_epi=true&__vxrail_rdt=micro-apps\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36\" \"192.168.1.2\" 192.168.1.20 - - 2022-08-31T17:22:09+00:00 \"GET /rest/vxm/v1/requests/1cb807b9-749e-4a77-942f-67370c76a230 HTTP/1.1\" 200 516 \"https://vcluster202-vcsa.demo.local/ui/vxrail/remote/~192.168.1.19~443/cluster/shutdown?locale=en_US&__vxrail_epi=true&__vxrail_rdt=micro-apps\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36\" \"192.168.1.2\" 192.168.1.20 - - 2022-08-31T17:22:13+00:00 \"GET /rest/vxm/v1/requests/a1ee234c-bb54-408e-bbf3-8d5b038bf138 HTTP/1.1\" 200 512 \"https://vcluster202-vcsa.demo.local/ui/vxrail/remote/~192.168.1.19~443/cluster/shutdown?locale=en_US&__vxrail_epi=true&__vxrail_rdt=micro-apps\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36\" \"192.168.1.2\" 192.168.1.20 - - 2022-08-31T17:22:18+00:00 \"POST /rest/vxm/v1/cluster/shutdown HTTP/1.1\" 202 95 \"https://vcluster202-vcsa.demo.local/ui/vxrail/remote/~192.168.1.19~443/cluster/shutdown?locale=en_US&__vxrail_epi=true&__vxrail_rdt=micro-apps\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36\" \"192.168.1.2\" 192.168.1.20 - - 2022-08-31T17:22:25+00:00 \"GET /rest/vxm/v1/requests/c0691b5d-3ada-4000-a603-73229580e6d9 HTTP/1.1\" 200 513 \"https://vcluster202-vcsa.demo.local/ui/vxrail/remote/~192.168.1.19~443/cluster/shutdown?locale=en_US&__vxrail_epi=true&__vxrail_rdt=micro-apps\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36\" \"192.168.1.2\" 192.168.1.20 - - 2022-08-31T17:22:57+00:00 \"GET /rest/vxm/v1/requests/c0691b5d-3ada-4000-a603-73229580e6d9 HTTP/1.1\" 200 536 \"https://vcluster202-vcsa.demo.local/ui/vxrail/remote/~192.168.1.19~443/cluster/shutdown?locale=en_US&__vxrail_epi=true&__vxrail_rdt=micro-apps\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36\" \"192.168.1.2\" 192.168.1.20 - - 2022-08-31T17:23:08+00:00 \"GET /rest/vxm/v1/requests/a1ee234c-bb54-408e-bbf3-8d5b038bf138 HTTP/1.1\" 200 534 \"https://vcluster202-vcsa.demo.local/ui/vxrail/remote/~192.168.1.19~443/cluster/shutdown?locale=en_US&__vxrail_epi=true&__vxrail_rdt=micro-apps\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36\" \"192.168.1.2\" 192.168.1.20 - - 2022-08-31T17:23:19+00:00 \"GET /rest/vxm/v1/requests/a1ee234c-bb54-408e-bbf3-8d5b038bf138 HTTP/1.1\" 200 539 \"https://vcluster202-vcsa.demo.local/ui/vxrail/remote/~192.168.1.19~443/cluster/shutdown?locale=en_US&__vxrail_epi=true&__vxrail_rdt=micro-apps\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36\" \"192.168.1.2\" 192.168.1.20 - - 2022-08-31T17:23:27+00:00 \"GET /rest/vxm/v1/requests/a1ee234c-bb54-408e-bbf3-8d5b038bf138 HTTP/1.1\" 200 544 \"https://vcluster202-vcsa.demo.local/ui/vxrail/remote/~192.168.1.19~443/cluster/shutdown?locale=en_US&__vxrail_epi=true&__vxrail_rdt=micro-apps\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36\" \"192.168.1.2\" 192.168.1.20 - - 2022-08-31T17:23:30+00:00 \"GET /rest/vxm/v1/requests/c0691b5d-3ada-4000-a603-73229580e6d9 HTTP/1.1\" 200 541 \"https://vcluster202-vcsa.demo.local/ui/vxrail/remote/~192.168.1.19~443/cluster/shutdown?locale=en_US&__vxrail_epi=true&__vxrail_rdt=micro-apps\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36\" \"192.168.1.2\" 192.168.1.20 - - 2022-08-31T17:23:39+00:00 \"GET /rest/vxm/v1/requests/c0691b5d-3ada-4000-a603-73229580e6d9 HTTP/1.1\" 200 541 \"https://vcluster202-vcsa.demo.local/ui/vxrail/remote/~192.168.1.19~443/cluster/shutdown?locale=en_US&__vxrail_epi=true&__vxrail_rdt=micro-apps\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36\" \"192.168.1.2\" 192.168.1.20 - - 2022-08-31T17:23:50+00:00 \"GET /rest/vxm/v1/requests/c0691b5d-3ada-4000-a603-73229580e6d9 HTTP/1.1\" 200 541 \"https://vcluster202-vcsa.demo.local/ui/vxrail/remote/~192.168.1.19~443/cluster/shutdown?locale=en_US&__vxrail_epi=true&__vxrail_rdt=micro-apps\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36\" \"192.168.1.2\" 192.168.1.20 - - 2022-08-31T17:23:53+00:00 \"GET /rest/vxm/v1/requests/a1ee234c-bb54-408e-bbf3-8d5b038bf138 HTTP/1.1\" 200 535 \"https://vcluster202-vcsa.demo.local/ui/vxrail/remote/~192.168.1.19~443/cluster/shutdown?locale=en_US&__vxrail_epi=true&__vxrail_rdt=micro-apps\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36\" \"192.168.1.2\" 192.168.1.20 - - 2022-08-31T17:24:00+00:00 \"POST /rest/vxm/v1/cluster/shutdown HTTP/1.1\" 202 90 \"https://vcluster202-vcsa.demo.local/ui/vxrail/remote/~192.168.1.19~443/cluster/shutdown?locale=en_US&__vxrail_epi=true&__vxrail_rdt=micro-apps\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36\" \"192.168.1.2\" 192.168.1.20 - - 2022-08-31T17:24:08+00:00 \"GET /rest/vxm/v1/requests/c0691b5d-3ada-4000-a603-73229580e6d9 HTTP/1.1\" 200 539 \"https://vcluster202-vcsa.demo.local/ui/vxrail/remote/~192.168.1.19~443/cluster/shutdown?locale=en_US&__vxrail_epi=true&__vxrail_rdt=micro-apps\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36\" \"192.168.1.2\" 192.168.1.20 - - 2022-08-31T17:24:12+00:00 \"GET /rest/vxm/v1/requests/a1ee234c-bb54-408e-bbf3-8d5b038bf138 HTTP/1.1\" 200 535 \"https://vcluster202-vcsa.demo.local/ui/vxrail/remote/~192.168.1.19~443/cluster/shutdown?locale=en_US&__vxrail_epi=true&__vxrail_rdt=micro-apps\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36\" \"192.168.1.2\" 192.168.1.20 - - 2022-08-31T17:24:22+00:00 \"POST /rest/vxm/v1/cluster/shutdown HTTP/1.1\" 202 95 \"https://vcluster202-vcsa.demo.local/ui/vxrail/remote/~192.168.1.19~443/cluster/shutdown?locale=en_US&__vxrail_epi=true&__vxrail_rdt=micro-apps\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36\" \"192.168.1.2\" 192.168.1.20 - - 2022-08-31T17:24:29+00:00 \"GET /rest/vxm/v1/requests/3c17bfe3-19ae-494f-b2e5-62f4ff2b347c HTTP/1.1\" 200 516 \"https://vcluster202-vcsa.demo.local/ui/vxrail/remote/~192.168.1.19~443/cluster/shutdown?locale=en_US&__vxrail_epi=true&__vxrail_rdt=micro-apps\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36\" \"192.168.1.2\" 192.168.1.20 - - 2022-08-31T17:24:38+00:00 \"GET /rest/vxm/v1/requests/3c17bfe3-19ae-494f-b2e5-62f4ff2b347c HTTP/1.1\" 200 511 \"https://vcluster202-vcsa.demo.local/ui/vxrail/remote/~192.168.1.19~443/cluster/shutdown?locale=en_US&__vxrail_epi=true&__vxrail_rdt=micro-apps\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36\" \"192.168.1.2\" 192.168.1.20 - - 2022-08-31T17:24:42+00:00 \"GET /rest/vxm/v1/requests/3c17bfe3-19ae-494f-b2e5-62f4ff2b347c HTTP/1.1\" 200 516 \"https://vcluster202-vcsa.demo.local/ui/vxrail/remote/~192.168.1.19~443/cluster/shutdown?locale=en_US&__vxrail_epi=true&__vxrail_rdt=micro-apps\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36\" \"192.168.1.2\" 192.168.1.20 - - 2022-08-31T17:24:55+00:00 \"GET /rest/vxm/v1/requests/3c17bfe3-19ae-494f-b2e5-62f4ff2b347c HTTP/1.1\" 200 533 \"https://vcluster202-vcsa.demo.local/ui/vxrail/remote/~192.168.1.19~443/cluster/shutdown?locale=en_US&__vxrail_epi=true&__vxrail_rdt=micro-apps\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36\" \"192.168.1.2\" 192.168.1.20 - - 2022-08-31T17:25:07+00:00 \"GET /rest/vxm/v1/requests/3c17bfe3-19ae-494f-b2e5-62f4ff2b347c HTTP/1.1\" 200 541 \"https://vcluster202-vcsa.demo.local/ui/vxrail/remote/~192.168.1.19~443/cluster/shutdown?locale=en_US&__vxrail_epi=true&__vxrail_rdt=micro-apps\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36\" \"192.168.1.2\" 192.168.1.20 - - 2022-08-31T17:25:09+00:00 \"GET /rest/vxm/v1/requests/c0691b5d-3ada-4000-a603-73229580e6d9 HTTP/1.1\" 200 517 \"https://vcluster202-vcsa.demo.local/ui/vxrail/remote/~192.168.1.19~443/cluster/shutdown?locale=en_US&__vxrail_epi=true&__vxrail_rdt=micro-apps\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36\" \"192.168.1.2\" 192.168.1.20 - - 2022-08-31T17:25:12+00:00 \"GET /rest/vxm/v1/requests/a1ee234c-bb54-408e-bbf3-8d5b038bf138 HTTP/1.1\" 200 516 \"https://vcluster202-vcsa.demo.local/ui/vxrail/remote/~192.168.1.19~443/cluster/shutdown?locale=en_US&__vxrail_epi=true&__vxrail_rdt=micro-apps\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36\" \"192.168.1.2\" 192.168.1.20 - - 2022-08-31T17:25:23+00:00 \"GET /rest/vxm/v1/requests/3c17bfe3-19ae-494f-b2e5-62f4ff2b347c HTTP/1.1\" 200 540 \"https://vcluster202-vcsa.demo.local/ui/vxrail/remote/~192.168.1.19~443/cluster/shutdown?locale=en_US&__vxrail_epi=true&__vxrail_rdt=micro-apps\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36\" \"192.168.1.2\" 192.168.1.20 - - 2022-08-31T17:25:32+00:00 \"GET /rest/vxm/v1/requests/3c17bfe3-19ae-494f-b2e5-62f4ff2b347c HTTP/1.1\" 200 532 \"https://vcluster202-vcsa.demo.local/ui/vxrail/remote/~192.168.1.19~443/cluster/shutdown?locale=en_US&__vxrail_epi=true&__vxrail_rdt=micro-apps\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36\" \"192.168.1.2\" 192.168.1.20 - - 2022-08-31T17:25:55+00:00 \"GET /rest/vxm/v1/requests/3c17bfe3-19ae-494f-b2e5-62f4ff2b347c HTTP/1.1\" 200 511 \"https://vcluster202-vcsa.demo.local/ui/vxrail/remote/~192.168.1.19~443/cluster/shutdown?locale=en_US&__vxrail_epi=true&__vxrail_rdt=micro-apps\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36\" \"192.168.1.2\"","title":"Nginx"},{"location":"VMWare/Change%20User%20on%20VxRail%20Plugin/#resolution","text":"You must make sure that the accounts match.","title":"Resolution"},{"location":"VMWare/Change%20User%20on%20VxRail%20Plugin/#vcenter-account-for-vxrail-manager","text":"","title":"vCenter Account for VxRail Manager"},{"location":"VMWare/Change%20User%20on%20VxRail%20Plugin/#checking-what-credentials-are-in-use","text":"The below will pull the credentials from the VxRail Manager keystore. You run this command on the command line in VxRail Manager. curl --unix-socket /var/lib/vxrail/nginx/socket/nginx.sock -X GET 'http://localhost/rest/vxm/internal/lockbox/v1/credentials?lockbox_name=SYSTEM&credential_names=management_account_vc'|jq vcluster202-vxm:~ # curl --unix-socket /var/lib/vxrail/nginx/socket/nginx.sock -X GET 'http://localhost/rest/vxm/internal/lockbox/v1/credentials?lockbox_name=SYSTEM&credential_names=management_account_vc'|jq % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 162 100 162 0 0 8526 0 --:--:-- --:--:-- --:--:-- 8526 { \"items\": [ { \"connection_type\": \"INVALID\", // Note: This is here because I deliberately broke the system. \"credential_name\": \"management_account_vc\", \"is_found\": true, \"password\": \"UEBzc3cwcmQxMjM0\", \"username\": \"management@localos\" } ] } Notice in the above that the password is base 64 encoded. You have to decode it with the following: vcluster202-vxm:~ # echo -n \"UEBzc3cwcmQxMjM0\" | base64 -d P@ssw0rd1234 In the above JSON output, credential_name refers to the name of the credential inside of the VxRail Manager lockbox program . The field username refers to the local account on vCenter. You will be able to see this account on vCenter by running cat /etc/passwd which shows all local users on the system. Note : Only accounts in the localos domain will show in /etc/passwd .","title":"Checking What Credentials are in Use"},{"location":"VMWare/Change%20User%20on%20VxRail%20Plugin/#changing-the-credentials","text":"To change the credentials in the VxRail Manager lockbox program use the following code on the VxRail Manager curl -X PUT --unix-socket /var/lib/vxrail/nginx/socket/nginx.sock -H \"accept: application/json\" -H \"Content-Type: application/json\" -d '{\"lockbox_name\":\"SYSTEM\",\"credentials\":[{\"credential_name\":\"management_account_vc\",\"username\":\"VC_management_account_user_name\",\"password\":\"encoded_password\"}]}' 'http://localhost/rest/vxm/internal/lockbox/v1/credentials' In the above where it says VC_management_account_user_name you will replace that with whatever is in username in your JSON output. In my case that is management@localos and you must replace encoded_password with YOUR base64 encoded password. In my case, I changed the password on vCenter to This.is.a.great.password1 which is why the system isn't currently working. So we have to base64 encode that and update in lockbox. vcluster202-vxm:~ # echo -n \"This.is.a.great.password1\" | base64 VGhpcy5pcy5hLmdyZWF0LnBhc3N3b3JkMQ== Next we have to update it with: curl -X PUT --unix-socket /var/lib/vxrail/nginx/socket/nginx.sock -H \"accept: application/json\" -H \"Content-Type: application/json\" -d '{\"lockbox_name\":\"SYSTEM\",\"credentials\":[{\"credential_name\":\"management_account_vc\",\"username\":\"management@localos\",\"password\":\"VGhpcy5pcy5hLmdyZWF0LnBhc3N3b3JkMQ==\"}]}' 'http://localhost/rest/vxm/internal/lockbox/v1/credentials' Now we need to restart the lockbox service. You can do this by rebooting the VxRail Manager or you can simply restart the lockbox container with docker service scale func_lockbox=0 && docker service scale func_lockbox=1 .","title":"Changing the Credentials"},{"location":"VMWare/Change%20User%20on%20VxRail%20Plugin/#setting-the-credentials-on-vcenter","text":"On the vCenter side, use passwd <vCenter account name> to reset the password on vCenter. After you change it, restart vCenter to force the change.","title":"Setting the Credentials on vCenter"},{"location":"VMWare/Change%20User%20on%20VxRail%20Plugin/#esxi-credentials","text":"The same problem exists with the VxRail Manager and the individual ESXi hosts. Repeat the same process as above for the management account for each ESXi host. For example: curl --unix-socket /var/lib/vxrail/nginx/socket/nginx.sock -X GET 'http://localhost/rest/vxm/internal/lockbox/v1/credentials?lockbox_name=SYSTEM&credential_names=management_account_esxi__<host-SN>'| jq where host-SN is the service tag for the server. This will return an output like: { \"items\": [ { \"credential_name\": \"management_account_esxi__V015001\", \"is_found\": true, \"password\": \"BASE64_ENCODED_PASSWORD\", \"username\": \"management\" } ] } As before you will have to update the password with: curl -X PUT --unix-socket /var/lib/vxrail/nginx/socket/nginx.sock -H \"accept: application/json\" -H \"Content-Type: application/json\" -d '{\"lockbox_name\":\"SYSTEM\",\"credentials\":[{\"credential_name\":\"management_account_esxi__<host-SN>\",\"username\":\"ESXi_management_account_user_name\",\"password\":\"encoded_password\"}]}' 'http://localhost/rest/vxm/internal/lockbox/v1/credentials' where <host-sn> is your service tag, ESXi_management_account_user_name is the name of your ESXi management account from above, and encoded password is your password base64 encoded with echo -n \"PASSWORD\" | base64 . Ensure that the password on ESXi matches by browsing to Manage->Security & Users->Users on the ESXi management interface: Click Edit User and then change the password to match whatever you used above.","title":"ESXi Credentials"},{"location":"VMWare/ESXi%20Architecture/","text":"VMWare Architecture Notes My Image Profile A copy of my image profile Example VIB Definition Example VIB definition What is the altbookbank Partition https://serverfault.com/questions/278895/how-is-the-altbootbank-partition-used-in-esxi VMWare Architecture Document https://www.vmware.com/content/dam/digitalmarketing/vmware/en/pdf/techpaper/ESXi_architecture.pdf What is dcism Appears to be the iDRAC service module ESXi Log File Locations https://docs.vmware.com/en/VMware-vSphere/7.0/com.vmware.vsphere.security.doc/GUID-832A2618-6B11-4A28-9672-93296DA931D0.html I suspect anything related to updates should be found in the hostd.log file on the ESXi server itself.","title":"VMWare Architecture Notes"},{"location":"VMWare/ESXi%20Architecture/#vmware-architecture-notes","text":"","title":"VMWare Architecture Notes"},{"location":"VMWare/ESXi%20Architecture/#my-image-profile","text":"A copy of my image profile","title":"My Image Profile"},{"location":"VMWare/ESXi%20Architecture/#example-vib-definition","text":"Example VIB definition","title":"Example VIB Definition"},{"location":"VMWare/ESXi%20Architecture/#what-is-the-altbookbank-partition","text":"https://serverfault.com/questions/278895/how-is-the-altbootbank-partition-used-in-esxi","title":"What is the altbookbank Partition"},{"location":"VMWare/ESXi%20Architecture/#vmware-architecture-document","text":"https://www.vmware.com/content/dam/digitalmarketing/vmware/en/pdf/techpaper/ESXi_architecture.pdf","title":"VMWare Architecture Document"},{"location":"VMWare/ESXi%20Architecture/#what-is-dcism","text":"Appears to be the iDRAC service module","title":"What is dcism"},{"location":"VMWare/ESXi%20Architecture/#esxi-log-file-locations","text":"https://docs.vmware.com/en/VMware-vSphere/7.0/com.vmware.vsphere.security.doc/GUID-832A2618-6B11-4A28-9672-93296DA931D0.html I suspect anything related to updates should be found in the hostd.log file on the ESXi server itself.","title":"ESXi Log File Locations"},{"location":"VMWare/How%20to%20Pull%20Usage%20Metrics/","text":"How to Pull Usage Metrics Log into vCenter Select your datacenter -> Click the monitor tab -> Under Performance, click Overview Change period to \"Last year\", create a screenshot On \"View\" take screenshots of Space Utilization and Datastores (make sure to select last year) Under Performance on the left, go to Advanced, change the Period to Last year, and take a screenshot.","title":"How to Pull Usage Metrics"},{"location":"VMWare/How%20to%20Pull%20Usage%20Metrics/#how-to-pull-usage-metrics","text":"Log into vCenter Select your datacenter -> Click the monitor tab -> Under Performance, click Overview Change period to \"Last year\", create a screenshot On \"View\" take screenshots of Space Utilization and Datastores (make sure to select last year) Under Performance on the left, go to Advanced, change the Period to Last year, and take a screenshot.","title":"How to Pull Usage Metrics"},{"location":"VMWare/Notes%20on%20VSAN/","text":"Notes on vSAN Notes on vSAN Disk Groups Deduplication and Replication What is a Replica (other source) Distributed Datastore Objects and Components RAID Architecture Required Networks Quarum Logic Fault Domains Sample Architecture Witness Alternate Explanation (includes stripes) Design Notes Networking Erasure Coding RAID 5 RAID 6 Internal Components vSAN Layers Objects and Components Another explanation vSAN Networking Roles vSAN RAID Tree Other Notes Minimum Drive Requirements Minimum Hosts for vSAN Minimum Hosts for Erasure Coding (RAID5/6) Maximum Size Deduplication and Compression Disk Groups Deduplication and Replication The scope of deduplication and compression exists only within each disk group. Hosken, Martin. VMware Software-Defined Storage (Kindle Locations 5278-5279). Wiley. Kindle Edition. Virtual SAN uses a distributed read cache mechanism whereby reads and writes are distributed to all hosts holding replicas. This way, if one host is busy, the other hosts holding replicas can still service I/ O requests. Hosken, Martin. VMware Software-Defined Storage (Kindle Locations 5317-5318). Wiley. Kindle Edition. Virtual SAN locates data in two or more locations across the distributed Virtual SAN datastore, in order to withstand host or disk failures. With data locality operating in this way, I/ O can come from any of the data replicas across the cluster, helping to mitigate potential host or disk bottlenecks and allowing Virtual SAN to run more efficiently, while still maintaining data availability and optimum performance. Hosken, Martin. VMware Software-Defined Storage (Kindle Locations 5324-5327). Wiley. Kindle Edition. The mechanism for destaging differs between the two Virtual SAN models, hybrid and all-flash; mechanical disks are typically good at handling sequential write workloads, so Virtual SAN uses this to make the process more efficient. In the hybrid model, an elevator algorithm runs independently on each disk group and decides, locally, whether to move any data to its capacity disks, and if so, when. This algorithm uses multiple criteria and batches together larger chunks of data that are physically proximal on a mechanical disk, and destages them together asynchronously. This mechanism writes to the disk sequentially for improved performance. However, the destaging mechanism is also conservative: it will not rush to move data if the space in the write buffer is not constringed. In addition, as data that is written tends to be overwritten quickly within a short period of time, this approach avoids writing the same blocks of data multiple times to the mechanical disks. Also note that the write buffers of the capacity layer disks are flushed onto the persistent storage devices before writes are discarded from the caching device. In the all-flash model, Virtual SAN uses 100 percent of the available capacity on the endurance flash device as a write buffer. In all-flash configurations, essentially the same mechanism is in place as that in the hybrid model. However, Virtual SAN does not take into account the proximal algorithm, making it a more efficient mechanism for destaging to capacity flash devices. Also, in the all-flash model, changes to the elevator algorithm allow the destaging of cold data from the write cache to the capacity tier, based on their data blocks' relative hotness or coldness. In addition, data blocks that are overwritten stay in the caching tier longer, which results in reducing the overall wear on the capacity tier flash devices, increasing their life expectancy. Hosken, Martin. VMware Software-Defined Storage (Kindle Locations 5339-5353). Wiley. Kindle Edition. What is a Replica (other source) Distributed Datastore Objects and Components RAID Architecture Required Networks Quarum Logic A Virtual SAN\u2013 enabled cluster uses a quorum-based system with witness components to ensure consistent operations across the distributed datastore. The quorum is the minimum number of votes that a distributed system must be able to obtain, in order to be allowed to perform an operation. In Virtual SAN, 50 percent of the votes that make up a virtual machine's storage object must be accessible at all times for that replica to be active. If less than 50 percent of the votes are accessible to the host, the object is not available and is marked as inaccessible in the Virtual SAN datastore. This can become a problem for Virtual SAN that can affect the availability of virtual machines, if after a host failure, the loss of quorum for a virtual machine object results in vSphere High Availability not being able to restart the virtual machine until the cluster quorum is restored. vSphere HA can guarantee that a virtual machine will restart only when it has a cluster quorum and can access the most recent copy of the virtual machine object. For instance, Figure 4.48 shows a three-node cluster that has a single virtual machine running on host 1, which has been assigned a storage policy with an FTT (Failures to Tolerate) of 1. If all three hosts fail in sequence (with host 3 the last host to fail), when host 1 and host 2 come back online, vSphere HA will be unable to restart the virtual machine because the last host that failed, host 3, retains the most recent copy of the virtual machine object components and is currently inaccessible. In this scenario, either all three hosts must recover at the same time or the two-host quorum must include host 3. If neither of these conditions is satisfied, vSphere High Availability will attempt to restart the virtual machine again when host 3 comes back online. Hosken, Martin. VMware Software-Defined Storage (p. 302). Wiley. Kindle Edition. Fault Domains Sample Architecture Witness Witnesses are zero-length components, containing just metadata. The purpose of the witness is to ensure that only one network partition can access an object at any one time. For instance, consider a failure scenario in which two vSphere hosts communicate over a Virtual SAN network. The VMDK object has been configured with a replica, so there is a copy of the VMDK on a second vSphere host. If the Virtual SAN network goes down, the two hosts are no longer able to communicate with one another, even though both hosts are still up and running and accessing other networks successfully. From which partition does the virtual machine access the data? Hosken, Martin. VMware Software-Defined Storage (p. 211). Wiley. Kindle Edition. The logic for the equation 2n+1 comes from the fact that you have to have replicas on two different hosts and then a third to act as a tie breaker with the witness. Alternate Explanation (includes stripes) Design Notes Networking Comparing the vSphere Standard and Distributed Virtual Switches When designing the virtual switch configuration, if not already dictated by other design factors, one required design decision may be whether to adopt the vSphere standard switch or to implement the vSphere Distributed Switch (VDS), with Virtual SAN deployment being supported on both options. The major benefit of the vSphere standard switch is simplicity of implementation. However, as the Virtual SAN environment grows, the design might benefit from several features offered only by the VDS, including Network I/ O Control (NIOC), Link Aggregation Control Protocol (LACP), and NetFlow. Another key consideration that factors into this design decision is whether VMware NSX is included in the overall environment architecture. One of the key benefits of using the vSphere Distributed Switch in a Virtual SAN environment is that NIOC can be used, which allows for the prioritization of bandwidth when there is network contention. For instance, replication and synchronization activities that Virtual SAN will impose on the network can cause contention. Depending on the number of virtual machines, their level of network activity, and Virtual SAN network utilization, 1 Gb/ s networks can easily be saturated and overwhelmed, particularly during rebuild and synchronization operations. Through the use of NIOC and QoS, vSphere Hosken, Martin. VMware Software-Defined Storage (p. 238). Wiley. Kindle Edition. Erasure Coding How it works: https://stonefly.com/blog/understanding-erasure-coding RAID 5 RAID 6 Internal Components vSAN Layers Objects and Components See https://masteringvmware.com/vsan-objects-and-components/ Component is an single file which you can say it as single VMDK. When you apply an storage policy to the Virtual Machine based on the policy components gets created and replicated. Let\u2019s take an Example where you have created a VM with RAID-1 (Mirroring). So now when you see at the VM placement you will see that different components gets created. Component has the maximum limit of 255GB. So that means if your VMDK is more then 255 GB in size then it will be striped and if the VMDK is less then 255GB in size then it will be single component. In vSAN 6.6 there is limit of maximum 9000 components per vSAN Host. vSAN Distributes the components across the hosts evenly for the availability and to maintain the balance. Another explanation This also describes stripe width vSAN Networking Roles CMMDS = Clustered metadata database and monitoring service Multicast addresses used are as follows: vSAN RAID Tree Other Notes Minimum Drive Requirements Each host contributing storage capacity to the vSAN cluster will require at least one flash device and one capacity device (magnetic disk or flash). Note that the capacity tier is either all-flash or all magnetic disk; they cannot be mixed in the same vSAN cluster. Hogan, Cormac; Epping, Duncan. VMware vSAN 6.7 U1 Deep Dive . Kindle Edition. Minimum Hosts for vSAN At a minimum, vSAN requires three hosts in your cluster to contribute storage (or two hosts if you decide to use a witness host, which is a common configuration for ROBO, this is discussed in chapter 8); other hosts in your cluster could leverage these storage resources without contributing storage resources to the cluster itself, although this is not common. Hogan, Cormac; Epping, Duncan. VMware vSAN 6.7 U1 Deep Dive . Kindle Edition. Minimum Hosts for Erasure Coding (RAID5/6) Maximum Size Today\u2019s boundary for vSAN in terms of both size and connectivity is a vSphere cluster. This means that vSAN supports single clusters/datastores of up to 64 hosts, but of course a single vCenter Server instance can manage many 64 host clusters. It is a common practice for most customers however to limit their clusters to around 20 hosts. This is for operational considerations like the time it takes to update a full cluster. Each host can run a supported maximum of 200 VMs, up to a total of 6,400 VMs within a 64-host vSAN cluster. Hogan, Cormac; Epping, Duncan. VMware vSAN 6.7 U1 Deep Dive . Kindle Edition. Deduplication and Compression When creating your vSAN cluster, if your vSAN cluster is an all-flash configuration, you have the option to enable \u201cdeduplication and compression.\u201d Deduplication and compression will play a big factor in available capacity for an all-flash configuration. Note that these data services are not available in a hybrid configuration. Hogan, Cormac; Epping, Duncan. VMware vSAN 6.7 U1 Deep Dive . Kindle Edition. But it is not just deduplication and compression that can provide space saving on the vSAN datastore. There is also the number of replica copies configured if the VM is using a RAID-1 policy. This is enabled through the policy-based management framework. Conversely, you may decide to use erasure coding polices such as RAID-5 and RAID-6 (but note that this space efficiency feature is only available on all-flash vSAN). These all determine how many VMs can be deployed on the datastore. Hogan, Cormac; Epping, Duncan. VMware vSAN 6.7 U1 Deep Dive . Kindle Edition.","title":"Notes on vSAN"},{"location":"VMWare/Notes%20on%20VSAN/#notes-on-vsan","text":"Notes on vSAN Disk Groups Deduplication and Replication What is a Replica (other source) Distributed Datastore Objects and Components RAID Architecture Required Networks Quarum Logic Fault Domains Sample Architecture Witness Alternate Explanation (includes stripes) Design Notes Networking Erasure Coding RAID 5 RAID 6 Internal Components vSAN Layers Objects and Components Another explanation vSAN Networking Roles vSAN RAID Tree Other Notes Minimum Drive Requirements Minimum Hosts for vSAN Minimum Hosts for Erasure Coding (RAID5/6) Maximum Size Deduplication and Compression","title":"Notes on vSAN"},{"location":"VMWare/Notes%20on%20VSAN/#disk-groups","text":"","title":"Disk Groups"},{"location":"VMWare/Notes%20on%20VSAN/#deduplication-and-replication","text":"The scope of deduplication and compression exists only within each disk group. Hosken, Martin. VMware Software-Defined Storage (Kindle Locations 5278-5279). Wiley. Kindle Edition. Virtual SAN uses a distributed read cache mechanism whereby reads and writes are distributed to all hosts holding replicas. This way, if one host is busy, the other hosts holding replicas can still service I/ O requests. Hosken, Martin. VMware Software-Defined Storage (Kindle Locations 5317-5318). Wiley. Kindle Edition. Virtual SAN locates data in two or more locations across the distributed Virtual SAN datastore, in order to withstand host or disk failures. With data locality operating in this way, I/ O can come from any of the data replicas across the cluster, helping to mitigate potential host or disk bottlenecks and allowing Virtual SAN to run more efficiently, while still maintaining data availability and optimum performance. Hosken, Martin. VMware Software-Defined Storage (Kindle Locations 5324-5327). Wiley. Kindle Edition. The mechanism for destaging differs between the two Virtual SAN models, hybrid and all-flash; mechanical disks are typically good at handling sequential write workloads, so Virtual SAN uses this to make the process more efficient. In the hybrid model, an elevator algorithm runs independently on each disk group and decides, locally, whether to move any data to its capacity disks, and if so, when. This algorithm uses multiple criteria and batches together larger chunks of data that are physically proximal on a mechanical disk, and destages them together asynchronously. This mechanism writes to the disk sequentially for improved performance. However, the destaging mechanism is also conservative: it will not rush to move data if the space in the write buffer is not constringed. In addition, as data that is written tends to be overwritten quickly within a short period of time, this approach avoids writing the same blocks of data multiple times to the mechanical disks. Also note that the write buffers of the capacity layer disks are flushed onto the persistent storage devices before writes are discarded from the caching device. In the all-flash model, Virtual SAN uses 100 percent of the available capacity on the endurance flash device as a write buffer. In all-flash configurations, essentially the same mechanism is in place as that in the hybrid model. However, Virtual SAN does not take into account the proximal algorithm, making it a more efficient mechanism for destaging to capacity flash devices. Also, in the all-flash model, changes to the elevator algorithm allow the destaging of cold data from the write cache to the capacity tier, based on their data blocks' relative hotness or coldness. In addition, data blocks that are overwritten stay in the caching tier longer, which results in reducing the overall wear on the capacity tier flash devices, increasing their life expectancy. Hosken, Martin. VMware Software-Defined Storage (Kindle Locations 5339-5353). Wiley. Kindle Edition.","title":"Deduplication and Replication"},{"location":"VMWare/Notes%20on%20VSAN/#what-is-a-replica-other-source","text":"","title":"What is a Replica (other source)"},{"location":"VMWare/Notes%20on%20VSAN/#distributed-datastore","text":"","title":"Distributed Datastore"},{"location":"VMWare/Notes%20on%20VSAN/#objects-and-components","text":"","title":"Objects and Components"},{"location":"VMWare/Notes%20on%20VSAN/#raid-architecture","text":"","title":"RAID Architecture"},{"location":"VMWare/Notes%20on%20VSAN/#required-networks","text":"","title":"Required Networks"},{"location":"VMWare/Notes%20on%20VSAN/#quarum-logic","text":"A Virtual SAN\u2013 enabled cluster uses a quorum-based system with witness components to ensure consistent operations across the distributed datastore. The quorum is the minimum number of votes that a distributed system must be able to obtain, in order to be allowed to perform an operation. In Virtual SAN, 50 percent of the votes that make up a virtual machine's storage object must be accessible at all times for that replica to be active. If less than 50 percent of the votes are accessible to the host, the object is not available and is marked as inaccessible in the Virtual SAN datastore. This can become a problem for Virtual SAN that can affect the availability of virtual machines, if after a host failure, the loss of quorum for a virtual machine object results in vSphere High Availability not being able to restart the virtual machine until the cluster quorum is restored. vSphere HA can guarantee that a virtual machine will restart only when it has a cluster quorum and can access the most recent copy of the virtual machine object. For instance, Figure 4.48 shows a three-node cluster that has a single virtual machine running on host 1, which has been assigned a storage policy with an FTT (Failures to Tolerate) of 1. If all three hosts fail in sequence (with host 3 the last host to fail), when host 1 and host 2 come back online, vSphere HA will be unable to restart the virtual machine because the last host that failed, host 3, retains the most recent copy of the virtual machine object components and is currently inaccessible. In this scenario, either all three hosts must recover at the same time or the two-host quorum must include host 3. If neither of these conditions is satisfied, vSphere High Availability will attempt to restart the virtual machine again when host 3 comes back online. Hosken, Martin. VMware Software-Defined Storage (p. 302). Wiley. Kindle Edition.","title":"Quarum Logic"},{"location":"VMWare/Notes%20on%20VSAN/#fault-domains","text":"","title":"Fault Domains"},{"location":"VMWare/Notes%20on%20VSAN/#sample-architecture","text":"","title":"Sample Architecture"},{"location":"VMWare/Notes%20on%20VSAN/#witness","text":"Witnesses are zero-length components, containing just metadata. The purpose of the witness is to ensure that only one network partition can access an object at any one time. For instance, consider a failure scenario in which two vSphere hosts communicate over a Virtual SAN network. The VMDK object has been configured with a replica, so there is a copy of the VMDK on a second vSphere host. If the Virtual SAN network goes down, the two hosts are no longer able to communicate with one another, even though both hosts are still up and running and accessing other networks successfully. From which partition does the virtual machine access the data? Hosken, Martin. VMware Software-Defined Storage (p. 211). Wiley. Kindle Edition. The logic for the equation 2n+1 comes from the fact that you have to have replicas on two different hosts and then a third to act as a tie breaker with the witness.","title":"Witness"},{"location":"VMWare/Notes%20on%20VSAN/#alternate-explanation-includes-stripes","text":"","title":"Alternate Explanation (includes stripes)"},{"location":"VMWare/Notes%20on%20VSAN/#design-notes","text":"","title":"Design Notes"},{"location":"VMWare/Notes%20on%20VSAN/#networking","text":"Comparing the vSphere Standard and Distributed Virtual Switches When designing the virtual switch configuration, if not already dictated by other design factors, one required design decision may be whether to adopt the vSphere standard switch or to implement the vSphere Distributed Switch (VDS), with Virtual SAN deployment being supported on both options. The major benefit of the vSphere standard switch is simplicity of implementation. However, as the Virtual SAN environment grows, the design might benefit from several features offered only by the VDS, including Network I/ O Control (NIOC), Link Aggregation Control Protocol (LACP), and NetFlow. Another key consideration that factors into this design decision is whether VMware NSX is included in the overall environment architecture. One of the key benefits of using the vSphere Distributed Switch in a Virtual SAN environment is that NIOC can be used, which allows for the prioritization of bandwidth when there is network contention. For instance, replication and synchronization activities that Virtual SAN will impose on the network can cause contention. Depending on the number of virtual machines, their level of network activity, and Virtual SAN network utilization, 1 Gb/ s networks can easily be saturated and overwhelmed, particularly during rebuild and synchronization operations. Through the use of NIOC and QoS, vSphere Hosken, Martin. VMware Software-Defined Storage (p. 238). Wiley. Kindle Edition.","title":"Networking"},{"location":"VMWare/Notes%20on%20VSAN/#erasure-coding","text":"How it works: https://stonefly.com/blog/understanding-erasure-coding","title":"Erasure Coding"},{"location":"VMWare/Notes%20on%20VSAN/#raid-5","text":"","title":"RAID 5"},{"location":"VMWare/Notes%20on%20VSAN/#raid-6","text":"","title":"RAID 6"},{"location":"VMWare/Notes%20on%20VSAN/#internal-components","text":"","title":"Internal Components"},{"location":"VMWare/Notes%20on%20VSAN/#vsan-layers","text":"","title":"vSAN Layers"},{"location":"VMWare/Notes%20on%20VSAN/#objects-and-components_1","text":"See https://masteringvmware.com/vsan-objects-and-components/ Component is an single file which you can say it as single VMDK. When you apply an storage policy to the Virtual Machine based on the policy components gets created and replicated. Let\u2019s take an Example where you have created a VM with RAID-1 (Mirroring). So now when you see at the VM placement you will see that different components gets created. Component has the maximum limit of 255GB. So that means if your VMDK is more then 255 GB in size then it will be striped and if the VMDK is less then 255GB in size then it will be single component. In vSAN 6.6 there is limit of maximum 9000 components per vSAN Host. vSAN Distributes the components across the hosts evenly for the availability and to maintain the balance.","title":"Objects and Components"},{"location":"VMWare/Notes%20on%20VSAN/#another-explanation","text":"This also describes stripe width","title":"Another explanation"},{"location":"VMWare/Notes%20on%20VSAN/#vsan-networking-roles","text":"CMMDS = Clustered metadata database and monitoring service Multicast addresses used are as follows:","title":"vSAN Networking Roles"},{"location":"VMWare/Notes%20on%20VSAN/#vsan-raid-tree","text":"","title":"vSAN RAID Tree"},{"location":"VMWare/Notes%20on%20VSAN/#other-notes","text":"","title":"Other Notes"},{"location":"VMWare/Notes%20on%20VSAN/#minimum-drive-requirements","text":"Each host contributing storage capacity to the vSAN cluster will require at least one flash device and one capacity device (magnetic disk or flash). Note that the capacity tier is either all-flash or all magnetic disk; they cannot be mixed in the same vSAN cluster. Hogan, Cormac; Epping, Duncan. VMware vSAN 6.7 U1 Deep Dive . Kindle Edition.","title":"Minimum Drive Requirements"},{"location":"VMWare/Notes%20on%20VSAN/#minimum-hosts-for-vsan","text":"At a minimum, vSAN requires three hosts in your cluster to contribute storage (or two hosts if you decide to use a witness host, which is a common configuration for ROBO, this is discussed in chapter 8); other hosts in your cluster could leverage these storage resources without contributing storage resources to the cluster itself, although this is not common. Hogan, Cormac; Epping, Duncan. VMware vSAN 6.7 U1 Deep Dive . Kindle Edition.","title":"Minimum Hosts for vSAN"},{"location":"VMWare/Notes%20on%20VSAN/#minimum-hosts-for-erasure-coding-raid56","text":"","title":"Minimum Hosts for Erasure Coding (RAID5/6)"},{"location":"VMWare/Notes%20on%20VSAN/#maximum-size","text":"Today\u2019s boundary for vSAN in terms of both size and connectivity is a vSphere cluster. This means that vSAN supports single clusters/datastores of up to 64 hosts, but of course a single vCenter Server instance can manage many 64 host clusters. It is a common practice for most customers however to limit their clusters to around 20 hosts. This is for operational considerations like the time it takes to update a full cluster. Each host can run a supported maximum of 200 VMs, up to a total of 6,400 VMs within a 64-host vSAN cluster. Hogan, Cormac; Epping, Duncan. VMware vSAN 6.7 U1 Deep Dive . Kindle Edition.","title":"Maximum Size"},{"location":"VMWare/Notes%20on%20VSAN/#deduplication-and-compression","text":"When creating your vSAN cluster, if your vSAN cluster is an all-flash configuration, you have the option to enable \u201cdeduplication and compression.\u201d Deduplication and compression will play a big factor in available capacity for an all-flash configuration. Note that these data services are not available in a hybrid configuration. Hogan, Cormac; Epping, Duncan. VMware vSAN 6.7 U1 Deep Dive . Kindle Edition. But it is not just deduplication and compression that can provide space saving on the vSAN datastore. There is also the number of replica copies configured if the VM is using a RAID-1 policy. This is enabled through the policy-based management framework. Conversely, you may decide to use erasure coding polices such as RAID-5 and RAID-6 (but note that this space efficiency feature is only available on all-flash vSAN). These all determine how many VMs can be deployed on the datastore. Hogan, Cormac; Epping, Duncan. VMware vSAN 6.7 U1 Deep Dive . Kindle Edition.","title":"Deduplication and Compression"},{"location":"VMWare/Setup%20VXRail/","text":"VxRail Setup VxRail Setup License Keys Networking What does MLD Querying Do and Why Can it Break VxRail Discovery RASR Process Deploy Witness After deploying the Witness: Set Up vCenter Set up the Manager for Discovery Install Advanced Troubleshooting Check for ESXi Logs Marvin Logs Digging Through VxRail's Guts Helpful Commands Check if VxRail is Running Get a List of VMs Check if VM is On Check Networking Set the VLAN for a Switch Get Interface IPs Get Interface List License Keys In case you have to upgrade or downgrade license keys see this KB article Networking I followed the VxRail Network Planning Guide to set up the network. WARNING : The discovery process for VxRail uses IPv6 multicast to discover itself. Dell switches come with MLD snooping globally enabled, but do not have the MLD querier enabled! This will cause the nodes to flap! Servers will discover themselves and then disappear. You must enable MLD snooping and the mld querier on the correct vlans with interface vlan # and then ipv6 mld snooping querier . What does MLD Querying Do and Why Can it Break VxRail Discovery By default, switches are not multicast aware so they will broadcast any multicast traffic on all ports assigned to a VLAN. For bandwidth optimization the switch will block any multicast messages to a segment it thinks does not have a device which wants those multicast messages. It discovers if there is an interested host by using MLD general queries or group specific queries. Enabling the querier will allow the switch to use these messages to discover interested hosts and subsequently ensure those hosts receive the appropriate IPv6 messages. RASR Process There is an IDSDM module on the box with the factor image on the box. To perform the RASR process you'll boot from that and it will copy over all necessary files to the internal BOSS drive. Just follow the prompts. WARNING RASRing a node blows away absolutely everything on all drives! DO NOT RUN ON A PRODUCTION CLUSTER Deploy Witness Note For the two node setup, you will need a third site with vCenter deployed in a cluster separate from VxRail. WARNING If someone tells you that 1 proc, 16GB R240 is sufficient for running the Witness and vCenter in a 2 node setup... that is strictly speaking true. Do not plan on putting anything else on that box. Download the witness appliance from the VMWare site . Make sure you pull the witness appliance for your setup! On VxRail 4.7 which is what I'm installing the correct version of Witness is 6.7. Follow the instructions here for setup. The only thing that could be confusing depending on what you've previously read is that the second NIC for the Witness goes on its own special VLAN separate from VSAN. This VLAN will be used only for Witness traffic. Point of Curiosity The Witness host looks exactly like an instance of ESXi because it is. It's only purpose is to pretend to be a third ESXi host and avoid split brain on vSAN. After deploying the Witness: Give vmk0 an IP on your management network and vmk1 an IP address on the vSAN network. You can use the esxcli network ip interface ipv4 set -i vmk0 --type static -I 192.168.5.130 -N 255.255.255.0 -g 192.168.5.1 command to IP an interface Go into vCenter and add the Witness Make sure vmk0 of the Witness has the same MAC address of vmk0 on the host OR set the security settings of the management portgroup you are using to promiscuous Set Up vCenter Make sure your DNS server can resolve all the VxRail ESXi IPs first before continuing. On vCenter double check that DNS and NTP are setup correctly. You can do this from the network menu by going to Home->Administration->System Configuration->Click on vCenter->Login. This should bring up the appliance management screen. Check Time and Networking. vCenter has to be able to resolve all ESXi names itself. This is not checked by VxRail's validator ! To double check name resolution, go to vCenter's console, hit alt+F1, use nslookup to check all the ESXi node names. Set up the Manager for Discovery David Ring's Install Notes are helpful David Ring's notes on how host discovery works are also helpful Loudmouth requires IPv6 multicast in order for VxRail Manager to perform a successful VxRail node discovery. IPv6 multicast is required only on the \u2018Private Management Network\u2019, this is an isolated management network solely for auto-discovery of the VxRail nodes during install or expansion. The default VLAN ID for the private network is 3939, which is configured on each VxRail node from the factory, this VLAN needs to be configured on the TOR switches and remains isolated on the TORs. If you wish to deviate from the default of VLAN 3939 then each node will need to be modified onsite otherwise node discovery will fail. The VxRail Manager comes defaulted to 192.168.10.200 however, it does not accept SSH connections Turn on the VxRail manager. vxrail-primary --setup --vxrail-address 192.168.2.100 --vxrail-netmask 255.255.255.0 --vxrail-gateway 192.168.2.1 . The IP you assign is not used for discovery. It is for reaching the VxRail appliance. Give it an IP on whatever network/vlan you plan on using for management. Details below in step 3. 1.Make sure vmk0 of all ESXi hosts is on VLAN 3939 or whatever VLAN you're using for discovery. 2.On each ESXi host make sure Private Management Network and Private VM Network are both on VLAN 3939. (Picture from David Ring's guide) 3.The VxRail manager has two virtual NICs - eth0 and eth1. Eth1 is the NIC used for discovery. Make sure eth1 of the VxRail manager is on VLAN 3939. The first NIC (eth0) should be on whatever VLAN you are using for management. You will get to the VxRail manager webgui through eth0. 2. At this point you should have full IPv6 connectivity between vmk0 on all ESXi instances and the VxRail appliance. You can test this with the following: 1.Go to the ESXi console, press ALT+F1 and Pull the IPv6 address for vmk0 on ESXi with: esxcfg-vmknic -l | grep vmk0 2.On the ESXi host with the VxRail appliance, give another (not vmk0) VM kernel NIC you have access to an IP address with the esxcli network ip interface ipv4 set -i vmk0 --type static -I 192.168.5.130 -N 255.255.255.0 -g 192.168.5.1 command. For dhcp use the --type dhcp option. See Helpful Commands for how to list out the different portgroups and VMs 3.Once you know all the IPv6 addresses for the various vmk0 nics, get on the VxRail appliance and use ping6 -l eth1 <ipv6 address> to test your IPv6 ping. This should work against all devices. If it doesn't you probably have a networking problem. 4.You can also test the full server discovery process manually from the command line with: 5.On the switch side you can use show lldp discovery to see what interfaces it has discovered and it will list the interfaces. 6.On idrac it will show you what ports each nic is connected to under network devices. (My example is down - yours should not have a warning) 7.For further troubleshooting check the Marvin log 3. Assuming you've already checked DNS, the last thing you need to do is make sure NTP is up. I'm using chrony and checked like this on my NTP server: [root@services ~]# chronyc tracking Reference ID : D8EF230C (time4.google.com) Stratum : 2 Ref time (UTC) : Thu Dec 17 16:21:48 2020 System time : 0.000059723 seconds fast of NTP time Last offset : +0.000004093 seconds RMS offset : 0.000036436 seconds Frequency : 0.283 ppm fast Residual freq : -0.000 ppm Skew : 0.006 ppm Root delay : 0.023636952 seconds Root dispersion : 0.001011974 seconds Update interval : 1037.1 seconds Leap status : Normal Install Before running I strongly recommend you make sure NTP is up and running on any customer owned ESXi instances, customer vCenters, and the idracs. Mismatched time causes problems. All you have to do is browse to your VxRail Manager's address on follow the prompts. The only big gotcha I ran into is when it asks for vCenter Server Management Username what it wants is not administrator@vphere.whatever. It wants vCenter's root/password combo. Advanced Troubleshooting Check for ESXi Logs In my case I hit a failure where a node wouldn't add. A lot of times VxRail will have finished the ESXi deployment and you can actually go log into the ESXi instance in question. If you have a customer owned vCenter you can also go check its logs. A lot of the time failures during deployment can be found in one of those locations. Marvin Logs Another good place to look is to log into the VxRail appliance and check the Marvin logs at /var/log/vmware/marvin/tomcat Digging Through VxRail's Guts If you have problems during install you can pull VxRail's logs from the VxRail container by doing the below: Run docker ps -a and find the vxrail-system container. e8da567768e4 infra/openfaas/gateway:1.1.4 \"./gateway\" 2 days ago Up 2 days (healthy) 8080/tcp, 8082/tcp func_gateway.1.h5hmx70g8545d3apbwfxtfatr 2b71bfd81e90 infra/openfaas/queue-worker:1.1.4 \"./app\" 2 days ago Up 2 days (healthy) 8080/tcp func_queue-worker.1.897j8f0tbhas1989o0ygl5ez4 ba2f65a96664 infra/workflow_engine:1.1.6 \"./docker-entrypoint\u2026\" 2 days ago Up 2 days (healthy) 5000/tcp func_wfservice.1.wau7b1omhg3igjzzyvd865r6e d07929b37e2d infra/openfaas/gateway:1.1.4 \"./gateway\" 2 days ago Exited (1) 2 days ago func_gateway.1.stdin4eg1dysl3fajgpds5rwj e6f3ec734f8a infra/openfaas/queue-worker:1.1.4 \"./app\" 2 days ago Exited (2) 2 days ago func_queue-worker.1.uqy8d482650v4yquz98fnp9tq bc402e3b676c microservice/vxm-agent:1.1.6 \"/bin/bash entrypoin\u2026\" 2 days ago Up 2 days (healthy) 5000/tcp vxm-agent_vxm-agent_1 23e0ac7d3f92 microservice/nano-service:1.1.29 \"python -u index.py\" 2 days ago Up 2 days (healthy) 5000/tcp func_nano-service.1.dqo23zdkgmwbqf9ru6yn0t99r 54df5cb0af13 microservice/ms-day1-bringup:1.1.29 \"/bin/bash entrypoin\u2026\" 2 days ago Up 2 days (healthy) 5000/tcp func_ms-day1-bringup.1.xfe0xndtvsqbo17kapogyd7ke 87e26e7d27b2 infra/infra-lock-service:1.1.8 \"/bin/bash entrypoin\u2026\" 2 days ago Up 2 days (healthy) 5000/tcp func_lockservice.1.g2g7hg5viyph8r4ptcljeran9 4382600d0094 microservice/lockbox:1.1.11 \"/bin/bash entrypoin\u2026\" 2 days ago Up 2 days (healthy) 5000/tcp func_lockbox.1.q9hqbfuid54r5gelsazasksh2 f7964475a933 microservice/kgs-service:1.1.10 \"./boot.sh\" 2 days ago Up 2 days (healthy) 5000/tcp func_kgs-service.1.wpqm7hytpf5l9wo7ch3sc1wz7 75178ef078ea do-main/do-vxrail-system:1.1.35 \"/bin/bash entrypoin\u2026\" 2 days ago Up 2 days (healthy) 5000/tcp func_do-vxrail-system.1.e67dbhktpe88gw0wtmymmpthg 7abeddce2bb8 do-main/do-vm:1.1.35 \"/bin/bash entrypoin\u2026\" 2 days ago Up 2 days (healthy) 5000/tcp func_do-vm.1.g2fwikfzimyhv6di3zdalt1j3 3b557989c07e do-main/do-storage:1.1.35 \"/bin/bash entrypoin\u2026\" 2 days ago Up 2 days (healthy) 5000/tcp func_do-storage.1.w2nv1rfpidrv7g418xmknywso d0b15759e7f5 do-main/do-serviceability:1.1.35 \"/bin/bash entrypoin\u2026\" 2 days ago Up 2 days (healthy) func_do-serviceability.1.t5xkwt1eu2ypqvn0511i1gfy3 5e349b22f36a do-main/do-network:1.1.39 \"/bin/bash entrypoin\u2026\" 2 days ago Up 2 days (healthy) 5000/tcp func_do-network.1.vogoz7jtyrxf83h9jd0615r28 ae31c7cd94bd do-main/do-kgs:1.1.35 \"./boot.sh\" 2 days ago Up 2 days (healthy) 5000/tcp func_do-kgs.1.esudf6d39uy7e96dnvxoabaue 978447c9f6f8 do-main/do-host:1.1.39 \"/bin/bash entrypoin\u2026\" 2 days ago Up 2 days (healthy) 5000/tcp func_do-host.1.ravl83nt962k48up0v33qoviy 862926a58073 do-main/do-ecosystem:1.1.35 \"/bin/bash entrypoin\u2026\" 2 days ago Up 2 days (healthy) func_do-ecosystem.1.8k6cljhvo5p00632gt11f79uv 63dde78fc922 do-main/do-eservices:1.1.35 \"/bin/bash entrypoin\u2026\" 2 days ago Up 2 days (healthy) func_do-eservices.1.pnpnr1pofm0dvwv36trufl6g9 f1fb1d35457d do-main/do-cluster:1.1.35 \"/bin/bash entrypoin\u2026\" 2 days ago Up 2 days (healthy) 5000/tcp func_do-cluster.1.y4vs2diiyy5eau3ojuxdx12wx 2fc837e0726e infra/infra-config-service:1.1.9 \"/bin/bash entrypoin\u2026\" 2 days ago Up 2 days (healthy) 5000/tcp func_configservice.1.byjhptdpdxa3d04y7mr5lfp5z 685d833e0155 infra/openfaas/gateway:1.1.4 \"./gateway\" 2 days ago Exited (1) 2 days ago func_gateway.1.ztar1zozhfuh3skopwvlandwq 9af0891d5455 infra/openfaas/queue-worker:1.1.4 \"./app\" 2 days ago Exited (2) 2 days ago func_queue-worker.1.jmk9q8bkc6a2vlz3fg1e60y5f 0cb6d7948db9 infra/tracing:1.1.6 \"/go/bin/jaeger-all-\u2026\" 2 days ago Up 2 days (healthy) func_tracing.1.ymsjr3ba7ih2x21291xri4mz8 5e70fa63df88 infra/openfaas/faas-swarm:1.1.4 \"./faas-swarm\" 2 days ago Up 2 days (healthy) 8080/tcp func_faas-swarm.1.nfz0dm48hhx7vh9sh04mbagtz 620cce221e65 infra/nats-streaming:1.1.4 \"/nats-streaming-ser\u2026\" 2 days ago Up 2 days (healthy) 4222/tcp, 8222/tcp func_nats.1.l42vx07v08vhhtkv1lwxg1ipb dd16298b3e4c infra/nginx_gateway_vxrail:1.1.7 \"/bin/sh -c 'sudo /u\u2026\" 2 days ago Up 2 days (healthy) func_api-gateway.1.do9gzerxmmlti4rn8k3ab2gs4 4a8b6c976103 infra/gcr.io/etcd-development/etcd:1.1.4 \"/usr/local/bin/etcd\u2026\" 2 days ago Up 2 days (healthy) 2379-2380/tcp func_serviceregistry.1.u2gtw5wnsldulqgd1r47wkte5 acf803b61b04 infra/redis:1.1.4 \"docker-entrypoint.s\u2026\" 2 days ago Up 2 days (healthy) 6379/tcp func_cacheservice.1.subywzigvenobaskavjs6xyqc 3dac923f9bff infra/logging:1.1.4 \"tini -- /bin/entryp\u2026\" 2 days ago Up 2 days (healthy) 5140/tcp, 24224/tcp func_logging.1.kv7738umixulwryqpl38qvxvr bdd88fadc236 infra/prom/alertmanager:1.1.4 \"/bin/alertmanager -\u2026\" 2 days ago Up 2 days (healthy) 9093/tcp func_alertmanager.1.8uks430a7p10keeh8uqv625bv 7ec04254fee1 infra/prom/prometheus:1.1.4 \"/bin/prometheus --c\u2026\" 2 days ago Up 2 days (healthy) 9090/tcp func_prometheus.1.ti36b3wsder4w01v8nentdirx Find the log location with docker inspect --format='{{.LogPath}}' 75178ef078ea . They are in JSON format. For example, this is what my error looked like: This got me the reason for my failure: {\\\\\\\"error\\\\\\\": {\\\\\\\"result\\\\\\\": {\\\\\\\"error\\\\\\\": {\\\\\\\"code\\\\\\\": \\\\\\\"E3100_Cluster_22\\\\\\\", \\\\\\\"message\\\\\\\": \\\\\\\"failed to add host into cluster\\\\\\\", \\\\\\\"params\\\\\\\": [\\\\\\\"vxrail1-esxi.lan\\\\\\\"]}}}, \\\\\\\"id\\\\\\\": \\\\\\\"host_add_into_inventory\\\\\\\", \\\\\\\"internal_family\\\\\\\": \\\\\\\"node_one_bootstrap_with_cns_ready.primary_node_computer.host_add_into_inventory\\\\\\\", \\\\\\\"internal_id\\\\\\\": \\\\\\\"node_one_bootstrap_with_cns_ready.primary_node_computer.host_add_into_inventory_False____99ee8388_1147_4e_9d65c98cb2\\\\\\\", \", \"stream\": \"stderr\", \"time\": \"2020-12-18T13:24:42.028798775Z\" } {\"log\":\"\\\\\\\"params\\\\\\\": {\\\\\\\"cluster_name\\\\\\\": \\\\\\\"vxrail-cluster\\\\\\\", \\\\\\\"datacenter_name\\\\\\\": \\\\\\\"Datacenter\\\\\\\", \\\\\\\"host_conn_info\\\\\\\": {\\\\\\\"host\\\\\\\": \\\\\\\"192.168.2.31\\\\\\\", \\\\\\\"port\\\\\\\": 443, \\\\\\\"username\\\\\\\": \\\\\\\"root\\\\\\\"}, \\\\\\\"hostname\\\\\\\": \\\\\\\"vxrail1-esxi.lan\\\\\\\", \\\\\\\"ssl_thumbprint\\\\\\\": \\\\\\\"STUFF\\\\\\\", \\\\\\\"vc_conn_info\\\\\\\": {\\\\\\\"host\\\\\\\": \\\\\\\"vcenter.lan\\\\\\\", \\\\\\\"port\\\\\\\": 443, \\\\\\\"username\\\\\\\": \\\\\\\"administrator@vsphere.lan\\\\\\\"}, \\\\\\\"vm_folder_name\\\\\\\": \\\\\\\"VMware HCIA Folder vxrail-cluster\\\\\\\", \\\\\\\"vsan_name\\\\\\\": \\\\\\\"VxRail-Virtual-SAN-Datastore-22658cd0-5b17-428d-a23c-0d7e5a877bb7\\\\\\\"}, \\\\\\\"stage\\\\\\\": \\\\\\\"primary_node_computer\\\\\\\", \\\\\\\"startTime\\\\\\\": 1608297822107, \\\\\\\"status\\\\\\\": \\\\\\\"FAILED\\\\\\\"} Helpful Commands Check if VxRail is Running esxcli vm process list Get a List of VMs vim-cmd vmsvc/getallvms Check if VM is On vim-cmd vmsvc/power.getstate 1 Check Networking esxcli network vswitch standard portgroup list Set the VLAN for a Switch esxcli network vswitch standard portgroup set -p \"VM Network\" -v 120 Get Interface IPs esxcli network ip interface ipv4 get Get Interface List esxcli network ip interface list","title":"VxRail Setup"},{"location":"VMWare/Setup%20VXRail/#vxrail-setup","text":"VxRail Setup License Keys Networking What does MLD Querying Do and Why Can it Break VxRail Discovery RASR Process Deploy Witness After deploying the Witness: Set Up vCenter Set up the Manager for Discovery Install Advanced Troubleshooting Check for ESXi Logs Marvin Logs Digging Through VxRail's Guts Helpful Commands Check if VxRail is Running Get a List of VMs Check if VM is On Check Networking Set the VLAN for a Switch Get Interface IPs Get Interface List","title":"VxRail Setup"},{"location":"VMWare/Setup%20VXRail/#license-keys","text":"In case you have to upgrade or downgrade license keys see this KB article","title":"License Keys"},{"location":"VMWare/Setup%20VXRail/#networking","text":"I followed the VxRail Network Planning Guide to set up the network. WARNING : The discovery process for VxRail uses IPv6 multicast to discover itself. Dell switches come with MLD snooping globally enabled, but do not have the MLD querier enabled! This will cause the nodes to flap! Servers will discover themselves and then disappear. You must enable MLD snooping and the mld querier on the correct vlans with interface vlan # and then ipv6 mld snooping querier .","title":"Networking"},{"location":"VMWare/Setup%20VXRail/#what-does-mld-querying-do-and-why-can-it-break-vxrail-discovery","text":"By default, switches are not multicast aware so they will broadcast any multicast traffic on all ports assigned to a VLAN. For bandwidth optimization the switch will block any multicast messages to a segment it thinks does not have a device which wants those multicast messages. It discovers if there is an interested host by using MLD general queries or group specific queries. Enabling the querier will allow the switch to use these messages to discover interested hosts and subsequently ensure those hosts receive the appropriate IPv6 messages.","title":"What does MLD Querying Do and Why Can it Break VxRail Discovery"},{"location":"VMWare/Setup%20VXRail/#rasr-process","text":"There is an IDSDM module on the box with the factor image on the box. To perform the RASR process you'll boot from that and it will copy over all necessary files to the internal BOSS drive. Just follow the prompts. WARNING RASRing a node blows away absolutely everything on all drives! DO NOT RUN ON A PRODUCTION CLUSTER","title":"RASR Process"},{"location":"VMWare/Setup%20VXRail/#deploy-witness","text":"Note For the two node setup, you will need a third site with vCenter deployed in a cluster separate from VxRail. WARNING If someone tells you that 1 proc, 16GB R240 is sufficient for running the Witness and vCenter in a 2 node setup... that is strictly speaking true. Do not plan on putting anything else on that box. Download the witness appliance from the VMWare site . Make sure you pull the witness appliance for your setup! On VxRail 4.7 which is what I'm installing the correct version of Witness is 6.7. Follow the instructions here for setup. The only thing that could be confusing depending on what you've previously read is that the second NIC for the Witness goes on its own special VLAN separate from VSAN. This VLAN will be used only for Witness traffic. Point of Curiosity The Witness host looks exactly like an instance of ESXi because it is. It's only purpose is to pretend to be a third ESXi host and avoid split brain on vSAN.","title":"Deploy Witness"},{"location":"VMWare/Setup%20VXRail/#after-deploying-the-witness","text":"Give vmk0 an IP on your management network and vmk1 an IP address on the vSAN network. You can use the esxcli network ip interface ipv4 set -i vmk0 --type static -I 192.168.5.130 -N 255.255.255.0 -g 192.168.5.1 command to IP an interface Go into vCenter and add the Witness Make sure vmk0 of the Witness has the same MAC address of vmk0 on the host OR set the security settings of the management portgroup you are using to promiscuous","title":"After deploying the Witness:"},{"location":"VMWare/Setup%20VXRail/#set-up-vcenter","text":"Make sure your DNS server can resolve all the VxRail ESXi IPs first before continuing. On vCenter double check that DNS and NTP are setup correctly. You can do this from the network menu by going to Home->Administration->System Configuration->Click on vCenter->Login. This should bring up the appliance management screen. Check Time and Networking. vCenter has to be able to resolve all ESXi names itself. This is not checked by VxRail's validator ! To double check name resolution, go to vCenter's console, hit alt+F1, use nslookup to check all the ESXi node names.","title":"Set Up vCenter"},{"location":"VMWare/Setup%20VXRail/#set-up-the-manager-for-discovery","text":"David Ring's Install Notes are helpful David Ring's notes on how host discovery works are also helpful Loudmouth requires IPv6 multicast in order for VxRail Manager to perform a successful VxRail node discovery. IPv6 multicast is required only on the \u2018Private Management Network\u2019, this is an isolated management network solely for auto-discovery of the VxRail nodes during install or expansion. The default VLAN ID for the private network is 3939, which is configured on each VxRail node from the factory, this VLAN needs to be configured on the TOR switches and remains isolated on the TORs. If you wish to deviate from the default of VLAN 3939 then each node will need to be modified onsite otherwise node discovery will fail. The VxRail Manager comes defaulted to 192.168.10.200 however, it does not accept SSH connections Turn on the VxRail manager. vxrail-primary --setup --vxrail-address 192.168.2.100 --vxrail-netmask 255.255.255.0 --vxrail-gateway 192.168.2.1 . The IP you assign is not used for discovery. It is for reaching the VxRail appliance. Give it an IP on whatever network/vlan you plan on using for management. Details below in step 3. 1.Make sure vmk0 of all ESXi hosts is on VLAN 3939 or whatever VLAN you're using for discovery. 2.On each ESXi host make sure Private Management Network and Private VM Network are both on VLAN 3939. (Picture from David Ring's guide) 3.The VxRail manager has two virtual NICs - eth0 and eth1. Eth1 is the NIC used for discovery. Make sure eth1 of the VxRail manager is on VLAN 3939. The first NIC (eth0) should be on whatever VLAN you are using for management. You will get to the VxRail manager webgui through eth0. 2. At this point you should have full IPv6 connectivity between vmk0 on all ESXi instances and the VxRail appliance. You can test this with the following: 1.Go to the ESXi console, press ALT+F1 and Pull the IPv6 address for vmk0 on ESXi with: esxcfg-vmknic -l | grep vmk0 2.On the ESXi host with the VxRail appliance, give another (not vmk0) VM kernel NIC you have access to an IP address with the esxcli network ip interface ipv4 set -i vmk0 --type static -I 192.168.5.130 -N 255.255.255.0 -g 192.168.5.1 command. For dhcp use the --type dhcp option. See Helpful Commands for how to list out the different portgroups and VMs 3.Once you know all the IPv6 addresses for the various vmk0 nics, get on the VxRail appliance and use ping6 -l eth1 <ipv6 address> to test your IPv6 ping. This should work against all devices. If it doesn't you probably have a networking problem. 4.You can also test the full server discovery process manually from the command line with: 5.On the switch side you can use show lldp discovery to see what interfaces it has discovered and it will list the interfaces. 6.On idrac it will show you what ports each nic is connected to under network devices. (My example is down - yours should not have a warning) 7.For further troubleshooting check the Marvin log 3. Assuming you've already checked DNS, the last thing you need to do is make sure NTP is up. I'm using chrony and checked like this on my NTP server: [root@services ~]# chronyc tracking Reference ID : D8EF230C (time4.google.com) Stratum : 2 Ref time (UTC) : Thu Dec 17 16:21:48 2020 System time : 0.000059723 seconds fast of NTP time Last offset : +0.000004093 seconds RMS offset : 0.000036436 seconds Frequency : 0.283 ppm fast Residual freq : -0.000 ppm Skew : 0.006 ppm Root delay : 0.023636952 seconds Root dispersion : 0.001011974 seconds Update interval : 1037.1 seconds Leap status : Normal","title":"Set up the Manager for Discovery"},{"location":"VMWare/Setup%20VXRail/#install","text":"Before running I strongly recommend you make sure NTP is up and running on any customer owned ESXi instances, customer vCenters, and the idracs. Mismatched time causes problems. All you have to do is browse to your VxRail Manager's address on follow the prompts. The only big gotcha I ran into is when it asks for vCenter Server Management Username what it wants is not administrator@vphere.whatever. It wants vCenter's root/password combo.","title":"Install"},{"location":"VMWare/Setup%20VXRail/#advanced-troubleshooting","text":"","title":"Advanced Troubleshooting"},{"location":"VMWare/Setup%20VXRail/#check-for-esxi-logs","text":"In my case I hit a failure where a node wouldn't add. A lot of times VxRail will have finished the ESXi deployment and you can actually go log into the ESXi instance in question. If you have a customer owned vCenter you can also go check its logs. A lot of the time failures during deployment can be found in one of those locations.","title":"Check for ESXi Logs"},{"location":"VMWare/Setup%20VXRail/#marvin-logs","text":"Another good place to look is to log into the VxRail appliance and check the Marvin logs at /var/log/vmware/marvin/tomcat","title":"Marvin Logs"},{"location":"VMWare/Setup%20VXRail/#digging-through-vxrails-guts","text":"If you have problems during install you can pull VxRail's logs from the VxRail container by doing the below: Run docker ps -a and find the vxrail-system container. e8da567768e4 infra/openfaas/gateway:1.1.4 \"./gateway\" 2 days ago Up 2 days (healthy) 8080/tcp, 8082/tcp func_gateway.1.h5hmx70g8545d3apbwfxtfatr 2b71bfd81e90 infra/openfaas/queue-worker:1.1.4 \"./app\" 2 days ago Up 2 days (healthy) 8080/tcp func_queue-worker.1.897j8f0tbhas1989o0ygl5ez4 ba2f65a96664 infra/workflow_engine:1.1.6 \"./docker-entrypoint\u2026\" 2 days ago Up 2 days (healthy) 5000/tcp func_wfservice.1.wau7b1omhg3igjzzyvd865r6e d07929b37e2d infra/openfaas/gateway:1.1.4 \"./gateway\" 2 days ago Exited (1) 2 days ago func_gateway.1.stdin4eg1dysl3fajgpds5rwj e6f3ec734f8a infra/openfaas/queue-worker:1.1.4 \"./app\" 2 days ago Exited (2) 2 days ago func_queue-worker.1.uqy8d482650v4yquz98fnp9tq bc402e3b676c microservice/vxm-agent:1.1.6 \"/bin/bash entrypoin\u2026\" 2 days ago Up 2 days (healthy) 5000/tcp vxm-agent_vxm-agent_1 23e0ac7d3f92 microservice/nano-service:1.1.29 \"python -u index.py\" 2 days ago Up 2 days (healthy) 5000/tcp func_nano-service.1.dqo23zdkgmwbqf9ru6yn0t99r 54df5cb0af13 microservice/ms-day1-bringup:1.1.29 \"/bin/bash entrypoin\u2026\" 2 days ago Up 2 days (healthy) 5000/tcp func_ms-day1-bringup.1.xfe0xndtvsqbo17kapogyd7ke 87e26e7d27b2 infra/infra-lock-service:1.1.8 \"/bin/bash entrypoin\u2026\" 2 days ago Up 2 days (healthy) 5000/tcp func_lockservice.1.g2g7hg5viyph8r4ptcljeran9 4382600d0094 microservice/lockbox:1.1.11 \"/bin/bash entrypoin\u2026\" 2 days ago Up 2 days (healthy) 5000/tcp func_lockbox.1.q9hqbfuid54r5gelsazasksh2 f7964475a933 microservice/kgs-service:1.1.10 \"./boot.sh\" 2 days ago Up 2 days (healthy) 5000/tcp func_kgs-service.1.wpqm7hytpf5l9wo7ch3sc1wz7 75178ef078ea do-main/do-vxrail-system:1.1.35 \"/bin/bash entrypoin\u2026\" 2 days ago Up 2 days (healthy) 5000/tcp func_do-vxrail-system.1.e67dbhktpe88gw0wtmymmpthg 7abeddce2bb8 do-main/do-vm:1.1.35 \"/bin/bash entrypoin\u2026\" 2 days ago Up 2 days (healthy) 5000/tcp func_do-vm.1.g2fwikfzimyhv6di3zdalt1j3 3b557989c07e do-main/do-storage:1.1.35 \"/bin/bash entrypoin\u2026\" 2 days ago Up 2 days (healthy) 5000/tcp func_do-storage.1.w2nv1rfpidrv7g418xmknywso d0b15759e7f5 do-main/do-serviceability:1.1.35 \"/bin/bash entrypoin\u2026\" 2 days ago Up 2 days (healthy) func_do-serviceability.1.t5xkwt1eu2ypqvn0511i1gfy3 5e349b22f36a do-main/do-network:1.1.39 \"/bin/bash entrypoin\u2026\" 2 days ago Up 2 days (healthy) 5000/tcp func_do-network.1.vogoz7jtyrxf83h9jd0615r28 ae31c7cd94bd do-main/do-kgs:1.1.35 \"./boot.sh\" 2 days ago Up 2 days (healthy) 5000/tcp func_do-kgs.1.esudf6d39uy7e96dnvxoabaue 978447c9f6f8 do-main/do-host:1.1.39 \"/bin/bash entrypoin\u2026\" 2 days ago Up 2 days (healthy) 5000/tcp func_do-host.1.ravl83nt962k48up0v33qoviy 862926a58073 do-main/do-ecosystem:1.1.35 \"/bin/bash entrypoin\u2026\" 2 days ago Up 2 days (healthy) func_do-ecosystem.1.8k6cljhvo5p00632gt11f79uv 63dde78fc922 do-main/do-eservices:1.1.35 \"/bin/bash entrypoin\u2026\" 2 days ago Up 2 days (healthy) func_do-eservices.1.pnpnr1pofm0dvwv36trufl6g9 f1fb1d35457d do-main/do-cluster:1.1.35 \"/bin/bash entrypoin\u2026\" 2 days ago Up 2 days (healthy) 5000/tcp func_do-cluster.1.y4vs2diiyy5eau3ojuxdx12wx 2fc837e0726e infra/infra-config-service:1.1.9 \"/bin/bash entrypoin\u2026\" 2 days ago Up 2 days (healthy) 5000/tcp func_configservice.1.byjhptdpdxa3d04y7mr5lfp5z 685d833e0155 infra/openfaas/gateway:1.1.4 \"./gateway\" 2 days ago Exited (1) 2 days ago func_gateway.1.ztar1zozhfuh3skopwvlandwq 9af0891d5455 infra/openfaas/queue-worker:1.1.4 \"./app\" 2 days ago Exited (2) 2 days ago func_queue-worker.1.jmk9q8bkc6a2vlz3fg1e60y5f 0cb6d7948db9 infra/tracing:1.1.6 \"/go/bin/jaeger-all-\u2026\" 2 days ago Up 2 days (healthy) func_tracing.1.ymsjr3ba7ih2x21291xri4mz8 5e70fa63df88 infra/openfaas/faas-swarm:1.1.4 \"./faas-swarm\" 2 days ago Up 2 days (healthy) 8080/tcp func_faas-swarm.1.nfz0dm48hhx7vh9sh04mbagtz 620cce221e65 infra/nats-streaming:1.1.4 \"/nats-streaming-ser\u2026\" 2 days ago Up 2 days (healthy) 4222/tcp, 8222/tcp func_nats.1.l42vx07v08vhhtkv1lwxg1ipb dd16298b3e4c infra/nginx_gateway_vxrail:1.1.7 \"/bin/sh -c 'sudo /u\u2026\" 2 days ago Up 2 days (healthy) func_api-gateway.1.do9gzerxmmlti4rn8k3ab2gs4 4a8b6c976103 infra/gcr.io/etcd-development/etcd:1.1.4 \"/usr/local/bin/etcd\u2026\" 2 days ago Up 2 days (healthy) 2379-2380/tcp func_serviceregistry.1.u2gtw5wnsldulqgd1r47wkte5 acf803b61b04 infra/redis:1.1.4 \"docker-entrypoint.s\u2026\" 2 days ago Up 2 days (healthy) 6379/tcp func_cacheservice.1.subywzigvenobaskavjs6xyqc 3dac923f9bff infra/logging:1.1.4 \"tini -- /bin/entryp\u2026\" 2 days ago Up 2 days (healthy) 5140/tcp, 24224/tcp func_logging.1.kv7738umixulwryqpl38qvxvr bdd88fadc236 infra/prom/alertmanager:1.1.4 \"/bin/alertmanager -\u2026\" 2 days ago Up 2 days (healthy) 9093/tcp func_alertmanager.1.8uks430a7p10keeh8uqv625bv 7ec04254fee1 infra/prom/prometheus:1.1.4 \"/bin/prometheus --c\u2026\" 2 days ago Up 2 days (healthy) 9090/tcp func_prometheus.1.ti36b3wsder4w01v8nentdirx Find the log location with docker inspect --format='{{.LogPath}}' 75178ef078ea . They are in JSON format. For example, this is what my error looked like: This got me the reason for my failure: {\\\\\\\"error\\\\\\\": {\\\\\\\"result\\\\\\\": {\\\\\\\"error\\\\\\\": {\\\\\\\"code\\\\\\\": \\\\\\\"E3100_Cluster_22\\\\\\\", \\\\\\\"message\\\\\\\": \\\\\\\"failed to add host into cluster\\\\\\\", \\\\\\\"params\\\\\\\": [\\\\\\\"vxrail1-esxi.lan\\\\\\\"]}}}, \\\\\\\"id\\\\\\\": \\\\\\\"host_add_into_inventory\\\\\\\", \\\\\\\"internal_family\\\\\\\": \\\\\\\"node_one_bootstrap_with_cns_ready.primary_node_computer.host_add_into_inventory\\\\\\\", \\\\\\\"internal_id\\\\\\\": \\\\\\\"node_one_bootstrap_with_cns_ready.primary_node_computer.host_add_into_inventory_False____99ee8388_1147_4e_9d65c98cb2\\\\\\\", \", \"stream\": \"stderr\", \"time\": \"2020-12-18T13:24:42.028798775Z\" } {\"log\":\"\\\\\\\"params\\\\\\\": {\\\\\\\"cluster_name\\\\\\\": \\\\\\\"vxrail-cluster\\\\\\\", \\\\\\\"datacenter_name\\\\\\\": \\\\\\\"Datacenter\\\\\\\", \\\\\\\"host_conn_info\\\\\\\": {\\\\\\\"host\\\\\\\": \\\\\\\"192.168.2.31\\\\\\\", \\\\\\\"port\\\\\\\": 443, \\\\\\\"username\\\\\\\": \\\\\\\"root\\\\\\\"}, \\\\\\\"hostname\\\\\\\": \\\\\\\"vxrail1-esxi.lan\\\\\\\", \\\\\\\"ssl_thumbprint\\\\\\\": \\\\\\\"STUFF\\\\\\\", \\\\\\\"vc_conn_info\\\\\\\": {\\\\\\\"host\\\\\\\": \\\\\\\"vcenter.lan\\\\\\\", \\\\\\\"port\\\\\\\": 443, \\\\\\\"username\\\\\\\": \\\\\\\"administrator@vsphere.lan\\\\\\\"}, \\\\\\\"vm_folder_name\\\\\\\": \\\\\\\"VMware HCIA Folder vxrail-cluster\\\\\\\", \\\\\\\"vsan_name\\\\\\\": \\\\\\\"VxRail-Virtual-SAN-Datastore-22658cd0-5b17-428d-a23c-0d7e5a877bb7\\\\\\\"}, \\\\\\\"stage\\\\\\\": \\\\\\\"primary_node_computer\\\\\\\", \\\\\\\"startTime\\\\\\\": 1608297822107, \\\\\\\"status\\\\\\\": \\\\\\\"FAILED\\\\\\\"}","title":"Digging Through VxRail's Guts"},{"location":"VMWare/Setup%20VXRail/#helpful-commands","text":"","title":"Helpful Commands"},{"location":"VMWare/Setup%20VXRail/#check-if-vxrail-is-running","text":"esxcli vm process list","title":"Check if VxRail is Running"},{"location":"VMWare/Setup%20VXRail/#get-a-list-of-vms","text":"vim-cmd vmsvc/getallvms","title":"Get a List of VMs"},{"location":"VMWare/Setup%20VXRail/#check-if-vm-is-on","text":"vim-cmd vmsvc/power.getstate 1","title":"Check if VM is On"},{"location":"VMWare/Setup%20VXRail/#check-networking","text":"esxcli network vswitch standard portgroup list","title":"Check Networking"},{"location":"VMWare/Setup%20VXRail/#set-the-vlan-for-a-switch","text":"esxcli network vswitch standard portgroup set -p \"VM Network\" -v 120","title":"Set the VLAN for a Switch"},{"location":"VMWare/Setup%20VXRail/#get-interface-ips","text":"esxcli network ip interface ipv4 get","title":"Get Interface IPs"},{"location":"VMWare/Setup%20VXRail/#get-interface-list","text":"esxcli network ip interface list","title":"Get Interface List"},{"location":"VMWare/Setup%20VXRail/vcenter_deploy/","text":"vCenter Deploy It looks like deploying vCenter is controlled by the file vc_deploy.py : vxrailmanager:~ # find / -iname vc_deploy.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vcdeploy/vc_deploy.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vcdeploy/vc_deploy.py Run docker inspect $(docker ps -qa) | jq -r 'map([.Name, .GraphDriver.Data.MergedDir]) | .[] | \"\\(.[0])\\t\\(.[1])\"' to figure out which container owns that overlay. Based on that it looks like there is a container called nano-service which owns that file: vxrailmanager:~ # docker inspect $(docker ps -qa) | jq -r 'map([.Name, .GraphDriver.Data.MergedDir]) | .[] | \"\\(.[0])\\t\\(.[1])\"' | grep 5c284f1529c19ad /func_nano-service.1.dqo23zdkgmwbqf9ru6yn0t99r /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged vxrailmanager:~ # docker inspect $(docker ps -qa) | jq -r 'map([.Name, .GraphDriver.Data.MergedDir]) | .[] | \"\\(.[0])\\t\\(.[1])\"' | grep 6e4546b5beb8a Drop to container command line: docker exec -it 23e0ac7d3f92 /bin/bash Browse around and take a look at vc_deploy.py : logger = logging.getLogger(__name__) ns = api.namespace('', description='Operations related to internal vc case') host_info_model = get_host_conn_info_model(api) storage_info_model = get_storage_info_model(api) sync_call_result = build_api_model_sync_call_result(api) vc_deploy_model = api.model('VCDeploy', { 'host_conn_info': fields.Nested(host_info_model), 'storage_info': fields.Nested(storage_info_model), 'cluster_type': fields.String(required=True, description='vxrail cluster type') }) VC_VM_NAME = 'VMware vCenter Server Appliance' do_vm = DoCaller(do_host='do-vm', do_prefix='do-vm') def perform(self, request_body): logger.info(\"start to perform VCDeploy, cluster type: {}\".format(self.cluster_type)) src_datastore = self._get_source_datastore_name() dst_datastore = self.storage_info.get('datastore') if self.cluster_type == ClusterType.COMPUTE.value else 'vsanDatastore' body = { 'host_conn_info': self.host_conn_info, 'src_datastore_name': src_datastore, 'dst_datastore_name': dst_datastore, 'vm_name': VC_VM_NAME } result = None try: result = do_vm.sync_call_do(do_vm.build_call_do_service_task_url(HOST_VM_OVA_DEPLOY_URL), body) except Exception as e: self.logger.info(e) self.raise_error('E3100_Cluster_24', params=(), message='Failed to deploy vCenter.') return result Doing a search on grep -rli 'external vc' / 2> /dev/null gets ... /var/lib/docker/overlay2/19f438b6cef2857e6067e49802d04d61de2afb7275bc6fb1323a41bac03fec1c/diff/home/app/worker/task/cluster.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/common/conf/config-messages.properties /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vcmgmtaccountpermissiongrant/vc_mgmt_account_permission_grant.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/externalvclicensevalidator/external_vc_license_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/externalvcaccesslivevalidator/external_vc_access_live_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/externalvccapacitylimitlivevalidator/external_vc_capacity_limit_live_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostaddtovds/host_add_to_vds.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hosttimesync/host_time_sync.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/timeconfigurationlivevalidator/time_configuration_live_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostunusedsystemvmsremove/host_unused_system_vms_remove.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/externalvcversioncompatibilitylivevalidator/external_vc_version_compatibility_live_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vxmtimesync/vxm_time_sync.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/timesyncconfigurationvalidator/time_sync_configuration_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vcsyslogconfig/vc_syslog_config.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/dnshostnameslivevalidator/dns_hostnames_live_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/cursoryvalidator/hostname_configuration_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/cursoryvalidator/tests/test_hostname_configuration_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/cursoryvalidator/tests/test_none_value_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/cursoryvalidator/tests/test_vcenter_configuration_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/cursoryvalidator/tests/test_dns_servers_configuration_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/cursoryvalidator/vcenter_configuration_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/cursoryvalidator/none_value_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/cursoryvalidator/dns_servers_configuration_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/cursoryvalidator/passwords_configuration_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/cursoryvalidator/netmask_configuration_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vcuserpermissionlivevalidator/vc_user_permission_live_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/createvmfolder/create_vm_folder.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/externalvcentitieslivevalidator/external_vc_entities_live_validator.py /var/lib/docker/overlay2/6df33189ca552a920f2d6c3259a034fac3ec47f8065634a6ba17ff7c74299377/merged/home/app/messages/messages_en_US.properties /var/lib/docker/overlay2/6df33189ca552a920f2d6c3259a034fac3ec47f8065634a6ba17ff7c74299377/merged/home/app/dayone/dayone_processor.py /var/lib/docker/overlay2/6df33189ca552a920f2d6c3259a034fac3ec47f8065634a6ba17ff7c74299377/merged/home/app/dayone/api.py /var/lib/docker/overlay2/1bc6389a4238217d15b1f5d931c6049e53891a92cef4255f6b402138e130d4d9/merged/home/app/lockbox_app/dao/postgres_dao.py /var/lib/docker/overlay2/1bc6389a4238217d15b1f5d931c6049e53891a92cef4255f6b402138e130d4d9/merged/home/app/lockbox_app/resources/verify_error_messages.json ... Remove the cursory checks: (base) grant@DESKTOP-2SV1E9O:~$ grep -v cursory test.txt /var/lib/docker/overlay2/19f438b6cef2857e6067e49802d04d61de2afb7275bc6fb1323a41bac03fec1c/diff/home/app/worker/task/cluster.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/common/conf/config-messages.properties /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vcmgmtaccountpermissiongrant/vc_mgmt_account_permission_grant.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/externalvclicensevalidator/external_vc_license_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/externalvcaccesslivevalidator/external_vc_access_live_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/externalvccapacitylimitlivevalidator/external_vc_capacity_limit_live_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostaddtovds/host_add_to_vds.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hosttimesync/host_time_sync.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/timeconfigurationlivevalidator/time_configuration_live_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostunusedsystemvmsremove/host_unused_system_vms_remove.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/externalvcversioncompatibilitylivevalidator/external_vc_version_compatibility_live_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vxmtimesync/vxm_time_sync.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/timesyncconfigurationvalidator/time_sync_configuration_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vcsyslogconfig/vc_syslog_config.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/dnshostnameslivevalidator/dns_hostnames_live_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vcuserpermissionlivevalidator/vc_user_permission_live_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/createvmfolder/create_vm_folder.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/externalvcentitieslivevalidator/external_vc_entities_live_validator.py /var/lib/docker/overlay2/6df33189ca552a920f2d6c3259a034fac3ec47f8065634a6ba17ff7c74299377/merged/home/app/messages/messages_en_US.properties /var/lib/docker/overlay2/6df33189ca552a920f2d6c3259a034fac3ec47f8065634a6ba17ff7c74299377/merged/home/app/dayone/dayone_processor.py /var/lib/docker/overlay2/6df33189ca552a920f2d6c3259a034fac3ec47f8065634a6ba17ff7c74299377/merged/home/app/dayone/api.py /var/lib/docker/overlay2/1bc6389a4238217d15b1f5d931c6049e53891a92cef4255f6b402138e130d4d9/merged/home/app/lockbox_app/dao/postgres_dao.py /var/lib/docker/overlay2/1bc6389a4238217d15b1f5d931c6049e53891a92cef4255f6b402138e130d4d9/merged/home/app/lockbox_app/resources/verify_error_messages.json Looking for the call build_call_do_service_task_url vxrailmanager:/var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vcdeploy # grep -rli 'build_call_do_service_task_url' /var/lib/docker/ 2> /dev/null /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/common/do.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/common/validation/ip_live_check_utils.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/common/network_prepare_aware_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vxmconfigurationdatapopulate/vxm_configuration_data_populate.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostaddintocluster/host_add_into_cluster.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/clusterconverttostretchedcluster/cluster_convert_to_stretched_cluster.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostpsipupdate/host_ps_ip_update.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/multidiskgroupconfigurationvalidator/multi_disk_group_configuration_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vcinitialboot/vc_initial_boot.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/clusterdasconfig/cluster_das_config.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostnetworkmanagementipchange/host_network_management_ip_change.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vcmgmtaccountpermissiongrant/vc_mgmt_account_permission_grant.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vxrailkgsbaselineload/vxm_kgs_baseline_load.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vcretreatmodeconfigure/vc_retreat_mode_configure.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/enableteaming/enableteaming.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/externalvcaccesslivevalidator/external_vc_access_live_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hosthostnamechange/host_hostname_change.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vxmupdatecertification/vxm_update_certification.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hoststorageleavevsan/host_storage_leave_vsan.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vxmconfigureslaacip/vxm_configure_slaacip.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vxmsystemservicesstop/vxm_system_services_stop.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/removedisconnecthostfromcluster/remove_disconnect_host_from_cluster.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostaddtovds/host_add_to_vds.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostassignactivepnictovds/host_assign_active_pnic_to_vds.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostassignvmktovds/host_assign_vmk_to_vds.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/clusterhighavailabilityenable/cluster_high_availability_enable.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/switchcreatenetworks/switch_create_networks.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vxmruntimepropertiesgenerate/vxm_runtime_properties_generate.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostvsanstorageenable/host_vsan_storage_enable.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hosttimesync/host_time_sync.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vcguestopsalias/vc_guest_ops_alias.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostwitnesstrafficconnectivity/host_witness_traffic_connectivity.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostvsandiskgroupscreate/host_vsan_diskgroups_create.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/createvds/createvds.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/switchconfiguplinks/switch_configure_uplinks.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/createportgroup/createportgroup.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vcpluginregister/vc_plugin_register.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vcdatacentercreate/vc_datacenter_create.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hoststoragevsandiskremove/host_storage_vsan_disk_remove.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostptagentipchange/host_ptagent_ip_change.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/customersuppliedvdsvlanlivevalidator/customer_supplied_vds_vlan_live_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostversioncompatibilitylivevalidator/host_version_compatibility_live_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/esxihostaccountvalidator/esxi_host_account_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostvsanstoragepolicyconfiguration/host_vsan_storage_policy_configuration.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostunusedsystemvmsremove/host_unused_system_vms_remove.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/switchconfigjumpbox/switch_configure_jumpbox.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vccertsgenerate/vc_certs_generate.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/clusterdrsenable/cluster_drs_enable.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vclicensing/vc_licensing.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostelectionservicedisable/host_election_service_disable.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/clusteraddexceptionaccounts/cluster_add_exception_accounts.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vcpasswordexpirationdisable/vc_password_expiration_disable.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vxmnetworkvnicremove/vxm_network_vnic_remove.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/externalvcversioncompatibilitylivevalidator/external_vc_version_compatibility_live_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/omeintegration/ome_integration.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostmarkpnicworkingmode/host_mark_pnic_working_mode.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/switchcreatedownlinks/switch_create_downlinks.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vxmevopassword/vxm_evo_password.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/movevxmvnictovds/vxm_network_vnic_add_to_vds.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostloudmouthrestart/host_loudmouth_restart.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hoststorageaddtovsan/host_storage_add_to_vsan.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostloggingsystemconfigure/host_logging_system_configure.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/clusteradvanceparamssetting/cluster_advance_params_setting.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/resetvspherealarms/reset_vsphere_alarms.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/networkcompatibilitylivevalidator/network_compatibility_live_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/switchauthentication/switch_authentication.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vxmtimesync/vxm_time_sync.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostsecurityservicedisable/host_security_service_disable.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/timesyncconfigurationvalidator/time_sync_configuration_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vxmstaticipchange/vxm_static_ipchange.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/createstoragepolicy/create_storage_policy.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vxmclustercleanup/vxm_cluster_clean_up.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/switchchangepassword/switch_change_password.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostplatformservicerestart/host_platform_service_restart.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostdnschange/host_dns_change.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vxmautostartenable/vxm_auto_start_enable.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vlanlivevalidator/vlan_live_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vxmserviceaccountvalidator/vxm_service_account_validator_task.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vxmhostnameset/vxm_hostname_set.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vcautostartenable/vc_auto_start_enable.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vcdeploy/vc_deploy.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostwitnessnetworking/host_witness_networking.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostmanagementaccountcreate/host_management_account_create.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/enablenioc/enablenioc.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vcsyslogconfig/vc_syslog_config.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vxmsystemservicesrestart/vxm_system_services_restart.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/checknodestatus/check_node_status.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vxmmovetostorage/vxm_move_to_storage.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostassignpnictovss/host_assign_pnic_to_vss.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vcmgmntaccountcreate/vc_mgmnt_account_create.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/clusterevcenable/cluster_evc_enable.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vxmsetdns/vxm_set_dns.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/cursoryvalidator/cursory_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostvssremove/host_vss_remove.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/clustercreate/clustercreate.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostaccountpasswordchange/host_account_password_change.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vxmrootaccountvalidator/vxm_root_account_validator_task.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostproxyservicedisable/host_proxy_service_disable.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/clustersystemvmstoragepolicyapply/cluster_system_vm_storage_policy_apply.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostassignstandbypnictovds/host_assign_standby_pnic_to_vds.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/createvmfolder/create_vm_folder.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/postdayoneconfiguration/post_dayone_configuration.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostnetworkmanagementvlanchange/host_network_management_vlan_change.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vxmupdateinternaldns/vxm_update_internal_dns.py /var/lib/docker/overlay2/6df33189ca552a920f2d6c3259a034fac3ec47f8065634a6ba17ff7c74299377/merged/home/app/dayone/do.py /var/lib/docker/overlay2/6df33189ca552a920f2d6c3259a034fac3ec47f8065634a6ba17ff7c74299377/merged/home/app/dayone/transfer_handler.py /var/lib/docker/overlay2/6df33189ca552a920f2d6c3259a034fac3ec47f8065634a6ba17ff7c74299377/merged/home/app/dayone/storage_discovery_callback.py /var/lib/docker/overlay2/6df33189ca552a920f2d6c3259a034fac3ec47f8065634a6ba17ff7c74299377/merged/home/app/vxm_ip_change_scheduler.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vxmconfigurationdatapopulate/vxm_configuration_data_populate.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hostaddintocluster/host_add_into_cluster.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/clusterconverttostretchedcluster/cluster_convert_to_stretched_cluster.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hostpsipupdate/host_ps_ip_update.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/multidiskgroupconfigurationvalidator/multi_disk_group_configuration_validator.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vcinitialboot/vc_initial_boot.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/clusterdasconfig/cluster_das_config.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hostnetworkmanagementipchange/host_network_management_ip_change.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vcmgmtaccountpermissiongrant/vc_mgmt_account_permission_grant.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vxrailkgsbaselineload/vxm_kgs_baseline_load.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vcretreatmodeconfigure/vc_retreat_mode_configure.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/enableteaming/enableteaming.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/externalvcaccesslivevalidator/external_vc_access_live_validator.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hosthostnamechange/host_hostname_change.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vxmupdatecertification/vxm_update_certification.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hoststorageleavevsan/host_storage_leave_vsan.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vxmconfigureslaacip/vxm_configure_slaacip.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vxmsystemservicesstop/vxm_system_services_stop.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/removedisconnecthostfromcluster/remove_disconnect_host_from_cluster.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hostaddtovds/host_add_to_vds.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hostassignactivepnictovds/host_assign_active_pnic_to_vds.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hostassignvmktovds/host_assign_vmk_to_vds.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/clusterhighavailabilityenable/cluster_high_availability_enable.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/switchcreatenetworks/switch_create_networks.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vxmruntimepropertiesgenerate/vxm_runtime_properties_generate.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hostvsanstorageenable/host_vsan_storage_enable.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hosttimesync/host_time_sync.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vcguestopsalias/vc_guest_ops_alias.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hostwitnesstrafficconnectivity/host_witness_traffic_connectivity.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hostvsandiskgroupscreate/host_vsan_diskgroups_create.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/createvds/createvds.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/switchconfiguplinks/switch_configure_uplinks.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/createportgroup/createportgroup.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vcpluginregister/vc_plugin_register.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vcdatacentercreate/vc_datacenter_create.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hoststoragevsandiskremove/host_storage_vsan_disk_remove.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hostptagentipchange/host_ptagent_ip_change.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/customersuppliedvdsvlanlivevalidator/customer_supplied_vds_vlan_live_validator.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hostversioncompatibilitylivevalidator/host_version_compatibility_live_validator.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/esxihostaccountvalidator/esxi_host_account_validator.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hostvsanstoragepolicyconfiguration/host_vsan_storage_policy_configuration.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hostunusedsystemvmsremove/host_unused_system_vms_remove.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/switchconfigjumpbox/switch_configure_jumpbox.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vccertsgenerate/vc_certs_generate.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/clusterdrsenable/cluster_drs_enable.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vclicensing/vc_licensing.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hostelectionservicedisable/host_election_service_disable.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/clusteraddexceptionaccounts/cluster_add_exception_accounts.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vcpasswordexpirationdisable/vc_password_expiration_disable.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vxmnetworkvnicremove/vxm_network_vnic_remove.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/externalvcversioncompatibilitylivevalidator/external_vc_version_compatibility_live_validator.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/omeintegration/ome_integration.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hostmarkpnicworkingmode/host_mark_pnic_working_mode.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/switchcreatedownlinks/switch_create_downlinks.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vxmevopassword/vxm_evo_password.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/movevxmvnictovds/vxm_network_vnic_add_to_vds.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hostloudmouthrestart/host_loudmouth_restart.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hoststorageaddtovsan/host_storage_add_to_vsan.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hostloggingsystemconfigure/host_logging_system_configure.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/clusteradvanceparamssetting/cluster_advance_params_setting.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/resetvspherealarms/reset_vsphere_alarms.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/networkcompatibilitylivevalidator/network_compatibility_live_validator.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/switchauthentication/switch_authentication.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vxmtimesync/vxm_time_sync.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hostsecurityservicedisable/host_security_service_disable.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/timesyncconfigurationvalidator/time_sync_configuration_validator.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vxmstaticipchange/vxm_static_ipchange.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/createstoragepolicy/create_storage_policy.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vxmclustercleanup/vxm_cluster_clean_up.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/common/do.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/common/validation/ip_live_check_utils.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/common/network_prepare_aware_validator.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/switchchangepassword/switch_change_password.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hostplatformservicerestart/host_platform_service_restart.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hostdnschange/host_dns_change.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vxmautostartenable/vxm_auto_start_enable.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vlanlivevalidator/vlan_live_validator.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vxmserviceaccountvalidator/vxm_service_account_validator_task.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vxmhostnameset/vxm_hostname_set.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vcautostartenable/vc_auto_start_enable.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vcdeploy/vc_deploy.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hostwitnessnetworking/host_witness_networking.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hostmanagementaccountcreate/host_management_account_create.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/enablenioc/enablenioc.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vcsyslogconfig/vc_syslog_config.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vxmsystemservicesrestart/vxm_system_services_restart.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/checknodestatus/check_node_status.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vxmmovetostorage/vxm_move_to_storage.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hostassignpnictovss/host_assign_pnic_to_vss.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vcmgmntaccountcreate/vc_mgmnt_account_create.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/clusterevcenable/cluster_evc_enable.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vxmsetdns/vxm_set_dns.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/cursoryvalidator/cursory_validator.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hostvssremove/host_vss_remove.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/clustercreate/clustercreate.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hostaccountpasswordchange/host_account_password_change.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vxmrootaccountvalidator/vxm_root_account_validator_task.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hostproxyservicedisable/host_proxy_service_disable.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/clustersystemvmstoragepolicyapply/cluster_system_vm_storage_policy_apply.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hostassignstandbypnictovds/host_assign_standby_pnic_to_vds.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/createvmfolder/create_vm_folder.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/postdayoneconfiguration/post_dayone_configuration.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hostnetworkmanagementvlanchange/host_network_management_vlan_change.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vxmupdateinternaldns/vxm_update_internal_dns.py /var/lib/docker/overlay2/0d8ac81dbb9015f2f3526cd468c2c4e7e433661e38008b04f70539deee19272a/diff/home/app/dayone/do.py /var/lib/docker/overlay2/0d8ac81dbb9015f2f3526cd468c2c4e7e433661e38008b04f70539deee19272a/diff/home/app/dayone/transfer_handler.py /var/lib/docker/overlay2/0d8ac81dbb9015f2f3526cd468c2c4e7e433661e38008b04f70539deee19272a/diff/home/app/dayone/storage_discovery_callback.py /var/lib/docker/overlay2/02a3e09ef808a5f704f821bbf2fcc89ece02f262e0a20b1c5a599fbf949a8c9b/diff/home/app/vxm_ip_change_scheduler.py Finding the definition: vxrailmanager:/var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vcdeploy # grep -rli 'def build_call_do_service_task_url' /var/lib/docker/ 2> /dev/null /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/common/do.py /var/lib/docker/overlay2/6df33189ca552a920f2d6c3259a034fac3ec47f8065634a6ba17ff7c74299377/merged/home/app/dayone/do.py /var/lib/docker/overlay2/6df33189ca552a920f2d6c3259a034fac3ec47f8065634a6ba17ff7c74299377/merged/home/app/vxm_ip_change_scheduler.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/common/do.py /var/lib/docker/overlay2/0d8ac81dbb9015f2f3526cd468c2c4e7e433661e38008b04f70539deee19272a/diff/home/app/dayone/do.py /var/lib/docker/overlay2/02a3e09ef808a5f704f821bbf2fcc89ece02f262e0a20b1c5a599fbf949a8c9b/diff/home/app/vxm_ip_change_scheduler.py There seems to be a file for licensing: Find embedded license: vxrailmanager:/var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vcdeploy # find / -iname license-vc-700-e1-marvin-c1-201909 \\/usr/lib/vmware-marvin/marvind/webapps/lcm/WEB-INF/classes/licensing/7.0/license-vc-700-e1-marvin-c1-201909 /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/common/conf/license-vc-700-e1-marvin-c1-201909 /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/common/conf/license-vc-700-e1-marvin-c1-201909 vxrailmanager:/var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vcdeploy # cat /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/common/conf/license-vc-700-e1-marvin-c1-201909 # VMware software license StartFields = \"Cpt, ProductID, LicenseVersion, LicenseType, LicenseEdition, Option, Epoch\" Cpt = \"COPYRIGHT (c) VMware, Inc.\" ProductID = \"VMware VirtualCenter Server\" LicenseVersion = \"7.0\" LicenseType = \"Site\" Epoch = \"2019-9-1\" LicenseEdition = \"vc.standard.instance.marvin\" Option = \"7\" Data = \"FileVersion=7.0.0.2;capacityType=server;enable=linkedvc,woe,xvp,vcha,backuprestore,appliancemigration;addon=vsa;disallowedHostEditions=esx.hypervisor.cpuPackage,esx.hypervisorEmbeddedOem.cpuPackage,esx.essentials.cpuPackage,esx.essentialsPlus.cpuPackage,esx.hypervisor.cpuPackageCoreLimited,esx.hypervisorEmbeddedOem.cpuPackageCoreLimited,esx.essentials.cpuPackageCoreLimited,esx.essentialsPlus.cpuPackageCoreLimited;desc=vCenter Server 7 Standard\" DataHash = \"60aebea4-31a0050c-c4e49494-27690fad-5a8a5258\" Hash = \"441d0833-d2f3bed9-0bea3fec-7d13664b-e222d862\"","title":"vCenter Deploy"},{"location":"VMWare/Setup%20VXRail/vcenter_deploy/#vcenter-deploy","text":"It looks like deploying vCenter is controlled by the file vc_deploy.py : vxrailmanager:~ # find / -iname vc_deploy.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vcdeploy/vc_deploy.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vcdeploy/vc_deploy.py Run docker inspect $(docker ps -qa) | jq -r 'map([.Name, .GraphDriver.Data.MergedDir]) | .[] | \"\\(.[0])\\t\\(.[1])\"' to figure out which container owns that overlay. Based on that it looks like there is a container called nano-service which owns that file: vxrailmanager:~ # docker inspect $(docker ps -qa) | jq -r 'map([.Name, .GraphDriver.Data.MergedDir]) | .[] | \"\\(.[0])\\t\\(.[1])\"' | grep 5c284f1529c19ad /func_nano-service.1.dqo23zdkgmwbqf9ru6yn0t99r /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged vxrailmanager:~ # docker inspect $(docker ps -qa) | jq -r 'map([.Name, .GraphDriver.Data.MergedDir]) | .[] | \"\\(.[0])\\t\\(.[1])\"' | grep 6e4546b5beb8a Drop to container command line: docker exec -it 23e0ac7d3f92 /bin/bash Browse around and take a look at vc_deploy.py : logger = logging.getLogger(__name__) ns = api.namespace('', description='Operations related to internal vc case') host_info_model = get_host_conn_info_model(api) storage_info_model = get_storage_info_model(api) sync_call_result = build_api_model_sync_call_result(api) vc_deploy_model = api.model('VCDeploy', { 'host_conn_info': fields.Nested(host_info_model), 'storage_info': fields.Nested(storage_info_model), 'cluster_type': fields.String(required=True, description='vxrail cluster type') }) VC_VM_NAME = 'VMware vCenter Server Appliance' do_vm = DoCaller(do_host='do-vm', do_prefix='do-vm') def perform(self, request_body): logger.info(\"start to perform VCDeploy, cluster type: {}\".format(self.cluster_type)) src_datastore = self._get_source_datastore_name() dst_datastore = self.storage_info.get('datastore') if self.cluster_type == ClusterType.COMPUTE.value else 'vsanDatastore' body = { 'host_conn_info': self.host_conn_info, 'src_datastore_name': src_datastore, 'dst_datastore_name': dst_datastore, 'vm_name': VC_VM_NAME } result = None try: result = do_vm.sync_call_do(do_vm.build_call_do_service_task_url(HOST_VM_OVA_DEPLOY_URL), body) except Exception as e: self.logger.info(e) self.raise_error('E3100_Cluster_24', params=(), message='Failed to deploy vCenter.') return result Doing a search on grep -rli 'external vc' / 2> /dev/null gets ... /var/lib/docker/overlay2/19f438b6cef2857e6067e49802d04d61de2afb7275bc6fb1323a41bac03fec1c/diff/home/app/worker/task/cluster.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/common/conf/config-messages.properties /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vcmgmtaccountpermissiongrant/vc_mgmt_account_permission_grant.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/externalvclicensevalidator/external_vc_license_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/externalvcaccesslivevalidator/external_vc_access_live_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/externalvccapacitylimitlivevalidator/external_vc_capacity_limit_live_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostaddtovds/host_add_to_vds.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hosttimesync/host_time_sync.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/timeconfigurationlivevalidator/time_configuration_live_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostunusedsystemvmsremove/host_unused_system_vms_remove.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/externalvcversioncompatibilitylivevalidator/external_vc_version_compatibility_live_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vxmtimesync/vxm_time_sync.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/timesyncconfigurationvalidator/time_sync_configuration_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vcsyslogconfig/vc_syslog_config.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/dnshostnameslivevalidator/dns_hostnames_live_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/cursoryvalidator/hostname_configuration_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/cursoryvalidator/tests/test_hostname_configuration_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/cursoryvalidator/tests/test_none_value_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/cursoryvalidator/tests/test_vcenter_configuration_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/cursoryvalidator/tests/test_dns_servers_configuration_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/cursoryvalidator/vcenter_configuration_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/cursoryvalidator/none_value_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/cursoryvalidator/dns_servers_configuration_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/cursoryvalidator/passwords_configuration_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/cursoryvalidator/netmask_configuration_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vcuserpermissionlivevalidator/vc_user_permission_live_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/createvmfolder/create_vm_folder.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/externalvcentitieslivevalidator/external_vc_entities_live_validator.py /var/lib/docker/overlay2/6df33189ca552a920f2d6c3259a034fac3ec47f8065634a6ba17ff7c74299377/merged/home/app/messages/messages_en_US.properties /var/lib/docker/overlay2/6df33189ca552a920f2d6c3259a034fac3ec47f8065634a6ba17ff7c74299377/merged/home/app/dayone/dayone_processor.py /var/lib/docker/overlay2/6df33189ca552a920f2d6c3259a034fac3ec47f8065634a6ba17ff7c74299377/merged/home/app/dayone/api.py /var/lib/docker/overlay2/1bc6389a4238217d15b1f5d931c6049e53891a92cef4255f6b402138e130d4d9/merged/home/app/lockbox_app/dao/postgres_dao.py /var/lib/docker/overlay2/1bc6389a4238217d15b1f5d931c6049e53891a92cef4255f6b402138e130d4d9/merged/home/app/lockbox_app/resources/verify_error_messages.json ... Remove the cursory checks: (base) grant@DESKTOP-2SV1E9O:~$ grep -v cursory test.txt /var/lib/docker/overlay2/19f438b6cef2857e6067e49802d04d61de2afb7275bc6fb1323a41bac03fec1c/diff/home/app/worker/task/cluster.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/common/conf/config-messages.properties /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vcmgmtaccountpermissiongrant/vc_mgmt_account_permission_grant.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/externalvclicensevalidator/external_vc_license_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/externalvcaccesslivevalidator/external_vc_access_live_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/externalvccapacitylimitlivevalidator/external_vc_capacity_limit_live_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostaddtovds/host_add_to_vds.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hosttimesync/host_time_sync.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/timeconfigurationlivevalidator/time_configuration_live_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostunusedsystemvmsremove/host_unused_system_vms_remove.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/externalvcversioncompatibilitylivevalidator/external_vc_version_compatibility_live_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vxmtimesync/vxm_time_sync.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/timesyncconfigurationvalidator/time_sync_configuration_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vcsyslogconfig/vc_syslog_config.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/dnshostnameslivevalidator/dns_hostnames_live_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vcuserpermissionlivevalidator/vc_user_permission_live_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/createvmfolder/create_vm_folder.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/externalvcentitieslivevalidator/external_vc_entities_live_validator.py /var/lib/docker/overlay2/6df33189ca552a920f2d6c3259a034fac3ec47f8065634a6ba17ff7c74299377/merged/home/app/messages/messages_en_US.properties /var/lib/docker/overlay2/6df33189ca552a920f2d6c3259a034fac3ec47f8065634a6ba17ff7c74299377/merged/home/app/dayone/dayone_processor.py /var/lib/docker/overlay2/6df33189ca552a920f2d6c3259a034fac3ec47f8065634a6ba17ff7c74299377/merged/home/app/dayone/api.py /var/lib/docker/overlay2/1bc6389a4238217d15b1f5d931c6049e53891a92cef4255f6b402138e130d4d9/merged/home/app/lockbox_app/dao/postgres_dao.py /var/lib/docker/overlay2/1bc6389a4238217d15b1f5d931c6049e53891a92cef4255f6b402138e130d4d9/merged/home/app/lockbox_app/resources/verify_error_messages.json Looking for the call build_call_do_service_task_url vxrailmanager:/var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vcdeploy # grep -rli 'build_call_do_service_task_url' /var/lib/docker/ 2> /dev/null /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/common/do.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/common/validation/ip_live_check_utils.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/common/network_prepare_aware_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vxmconfigurationdatapopulate/vxm_configuration_data_populate.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostaddintocluster/host_add_into_cluster.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/clusterconverttostretchedcluster/cluster_convert_to_stretched_cluster.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostpsipupdate/host_ps_ip_update.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/multidiskgroupconfigurationvalidator/multi_disk_group_configuration_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vcinitialboot/vc_initial_boot.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/clusterdasconfig/cluster_das_config.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostnetworkmanagementipchange/host_network_management_ip_change.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vcmgmtaccountpermissiongrant/vc_mgmt_account_permission_grant.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vxrailkgsbaselineload/vxm_kgs_baseline_load.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vcretreatmodeconfigure/vc_retreat_mode_configure.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/enableteaming/enableteaming.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/externalvcaccesslivevalidator/external_vc_access_live_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hosthostnamechange/host_hostname_change.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vxmupdatecertification/vxm_update_certification.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hoststorageleavevsan/host_storage_leave_vsan.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vxmconfigureslaacip/vxm_configure_slaacip.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vxmsystemservicesstop/vxm_system_services_stop.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/removedisconnecthostfromcluster/remove_disconnect_host_from_cluster.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostaddtovds/host_add_to_vds.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostassignactivepnictovds/host_assign_active_pnic_to_vds.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostassignvmktovds/host_assign_vmk_to_vds.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/clusterhighavailabilityenable/cluster_high_availability_enable.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/switchcreatenetworks/switch_create_networks.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vxmruntimepropertiesgenerate/vxm_runtime_properties_generate.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostvsanstorageenable/host_vsan_storage_enable.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hosttimesync/host_time_sync.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vcguestopsalias/vc_guest_ops_alias.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostwitnesstrafficconnectivity/host_witness_traffic_connectivity.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostvsandiskgroupscreate/host_vsan_diskgroups_create.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/createvds/createvds.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/switchconfiguplinks/switch_configure_uplinks.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/createportgroup/createportgroup.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vcpluginregister/vc_plugin_register.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vcdatacentercreate/vc_datacenter_create.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hoststoragevsandiskremove/host_storage_vsan_disk_remove.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostptagentipchange/host_ptagent_ip_change.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/customersuppliedvdsvlanlivevalidator/customer_supplied_vds_vlan_live_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostversioncompatibilitylivevalidator/host_version_compatibility_live_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/esxihostaccountvalidator/esxi_host_account_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostvsanstoragepolicyconfiguration/host_vsan_storage_policy_configuration.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostunusedsystemvmsremove/host_unused_system_vms_remove.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/switchconfigjumpbox/switch_configure_jumpbox.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vccertsgenerate/vc_certs_generate.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/clusterdrsenable/cluster_drs_enable.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vclicensing/vc_licensing.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostelectionservicedisable/host_election_service_disable.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/clusteraddexceptionaccounts/cluster_add_exception_accounts.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vcpasswordexpirationdisable/vc_password_expiration_disable.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vxmnetworkvnicremove/vxm_network_vnic_remove.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/externalvcversioncompatibilitylivevalidator/external_vc_version_compatibility_live_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/omeintegration/ome_integration.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostmarkpnicworkingmode/host_mark_pnic_working_mode.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/switchcreatedownlinks/switch_create_downlinks.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vxmevopassword/vxm_evo_password.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/movevxmvnictovds/vxm_network_vnic_add_to_vds.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostloudmouthrestart/host_loudmouth_restart.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hoststorageaddtovsan/host_storage_add_to_vsan.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostloggingsystemconfigure/host_logging_system_configure.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/clusteradvanceparamssetting/cluster_advance_params_setting.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/resetvspherealarms/reset_vsphere_alarms.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/networkcompatibilitylivevalidator/network_compatibility_live_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/switchauthentication/switch_authentication.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vxmtimesync/vxm_time_sync.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostsecurityservicedisable/host_security_service_disable.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/timesyncconfigurationvalidator/time_sync_configuration_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vxmstaticipchange/vxm_static_ipchange.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/createstoragepolicy/create_storage_policy.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vxmclustercleanup/vxm_cluster_clean_up.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/switchchangepassword/switch_change_password.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostplatformservicerestart/host_platform_service_restart.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostdnschange/host_dns_change.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vxmautostartenable/vxm_auto_start_enable.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vlanlivevalidator/vlan_live_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vxmserviceaccountvalidator/vxm_service_account_validator_task.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vxmhostnameset/vxm_hostname_set.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vcautostartenable/vc_auto_start_enable.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vcdeploy/vc_deploy.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostwitnessnetworking/host_witness_networking.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostmanagementaccountcreate/host_management_account_create.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/enablenioc/enablenioc.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vcsyslogconfig/vc_syslog_config.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vxmsystemservicesrestart/vxm_system_services_restart.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/checknodestatus/check_node_status.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vxmmovetostorage/vxm_move_to_storage.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostassignpnictovss/host_assign_pnic_to_vss.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vcmgmntaccountcreate/vc_mgmnt_account_create.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/clusterevcenable/cluster_evc_enable.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vxmsetdns/vxm_set_dns.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/cursoryvalidator/cursory_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostvssremove/host_vss_remove.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/clustercreate/clustercreate.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostaccountpasswordchange/host_account_password_change.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vxmrootaccountvalidator/vxm_root_account_validator_task.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostproxyservicedisable/host_proxy_service_disable.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/clustersystemvmstoragepolicyapply/cluster_system_vm_storage_policy_apply.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostassignstandbypnictovds/host_assign_standby_pnic_to_vds.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/createvmfolder/create_vm_folder.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/postdayoneconfiguration/post_dayone_configuration.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostnetworkmanagementvlanchange/host_network_management_vlan_change.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vxmupdateinternaldns/vxm_update_internal_dns.py /var/lib/docker/overlay2/6df33189ca552a920f2d6c3259a034fac3ec47f8065634a6ba17ff7c74299377/merged/home/app/dayone/do.py /var/lib/docker/overlay2/6df33189ca552a920f2d6c3259a034fac3ec47f8065634a6ba17ff7c74299377/merged/home/app/dayone/transfer_handler.py /var/lib/docker/overlay2/6df33189ca552a920f2d6c3259a034fac3ec47f8065634a6ba17ff7c74299377/merged/home/app/dayone/storage_discovery_callback.py /var/lib/docker/overlay2/6df33189ca552a920f2d6c3259a034fac3ec47f8065634a6ba17ff7c74299377/merged/home/app/vxm_ip_change_scheduler.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vxmconfigurationdatapopulate/vxm_configuration_data_populate.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hostaddintocluster/host_add_into_cluster.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/clusterconverttostretchedcluster/cluster_convert_to_stretched_cluster.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hostpsipupdate/host_ps_ip_update.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/multidiskgroupconfigurationvalidator/multi_disk_group_configuration_validator.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vcinitialboot/vc_initial_boot.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/clusterdasconfig/cluster_das_config.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hostnetworkmanagementipchange/host_network_management_ip_change.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vcmgmtaccountpermissiongrant/vc_mgmt_account_permission_grant.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vxrailkgsbaselineload/vxm_kgs_baseline_load.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vcretreatmodeconfigure/vc_retreat_mode_configure.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/enableteaming/enableteaming.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/externalvcaccesslivevalidator/external_vc_access_live_validator.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hosthostnamechange/host_hostname_change.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vxmupdatecertification/vxm_update_certification.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hoststorageleavevsan/host_storage_leave_vsan.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vxmconfigureslaacip/vxm_configure_slaacip.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vxmsystemservicesstop/vxm_system_services_stop.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/removedisconnecthostfromcluster/remove_disconnect_host_from_cluster.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hostaddtovds/host_add_to_vds.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hostassignactivepnictovds/host_assign_active_pnic_to_vds.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hostassignvmktovds/host_assign_vmk_to_vds.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/clusterhighavailabilityenable/cluster_high_availability_enable.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/switchcreatenetworks/switch_create_networks.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vxmruntimepropertiesgenerate/vxm_runtime_properties_generate.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hostvsanstorageenable/host_vsan_storage_enable.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hosttimesync/host_time_sync.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vcguestopsalias/vc_guest_ops_alias.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hostwitnesstrafficconnectivity/host_witness_traffic_connectivity.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hostvsandiskgroupscreate/host_vsan_diskgroups_create.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/createvds/createvds.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/switchconfiguplinks/switch_configure_uplinks.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/createportgroup/createportgroup.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vcpluginregister/vc_plugin_register.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vcdatacentercreate/vc_datacenter_create.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hoststoragevsandiskremove/host_storage_vsan_disk_remove.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hostptagentipchange/host_ptagent_ip_change.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/customersuppliedvdsvlanlivevalidator/customer_supplied_vds_vlan_live_validator.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hostversioncompatibilitylivevalidator/host_version_compatibility_live_validator.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/esxihostaccountvalidator/esxi_host_account_validator.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hostvsanstoragepolicyconfiguration/host_vsan_storage_policy_configuration.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hostunusedsystemvmsremove/host_unused_system_vms_remove.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/switchconfigjumpbox/switch_configure_jumpbox.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vccertsgenerate/vc_certs_generate.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/clusterdrsenable/cluster_drs_enable.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vclicensing/vc_licensing.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hostelectionservicedisable/host_election_service_disable.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/clusteraddexceptionaccounts/cluster_add_exception_accounts.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vcpasswordexpirationdisable/vc_password_expiration_disable.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vxmnetworkvnicremove/vxm_network_vnic_remove.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/externalvcversioncompatibilitylivevalidator/external_vc_version_compatibility_live_validator.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/omeintegration/ome_integration.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hostmarkpnicworkingmode/host_mark_pnic_working_mode.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/switchcreatedownlinks/switch_create_downlinks.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vxmevopassword/vxm_evo_password.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/movevxmvnictovds/vxm_network_vnic_add_to_vds.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hostloudmouthrestart/host_loudmouth_restart.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hoststorageaddtovsan/host_storage_add_to_vsan.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hostloggingsystemconfigure/host_logging_system_configure.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/clusteradvanceparamssetting/cluster_advance_params_setting.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/resetvspherealarms/reset_vsphere_alarms.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/networkcompatibilitylivevalidator/network_compatibility_live_validator.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/switchauthentication/switch_authentication.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vxmtimesync/vxm_time_sync.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hostsecurityservicedisable/host_security_service_disable.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/timesyncconfigurationvalidator/time_sync_configuration_validator.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vxmstaticipchange/vxm_static_ipchange.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/createstoragepolicy/create_storage_policy.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vxmclustercleanup/vxm_cluster_clean_up.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/common/do.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/common/validation/ip_live_check_utils.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/common/network_prepare_aware_validator.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/switchchangepassword/switch_change_password.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hostplatformservicerestart/host_platform_service_restart.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hostdnschange/host_dns_change.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vxmautostartenable/vxm_auto_start_enable.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vlanlivevalidator/vlan_live_validator.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vxmserviceaccountvalidator/vxm_service_account_validator_task.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vxmhostnameset/vxm_hostname_set.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vcautostartenable/vc_auto_start_enable.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vcdeploy/vc_deploy.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hostwitnessnetworking/host_witness_networking.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hostmanagementaccountcreate/host_management_account_create.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/enablenioc/enablenioc.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vcsyslogconfig/vc_syslog_config.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vxmsystemservicesrestart/vxm_system_services_restart.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/checknodestatus/check_node_status.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vxmmovetostorage/vxm_move_to_storage.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hostassignpnictovss/host_assign_pnic_to_vss.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vcmgmntaccountcreate/vc_mgmnt_account_create.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/clusterevcenable/cluster_evc_enable.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vxmsetdns/vxm_set_dns.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/cursoryvalidator/cursory_validator.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hostvssremove/host_vss_remove.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/clustercreate/clustercreate.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hostaccountpasswordchange/host_account_password_change.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vxmrootaccountvalidator/vxm_root_account_validator_task.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hostproxyservicedisable/host_proxy_service_disable.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/clustersystemvmstoragepolicyapply/cluster_system_vm_storage_policy_apply.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hostassignstandbypnictovds/host_assign_standby_pnic_to_vds.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/createvmfolder/create_vm_folder.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/postdayoneconfiguration/post_dayone_configuration.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hostnetworkmanagementvlanchange/host_network_management_vlan_change.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vxmupdateinternaldns/vxm_update_internal_dns.py /var/lib/docker/overlay2/0d8ac81dbb9015f2f3526cd468c2c4e7e433661e38008b04f70539deee19272a/diff/home/app/dayone/do.py /var/lib/docker/overlay2/0d8ac81dbb9015f2f3526cd468c2c4e7e433661e38008b04f70539deee19272a/diff/home/app/dayone/transfer_handler.py /var/lib/docker/overlay2/0d8ac81dbb9015f2f3526cd468c2c4e7e433661e38008b04f70539deee19272a/diff/home/app/dayone/storage_discovery_callback.py /var/lib/docker/overlay2/02a3e09ef808a5f704f821bbf2fcc89ece02f262e0a20b1c5a599fbf949a8c9b/diff/home/app/vxm_ip_change_scheduler.py Finding the definition: vxrailmanager:/var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vcdeploy # grep -rli 'def build_call_do_service_task_url' /var/lib/docker/ 2> /dev/null /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/common/do.py /var/lib/docker/overlay2/6df33189ca552a920f2d6c3259a034fac3ec47f8065634a6ba17ff7c74299377/merged/home/app/dayone/do.py /var/lib/docker/overlay2/6df33189ca552a920f2d6c3259a034fac3ec47f8065634a6ba17ff7c74299377/merged/home/app/vxm_ip_change_scheduler.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/common/do.py /var/lib/docker/overlay2/0d8ac81dbb9015f2f3526cd468c2c4e7e433661e38008b04f70539deee19272a/diff/home/app/dayone/do.py /var/lib/docker/overlay2/02a3e09ef808a5f704f821bbf2fcc89ece02f262e0a20b1c5a599fbf949a8c9b/diff/home/app/vxm_ip_change_scheduler.py There seems to be a file for licensing: Find embedded license: vxrailmanager:/var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vcdeploy # find / -iname license-vc-700-e1-marvin-c1-201909 \\/usr/lib/vmware-marvin/marvind/webapps/lcm/WEB-INF/classes/licensing/7.0/license-vc-700-e1-marvin-c1-201909 /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/common/conf/license-vc-700-e1-marvin-c1-201909 /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/common/conf/license-vc-700-e1-marvin-c1-201909 vxrailmanager:/var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vcdeploy # cat /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/common/conf/license-vc-700-e1-marvin-c1-201909 # VMware software license StartFields = \"Cpt, ProductID, LicenseVersion, LicenseType, LicenseEdition, Option, Epoch\" Cpt = \"COPYRIGHT (c) VMware, Inc.\" ProductID = \"VMware VirtualCenter Server\" LicenseVersion = \"7.0\" LicenseType = \"Site\" Epoch = \"2019-9-1\" LicenseEdition = \"vc.standard.instance.marvin\" Option = \"7\" Data = \"FileVersion=7.0.0.2;capacityType=server;enable=linkedvc,woe,xvp,vcha,backuprestore,appliancemigration;addon=vsa;disallowedHostEditions=esx.hypervisor.cpuPackage,esx.hypervisorEmbeddedOem.cpuPackage,esx.essentials.cpuPackage,esx.essentialsPlus.cpuPackage,esx.hypervisor.cpuPackageCoreLimited,esx.hypervisorEmbeddedOem.cpuPackageCoreLimited,esx.essentials.cpuPackageCoreLimited,esx.essentialsPlus.cpuPackageCoreLimited;desc=vCenter Server 7 Standard\" DataHash = \"60aebea4-31a0050c-c4e49494-27690fad-5a8a5258\" Hash = \"441d0833-d2f3bed9-0bea3fec-7d13664b-e222d862\"","title":"vCenter Deploy"},{"location":"VMWare/Troubleshooting%20vSAN/","text":"Troubleshooting vSAN Troubleshooting vSAN Helpful Resources vSAN Support Insight Overall Approach Technical Approach Identify Using vSAN Observer Running from a remote machine Running vSAN Observer without Internet Access vSAN Observer Case Studies Anatomy of a Write/Read Operation in Hybrid Config Helpful Commands Networking Determine Disk Group Assignmnts Check Failed Disks Determine what drives are attached to which storage adapters Get Detailed Version Simple List Interpreting the Output List SCSI devices and their sizes Get a list of all PCI devices (allows you to check for rebranding) Determine Adapter Driver and Version Determine Driver Parameters Get a list of VIBs Check failures to tolerate and stripe Check what the fault domains are Inspect a Specific Object (VM) in vSAN Understanding Drive Types VCG Check Summary Absent vs Degraded Failure Object Compliance Status: Compliant vs Non Compliant Object Operational State: Healthy vs Unhealthy VM Accessibility: Inaccessible vs Orphaned TODO Helpful Resources Virtual SAN Diagnostics and Troubleshooting Reference Manual - Outside of what I have recorded here, chapter 7 - Understanding expected failure behavior may be useful. It covers what you'll see in the logs, UI, etc when different types of failure occur. It also describes things like what happens when you plug in a new disk in both the capacity and cache tiers. If you don't know how to handle a problem this is the place to look. I have a lot of the highlights below but there are a lot of scenarios they cover that I don't. VMware Ruby vSphere Console Command Reference for Virtual SAN Troubleshooting vSAN Performance Monitoring with vSAN Observer - Covers interpreting and understanding all the functionality of vSAN observer. My Notes on vSAN - I documented how a number of vSAN features function and the variosu highlights. vSAN Support Insight Overall Approach vSAN observer vSAN health test Time/resource permitting vSAN HCI benchmark I'll also do a general network survey to make sure I understand the physical and logical layout paying particular attention to the various VLANs https://core.vmware.com/resource/troubleshooting-vsan-performance Identify and quantify. This step helps to clearly define the issue. Clarifying questions can help properly qualify the problem statement, which will allow for a more targeted approach to addressing. This process helps sort out real versus perceived issues, and focuses on the end result and supporting symptoms, without implying the cause of the issue. Discovery/Review - Environment. This step takes a review of the current configuration. This will help eliminate previously unnoticed basic configuration or topology issues that might be plaguing the environment. Discovery/Review - Workload. This step will help the reader review the applications and workflows. This will help a virtualization administrator better understand what the application is attempting to perform, and why. Performance Metrics - Insight. This step will review some of the key performance metrics to view, and how to interpret some of the findings the reader may see when observing their workloads. It clarifies what the performance metrics means, and how they relate to each other.. Mitigation - Options in potential software and hardware changes. This step will help the reader step through the potential actions for mitigation. Technical Approach Potential Bottlenecks: https://core.vmware.com/blog/understanding-performance-bottlenecks Identify Check VxRail version Perform a network survey and understand what switches and devices are in between each of the hosts Verify everything is licensed Hosts that are participating in the vSAN cluster but not providing storage still require a license Check that the flash cache to capacity ratio is 1:10 Check cluster health with esxcli vsan health cluster list Cross reference in RVC with vsan.cluster_info <target> . This is good for getting a big picture view of what is going on. Check the overall state of the cluster in RVC with vsan.check_state <target> . This will check for inaccessible vSAN objects, invalid/inaccessible VMs, VMs for which VC/hostd/vmx are out of sync. 1. Inaccessible vSAN Objects : Inaccessible Virtual SAN objects are an indication that there is probably a failure somewhere in the cluster, but that Virtual SAN is still able to track the virtual machine. An invalid or inaccessible object is when the VM has objects that have lost the majority of its components or votes, again due to hardware failures. Note that for a VM\u2019s object to be accessible, it must have a full, intact mirror and greater than 50% of its components/votes available. 2. Invalid/inaccessible VMs : The next check is for invalid or inaccessible VMs. These are VMs that, most likely due to the fact that the failure(s) that have occurred in the cluster, have been impacted so much that it is no longer accessible by the vCenter server or the ESXi hosts. This is likely be due to the fact that the VM Home Namespace, where the .vmx file resides, is no longer online. Common causes are clusters that have had multiple failures, but the virtual machines have been configured to tolerate only one failure, or network outages. 3. Check that vCenter and hosts are in sync : The final check just makes sure vCenter Server (VC) and hosts agree on the state of the cluster. Use RVC to check that the cluster is operating within limits with vsan.check_limits 1. From a network perspective the most important things to check are associations (Assocs) and the the socket count. Associations track peer-to-peer network state within the vSAN and you should not run out (max 45k). Sockts are used for various things. The max is 10,000. Check that the cluster has enough resources to tolerate a failure with RVC command vsan.whatif_host_failures 1. RC reservations means Read Cache reservations and is only used in hybrid configurations. You can dedicate a certain amount of read cache to a specific VM. 2. HDD capacity only refers to the capacity tier. Check host connectivity Note it may be helpful if you need to identify where a host is plugged in to use LLDP with the RVC command vsan.lldpnetmap localhost/datacenter/computers/vSAN\\ Cluster/ Check cluster ping with vmkping -I <vmk_interface> <target_ip> Check to make sure all hosts are on the same network segment with esxcli network ip neighbor list Check round trip time with esxcli network diag ping Check to make sure multicast is working and a host is receiving heartbeats with tcpdump-uw \u2013i <your_vmk> udp port 23451 \u2013v -s0 Each ESXi node in the vSAN cluster will send out IGMP membership reports (aka joins) every 90-300 seconds. Check for receipt with tcpdump-uw -i <yourvmk> igmp Make sure the following ports are accessible through any firewalls: Use iperf to get a baseline idea of the network bandwidth available. This will cause degredation of performance! Most useful for initial setup. See this KB article You can also use RVC's vsan.vm_perf_stats command to get a feel for the performance. Ex vsan.vm_perf_stats ~/vms/W2k12-SQL2k12 --interval 10 --show-objects . IOPS = (MBps Throughput / KB per IO) * 1024. MBps = (IOPS * KB per IO) / 1024 NOTE : Virtual SAN Diagnostics and Troubleshooting Reference Manual covers a lot of other scenarios I didn't here. Consider Kick off vSAN Observer. See Using vSAN Observer . Details on using vSAN observer available in this PDF . It is also covered starting on page 197 in Virtual SAN Diagnostics and Troubleshooting Reference Manual Check the latency as seen by a guest VM running the application of interest (check on the VMs tab) VM Home - This is where the VM's configuration file, log files, and other VM related small files are stored. The RAID tree subsection shows the different component owners for VM Home. Virtual Disk - This shows the different virtual disks attached to the VM. Each disk displas stats specific to that disk. You can drill down to individual virtual disks. The VSCSI layer shows the aggregate latency, IOPS and throughput numbers for a particular virtual disk of a VM. NOTE : On the various full graphs you may see RecovWrite - these are the number of writes that are being used for component rebuilds. Note that this metric will be 0 for vSAN clients because they do not have visibility into rebuild I/O - DOM does that. You will also see Write LE - this refers to write log entry. For every write there are actually two I/Os: the actual data and a log entry. Check for high outstanding IOPs (vSAN client tab and vSAN disks). On the vSAN disks tab make sure that outstanding IO is well balanced across the hosts. A high outstanding IO may be something like 200. On the vSAN disks tab look for a high number of evictions. These occur when vSAN has to evict data blocks from the write buffer to make room for new data blocks. In an optimized system, the working data for an application should mostly reside in write cache. We should not see too many evictions. Maybe this workload is not suitable? Check for high latencies (time it takes to complete on I/O operation from application viewpoint). (vSAN client tab and vSAN disk tab). Consier that if the latency in the vSAN client tab is much higher than the disk tab than it is more likely the network is the problem. Make sure that what we see on the vSAN client tab correspondings to what is on the vSAN disk tabs Common causes of high latency: Large average I/O sizes, which leads to increased latencies Large number of writes Large number of I/Os Slow SSD that isn't keeping up Too many random reads causing cache misses in the SSD. Latency formula: outstanding IOs / drive max write or I/O = x ms If we kick off a lot of read ops on something we generally expect there to a spike in latency followed by a drop as things are cached (assuming the same thing is being read) The standard deviation graph is telling you the frequency you are outside a single standard deviation Check bandwidth utilization Lots of small I/Os could cause you to hit I/O ceiling before bandwidth Large I/Os may exhaust bandwidth Check buffer utilization. You can see this on the client and disk tabs. On the deep dive tab you can check RC hit rate for the various hosts. If we are seeing a lot of misse on the read cache this may indicate the cache isn't large enough and we expect to see a spike in hits against our capacity drives. Check PCPU utilization. It isn't uncommon to see 30-50% utilization when under I/O load. Sustained high CPU utilization could indicate a problem. If you have problems but don't expect them, make sure that if the server has a power management setting that it isn't set to a lower performance setting Check memory consumption paying specific attention to congestion. The vmkernel log will commonly display this error in the event of a memory shortage: Or this error when trying to add a disk group: An error will also be displayed in the GUI: Check distribution of components . The line should be uniform indicating roughly equal distribution of components Check that the storage controller is supported and running normaly with esxcli storage core device list . Compare to VCG. You can cross reference this with esxcli core storage adapter list and esxcfg-scsidevs -a You may see the word degraded - this occurs when there is only a single path to teh device. If there are multiple paths this will not show. This is not an issue for local disk configurations. If you see Logical Volume in the model field it implies there is a RAID volume configuration on the disk - probably RAID0. Certain storage controllers can pass disk devices directly to the ESXi host; this is called pass-through mode. You may also see it called JBOD or HBA mode. Other storage controllers require each disk device be configured as a RAID0 volume before ESXi can recognize it. You can check the VCG to determine whether something supports pass through or not: Double check the VCG! If a controller is supported in RAID0 mode only, it may still work in pass-through mode but the disks will frequently error. Make sure the storage adapter is supported and the driver is up to date. See Get a list of all PCI devices Review the VCG check summary and make sure everything is good to go If you are having general performace issues with vSAN you can go to a host seeing the issues and check the controller queue depth with esxtop . You can use the D option to see the queue stats. Hit enter to return to the general view and then you can check AQLEN. ESXi requires a depth greater than 256. You can also check it with the command esxcfg-info \u2013s | grep \u201c==+SCSI Interface\u201d \u2013A 1 Disable controller caching. This is the VMWare recommended setting. If you can't disable it try setting the cache to 100% read. Make sure all disks that are meant to be claimed by vSAN are with the command esxcli vsan storage list . Make sure in the output that CMMDS is set to true. This implies that the cluster membership and directory services know about the disk in question and that the capacity of the disk is contributing to the capactiy of the vSAN datastore. Using vSAN Observer SSH to vCenter and then run rvc administrator@vsphere.lan@localhost vsan.observer /localhost/datacenter/computers/vSAN\\ Cluster/ --run-webserver --force Go to https://vcenter.lan:8010/ (it must be https) You can also write the output to a JSON file. If you only have the JSON output you can produce static HTML with the vsan.observer_process_statsfile <stats_file> <output_folder> Running from a remote machine Install Ruby: https://rubyinstaller.org/downloads/ Install Ruby vSphere Console: https://rubydoc.info/gems/rvc/1.6.0 gem install rvc Running vSAN Observer without Internet Access See page 198 of Virtual SAN Diagnostics and Troubleshooting Reference Manual vSAN Observer Case Studies Starts on page 262 of Virtual SAN Diagnostics and Troubleshooting Reference Manual Anatomy of a Write/Read Operation in Hybrid Config For more details see page 245 of Virtual SAN Diagnostics and Troubleshooting Reference Manual Helpful Commands Networking esxcli network ip interface list esxcli network ip interface ipv4 get esxcli network ip neighbor list # view ARP table esxcli vsan network list # Determine which vmk interface is used for vSAN esxcli network nic list # List the physical interfaces # This can be run at the cluster level and also gives vmk interface for vSAN vsan.cluster_info <vSAN cluster> # Use LLDP to obtain upstream switch info vsan.lldpnetmap localhost/datacenter/computers/vSAN\\ Cluster/ Determine Disk Group Assignmnts Check Failed Disks esxcli storage core device stats get Determine what drives are attached to which storage adapters Get Detailed Version esxcli storage core path list Simple List esxcfg-scsidevs \u2013A Interpreting the Output NAA stands for Network Addressing Authority identifier. EUI stands for Extended Unique Identifier. The number is guaranteed to be unique to that LUN. The NAA or EUI identifier is the preferred method of identifying LUNs and the storage device generates the number. Since the NAA or EUI is unique to the LUN, if the LUN is presented the same way across all ESXi hosts, the NAA or EUI identifier remains the same. Some devices do not provide the NAA number described above. In these circumstances, an MPX Identifier is generated by ESXi to represent the LUN or disk. The identifier takes the form similar to that of the canonical name of previous versions of ESXi with the \u201cmpx.\u201d prefix. This identifier can be used in the exact same way as the NAA Identifier described above.\u201d Refer to VMware KB article 1014953 for further information. List SCSI devices and their sizes esxcfg-scsidevs \u2013c Get a list of all PCI devices (allows you to check for rebranding) You an use a combination of the vendor IDs, device IDs, sub-vendor IDs, and sub-device IDs, to verify an adapter is on the VCG. Some vendors rebrand devices to reflect their own range of products however, the vendor and device ids remain the same. It is not uncommon to find that one vendor's controller is not listed under the vendor name in the VCG but is actually listed under another vendor. esxcli hardware pci list Determine Adapter Driver and Version Module Name indicates the driver that is currently in use. You can use the vmkload_mod -s <driver name> or esxcli system module get -m command to determine the driver version. Check that against the VCG; as bugs arise the supported drivers can change. You can also check the adapters with vsan.disks_info localhost/datacenter/computers/vSAN\\ Cluster/hosts/vsan1.lan --show-adapters in RVC. Determine Driver Parameters vmkload_mod -s <driver name> Get a list of VIBs esxcli software vib list If someone built using a custom ISO, it is common that they may have upgraded the driver to too high a version and that this version is unsupported. Check failures to tolerate and stripe You can see this on a per object level in vsan.object_info Check what the fault domains are vsan.cluster_info computers/vSAN\\ Cluster/ Inspect a Specific Object (VM) in vSAN vsan.vm_object_info <target> Understanding Drive Types Flash devices are placed in classes. Performance classes are measured in writes per second (not sure if this is up to date): Endurance is measured in Terabytes Written. Drives are qualified by the number of terabytes they are capable of writing per some time period per the vendor. VCG Check Summary Absent vs Degraded Failure Object Compliance Status: Compliant vs Non Compliant Object Operational State: Healthy vs Unhealthy VM Accessibility: Inaccessible vs Orphaned TODO Review P141 - Verifying Virtual SAN storage operation - RVC in Virtual SAN Diagnostics and Troubleshooting Reference Manual Review P152 - Testing vSAN functionality - deploying VMs in Virtual SAN Diagnostics and Troubleshooting Reference Manual P156 of Virtual SAN Diagnostics and Troubleshooting Reference Manual","title":"Troubleshooting vSAN"},{"location":"VMWare/Troubleshooting%20vSAN/#troubleshooting-vsan","text":"Troubleshooting vSAN Helpful Resources vSAN Support Insight Overall Approach Technical Approach Identify Using vSAN Observer Running from a remote machine Running vSAN Observer without Internet Access vSAN Observer Case Studies Anatomy of a Write/Read Operation in Hybrid Config Helpful Commands Networking Determine Disk Group Assignmnts Check Failed Disks Determine what drives are attached to which storage adapters Get Detailed Version Simple List Interpreting the Output List SCSI devices and their sizes Get a list of all PCI devices (allows you to check for rebranding) Determine Adapter Driver and Version Determine Driver Parameters Get a list of VIBs Check failures to tolerate and stripe Check what the fault domains are Inspect a Specific Object (VM) in vSAN Understanding Drive Types VCG Check Summary Absent vs Degraded Failure Object Compliance Status: Compliant vs Non Compliant Object Operational State: Healthy vs Unhealthy VM Accessibility: Inaccessible vs Orphaned TODO","title":"Troubleshooting vSAN"},{"location":"VMWare/Troubleshooting%20vSAN/#helpful-resources","text":"Virtual SAN Diagnostics and Troubleshooting Reference Manual - Outside of what I have recorded here, chapter 7 - Understanding expected failure behavior may be useful. It covers what you'll see in the logs, UI, etc when different types of failure occur. It also describes things like what happens when you plug in a new disk in both the capacity and cache tiers. If you don't know how to handle a problem this is the place to look. I have a lot of the highlights below but there are a lot of scenarios they cover that I don't. VMware Ruby vSphere Console Command Reference for Virtual SAN Troubleshooting vSAN Performance Monitoring with vSAN Observer - Covers interpreting and understanding all the functionality of vSAN observer. My Notes on vSAN - I documented how a number of vSAN features function and the variosu highlights.","title":"Helpful Resources"},{"location":"VMWare/Troubleshooting%20vSAN/#vsan-support-insight","text":"","title":"vSAN Support Insight"},{"location":"VMWare/Troubleshooting%20vSAN/#overall-approach","text":"vSAN observer vSAN health test Time/resource permitting vSAN HCI benchmark I'll also do a general network survey to make sure I understand the physical and logical layout paying particular attention to the various VLANs https://core.vmware.com/resource/troubleshooting-vsan-performance Identify and quantify. This step helps to clearly define the issue. Clarifying questions can help properly qualify the problem statement, which will allow for a more targeted approach to addressing. This process helps sort out real versus perceived issues, and focuses on the end result and supporting symptoms, without implying the cause of the issue. Discovery/Review - Environment. This step takes a review of the current configuration. This will help eliminate previously unnoticed basic configuration or topology issues that might be plaguing the environment. Discovery/Review - Workload. This step will help the reader review the applications and workflows. This will help a virtualization administrator better understand what the application is attempting to perform, and why. Performance Metrics - Insight. This step will review some of the key performance metrics to view, and how to interpret some of the findings the reader may see when observing their workloads. It clarifies what the performance metrics means, and how they relate to each other.. Mitigation - Options in potential software and hardware changes. This step will help the reader step through the potential actions for mitigation.","title":"Overall Approach"},{"location":"VMWare/Troubleshooting%20vSAN/#technical-approach","text":"Potential Bottlenecks: https://core.vmware.com/blog/understanding-performance-bottlenecks","title":"Technical Approach"},{"location":"VMWare/Troubleshooting%20vSAN/#identify","text":"Check VxRail version Perform a network survey and understand what switches and devices are in between each of the hosts Verify everything is licensed Hosts that are participating in the vSAN cluster but not providing storage still require a license Check that the flash cache to capacity ratio is 1:10 Check cluster health with esxcli vsan health cluster list Cross reference in RVC with vsan.cluster_info <target> . This is good for getting a big picture view of what is going on. Check the overall state of the cluster in RVC with vsan.check_state <target> . This will check for inaccessible vSAN objects, invalid/inaccessible VMs, VMs for which VC/hostd/vmx are out of sync. 1. Inaccessible vSAN Objects : Inaccessible Virtual SAN objects are an indication that there is probably a failure somewhere in the cluster, but that Virtual SAN is still able to track the virtual machine. An invalid or inaccessible object is when the VM has objects that have lost the majority of its components or votes, again due to hardware failures. Note that for a VM\u2019s object to be accessible, it must have a full, intact mirror and greater than 50% of its components/votes available. 2. Invalid/inaccessible VMs : The next check is for invalid or inaccessible VMs. These are VMs that, most likely due to the fact that the failure(s) that have occurred in the cluster, have been impacted so much that it is no longer accessible by the vCenter server or the ESXi hosts. This is likely be due to the fact that the VM Home Namespace, where the .vmx file resides, is no longer online. Common causes are clusters that have had multiple failures, but the virtual machines have been configured to tolerate only one failure, or network outages. 3. Check that vCenter and hosts are in sync : The final check just makes sure vCenter Server (VC) and hosts agree on the state of the cluster. Use RVC to check that the cluster is operating within limits with vsan.check_limits 1. From a network perspective the most important things to check are associations (Assocs) and the the socket count. Associations track peer-to-peer network state within the vSAN and you should not run out (max 45k). Sockts are used for various things. The max is 10,000. Check that the cluster has enough resources to tolerate a failure with RVC command vsan.whatif_host_failures 1. RC reservations means Read Cache reservations and is only used in hybrid configurations. You can dedicate a certain amount of read cache to a specific VM. 2. HDD capacity only refers to the capacity tier. Check host connectivity Note it may be helpful if you need to identify where a host is plugged in to use LLDP with the RVC command vsan.lldpnetmap localhost/datacenter/computers/vSAN\\ Cluster/ Check cluster ping with vmkping -I <vmk_interface> <target_ip> Check to make sure all hosts are on the same network segment with esxcli network ip neighbor list Check round trip time with esxcli network diag ping Check to make sure multicast is working and a host is receiving heartbeats with tcpdump-uw \u2013i <your_vmk> udp port 23451 \u2013v -s0 Each ESXi node in the vSAN cluster will send out IGMP membership reports (aka joins) every 90-300 seconds. Check for receipt with tcpdump-uw -i <yourvmk> igmp Make sure the following ports are accessible through any firewalls: Use iperf to get a baseline idea of the network bandwidth available. This will cause degredation of performance! Most useful for initial setup. See this KB article You can also use RVC's vsan.vm_perf_stats command to get a feel for the performance. Ex vsan.vm_perf_stats ~/vms/W2k12-SQL2k12 --interval 10 --show-objects . IOPS = (MBps Throughput / KB per IO) * 1024. MBps = (IOPS * KB per IO) / 1024 NOTE : Virtual SAN Diagnostics and Troubleshooting Reference Manual covers a lot of other scenarios I didn't here. Consider Kick off vSAN Observer. See Using vSAN Observer . Details on using vSAN observer available in this PDF . It is also covered starting on page 197 in Virtual SAN Diagnostics and Troubleshooting Reference Manual Check the latency as seen by a guest VM running the application of interest (check on the VMs tab) VM Home - This is where the VM's configuration file, log files, and other VM related small files are stored. The RAID tree subsection shows the different component owners for VM Home. Virtual Disk - This shows the different virtual disks attached to the VM. Each disk displas stats specific to that disk. You can drill down to individual virtual disks. The VSCSI layer shows the aggregate latency, IOPS and throughput numbers for a particular virtual disk of a VM. NOTE : On the various full graphs you may see RecovWrite - these are the number of writes that are being used for component rebuilds. Note that this metric will be 0 for vSAN clients because they do not have visibility into rebuild I/O - DOM does that. You will also see Write LE - this refers to write log entry. For every write there are actually two I/Os: the actual data and a log entry. Check for high outstanding IOPs (vSAN client tab and vSAN disks). On the vSAN disks tab make sure that outstanding IO is well balanced across the hosts. A high outstanding IO may be something like 200. On the vSAN disks tab look for a high number of evictions. These occur when vSAN has to evict data blocks from the write buffer to make room for new data blocks. In an optimized system, the working data for an application should mostly reside in write cache. We should not see too many evictions. Maybe this workload is not suitable? Check for high latencies (time it takes to complete on I/O operation from application viewpoint). (vSAN client tab and vSAN disk tab). Consier that if the latency in the vSAN client tab is much higher than the disk tab than it is more likely the network is the problem. Make sure that what we see on the vSAN client tab correspondings to what is on the vSAN disk tabs Common causes of high latency: Large average I/O sizes, which leads to increased latencies Large number of writes Large number of I/Os Slow SSD that isn't keeping up Too many random reads causing cache misses in the SSD. Latency formula: outstanding IOs / drive max write or I/O = x ms If we kick off a lot of read ops on something we generally expect there to a spike in latency followed by a drop as things are cached (assuming the same thing is being read) The standard deviation graph is telling you the frequency you are outside a single standard deviation Check bandwidth utilization Lots of small I/Os could cause you to hit I/O ceiling before bandwidth Large I/Os may exhaust bandwidth Check buffer utilization. You can see this on the client and disk tabs. On the deep dive tab you can check RC hit rate for the various hosts. If we are seeing a lot of misse on the read cache this may indicate the cache isn't large enough and we expect to see a spike in hits against our capacity drives. Check PCPU utilization. It isn't uncommon to see 30-50% utilization when under I/O load. Sustained high CPU utilization could indicate a problem. If you have problems but don't expect them, make sure that if the server has a power management setting that it isn't set to a lower performance setting Check memory consumption paying specific attention to congestion. The vmkernel log will commonly display this error in the event of a memory shortage: Or this error when trying to add a disk group: An error will also be displayed in the GUI: Check distribution of components . The line should be uniform indicating roughly equal distribution of components Check that the storage controller is supported and running normaly with esxcli storage core device list . Compare to VCG. You can cross reference this with esxcli core storage adapter list and esxcfg-scsidevs -a You may see the word degraded - this occurs when there is only a single path to teh device. If there are multiple paths this will not show. This is not an issue for local disk configurations. If you see Logical Volume in the model field it implies there is a RAID volume configuration on the disk - probably RAID0. Certain storage controllers can pass disk devices directly to the ESXi host; this is called pass-through mode. You may also see it called JBOD or HBA mode. Other storage controllers require each disk device be configured as a RAID0 volume before ESXi can recognize it. You can check the VCG to determine whether something supports pass through or not: Double check the VCG! If a controller is supported in RAID0 mode only, it may still work in pass-through mode but the disks will frequently error. Make sure the storage adapter is supported and the driver is up to date. See Get a list of all PCI devices Review the VCG check summary and make sure everything is good to go If you are having general performace issues with vSAN you can go to a host seeing the issues and check the controller queue depth with esxtop . You can use the D option to see the queue stats. Hit enter to return to the general view and then you can check AQLEN. ESXi requires a depth greater than 256. You can also check it with the command esxcfg-info \u2013s | grep \u201c==+SCSI Interface\u201d \u2013A 1 Disable controller caching. This is the VMWare recommended setting. If you can't disable it try setting the cache to 100% read. Make sure all disks that are meant to be claimed by vSAN are with the command esxcli vsan storage list . Make sure in the output that CMMDS is set to true. This implies that the cluster membership and directory services know about the disk in question and that the capacity of the disk is contributing to the capactiy of the vSAN datastore.","title":"Identify"},{"location":"VMWare/Troubleshooting%20vSAN/#using-vsan-observer","text":"SSH to vCenter and then run rvc administrator@vsphere.lan@localhost vsan.observer /localhost/datacenter/computers/vSAN\\ Cluster/ --run-webserver --force Go to https://vcenter.lan:8010/ (it must be https) You can also write the output to a JSON file. If you only have the JSON output you can produce static HTML with the vsan.observer_process_statsfile <stats_file> <output_folder>","title":"Using vSAN Observer"},{"location":"VMWare/Troubleshooting%20vSAN/#running-from-a-remote-machine","text":"Install Ruby: https://rubyinstaller.org/downloads/ Install Ruby vSphere Console: https://rubydoc.info/gems/rvc/1.6.0 gem install rvc","title":"Running from a remote machine"},{"location":"VMWare/Troubleshooting%20vSAN/#running-vsan-observer-without-internet-access","text":"See page 198 of Virtual SAN Diagnostics and Troubleshooting Reference Manual","title":"Running vSAN Observer without Internet Access"},{"location":"VMWare/Troubleshooting%20vSAN/#vsan-observer-case-studies","text":"Starts on page 262 of Virtual SAN Diagnostics and Troubleshooting Reference Manual","title":"vSAN Observer Case Studies"},{"location":"VMWare/Troubleshooting%20vSAN/#anatomy-of-a-writeread-operation-in-hybrid-config","text":"For more details see page 245 of Virtual SAN Diagnostics and Troubleshooting Reference Manual","title":"Anatomy of a Write/Read Operation in Hybrid Config"},{"location":"VMWare/Troubleshooting%20vSAN/#helpful-commands","text":"","title":"Helpful Commands"},{"location":"VMWare/Troubleshooting%20vSAN/#networking","text":"esxcli network ip interface list esxcli network ip interface ipv4 get esxcli network ip neighbor list # view ARP table esxcli vsan network list # Determine which vmk interface is used for vSAN esxcli network nic list # List the physical interfaces # This can be run at the cluster level and also gives vmk interface for vSAN vsan.cluster_info <vSAN cluster> # Use LLDP to obtain upstream switch info vsan.lldpnetmap localhost/datacenter/computers/vSAN\\ Cluster/","title":"Networking"},{"location":"VMWare/Troubleshooting%20vSAN/#determine-disk-group-assignmnts","text":"","title":"Determine Disk Group Assignmnts"},{"location":"VMWare/Troubleshooting%20vSAN/#check-failed-disks","text":"esxcli storage core device stats get","title":"Check Failed Disks"},{"location":"VMWare/Troubleshooting%20vSAN/#determine-what-drives-are-attached-to-which-storage-adapters","text":"","title":"Determine what drives are attached to which storage adapters"},{"location":"VMWare/Troubleshooting%20vSAN/#get-detailed-version","text":"esxcli storage core path list","title":"Get Detailed Version"},{"location":"VMWare/Troubleshooting%20vSAN/#simple-list","text":"esxcfg-scsidevs \u2013A","title":"Simple List"},{"location":"VMWare/Troubleshooting%20vSAN/#interpreting-the-output","text":"NAA stands for Network Addressing Authority identifier. EUI stands for Extended Unique Identifier. The number is guaranteed to be unique to that LUN. The NAA or EUI identifier is the preferred method of identifying LUNs and the storage device generates the number. Since the NAA or EUI is unique to the LUN, if the LUN is presented the same way across all ESXi hosts, the NAA or EUI identifier remains the same. Some devices do not provide the NAA number described above. In these circumstances, an MPX Identifier is generated by ESXi to represent the LUN or disk. The identifier takes the form similar to that of the canonical name of previous versions of ESXi with the \u201cmpx.\u201d prefix. This identifier can be used in the exact same way as the NAA Identifier described above.\u201d Refer to VMware KB article 1014953 for further information.","title":"Interpreting the Output"},{"location":"VMWare/Troubleshooting%20vSAN/#list-scsi-devices-and-their-sizes","text":"esxcfg-scsidevs \u2013c","title":"List SCSI devices and their sizes"},{"location":"VMWare/Troubleshooting%20vSAN/#get-a-list-of-all-pci-devices-allows-you-to-check-for-rebranding","text":"You an use a combination of the vendor IDs, device IDs, sub-vendor IDs, and sub-device IDs, to verify an adapter is on the VCG. Some vendors rebrand devices to reflect their own range of products however, the vendor and device ids remain the same. It is not uncommon to find that one vendor's controller is not listed under the vendor name in the VCG but is actually listed under another vendor. esxcli hardware pci list","title":"Get a list of all PCI devices (allows you to check for rebranding)"},{"location":"VMWare/Troubleshooting%20vSAN/#determine-adapter-driver-and-version","text":"Module Name indicates the driver that is currently in use. You can use the vmkload_mod -s <driver name> or esxcli system module get -m command to determine the driver version. Check that against the VCG; as bugs arise the supported drivers can change. You can also check the adapters with vsan.disks_info localhost/datacenter/computers/vSAN\\ Cluster/hosts/vsan1.lan --show-adapters in RVC.","title":"Determine Adapter Driver and Version"},{"location":"VMWare/Troubleshooting%20vSAN/#determine-driver-parameters","text":"vmkload_mod -s <driver name>","title":"Determine Driver Parameters"},{"location":"VMWare/Troubleshooting%20vSAN/#get-a-list-of-vibs","text":"esxcli software vib list If someone built using a custom ISO, it is common that they may have upgraded the driver to too high a version and that this version is unsupported.","title":"Get a list of VIBs"},{"location":"VMWare/Troubleshooting%20vSAN/#check-failures-to-tolerate-and-stripe","text":"You can see this on a per object level in vsan.object_info","title":"Check failures to tolerate and stripe"},{"location":"VMWare/Troubleshooting%20vSAN/#check-what-the-fault-domains-are","text":"vsan.cluster_info computers/vSAN\\ Cluster/","title":"Check what the fault domains are"},{"location":"VMWare/Troubleshooting%20vSAN/#inspect-a-specific-object-vm-in-vsan","text":"vsan.vm_object_info <target>","title":"Inspect a Specific Object (VM) in vSAN"},{"location":"VMWare/Troubleshooting%20vSAN/#understanding-drive-types","text":"Flash devices are placed in classes. Performance classes are measured in writes per second (not sure if this is up to date): Endurance is measured in Terabytes Written. Drives are qualified by the number of terabytes they are capable of writing per some time period per the vendor.","title":"Understanding Drive Types"},{"location":"VMWare/Troubleshooting%20vSAN/#vcg-check-summary","text":"","title":"VCG Check Summary"},{"location":"VMWare/Troubleshooting%20vSAN/#absent-vs-degraded-failure","text":"","title":"Absent vs Degraded Failure"},{"location":"VMWare/Troubleshooting%20vSAN/#object-compliance-status-compliant-vs-non-compliant","text":"","title":"Object Compliance Status: Compliant vs Non Compliant"},{"location":"VMWare/Troubleshooting%20vSAN/#object-operational-state-healthy-vs-unhealthy","text":"","title":"Object Operational State: Healthy vs Unhealthy"},{"location":"VMWare/Troubleshooting%20vSAN/#vm-accessibility-inaccessible-vs-orphaned","text":"","title":"VM Accessibility: Inaccessible vs Orphaned"},{"location":"VMWare/Troubleshooting%20vSAN/#todo","text":"Review P141 - Verifying Virtual SAN storage operation - RVC in Virtual SAN Diagnostics and Troubleshooting Reference Manual Review P152 - Testing vSAN functionality - deploying VMs in Virtual SAN Diagnostics and Troubleshooting Reference Manual P156 of Virtual SAN Diagnostics and Troubleshooting Reference Manual","title":"TODO"},{"location":"VMWare/Troubleshooting%20vSAN/setting_up_vsan_lab/","text":"Testing vSAN See notes on vSAN for how vSAN works. VM Setup I created three VMs with the following config: - I had to use the VMXNET 3 adapter to get ESXi to recognize the network card. - I used all NVMe drives - For the networks you must use a portgroup with the security settings disabled (because under the hood ESXi is effectively \"spoofing\" MAC addresses - or so the security settings will think): I then setup DNS entries for vsan1.lan, vsan2.lan, and vsan3.lan Configure NTP on vCenter (time must be synched before build) https://kb.vmware.com/s/article/57146 If your root password is expired: https://buildvirtual.net/vcenter-exception-in-invoking-authentication-handler-user-password-expired/ Make sure time on ESXi is working https://docs.vmware.com/en/VMware-vSphere/7.0/com.vmware.vsphere.vcenterhost.doc/GUID-8756D419-A878-4AE0-9183-C6D5A91A8FB1.html Configure vSAN https://docs.vmware.com/en/VMware-vSphere/7.0/com.vmware.vsphere.vsan-planning.doc/GUID-CF9767B6-B3F5-4787-9AF3-D661987AE525.html - I had to update my lab's HCL: https://kb.vmware.com/s/article/2145116 (the button didn't work for me)","title":"Testing vSAN"},{"location":"VMWare/Troubleshooting%20vSAN/setting_up_vsan_lab/#testing-vsan","text":"See notes on vSAN for how vSAN works.","title":"Testing vSAN"},{"location":"VMWare/Troubleshooting%20vSAN/setting_up_vsan_lab/#vm-setup","text":"I created three VMs with the following config: - I had to use the VMXNET 3 adapter to get ESXi to recognize the network card. - I used all NVMe drives - For the networks you must use a portgroup with the security settings disabled (because under the hood ESXi is effectively \"spoofing\" MAC addresses - or so the security settings will think): I then setup DNS entries for vsan1.lan, vsan2.lan, and vsan3.lan Configure NTP on vCenter (time must be synched before build) https://kb.vmware.com/s/article/57146 If your root password is expired: https://buildvirtual.net/vcenter-exception-in-invoking-authentication-handler-user-password-expired/ Make sure time on ESXi is working https://docs.vmware.com/en/VMware-vSphere/7.0/com.vmware.vsphere.vcenterhost.doc/GUID-8756D419-A878-4AE0-9183-C6D5A91A8FB1.html","title":"VM Setup"},{"location":"VMWare/Troubleshooting%20vSAN/setting_up_vsan_lab/#configure-vsan","text":"https://docs.vmware.com/en/VMware-vSphere/7.0/com.vmware.vsphere.vsan-planning.doc/GUID-CF9767B6-B3F5-4787-9AF3-D661987AE525.html - I had to update my lab's HCL: https://kb.vmware.com/s/article/2145116 (the button didn't work for me)","title":"Configure vSAN"},{"location":"VMWare/VMWare%20APIs/","text":"VMWare APIs VMWare APIs VxRail Upgrading VxRail with Windows Curl Example Code for PowerShell 5 Version Used for Testing Log Locations VxRail API Cookbook Upgrading VxRail with Windows Curl curl -k --user \"administrator@vsphere.local:<PASSWORD>\" --request POST \"https://<VXRAIL_IP_ADDRESS>/rest/vxm/v1/lcm/upgrade\" --header \"Content-Type: application/json\" --data \"{\\\"bundle_file_locator\\\": \\\"/tmp/VXRAIL_COMPOSITE-4.7.for_4.x.x.zip \\\",\\\"vxrail\\\": {\\\"vxm_root_user\\\": {\\\"username\\\": \\\"root\\\",\\\"password\\\": \\\"<PASSWORD>\\\"}},\\\"vcenter\\\":{\\\"vc_admin_user\\\": {\\\"username\\\": \\\"administrator@vsphere.local\\\",\\\"password\\\": \\\"<PASSWORD>\\\"},\\\"vcsa_root_user\\\": {\\\"username\\\": \\\"root\\\",\\\"password\\\": \\\"<PASSWORD>\\\"},\\\"psc_root_user\\\": {\\\"username\\\": \\\"root\\\",\\\"password\\\": \\\"<PASSWORD>\\\"}}}\" Note: psc_root_user should be the same as vCenter root user Example Code for PowerShell 5 $RESTAPIServer = \"YOUR_VCENTER\" $RESTAPIUser = \"administrator@vsphere.local\" // Or whatever user you want $RESTAPIPassword = \"PASSWORD\" add-type @\" using System.Net; using System.Security.Cryptography.X509Certificates; public class TrustAllCertsPolicy : ICertificatePolicy { public bool CheckValidationResult( ServicePoint srvPoint, X509Certificate certificate, WebRequest request, int certificateProblem) { return true; } } \"@ [System.Net.ServicePointManager]::CertificatePolicy = New-Object TrustAllCertsPolicy [System.Net.ServicePointManager]::SecurityProtocol = [System.Net.SecurityProtocolType]'Ssl3,Tls,Tls11,Tls12' $BaseAuthURL = \"https://\" + $RESTAPIServer + \"/rest/com/vmware/cis/\" $BaseURL = \"https://\" + $RESTAPIServer + \"/rest/vcenter/\" $vCenterSessionURL = $BaseAuthURL + \"session\" $Header = @{\"Authorization\" = \"Basic \"+[System.Convert]::ToBase64String([System.Text.Encoding]::UTF8.GetBytes($RESTAPIUser+\":\"+$RESTAPIPassword))} $Type = \"application/json\" Try { $vCenterSessionResponse = Invoke-RestMethod -Uri $vCenterSessionURL -Headers $Header -Method POST -ContentType $Type } Catch { $_.Exception.ToString() $error[0] | Format-List -Force } # Extracting the session ID from the response $vCenterSessionHeader = @{'vmware-api-session-id' = $vCenterSessionResponse.value} $VMListURL = $BaseURL+\"datacenter\" Write-Host \"Sending request to URL: ${VMListURL}\" Try { $VMListJSON = Invoke-RestMethod -Method Get -Uri $VMListURL -TimeoutSec 100 -Headers $vCenterSessionHeader -ContentType $Type $VMList = $VMListJSON.value } Catch { $_.Exception.ToString() $error[0] | Format-List -Force } $VMList | Format-Table -AutoSize Version Used for Testing PS C:\\Users\\grant\\Downloads> (Get-Host).Version Major Minor Build Revision ----- ----- ----- -------- 5 1 19041 1023 Log Locations Relevant logs are in /var/log/vmware/vapi/endpoint The vcentershim.log will tell you about errors occurring (like a 500 internal server error) The endpoint.log will tell you about things like 404 errors endpoint-access.log will show you who is accessing the server.","title":"VMWare APIs"},{"location":"VMWare/VMWare%20APIs/#vmware-apis","text":"VMWare APIs VxRail Upgrading VxRail with Windows Curl Example Code for PowerShell 5 Version Used for Testing Log Locations","title":"VMWare APIs"},{"location":"VMWare/VMWare%20APIs/#vxrail","text":"API Cookbook","title":"VxRail"},{"location":"VMWare/VMWare%20APIs/#upgrading-vxrail-with-windows-curl","text":"curl -k --user \"administrator@vsphere.local:<PASSWORD>\" --request POST \"https://<VXRAIL_IP_ADDRESS>/rest/vxm/v1/lcm/upgrade\" --header \"Content-Type: application/json\" --data \"{\\\"bundle_file_locator\\\": \\\"/tmp/VXRAIL_COMPOSITE-4.7.for_4.x.x.zip \\\",\\\"vxrail\\\": {\\\"vxm_root_user\\\": {\\\"username\\\": \\\"root\\\",\\\"password\\\": \\\"<PASSWORD>\\\"}},\\\"vcenter\\\":{\\\"vc_admin_user\\\": {\\\"username\\\": \\\"administrator@vsphere.local\\\",\\\"password\\\": \\\"<PASSWORD>\\\"},\\\"vcsa_root_user\\\": {\\\"username\\\": \\\"root\\\",\\\"password\\\": \\\"<PASSWORD>\\\"},\\\"psc_root_user\\\": {\\\"username\\\": \\\"root\\\",\\\"password\\\": \\\"<PASSWORD>\\\"}}}\" Note: psc_root_user should be the same as vCenter root user","title":"Upgrading VxRail with Windows Curl"},{"location":"VMWare/VMWare%20APIs/#example-code-for-powershell-5","text":"$RESTAPIServer = \"YOUR_VCENTER\" $RESTAPIUser = \"administrator@vsphere.local\" // Or whatever user you want $RESTAPIPassword = \"PASSWORD\" add-type @\" using System.Net; using System.Security.Cryptography.X509Certificates; public class TrustAllCertsPolicy : ICertificatePolicy { public bool CheckValidationResult( ServicePoint srvPoint, X509Certificate certificate, WebRequest request, int certificateProblem) { return true; } } \"@ [System.Net.ServicePointManager]::CertificatePolicy = New-Object TrustAllCertsPolicy [System.Net.ServicePointManager]::SecurityProtocol = [System.Net.SecurityProtocolType]'Ssl3,Tls,Tls11,Tls12' $BaseAuthURL = \"https://\" + $RESTAPIServer + \"/rest/com/vmware/cis/\" $BaseURL = \"https://\" + $RESTAPIServer + \"/rest/vcenter/\" $vCenterSessionURL = $BaseAuthURL + \"session\" $Header = @{\"Authorization\" = \"Basic \"+[System.Convert]::ToBase64String([System.Text.Encoding]::UTF8.GetBytes($RESTAPIUser+\":\"+$RESTAPIPassword))} $Type = \"application/json\" Try { $vCenterSessionResponse = Invoke-RestMethod -Uri $vCenterSessionURL -Headers $Header -Method POST -ContentType $Type } Catch { $_.Exception.ToString() $error[0] | Format-List -Force } # Extracting the session ID from the response $vCenterSessionHeader = @{'vmware-api-session-id' = $vCenterSessionResponse.value} $VMListURL = $BaseURL+\"datacenter\" Write-Host \"Sending request to URL: ${VMListURL}\" Try { $VMListJSON = Invoke-RestMethod -Method Get -Uri $VMListURL -TimeoutSec 100 -Headers $vCenterSessionHeader -ContentType $Type $VMList = $VMListJSON.value } Catch { $_.Exception.ToString() $error[0] | Format-List -Force } $VMList | Format-Table -AutoSize","title":"Example Code for PowerShell 5"},{"location":"VMWare/VMWare%20APIs/#version-used-for-testing","text":"PS C:\\Users\\grant\\Downloads> (Get-Host).Version Major Minor Build Revision ----- ----- ----- -------- 5 1 19041 1023","title":"Version Used for Testing"},{"location":"VMWare/VMWare%20APIs/#log-locations","text":"Relevant logs are in /var/log/vmware/vapi/endpoint The vcentershim.log will tell you about errors occurring (like a 500 internal server error) The endpoint.log will tell you about things like 404 errors endpoint-access.log will show you who is accessing the server.","title":"Log Locations"},{"location":"VMWare/VxRail%20Architecture%20and%20Troubleshooting/","text":"VxRail Architecture and Troubleshooting VxRail Architecture and Troubleshooting VxRail Architecture vxrail-mystic-lcm-7.0.202.rpm vxrail-system.zip VXRAIL_7.0.200-17911444.zip Inspecting VIB files Inspecting Java Class Files Upgrades How Upgrades Work Failed Upgrade Observed Errors Fix Action Understanding Upgrades in ESXi Understanding the VxRail Update Bundle Manifest File and Update Order Troubleshooting Docker Restarting a Container Checking Swarm Status Check for a List of Services Adding Nodes Troubleshooting How Adding Nodes Works During Cluster Construction Disappearing Browse Button Troubleshooting Script 15_setup_node.sh execution error Node 3 Password Change RASR process fails with \"Script 15_setup_node.sh execution error\" RASR on 192.168.0.172 Debugging error Log Info Fixing Half Upgrade vCenter Logs Altbootbank Add nodes NIC config page Add Node Error Error Node General Error Redeploying VxRail Manager Logs Generating Logs VxRail Microservice (container) Logs RASR Process Tomcat Logs Upgrade Logs API Logs Helpful Commands Helpful KB Articles VxRail API Info VxRail Architecture Most meaningful information on VxRail can be gleaned by reverse engineering the update package. There are three files in the update package of particular interest: bundles/vxrail-mystic-lcm-7.0.202.rpm vxrail-system.zip VXRAIL_7.0.200-17911444.zip vxrail-mystic-lcm-7.0.202.rpm The mystic LCM process governs all upgrades. Any code related to the upgrade seems to be part of this process. RPMs are just archives and you can unarchive this file with 7-zip and take a look at its contents for any debugging desired. vxrail-system.zip Defines all the baselines available in JSON format. VXRAIL_7.0.200-17911444.zip This file contains all the files for VxRail that aren't associated with the upgrade subsystem. Inspecting VIB files There's a good chance you may want to be able to look inside VIB files. VIB files use a proprietary compression technology. You can get the type by opening the file in a hex editor and inspecting the headers: On any ESXi host you can use the vmtar program to open these with the command: vmtar -x <INPUT_VMTAR_FILE> -o <OUTPUT>.tar . The output will be a regular tar file which you can then open with tar xvf <OUTPUT>.tar Inspecting Java Class Files Most of the Java I saw was contained in the vmware-marvin file or in the LCM Marvin RPM. I was wrestling with the error: web.log: ERROR [tomcat-http--48] com.emc.mystic.manager.commons.emc.webutil.LocaleUtil LocaleUtil.getBundleMessage:198 - No en_US resource bundle for ExpansionValidation with key <SERIAL_NUMBER> I was able to find the offending file in question at VXRAIL_COMPOSITE-7.0.202-27047874_for_4.7.x\\bundles\\vxrail-mystic-lcm-7.0.202\\vxrail-mystic-lcm-7.0.202-27047874.noarch\\usr\\lib\\vmware-marvin\\marvind\\webapps\\lcm\\WEB-INF\\lib\\lcm-module-7.0.202\\com\\vce\\lcm\\emc (that's post decompression of the vmware-marvin file inside the VIB). I find that compressing the three core files for VxRail and searching them with grep for keywords from the error messages was pretty effective for locating what I wanted. In my case the following got me what I wanted for the above error: (base) grant@DESKTOP-2SV1E9O:/mnt/c/Users/grant/Downloads/VXRAIL_COMPOSITE-7.0.202-27047874_for_4.7.x/bundles/vxrail-mystic-lcm-7.0.202/vxrail-mystic-lcm-7.0.202-27047874.noarch$ grep -inr 'resource bundle' ./* Binary file ./usr/lib/vmware-marvin/marvind/webapps/lcm/WEB-INF/lib/lcm-module-7.0.202/com/vce/lcm/emc/LocaleUtil.class matches I used the program JD Project to decompile the Java bytecode in question and it worked quite well. I was able to search the decompiled bytecode normally and find the offending function: which then allowed me to determine that this error message was actually erronious and just a product of VxRail trying and failing to localize an error message related to disk validation. Upgrades How Upgrades Work During the upgrade process the old VxRail manager will have its IP and hostname removed and be shutdown while the new one assumes the previous manager's hostname and IP. Upgrades are performed with the service account you configured when you built VxRail - not root . Failed Upgrade If an upgrade fails midflight this typically leaves the cluster in a state where there are two VxRail managers. In my team's experience trying to resume the upgrade using the new VxRail manager leads to a host of issues. Observed Errors Specifically, we encountered the following errors: VxRail Update ran into a problem... Please refer to KB488889 article. Failed to upload bundle: VXRAIL_COMPOSITE-SLIM-7.0.132-26894200_for_4.7x.zip: Failure occured in executing the checker: Pre-checking the version compatibility among components. General unexpected error occurs during getting the information of firmwares.. Please refer to the KB517433 article. Error extracting upgrade bundle 7.0.132-26894200. Failed to upload bundle. Please refer to log for details. This error is covered by Dell EMC VxRail: LCM failed at Error extracting update bundle 7.0.132 We also saw: Failure occurred while running an upgrade for bundle VXRAIL_COMPOSITE-SLIM-7.0.132-26894200_for_4.7.x.zip. The error message [DependencyError] VIB DEL_bootbank_dcism_3.6.0.2249-DEL.700.0.0.15843807 requires esx-version >= 7.0.0 but the requirement cannot be satisfied within the ImageProfile. Please refer to the log file for more details. Fix Action The most reliable way to fix a stuck upgrade is to revert to the old VxRail Manager and then run the upgrade again. You can do this by shutting down the new VxRail Manager, deleting it, and then logging into the previous VxRail Manager on the console, re-IPing it, give it it's previous hostname, and then restarting the upgrade. Understanding Upgrades in ESXi See What is the altbookbank Partition Understanding the VxRail Update Bundle At the top level the upgrade is governed and controlled by the file node-upgrade.py in the root of the upgrade bundle: The bundles directory has all the possible files that could be installed with the update. The exact files which are installed vary by system. More on that in the section Manifest File and Update Order . Manifest File and Update Order The manifest file defines which VIBs will be installed based on system parameters and in what order they will be installed in. For example: ESXi: <Package InstallOrder=\"100\"> <ComponentType>ESXi</ComponentType> <DisplayName>ESXi</DisplayName> <Version>7.0.2</Version> <Build>17867351</Build> <File>bundles/ESXi-7.0.2-ep1_17867351-34777dce-standard.zip</File> <Size>393239990</Size> <HighestFormatVersionSupported>11</HighestFormatVersionSupported> <UpgradeTime>5</UpgradeTime> <RebootFlag>True</RebootFlag> <Package InstallOrder=\"101\"> <ComponentType>ESXi_No_Tools</ComponentType> <DisplayName>VMware ESXi No Tools</DisplayName> <Version>7.0.2</Version> <Build>17867351</Build> <File>bundles/ESXi-7.0.2-ep1_17867351-34777dce-upgrade-no-tools.zip</File> <Size>207115430</Size> <HighestFormatVersionSupported>11</HighestFormatVersionSupported> <UpgradeTime>5</UpgradeTime> <RebootFlag>True</RebootFlag> </Package> <Package InstallOrder=\"1001\"> <ComponentType>ESXi_VIB</ComponentType> <DisplayName>Dell iSM for vSphere 7</DisplayName> <Version>3.6.0.2249</Version> <Build>DEL.700.0.0.15843807</Build> <SystemName>dcism</SystemName> This is parsed by the function _parse_esxi_patch, _parse_install_vib, and _parse_update_firmware in node-upgrade.py on line 1381, 1454, and 1492: def _parse_esxi_patch(self, element): task = { 'type': \"esxi_patch\", 'name': \"Install ESXi VMware patch\", 'async': False, 'visible': True, } task['install_order'] = int(element.get('InstallOrder')) task['args'] = self._extract_args(element, [ 'ComponentType', 'DisplayName', 'Version', 'Build', 'File', 'Size', 'HighestFormatVersionSupported', ]) return task ---SNIP--- def _parse_install_vib(self, element): task = { 'type': \"install_vib\", 'async': False, 'visible': True, 'package_type': 'vib', } task['name'] = \"Install %s\" % element.find('DisplayName').text task['install_order'] = int(element.get('InstallOrder')) task['args'] = self._extract_args(element, [ 'ComponentType', 'DisplayName', 'Version', 'Build', 'SystemName', 'File', 'Size', 'ReplaceTargetInfo/ReplaceTarget/SystemName', ]) component_type = task['args'].get('ComponentType', '') display_name = task['args'].get('DisplayName', '') if equals_ignore_case(display_name, 'VxRail VIB') or \\ equals_ignore_case(component_type, 'VXRAIL_'): task['args']['SystemName'] = \"vmware-marvin\" pkg_file = task['args'].get('File', '') file_path = os.path.join(self._bundle_dir, pkg_file) vlcm_bundle_info = self._vlcm_bundle_info(file_path) if vlcm_bundle_info: task['package_type'] = 'component' task['component_name'] = vlcm_bundle_info[0] task['component_version'] = vlcm_bundle_info[1] return task def _parse_update_firmware(self, element): task = { 'type': \"update_firmware\", 'async': True, 'visible': True, 'runtime_check': False, } task['name'] = \"Update %s\" % element.find('DisplayName').text task['install_order'] = int(element.get('InstallOrder')) task['args'] = self._extract_args(element, [ 'ComponentType', 'DisplayName', 'Version', 'Build', 'File', 'Size', ]) nic_models = self._extract_target_models(element, 'TargetNicModelInfo') component_models = self._extract_target_models( element, 'TargetComponentModelInfo') fw_models = nic_models + component_models if fw_models: task['args']['FirmwareModels'] = fw_models else: task['args']['FirmwareModels'] = None return task These functions are used to create tasks which are stored in the required_tasks variable: After all tasks are added to required tasks they are sorted with a lambda function on line 1898: required_tasks.sort(key=lambda t: t['install_order']) Subsequently it is safe to assume a linear sort on the integer value. We can use this to diagnose any problems we encounter with install order. Troubleshooting Docker Restarting a Container You can add and remove a container with docker service scale func_lockbox=0 && docker service scale func_lockbox=1 Checking Swarm Status Use docker info to get the swarm info and docker node ls to list the nodes vcluster202-vxm:~ # docker info Client: Context: default Debug Mode: false Server: Containers: 110 Running: 53 Paused: 0 Stopped: 57 Images: 43 Server Version: 20.10.9-ce Storage Driver: overlay2 Backing Filesystem: extfs Supports d_type: true Native Overlay Diff: true userxattr: false Logging Driver: json-file Cgroup Driver: cgroupfs Cgroup Version: 1 Plugins: Volume: local Network: bridge host ipvlan macvlan null overlay Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog Swarm: active NodeID: gvqywccvm7g1qcbhud1kej1y5 Is Manager: true ClusterID: 3uj8bybr66y3edkgp4txgmhse Managers: 1 Nodes: 1 Default Address Pool: 169.254.170.0/24 SubnetSize: 24 Data Path Port: 4789 Orchestration: Task History Retention Limit: 5 Raft: Snapshot Interval: 10000 Number of Old Snapshots to Retain: 0 Heartbeat Tick: 1 Election Tick: 10 Dispatcher: Heartbeat Period: 10 years CA Configuration: Expiry Duration: 10 years Force Rotate: 1 Autolock Managers: false Root Rotation In Progress: false Node Address: 127.0.0.1 Manager Addresses: 127.0.0.1:2377 Runtimes: io.containerd.runc.v2 io.containerd.runtime.v1.linux oci runc Default Runtime: runc Init Binary: docker-init containerd version: 5b46e404f6b9f661a205e28d59c982d3634148f8 runc version: init version: Security Options: apparmor seccomp Profile: default Kernel Version: 5.3.18-24.96-default Operating System: SUSE Linux Enterprise Server 15 SP2 OSType: linux Architecture: x86_64 CPUs: 4 Total Memory: 11.69GiB Name: vcluster202-vxm ID: XFEN:HBJA:CCKR:7CGG:QKXR:DDQP:ZEB3:YPOW:NAUC:AASS:3WM7:3A2F Docker Root Dir: /var/lib/docker Debug Mode: false Registry: https://index.docker.io/v1/ Labels: Experimental: false Insecure Registries: 127.0.0.0/8 Live Restore Enabled: false WARNING: No swap limit support vcluster202-vxm:~ # docker node ls ID HOSTNAME STATUS AVAILABILITY MANAGER STATUS ENGINE VERSION gvqywccvm7g1qcbhud1kej1y5 * vcluster202-vxm Ready Active Leader 20.10.9-ce Check for a List of Services Each container on VxRail runs as a separate service. You can check their status with docker service ls . vcluster202-vxm:~ # docker service ls ID NAME MODE REPLICAS IMAGE PORTS r4mawp339y88 func_alertmanager replicated 1/1 infra/prom/alertmanager:1.8.9 iduap6ujbho1 func_api-gateway replicated 1/1 infra/nginx_gateway_vxrail:1.8.14 5e18xm020k0f func_auth-service replicated 1/1 microservice/auth-service:1.8.13 j4pg14v6ajts func_cacheservice replicated 1/1 infra/redis:1.8.11 5e1q0w6v2exg func_cms-service replicated 1/1 microservice/cms-service:1.8.31 i4xpa8ltrla0 func_configservice replicated 1/1 infra/infra-config-service:1.8.10 uma1pjrkwjcq func_do-cluster replicated 1/1 do-main/do-cluster:1.8.26 uprraqe4lxo4 func_do-ecosystem replicated 1/1 do-main/do-ecosystem:1.8.26 0bpnsjy617yx func_do-eservices replicated 1/1 do-main/do-eservices:1.8.26 uaza36n6v9hl func_do-host replicated 1/1 do-main/do-host:1.8.26 jby3wca4pgxh func_do-kgs replicated 1/1 do-main/do-kgs:1.8.26 7nh07m0qnn10 func_do-network replicated 1/1 do-main/do-network:1.8.26 ta846rozl38e func_do-serviceability replicated 1/1 do-main/do-serviceability:1.8.26 yczljgv2hfki func_do-storage replicated 1/1 do-main/do-storage:1.8.26 tizxv31eqr6z func_do-vm replicated 1/1 do-main/do-vm:1.8.26 pnwq69rtx8b9 func_do-vxrail-system replicated 1/1 do-main/do-vxrail-system:1.8.26 pescqbk5gdm6 func_event-history-service replicated 1/1 microservice/event-history-service:1.8.18 uvj4grv92xjx func_event-service replicated 1/1 microservice/event-service:1.8.18 i1ajjwa20o2h func_faas-swarm replicated 1/1 infra/openfaas/faas-swarm:1.8.9 xdnl8mbbfnhw func_gateway replicated 1/1 infra/openfaas/gateway:1.8.9 w7yz4wdlurgx func_hsm replicated 1/1 microservice/hsm:1.8.25 txlipzyfxe5k func_infra-scale-service replicated 1/1 infra/infra-scale-service:1.8.8 hrzxht5zp33u func_kgs-service replicated 1/1 microservice/kgs-service:1.8.9 iweqxas2urpn func_lockbox replicated 1/1 microservice/lockbox:1.8.10 q75sa2hvxc0z func_lockservice replicated 1/1 infra/infra-lock-service:1.8.8 zd10d8q5vmml func_logging replicated 1/1 infra/logging:1.8.9 nob97ajl4wjl func_ms-day1-bringup replicated 1/1 microservice/ms-day1-bringup:1.8.21 339zceysq1w7 func_ms-day2 replicated 1/1 microservice/ms-day2:1.8.40 vwofhpygnrvz func_nano-service replicated 1/1 microservice/nano-service:1.8.21 ekm0s9o6fse3 func_nats replicated 1/1 infra/nats-streaming:1.8.9 o9ft5wqvkgyq func_node-service replicated 1/1 microservice/node-service:1.8.31 q9ewku1t3xjy func_prometheus replicated 1/1 infra/prom/prometheus:1.8.9 om3gislgsh82 func_property-collector replicated 1/1 do-main/property-collector:1.8.26 pkhsz674xtde func_queue-worker replicated 1/1 infra/openfaas/queue-worker:1.8.9 anmzrcv2plzy func_rcs-service replicated 1/1 microservice/rcs-service:1.8.11 t16j0wln8w0s func_serviceregistry replicated 1/1 infra/gcr.io/etcd-development/etcd:1.8.9 rny34tgzu7jq func_storage-service replicated 1/1 microservice/storage-service:1.8.16 sbc1n8rr25n3 func_tracing replicated 1/1 infra/tracing:1.8.9 vdf4hdtl957a func_vlcm replicated 1/1 microservice/vlcm:1.8.23 dkfjjajjx8tj func_vxdoctor-service replicated 1/1 microservice/vxdoctor-service:1.8.18 rsr9plg95qhw func_wfservice replicated 1/1 infra/workflow_engine:1.8.10 You can check the details of a specific service with docker service inspect --pretty <SERVICE> vcluster202-vxm:~ # docker service inspect --pretty func_do-storage ID: yczljgv2hfkiou1zh6liq6tbr Name: func_do-storage Labels: com.docker.stack.image=do-main/do-storage:1.8.26 com.docker.stack.namespace=func Service Mode: Replicated Replicas: 1 UpdateStatus: State: paused Started: 6 months ago Message: update paused due to failure or early termination of task qgjut6lu088orf8dmi8r3xxt8 Placement: UpdateConfig: Parallelism: 1 On failure: pause Monitoring Period: 5s Max failure ratio: 0 Update order: stop-first RollbackConfig: Parallelism: 1 On failure: pause Monitoring Period: 5s Max failure ratio: 0 Rollback order: stop-first ContainerSpec: Image: do-main/do-storage:1.8.26 Env: API_RESOURCES=[\"v1/storage:do-storage/v1/storage\"] REGISTERED=true SERVICE_LAYER=do SERVICE_NAME=do-storage SERVICE_PORT=5000 SERVICE_TYPE=micro TRACING_HOST=tracing TRACING_SERVICE_NAME=do-storage User: app:475 Mounts: Target: /vxm_cert Source: /etc/vmware-marvin/ssl ReadOnly: false Type: bind Target: /certificate Source: /var/lib/vmware-marvin/trust ReadOnly: false Type: bind Target: /service_data Source: /var/lib/vxrail/do_storage/data ReadOnly: false Type: bind Resources: Limits: Memory: 1GiB Networks: func_functions Endpoint Mode: vip Healthcheck: Interval = 30s Retries = 10 StartPeriod = 1m0s Timeout = 1m0s Tests: Test = CMD-SHELL Test = curl -f http://localhost:5000/do-storage/v1/storage/health-check || exit 1 Adding Nodes Version Compatibility Matrix: Link Make sure before you try to do anything with adding nodes that you verify the versions are mutually compatible. Troubleshooting We bumped into several errors while attempting to add nodes and they varied in type. One of the errors we encountered was when adding the node the interface for the VxRail plugin failed entirely. We were able to fix it with the following process: Take a snapshot of VxM psql -U postgres vxrail -c \"DELETE FROM system.operation_status WHERE state='IN_PROGRESS';\" enable advanced mode by changing it in /var/lib/vmware-marvin/lcm_advanced_mode.properties Change the contents of /var/lib/vmware-mariv to {\"state\":\"NONE\",\"vc_plugin_updated\":false,\"deployed_for_public_api\":false} Restart Marvin with system restart vmware-marvin In one instance we were able to clear all problems by restarting all nodes in the cluster, the VxRail manager, and vCenter. After a full reboot of everything, a series of general errors we had been receiving during validation cleared. How Adding Nodes Works During Cluster Construction All nodes after being RASR have a fully built vSAN disk group. During the initial cluster construction VxRail will select a primary node and then add it to the cluster. On every subsequent node it will delete that node's vSAN disk group and then add it to the cluster's existing disk group. Disappearing Browse Button Take a snapshot of VxM psql -U postgres vxrail -c \"DELETE FROM system.operation_status WHERE state='IN_PROGRESS';\" enable advanced mode by change it in /var/lib/vmware-marvin/lcm_advanced_mode.properties Change the contents of /var/lib/vmware-mariv to {\"state\":\"NONE\",\"vc_plugin_updated\":false,\"deployed_for_public_api\":false} Result system restart vmware-marvin Troubleshooting Script 15_setup_node.sh execution error Node 3 Password Change Somehow 192.168.0.172 changed iDRAC passwords overnight. Last I left it, it was in a reboot loop RASR process fails with \"Script 15_setup_node.sh execution error\" Both 192.168.0.171 and 192.168.0.172 are failed with \"Script 15_setup_node.sh execution error\". I confirmed they both had failed to generate disk mapping. This does not exactly match the kb but it does appear to be fatal. RASR on 192.168.0.172 I ran with no DUP install Copying these temp files is taking hours. They're only 12 GB Same problem Debugging error The error looks like https://www.dell.com/support/kbdoc/en-th/000193618?lang=en For starters my fist.log was in /scratch instead of /mnt The observed 15_setup_node.sh is in: [root@fedora media]# grep -Ro 15_setup* . grep: ./release/gmywd_is.tar: binary file matches See 15_setup_node.sh The referenced vmware-marvin seems to be a VIB The failed post is here: I can't tell if this error is related but in another error this function failes: drives is none. Whatever is populating self.getDrivesInfo(host, port) isn't working. Whatever this is in vxrail_primary.py (line 530) HostConnection is not returning something valid. If we know what that is we can debug manually on the box. Moreover, this function is called is_dell_non_13G_platform but this is a P470F - which IS a 13G platform. Log Info lcm-web.log - Shows upgrade related info web.log - combined web output Fixing Half Upgrade Roll back to previous VxRail manager, then use it to complete the upgrade. Uses ESXi management account for updates Uses PSC log vCenter Logs Altbootbank You can load the old version with shift+r Add nodes NIC config page When you hit https://<host>/ui/vxrail/rest/vxm/private/system/cluster-hosts?$$objectID=urn:vmomi:ClusterComputeResource:domain<ID>&lang=en-us you should get back the existing hosts in the cluster with their hostname, model, serial_number, and vmnics https://<HOST>/ui/vxrail/rest/vxm/private/system/available-hosts?$filter=serial_number%20in%20(SERIAL,SERIAL)&$$objectId=urn:vmomi:ClusterComputeResource:domain-<ID>&lang=en-us should get you the two hosts you are going to add Add Node Error \"Could not find NSX-T network information\" Http failure response for https://<address>/ui/vxrail/rest/vxm/private/cluster/network/nsxt???objectId=urn:vmomi:ClusterComputeResource:domain-ID=en-US: 404 OK Error Node Validation Errors Disk Grouping Error (JKSLH63) Error occurs when validating disk group on host JKSLH63 web.log: ERROR [tomcat-http--48] com.emc.mystic.manager.commons.emc.webutil.LocaleUtil LocaleUtil.getBundleMessage:198 - No en_US resource bundle for ExpansionValidation with key JKSLH63 General Error web.log: [WARN] com.vce.commons.config.ConfigServiceImpl$NotFoundHandler ConfigServiceImpl$NotFoundHandler.handleNotFound:114 - provided key is not present: [404, {\"message\":\"404 Not Found: bandwidth_throttling_level does not exist\"}] Redeploying VxRail Manager Note: My testing was done on VxRail 7.0.320 Go to Edit Settings on your existing VxRail Manager and check its existing networking. There should be two networks. Make note of what they are as you will need these settings later. They are probably something like vCenter Server Network-<uuid> and VxRail Manager-<uuid> . Make note of the existing VxRail manager's IP address. Log into the existing VxRail Manager. Run cat /var/lib/vmware-marvin/config-initial.json | jq | less . This is the current settings for VxRail Manager. I suggest you back this up and use it for your answers to the script but this is not required. You will need to backup the following files in this directory: manifest.xml vcmRuleset.xml nodeManifestfile.xml node-upgrade.py baseline.json Power down the existing VxRail Manager. Import the new VxRail Manager. The values for the import are specific to the user except the networking. When selecting the networking for the new VxRail Manager there will be two networks. Match these to what you saw in the original VxRail Manager. Before powering on your VxRail Manager you will need to change the guest operating system. Right click the VM, edit settings, vm options, general options, change the operating system to SUSE Linux 12. After you power on this should autocorrect itself to the correct OS. If you don't do this you will get an error from vCenter saying the operating system is unsupported. Login to the VxRail Manager in the web console with root / Passw0rd! Set the ip address of eth0 with ifconfig eth0 <IP> , ifconfig eth0 netmask <NETMASK> , and ip route add 0.0.0.0/0 via <your_default_gateway> If you do not already have it, contact Dell Support, ask for the script which comes with the KB Dell VxRail: Recover VxRail Manager VM from Scratch . Use credentials mystic / VxRailManager@201602! to upload the script to the IP address you established for VxRail Manager with WinSCP or other utility. Alternatively, you can copy and paste it over SSH with vim or your favorite editor. SSH to the VxRail Manager. Copy the files you backed up earlier (manifest.xml, vcmRuleset.xml, nodeManifestfile.xml, node-upgrade.py, baseline.json) to the /var/lib/vmware-marvin folder and change their owner to tcserver:pivotal . Next, you will need to enable root login for SSH. Run vim /etc/ssh/sshd_config (or your favorite text editor). Change PermitRootLogin from no to yes. Save and restart sshd with systemctl restart sshd Finally, run the uploaded script vxm_recovery_70x.py . Use the JSON you downloaded from the old VxRail Manager as a reference if you need it. When asked if the VxRail Manager networking is configured as expected say no and then re-input all networking values. Domain Search Path: This is top_level_domain in your JSON. Is the Domain Name server external: check the field is_internal_dns in your JSON. WARNING it asks you if you domain server is external. If the value of is_internal_dns is false then make sure you answer yes to this question. Domain Name Server IP address: Check the array dns_servers . If you only have one just import the one DNS server IP. If there is more than one, split each ip with a , . VxRail Manager hostname: This can be whatever you want it to be. You do not need to add the domain suffix here. NTP server IPs: see the array ntp_servers in your JSON Config root and mystic password of VxM: Select y if you haven't done this already Retrieve vRealize Log Insight Info - n unless you have vRealize Log Insight vCenter IP: See vxrail_supplied_vc_ip in your JSON (this should be the IP address of your vCenter server) vcenter FQDN: The FQDN of vCenter. Ex: vcluster202-vcsa.demo.local Admin account name: This is typically administrator@something.something Management account name: This is the VxRail manager account name. NOT root. You can find it here: . WARNING if authentication fails, you may have to add @localos depending on your VxRail version. Is PSC external: n Is customized vds created? N Can you confirm the management account info of the ESXi host: Answer yes. You can find the ESXi Management account info here: On vCenter, browse to VxRail here: Confirm that the dashboard populates. It is normal for it to have warnings if it cannot connect to the internet. If it is not working, the screen will be completely blank. Make sure if you alread had vCenter open that you refresh the page. Next, we are going to simulate a shutdown. You do not actually need to shutdown the cluster, but executing the shutdown simulation will confirm everything is working as expected. In my instance I received the following: If you see this, it is more than likely a cert problem. You can confirm this by checking the /var/log/microservice_log/short.term.log on the VxRail Manager. If this is indeed the problem you should see: 2022-10-20-04:18:26 microservice.do-cluster \"[2022-10-20 04:18:26,812: ERROR/MainProcess] Failed to login by session cookie!\" 2022-10-20-04:18:26 microservice.do-cluster \"Traceback (most recent call last):\" 2022-10-20-04:18:26 microservice.do-cluster \" File \"\"/home/app/worker/task/vcenter.py\"\", line 414, in login_by_session_cookie\" 2022-10-20-04:18:26 microservice.do-cluster \" username = si.content.sessionManager.currentSession.userName\" 2022-10-20-04:18:26 microservice.do-cluster \" File \"\"/usr/local/venv/lib64/python3.6/site-packages/pyVmomi/VmomiSupport.py\"\", line 700, in __call__\" 2022-10-20-04:18:26 microservice.do-cluster \" return self.f(*args, **kwargs)\" 2022-10-20-04:18:26 microservice.do-cluster \" File \"\"/usr/local/venv/lib64/python3.6/site-packages/pyVmomi/VmomiSupport.py\"\", line 520, in _InvokeAccessor\" 2022-10-20-04:18:26 microservice.do-cluster \" return self._stub.InvokeAccessor(self, info)\" 2022-10-20-04:18:26 microservice.do-cluster \" File \"\"/usr/local/venv/lib64/python3.6/site-packages/pyVmomi/StubAdapterAccessorImpl.py\"\", line 40, in InvokeAccessor\" 2022-10-20-04:18:26 microservice.do-cluster \" self._pc = si.RetrieveContent().propertyCollector\" 2022-10-20-04:18:26 microservice.do-cluster \" File \"\"/usr/local/venv/lib64/python3.6/site-packages/pyVmomi/VmomiSupport.py\"\", line 706, in <lambda>\" 2022-10-20-04:18:26 microservice.do-cluster \" self.f(*(self.args + (obj,) + args), **kwargs)\" 2022-10-20-04:18:26 microservice.do-cluster \" File \"\"/usr/local/venv/lib64/python3.6/site-packages/pyVmomi/VmomiSupport.py\"\", line 512, in _InvokeMethod\" 2022-10-20-04:18:26 microservice.do-cluster \" return self._stub.InvokeMethod(self, info, args)\" 2022-10-20-04:18:26 microservice.do-cluster \" File \"\"/usr/local/venv/lib64/python3.6/site-packages/pyVmomi/SoapAdapter.py\"\", line 1350, in InvokeMethod\" 2022-10-20-04:18:26 microservice.do-cluster \" conn.request('POST', self.path, req, headers)\" 2022-10-20-04:18:26 microservice.do-cluster \" File \"\"/usr/lib64/python3.6/http/client.py\"\", line 1291, in request\" 2022-10-20-04:18:26 microservice.do-cluster \" self._send_request(method, url, body, headers, encode_chunked)\" 2022-10-20-04:18:26 microservice.do-cluster \" File \"\"/usr/lib64/python3.6/http/client.py\"\", line 1337, in _send_request\" 2022-10-20-04:18:26 microservice.do-cluster \" self.endheaders(body, encode_chunked=encode_chunked)\" 2022-10-20-04:18:26 microservice.do-cluster \" File \"\"/usr/lib64/python3.6/http/client.py\"\", line 1286, in endheaders\" 2022-10-20-04:18:26 microservice.do-cluster \" self._send_output(message_body, encode_chunked=encode_chunked)\" 2022-10-20-04:18:26 microservice.do-cluster \" File \"\"/usr/lib64/python3.6/http/client.py\"\", line 1046, in _send_output\" 2022-10-20-04:18:26 microservice.do-cluster \" self.send(msg)\" 2022-10-20-04:18:26 microservice.do-cluster \" File \"\"/usr/lib64/python3.6/http/client.py\"\", line 984, in send\" 2022-10-20-04:18:26 microservice.do-cluster \" self.connect()\" 2022-10-20-04:18:26 microservice.do-cluster \" File \"\"/usr/local/venv/lib64/python3.6/site-packages/pyVmomi/SoapAdapter.py\"\", line 1039, in connect\" 2022-10-20-04:18:26 microservice.do-cluster \" http_client.HTTPSConnection.connect(self)\" 2022-10-20-04:18:26 microservice.do-cluster \" File \"\"/usr/lib64/python3.6/http/client.py\"\", line 1452, in connect\" 2022-10-20-04:18:26 microservice.do-cluster \" server_hostname=server_hostname)\" 2022-10-20-04:18:26 microservice.do-cluster \" File \"\"/usr/local/venv/lib64/python3.6/site-packages/gevent/_ssl3.py\"\", line 120, in wrap_socket\" 2022-10-20-04:18:26 microservice.do-cluster \" _session=session)\" 2022-10-20-04:18:26 microservice.do-cluster \" File \"\"/usr/local/venv/lib64/python3.6/site-packages/gevent/_ssl3.py\"\", line 308, in __init__\" 2022-10-20-04:18:26 microservice.do-cluster \" self.do_handshake()\" 2022-10-20-04:18:26 microservice.do-cluster \" File \"\"/usr/local/venv/lib64/python3.6/site-packages/gevent/_ssl3.py\"\", line 667, in do_handshake\" 2022-10-20-04:18:26 microservice.do-cluster \" self._sslobj.do_handshake()\" 2022-10-20-04:18:26 microservice.do-cluster \" File \"\"/usr/lib64/python3.6/ssl.py\"\", line 694, in do_handshake\" 2022-10-20-04:18:26 microservice.do-cluster \" match_hostname(self.getpeercert(), self.server_hostname)\" 2022-10-20-04:18:26 microservice.do-cluster \" File \"\"/usr/lib64/python3.6/ssl.py\"\", line 331, in match_hostname\" 2022-10-20-04:18:26 microservice.do-cluster \" % (hostname, dnsnames[0]))\" 2022-10-20-04:18:26 microservice.do-cluster \"ssl.CertificateError: hostname '192.168.1.20' doesn't match 'vcluster202-vcsa.demo.local'\" 2022-10-20-04:18:26 microservice.do-cluster \"[2022-10-20 04:18:26,816: INFO/MainProcess] Task worker.celery_task.rest_cluster.login_by_session_cookie[90e54bf5-a9d3-4d28-a76e-70fcbbe062cb] succeeded in 0.03339903799860622s: {'result': 'LOGIN_BY_SESSION_C OOKIE_FAIL'}\" On the Vxrail manager, confirm that in /etc/hosts you have an entry for your vCenter. If not, add it. On vCenter as root run mkdir -p /var/tmp/vmware && chmod 755 /var/tmp/vmware Next run /usr/lib/vmware-vmca/bin/certificate-manager . Select option 8 Do you wish to generate all certs using configuration file: n Enter your credentials You can leave everything to default until you get to Hostname. Here I used the FQDN for VCSA. For VMCA I used the shortname of VCSA When asked if you want to continue select yes. The script will run. This takes a while to run. For me it stopped at 85% for a long time. This appears to be normal. Grab cert_util.py and upload it to VxRail Manager. Run the script. If it fails saying No host configured run curl -k -H \"Content-Type: application/json\" -X PUT --unix-socket /var/lib/vxrail/nginx/socket/nginx.sock -d '{\"value\": \"false\"}' http://127.0.0.1/rest/vxm/internal/configservice/v1/configuration/keys/certificate_verification_enable . This will disable cert checking for it. After you run that command, rerun the script and it should complete successfully. Logs Generating Logs If you want to get a full dump of all the logs associated with VxRail you can generate a dump by going to the VxRail plugin on vCenter, navigating to the support tab, and then clicking the generate log button. VxRail Microservice (container) Logs /var/log/microservice/vxrail-manager RASR Process The totality of the RASR process' output is stored in the images folder on the local datastore in fist.log. Tomcat Logs /var/log/marvin/tomcat/logs /var/log/mystic/web.log Upgrade Logs The LCM process handles upgrades. It's logs are all in /var/log/mystic/lcm*. You can also try checking the PSC log (haven't personally done this) API Logs These are on vCenter. See VMWARE API Logs Helpful Commands See Helpful Commands Helpful KB Articles https://www.dell.com/support/kbdoc/en-uk/000021742/dell-emc-vxrail-troubleshooting-guide-for-vxrail-micro-services-infrastructure https://www.dell.com/support/kbdoc/en-uk/000157667/dell-emc-vxrail-how-to-query-vxrail-cluster-configuration-data-in-7-0-010-or-higher https://www.dell.com/support/kbdoc/en-us/000157715/dell-emc-vxrail-troubleshooting-guide-for-upgrading-vxrail-manager-to-7-0-010-or-higher https://www.dell.com/support/kbdoc/en-us/000157662/dell-emc-vxrail-how-to-get-or-update-management-account-in-vxrail-7-0-010-or-higher https://www.dell.com/support/kbdoc/en-us/000181759/dell-emc-vxrail-upgrading-vxrail-manager-to-7-0-x-release-failed-vcenter-plugin-no-loading https://www.dell.com/support/kbdoc/en-us/000181712/dell-emc-vxrail-how-to-update-vxrail-manager-without-lcm VxRail API Info See VMWare APIs","title":"VxRail Architecture and Troubleshooting"},{"location":"VMWare/VxRail%20Architecture%20and%20Troubleshooting/#vxrail-architecture-and-troubleshooting","text":"VxRail Architecture and Troubleshooting VxRail Architecture vxrail-mystic-lcm-7.0.202.rpm vxrail-system.zip VXRAIL_7.0.200-17911444.zip Inspecting VIB files Inspecting Java Class Files Upgrades How Upgrades Work Failed Upgrade Observed Errors Fix Action Understanding Upgrades in ESXi Understanding the VxRail Update Bundle Manifest File and Update Order Troubleshooting Docker Restarting a Container Checking Swarm Status Check for a List of Services Adding Nodes Troubleshooting How Adding Nodes Works During Cluster Construction Disappearing Browse Button Troubleshooting Script 15_setup_node.sh execution error Node 3 Password Change RASR process fails with \"Script 15_setup_node.sh execution error\" RASR on 192.168.0.172 Debugging error Log Info Fixing Half Upgrade vCenter Logs Altbootbank Add nodes NIC config page Add Node Error Error Node General Error Redeploying VxRail Manager Logs Generating Logs VxRail Microservice (container) Logs RASR Process Tomcat Logs Upgrade Logs API Logs Helpful Commands Helpful KB Articles VxRail API Info","title":"VxRail Architecture and Troubleshooting"},{"location":"VMWare/VxRail%20Architecture%20and%20Troubleshooting/#vxrail-architecture","text":"Most meaningful information on VxRail can be gleaned by reverse engineering the update package. There are three files in the update package of particular interest: bundles/vxrail-mystic-lcm-7.0.202.rpm vxrail-system.zip VXRAIL_7.0.200-17911444.zip","title":"VxRail Architecture"},{"location":"VMWare/VxRail%20Architecture%20and%20Troubleshooting/#vxrail-mystic-lcm-70202rpm","text":"The mystic LCM process governs all upgrades. Any code related to the upgrade seems to be part of this process. RPMs are just archives and you can unarchive this file with 7-zip and take a look at its contents for any debugging desired.","title":"vxrail-mystic-lcm-7.0.202.rpm"},{"location":"VMWare/VxRail%20Architecture%20and%20Troubleshooting/#vxrail-systemzip","text":"Defines all the baselines available in JSON format.","title":"vxrail-system.zip"},{"location":"VMWare/VxRail%20Architecture%20and%20Troubleshooting/#vxrail_70200-17911444zip","text":"This file contains all the files for VxRail that aren't associated with the upgrade subsystem.","title":"VXRAIL_7.0.200-17911444.zip"},{"location":"VMWare/VxRail%20Architecture%20and%20Troubleshooting/#inspecting-vib-files","text":"There's a good chance you may want to be able to look inside VIB files. VIB files use a proprietary compression technology. You can get the type by opening the file in a hex editor and inspecting the headers: On any ESXi host you can use the vmtar program to open these with the command: vmtar -x <INPUT_VMTAR_FILE> -o <OUTPUT>.tar . The output will be a regular tar file which you can then open with tar xvf <OUTPUT>.tar","title":"Inspecting VIB files"},{"location":"VMWare/VxRail%20Architecture%20and%20Troubleshooting/#inspecting-java-class-files","text":"Most of the Java I saw was contained in the vmware-marvin file or in the LCM Marvin RPM. I was wrestling with the error: web.log: ERROR [tomcat-http--48] com.emc.mystic.manager.commons.emc.webutil.LocaleUtil LocaleUtil.getBundleMessage:198 - No en_US resource bundle for ExpansionValidation with key <SERIAL_NUMBER> I was able to find the offending file in question at VXRAIL_COMPOSITE-7.0.202-27047874_for_4.7.x\\bundles\\vxrail-mystic-lcm-7.0.202\\vxrail-mystic-lcm-7.0.202-27047874.noarch\\usr\\lib\\vmware-marvin\\marvind\\webapps\\lcm\\WEB-INF\\lib\\lcm-module-7.0.202\\com\\vce\\lcm\\emc (that's post decompression of the vmware-marvin file inside the VIB). I find that compressing the three core files for VxRail and searching them with grep for keywords from the error messages was pretty effective for locating what I wanted. In my case the following got me what I wanted for the above error: (base) grant@DESKTOP-2SV1E9O:/mnt/c/Users/grant/Downloads/VXRAIL_COMPOSITE-7.0.202-27047874_for_4.7.x/bundles/vxrail-mystic-lcm-7.0.202/vxrail-mystic-lcm-7.0.202-27047874.noarch$ grep -inr 'resource bundle' ./* Binary file ./usr/lib/vmware-marvin/marvind/webapps/lcm/WEB-INF/lib/lcm-module-7.0.202/com/vce/lcm/emc/LocaleUtil.class matches I used the program JD Project to decompile the Java bytecode in question and it worked quite well. I was able to search the decompiled bytecode normally and find the offending function: which then allowed me to determine that this error message was actually erronious and just a product of VxRail trying and failing to localize an error message related to disk validation.","title":"Inspecting Java Class Files"},{"location":"VMWare/VxRail%20Architecture%20and%20Troubleshooting/#upgrades","text":"","title":"Upgrades"},{"location":"VMWare/VxRail%20Architecture%20and%20Troubleshooting/#how-upgrades-work","text":"During the upgrade process the old VxRail manager will have its IP and hostname removed and be shutdown while the new one assumes the previous manager's hostname and IP. Upgrades are performed with the service account you configured when you built VxRail - not root .","title":"How Upgrades Work"},{"location":"VMWare/VxRail%20Architecture%20and%20Troubleshooting/#failed-upgrade","text":"If an upgrade fails midflight this typically leaves the cluster in a state where there are two VxRail managers. In my team's experience trying to resume the upgrade using the new VxRail manager leads to a host of issues.","title":"Failed Upgrade"},{"location":"VMWare/VxRail%20Architecture%20and%20Troubleshooting/#observed-errors","text":"Specifically, we encountered the following errors: VxRail Update ran into a problem... Please refer to KB488889 article. Failed to upload bundle: VXRAIL_COMPOSITE-SLIM-7.0.132-26894200_for_4.7x.zip: Failure occured in executing the checker: Pre-checking the version compatibility among components. General unexpected error occurs during getting the information of firmwares.. Please refer to the KB517433 article. Error extracting upgrade bundle 7.0.132-26894200. Failed to upload bundle. Please refer to log for details. This error is covered by Dell EMC VxRail: LCM failed at Error extracting update bundle 7.0.132 We also saw: Failure occurred while running an upgrade for bundle VXRAIL_COMPOSITE-SLIM-7.0.132-26894200_for_4.7.x.zip. The error message [DependencyError] VIB DEL_bootbank_dcism_3.6.0.2249-DEL.700.0.0.15843807 requires esx-version >= 7.0.0 but the requirement cannot be satisfied within the ImageProfile. Please refer to the log file for more details.","title":"Observed Errors"},{"location":"VMWare/VxRail%20Architecture%20and%20Troubleshooting/#fix-action","text":"The most reliable way to fix a stuck upgrade is to revert to the old VxRail Manager and then run the upgrade again. You can do this by shutting down the new VxRail Manager, deleting it, and then logging into the previous VxRail Manager on the console, re-IPing it, give it it's previous hostname, and then restarting the upgrade.","title":"Fix Action"},{"location":"VMWare/VxRail%20Architecture%20and%20Troubleshooting/#understanding-upgrades-in-esxi","text":"See What is the altbookbank Partition","title":"Understanding Upgrades in ESXi"},{"location":"VMWare/VxRail%20Architecture%20and%20Troubleshooting/#understanding-the-vxrail-update-bundle","text":"At the top level the upgrade is governed and controlled by the file node-upgrade.py in the root of the upgrade bundle: The bundles directory has all the possible files that could be installed with the update. The exact files which are installed vary by system. More on that in the section Manifest File and Update Order .","title":"Understanding the VxRail Update Bundle"},{"location":"VMWare/VxRail%20Architecture%20and%20Troubleshooting/#manifest-file-and-update-order","text":"The manifest file defines which VIBs will be installed based on system parameters and in what order they will be installed in. For example: ESXi: <Package InstallOrder=\"100\"> <ComponentType>ESXi</ComponentType> <DisplayName>ESXi</DisplayName> <Version>7.0.2</Version> <Build>17867351</Build> <File>bundles/ESXi-7.0.2-ep1_17867351-34777dce-standard.zip</File> <Size>393239990</Size> <HighestFormatVersionSupported>11</HighestFormatVersionSupported> <UpgradeTime>5</UpgradeTime> <RebootFlag>True</RebootFlag> <Package InstallOrder=\"101\"> <ComponentType>ESXi_No_Tools</ComponentType> <DisplayName>VMware ESXi No Tools</DisplayName> <Version>7.0.2</Version> <Build>17867351</Build> <File>bundles/ESXi-7.0.2-ep1_17867351-34777dce-upgrade-no-tools.zip</File> <Size>207115430</Size> <HighestFormatVersionSupported>11</HighestFormatVersionSupported> <UpgradeTime>5</UpgradeTime> <RebootFlag>True</RebootFlag> </Package> <Package InstallOrder=\"1001\"> <ComponentType>ESXi_VIB</ComponentType> <DisplayName>Dell iSM for vSphere 7</DisplayName> <Version>3.6.0.2249</Version> <Build>DEL.700.0.0.15843807</Build> <SystemName>dcism</SystemName> This is parsed by the function _parse_esxi_patch, _parse_install_vib, and _parse_update_firmware in node-upgrade.py on line 1381, 1454, and 1492: def _parse_esxi_patch(self, element): task = { 'type': \"esxi_patch\", 'name': \"Install ESXi VMware patch\", 'async': False, 'visible': True, } task['install_order'] = int(element.get('InstallOrder')) task['args'] = self._extract_args(element, [ 'ComponentType', 'DisplayName', 'Version', 'Build', 'File', 'Size', 'HighestFormatVersionSupported', ]) return task ---SNIP--- def _parse_install_vib(self, element): task = { 'type': \"install_vib\", 'async': False, 'visible': True, 'package_type': 'vib', } task['name'] = \"Install %s\" % element.find('DisplayName').text task['install_order'] = int(element.get('InstallOrder')) task['args'] = self._extract_args(element, [ 'ComponentType', 'DisplayName', 'Version', 'Build', 'SystemName', 'File', 'Size', 'ReplaceTargetInfo/ReplaceTarget/SystemName', ]) component_type = task['args'].get('ComponentType', '') display_name = task['args'].get('DisplayName', '') if equals_ignore_case(display_name, 'VxRail VIB') or \\ equals_ignore_case(component_type, 'VXRAIL_'): task['args']['SystemName'] = \"vmware-marvin\" pkg_file = task['args'].get('File', '') file_path = os.path.join(self._bundle_dir, pkg_file) vlcm_bundle_info = self._vlcm_bundle_info(file_path) if vlcm_bundle_info: task['package_type'] = 'component' task['component_name'] = vlcm_bundle_info[0] task['component_version'] = vlcm_bundle_info[1] return task def _parse_update_firmware(self, element): task = { 'type': \"update_firmware\", 'async': True, 'visible': True, 'runtime_check': False, } task['name'] = \"Update %s\" % element.find('DisplayName').text task['install_order'] = int(element.get('InstallOrder')) task['args'] = self._extract_args(element, [ 'ComponentType', 'DisplayName', 'Version', 'Build', 'File', 'Size', ]) nic_models = self._extract_target_models(element, 'TargetNicModelInfo') component_models = self._extract_target_models( element, 'TargetComponentModelInfo') fw_models = nic_models + component_models if fw_models: task['args']['FirmwareModels'] = fw_models else: task['args']['FirmwareModels'] = None return task These functions are used to create tasks which are stored in the required_tasks variable: After all tasks are added to required tasks they are sorted with a lambda function on line 1898: required_tasks.sort(key=lambda t: t['install_order']) Subsequently it is safe to assume a linear sort on the integer value. We can use this to diagnose any problems we encounter with install order.","title":"Manifest File and Update Order"},{"location":"VMWare/VxRail%20Architecture%20and%20Troubleshooting/#troubleshooting-docker","text":"","title":"Troubleshooting Docker"},{"location":"VMWare/VxRail%20Architecture%20and%20Troubleshooting/#restarting-a-container","text":"You can add and remove a container with docker service scale func_lockbox=0 && docker service scale func_lockbox=1","title":"Restarting a Container"},{"location":"VMWare/VxRail%20Architecture%20and%20Troubleshooting/#checking-swarm-status","text":"Use docker info to get the swarm info and docker node ls to list the nodes vcluster202-vxm:~ # docker info Client: Context: default Debug Mode: false Server: Containers: 110 Running: 53 Paused: 0 Stopped: 57 Images: 43 Server Version: 20.10.9-ce Storage Driver: overlay2 Backing Filesystem: extfs Supports d_type: true Native Overlay Diff: true userxattr: false Logging Driver: json-file Cgroup Driver: cgroupfs Cgroup Version: 1 Plugins: Volume: local Network: bridge host ipvlan macvlan null overlay Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog Swarm: active NodeID: gvqywccvm7g1qcbhud1kej1y5 Is Manager: true ClusterID: 3uj8bybr66y3edkgp4txgmhse Managers: 1 Nodes: 1 Default Address Pool: 169.254.170.0/24 SubnetSize: 24 Data Path Port: 4789 Orchestration: Task History Retention Limit: 5 Raft: Snapshot Interval: 10000 Number of Old Snapshots to Retain: 0 Heartbeat Tick: 1 Election Tick: 10 Dispatcher: Heartbeat Period: 10 years CA Configuration: Expiry Duration: 10 years Force Rotate: 1 Autolock Managers: false Root Rotation In Progress: false Node Address: 127.0.0.1 Manager Addresses: 127.0.0.1:2377 Runtimes: io.containerd.runc.v2 io.containerd.runtime.v1.linux oci runc Default Runtime: runc Init Binary: docker-init containerd version: 5b46e404f6b9f661a205e28d59c982d3634148f8 runc version: init version: Security Options: apparmor seccomp Profile: default Kernel Version: 5.3.18-24.96-default Operating System: SUSE Linux Enterprise Server 15 SP2 OSType: linux Architecture: x86_64 CPUs: 4 Total Memory: 11.69GiB Name: vcluster202-vxm ID: XFEN:HBJA:CCKR:7CGG:QKXR:DDQP:ZEB3:YPOW:NAUC:AASS:3WM7:3A2F Docker Root Dir: /var/lib/docker Debug Mode: false Registry: https://index.docker.io/v1/ Labels: Experimental: false Insecure Registries: 127.0.0.0/8 Live Restore Enabled: false WARNING: No swap limit support vcluster202-vxm:~ # docker node ls ID HOSTNAME STATUS AVAILABILITY MANAGER STATUS ENGINE VERSION gvqywccvm7g1qcbhud1kej1y5 * vcluster202-vxm Ready Active Leader 20.10.9-ce","title":"Checking Swarm Status"},{"location":"VMWare/VxRail%20Architecture%20and%20Troubleshooting/#check-for-a-list-of-services","text":"Each container on VxRail runs as a separate service. You can check their status with docker service ls . vcluster202-vxm:~ # docker service ls ID NAME MODE REPLICAS IMAGE PORTS r4mawp339y88 func_alertmanager replicated 1/1 infra/prom/alertmanager:1.8.9 iduap6ujbho1 func_api-gateway replicated 1/1 infra/nginx_gateway_vxrail:1.8.14 5e18xm020k0f func_auth-service replicated 1/1 microservice/auth-service:1.8.13 j4pg14v6ajts func_cacheservice replicated 1/1 infra/redis:1.8.11 5e1q0w6v2exg func_cms-service replicated 1/1 microservice/cms-service:1.8.31 i4xpa8ltrla0 func_configservice replicated 1/1 infra/infra-config-service:1.8.10 uma1pjrkwjcq func_do-cluster replicated 1/1 do-main/do-cluster:1.8.26 uprraqe4lxo4 func_do-ecosystem replicated 1/1 do-main/do-ecosystem:1.8.26 0bpnsjy617yx func_do-eservices replicated 1/1 do-main/do-eservices:1.8.26 uaza36n6v9hl func_do-host replicated 1/1 do-main/do-host:1.8.26 jby3wca4pgxh func_do-kgs replicated 1/1 do-main/do-kgs:1.8.26 7nh07m0qnn10 func_do-network replicated 1/1 do-main/do-network:1.8.26 ta846rozl38e func_do-serviceability replicated 1/1 do-main/do-serviceability:1.8.26 yczljgv2hfki func_do-storage replicated 1/1 do-main/do-storage:1.8.26 tizxv31eqr6z func_do-vm replicated 1/1 do-main/do-vm:1.8.26 pnwq69rtx8b9 func_do-vxrail-system replicated 1/1 do-main/do-vxrail-system:1.8.26 pescqbk5gdm6 func_event-history-service replicated 1/1 microservice/event-history-service:1.8.18 uvj4grv92xjx func_event-service replicated 1/1 microservice/event-service:1.8.18 i1ajjwa20o2h func_faas-swarm replicated 1/1 infra/openfaas/faas-swarm:1.8.9 xdnl8mbbfnhw func_gateway replicated 1/1 infra/openfaas/gateway:1.8.9 w7yz4wdlurgx func_hsm replicated 1/1 microservice/hsm:1.8.25 txlipzyfxe5k func_infra-scale-service replicated 1/1 infra/infra-scale-service:1.8.8 hrzxht5zp33u func_kgs-service replicated 1/1 microservice/kgs-service:1.8.9 iweqxas2urpn func_lockbox replicated 1/1 microservice/lockbox:1.8.10 q75sa2hvxc0z func_lockservice replicated 1/1 infra/infra-lock-service:1.8.8 zd10d8q5vmml func_logging replicated 1/1 infra/logging:1.8.9 nob97ajl4wjl func_ms-day1-bringup replicated 1/1 microservice/ms-day1-bringup:1.8.21 339zceysq1w7 func_ms-day2 replicated 1/1 microservice/ms-day2:1.8.40 vwofhpygnrvz func_nano-service replicated 1/1 microservice/nano-service:1.8.21 ekm0s9o6fse3 func_nats replicated 1/1 infra/nats-streaming:1.8.9 o9ft5wqvkgyq func_node-service replicated 1/1 microservice/node-service:1.8.31 q9ewku1t3xjy func_prometheus replicated 1/1 infra/prom/prometheus:1.8.9 om3gislgsh82 func_property-collector replicated 1/1 do-main/property-collector:1.8.26 pkhsz674xtde func_queue-worker replicated 1/1 infra/openfaas/queue-worker:1.8.9 anmzrcv2plzy func_rcs-service replicated 1/1 microservice/rcs-service:1.8.11 t16j0wln8w0s func_serviceregistry replicated 1/1 infra/gcr.io/etcd-development/etcd:1.8.9 rny34tgzu7jq func_storage-service replicated 1/1 microservice/storage-service:1.8.16 sbc1n8rr25n3 func_tracing replicated 1/1 infra/tracing:1.8.9 vdf4hdtl957a func_vlcm replicated 1/1 microservice/vlcm:1.8.23 dkfjjajjx8tj func_vxdoctor-service replicated 1/1 microservice/vxdoctor-service:1.8.18 rsr9plg95qhw func_wfservice replicated 1/1 infra/workflow_engine:1.8.10 You can check the details of a specific service with docker service inspect --pretty <SERVICE> vcluster202-vxm:~ # docker service inspect --pretty func_do-storage ID: yczljgv2hfkiou1zh6liq6tbr Name: func_do-storage Labels: com.docker.stack.image=do-main/do-storage:1.8.26 com.docker.stack.namespace=func Service Mode: Replicated Replicas: 1 UpdateStatus: State: paused Started: 6 months ago Message: update paused due to failure or early termination of task qgjut6lu088orf8dmi8r3xxt8 Placement: UpdateConfig: Parallelism: 1 On failure: pause Monitoring Period: 5s Max failure ratio: 0 Update order: stop-first RollbackConfig: Parallelism: 1 On failure: pause Monitoring Period: 5s Max failure ratio: 0 Rollback order: stop-first ContainerSpec: Image: do-main/do-storage:1.8.26 Env: API_RESOURCES=[\"v1/storage:do-storage/v1/storage\"] REGISTERED=true SERVICE_LAYER=do SERVICE_NAME=do-storage SERVICE_PORT=5000 SERVICE_TYPE=micro TRACING_HOST=tracing TRACING_SERVICE_NAME=do-storage User: app:475 Mounts: Target: /vxm_cert Source: /etc/vmware-marvin/ssl ReadOnly: false Type: bind Target: /certificate Source: /var/lib/vmware-marvin/trust ReadOnly: false Type: bind Target: /service_data Source: /var/lib/vxrail/do_storage/data ReadOnly: false Type: bind Resources: Limits: Memory: 1GiB Networks: func_functions Endpoint Mode: vip Healthcheck: Interval = 30s Retries = 10 StartPeriod = 1m0s Timeout = 1m0s Tests: Test = CMD-SHELL Test = curl -f http://localhost:5000/do-storage/v1/storage/health-check || exit 1","title":"Check for a List of Services"},{"location":"VMWare/VxRail%20Architecture%20and%20Troubleshooting/#adding-nodes","text":"Version Compatibility Matrix: Link Make sure before you try to do anything with adding nodes that you verify the versions are mutually compatible.","title":"Adding Nodes"},{"location":"VMWare/VxRail%20Architecture%20and%20Troubleshooting/#troubleshooting","text":"We bumped into several errors while attempting to add nodes and they varied in type. One of the errors we encountered was when adding the node the interface for the VxRail plugin failed entirely. We were able to fix it with the following process: Take a snapshot of VxM psql -U postgres vxrail -c \"DELETE FROM system.operation_status WHERE state='IN_PROGRESS';\" enable advanced mode by changing it in /var/lib/vmware-marvin/lcm_advanced_mode.properties Change the contents of /var/lib/vmware-mariv to {\"state\":\"NONE\",\"vc_plugin_updated\":false,\"deployed_for_public_api\":false} Restart Marvin with system restart vmware-marvin In one instance we were able to clear all problems by restarting all nodes in the cluster, the VxRail manager, and vCenter. After a full reboot of everything, a series of general errors we had been receiving during validation cleared.","title":"Troubleshooting"},{"location":"VMWare/VxRail%20Architecture%20and%20Troubleshooting/#how-adding-nodes-works-during-cluster-construction","text":"All nodes after being RASR have a fully built vSAN disk group. During the initial cluster construction VxRail will select a primary node and then add it to the cluster. On every subsequent node it will delete that node's vSAN disk group and then add it to the cluster's existing disk group.","title":"How Adding Nodes Works During Cluster Construction"},{"location":"VMWare/VxRail%20Architecture%20and%20Troubleshooting/#disappearing-browse-button","text":"Take a snapshot of VxM psql -U postgres vxrail -c \"DELETE FROM system.operation_status WHERE state='IN_PROGRESS';\" enable advanced mode by change it in /var/lib/vmware-marvin/lcm_advanced_mode.properties Change the contents of /var/lib/vmware-mariv to {\"state\":\"NONE\",\"vc_plugin_updated\":false,\"deployed_for_public_api\":false} Result system restart vmware-marvin","title":"Disappearing Browse Button"},{"location":"VMWare/VxRail%20Architecture%20and%20Troubleshooting/#troubleshooting-script-15_setup_nodesh-execution-error","text":"","title":"Troubleshooting Script 15_setup_node.sh execution error"},{"location":"VMWare/VxRail%20Architecture%20and%20Troubleshooting/#node-3-password-change","text":"Somehow 192.168.0.172 changed iDRAC passwords overnight. Last I left it, it was in a reboot loop","title":"Node 3 Password Change"},{"location":"VMWare/VxRail%20Architecture%20and%20Troubleshooting/#rasr-process-fails-with-script-15_setup_nodesh-execution-error","text":"Both 192.168.0.171 and 192.168.0.172 are failed with \"Script 15_setup_node.sh execution error\". I confirmed they both had failed to generate disk mapping. This does not exactly match the kb but it does appear to be fatal.","title":"RASR process fails with \"Script 15_setup_node.sh execution error\""},{"location":"VMWare/VxRail%20Architecture%20and%20Troubleshooting/#rasr-on-1921680172","text":"I ran with no DUP install Copying these temp files is taking hours. They're only 12 GB Same problem","title":"RASR on 192.168.0.172"},{"location":"VMWare/VxRail%20Architecture%20and%20Troubleshooting/#debugging-error","text":"The error looks like https://www.dell.com/support/kbdoc/en-th/000193618?lang=en For starters my fist.log was in /scratch instead of /mnt The observed 15_setup_node.sh is in: [root@fedora media]# grep -Ro 15_setup* . grep: ./release/gmywd_is.tar: binary file matches See 15_setup_node.sh The referenced vmware-marvin seems to be a VIB The failed post is here: I can't tell if this error is related but in another error this function failes: drives is none. Whatever is populating self.getDrivesInfo(host, port) isn't working. Whatever this is in vxrail_primary.py (line 530) HostConnection is not returning something valid. If we know what that is we can debug manually on the box. Moreover, this function is called is_dell_non_13G_platform but this is a P470F - which IS a 13G platform.","title":"Debugging error"},{"location":"VMWare/VxRail%20Architecture%20and%20Troubleshooting/#log-info","text":"lcm-web.log - Shows upgrade related info web.log - combined web output","title":"Log Info"},{"location":"VMWare/VxRail%20Architecture%20and%20Troubleshooting/#fixing-half-upgrade","text":"Roll back to previous VxRail manager, then use it to complete the upgrade. Uses ESXi management account for updates Uses PSC log","title":"Fixing Half Upgrade"},{"location":"VMWare/VxRail%20Architecture%20and%20Troubleshooting/#vcenter-logs","text":"","title":"vCenter Logs"},{"location":"VMWare/VxRail%20Architecture%20and%20Troubleshooting/#altbootbank","text":"You can load the old version with shift+r","title":"Altbootbank"},{"location":"VMWare/VxRail%20Architecture%20and%20Troubleshooting/#add-nodes-nic-config-page","text":"When you hit https://<host>/ui/vxrail/rest/vxm/private/system/cluster-hosts?$$objectID=urn:vmomi:ClusterComputeResource:domain<ID>&lang=en-us you should get back the existing hosts in the cluster with their hostname, model, serial_number, and vmnics https://<HOST>/ui/vxrail/rest/vxm/private/system/available-hosts?$filter=serial_number%20in%20(SERIAL,SERIAL)&$$objectId=urn:vmomi:ClusterComputeResource:domain-<ID>&lang=en-us should get you the two hosts you are going to add","title":"Add nodes NIC config page"},{"location":"VMWare/VxRail%20Architecture%20and%20Troubleshooting/#add-node-error","text":"\"Could not find NSX-T network information\" Http failure response for https://<address>/ui/vxrail/rest/vxm/private/cluster/network/nsxt???objectId=urn:vmomi:ClusterComputeResource:domain-ID=en-US: 404 OK","title":"Add Node Error"},{"location":"VMWare/VxRail%20Architecture%20and%20Troubleshooting/#error-node","text":"Validation Errors Disk Grouping Error (JKSLH63) Error occurs when validating disk group on host JKSLH63 web.log: ERROR [tomcat-http--48] com.emc.mystic.manager.commons.emc.webutil.LocaleUtil LocaleUtil.getBundleMessage:198 - No en_US resource bundle for ExpansionValidation with key JKSLH63","title":"Error Node"},{"location":"VMWare/VxRail%20Architecture%20and%20Troubleshooting/#general-error","text":"web.log: [WARN] com.vce.commons.config.ConfigServiceImpl$NotFoundHandler ConfigServiceImpl$NotFoundHandler.handleNotFound:114 - provided key is not present: [404, {\"message\":\"404 Not Found: bandwidth_throttling_level does not exist\"}]","title":"General Error"},{"location":"VMWare/VxRail%20Architecture%20and%20Troubleshooting/#redeploying-vxrail-manager","text":"Note: My testing was done on VxRail 7.0.320 Go to Edit Settings on your existing VxRail Manager and check its existing networking. There should be two networks. Make note of what they are as you will need these settings later. They are probably something like vCenter Server Network-<uuid> and VxRail Manager-<uuid> . Make note of the existing VxRail manager's IP address. Log into the existing VxRail Manager. Run cat /var/lib/vmware-marvin/config-initial.json | jq | less . This is the current settings for VxRail Manager. I suggest you back this up and use it for your answers to the script but this is not required. You will need to backup the following files in this directory: manifest.xml vcmRuleset.xml nodeManifestfile.xml node-upgrade.py baseline.json Power down the existing VxRail Manager. Import the new VxRail Manager. The values for the import are specific to the user except the networking. When selecting the networking for the new VxRail Manager there will be two networks. Match these to what you saw in the original VxRail Manager. Before powering on your VxRail Manager you will need to change the guest operating system. Right click the VM, edit settings, vm options, general options, change the operating system to SUSE Linux 12. After you power on this should autocorrect itself to the correct OS. If you don't do this you will get an error from vCenter saying the operating system is unsupported. Login to the VxRail Manager in the web console with root / Passw0rd! Set the ip address of eth0 with ifconfig eth0 <IP> , ifconfig eth0 netmask <NETMASK> , and ip route add 0.0.0.0/0 via <your_default_gateway> If you do not already have it, contact Dell Support, ask for the script which comes with the KB Dell VxRail: Recover VxRail Manager VM from Scratch . Use credentials mystic / VxRailManager@201602! to upload the script to the IP address you established for VxRail Manager with WinSCP or other utility. Alternatively, you can copy and paste it over SSH with vim or your favorite editor. SSH to the VxRail Manager. Copy the files you backed up earlier (manifest.xml, vcmRuleset.xml, nodeManifestfile.xml, node-upgrade.py, baseline.json) to the /var/lib/vmware-marvin folder and change their owner to tcserver:pivotal . Next, you will need to enable root login for SSH. Run vim /etc/ssh/sshd_config (or your favorite text editor). Change PermitRootLogin from no to yes. Save and restart sshd with systemctl restart sshd Finally, run the uploaded script vxm_recovery_70x.py . Use the JSON you downloaded from the old VxRail Manager as a reference if you need it. When asked if the VxRail Manager networking is configured as expected say no and then re-input all networking values. Domain Search Path: This is top_level_domain in your JSON. Is the Domain Name server external: check the field is_internal_dns in your JSON. WARNING it asks you if you domain server is external. If the value of is_internal_dns is false then make sure you answer yes to this question. Domain Name Server IP address: Check the array dns_servers . If you only have one just import the one DNS server IP. If there is more than one, split each ip with a , . VxRail Manager hostname: This can be whatever you want it to be. You do not need to add the domain suffix here. NTP server IPs: see the array ntp_servers in your JSON Config root and mystic password of VxM: Select y if you haven't done this already Retrieve vRealize Log Insight Info - n unless you have vRealize Log Insight vCenter IP: See vxrail_supplied_vc_ip in your JSON (this should be the IP address of your vCenter server) vcenter FQDN: The FQDN of vCenter. Ex: vcluster202-vcsa.demo.local Admin account name: This is typically administrator@something.something Management account name: This is the VxRail manager account name. NOT root. You can find it here: . WARNING if authentication fails, you may have to add @localos depending on your VxRail version. Is PSC external: n Is customized vds created? N Can you confirm the management account info of the ESXi host: Answer yes. You can find the ESXi Management account info here: On vCenter, browse to VxRail here: Confirm that the dashboard populates. It is normal for it to have warnings if it cannot connect to the internet. If it is not working, the screen will be completely blank. Make sure if you alread had vCenter open that you refresh the page. Next, we are going to simulate a shutdown. You do not actually need to shutdown the cluster, but executing the shutdown simulation will confirm everything is working as expected. In my instance I received the following: If you see this, it is more than likely a cert problem. You can confirm this by checking the /var/log/microservice_log/short.term.log on the VxRail Manager. If this is indeed the problem you should see: 2022-10-20-04:18:26 microservice.do-cluster \"[2022-10-20 04:18:26,812: ERROR/MainProcess] Failed to login by session cookie!\" 2022-10-20-04:18:26 microservice.do-cluster \"Traceback (most recent call last):\" 2022-10-20-04:18:26 microservice.do-cluster \" File \"\"/home/app/worker/task/vcenter.py\"\", line 414, in login_by_session_cookie\" 2022-10-20-04:18:26 microservice.do-cluster \" username = si.content.sessionManager.currentSession.userName\" 2022-10-20-04:18:26 microservice.do-cluster \" File \"\"/usr/local/venv/lib64/python3.6/site-packages/pyVmomi/VmomiSupport.py\"\", line 700, in __call__\" 2022-10-20-04:18:26 microservice.do-cluster \" return self.f(*args, **kwargs)\" 2022-10-20-04:18:26 microservice.do-cluster \" File \"\"/usr/local/venv/lib64/python3.6/site-packages/pyVmomi/VmomiSupport.py\"\", line 520, in _InvokeAccessor\" 2022-10-20-04:18:26 microservice.do-cluster \" return self._stub.InvokeAccessor(self, info)\" 2022-10-20-04:18:26 microservice.do-cluster \" File \"\"/usr/local/venv/lib64/python3.6/site-packages/pyVmomi/StubAdapterAccessorImpl.py\"\", line 40, in InvokeAccessor\" 2022-10-20-04:18:26 microservice.do-cluster \" self._pc = si.RetrieveContent().propertyCollector\" 2022-10-20-04:18:26 microservice.do-cluster \" File \"\"/usr/local/venv/lib64/python3.6/site-packages/pyVmomi/VmomiSupport.py\"\", line 706, in <lambda>\" 2022-10-20-04:18:26 microservice.do-cluster \" self.f(*(self.args + (obj,) + args), **kwargs)\" 2022-10-20-04:18:26 microservice.do-cluster \" File \"\"/usr/local/venv/lib64/python3.6/site-packages/pyVmomi/VmomiSupport.py\"\", line 512, in _InvokeMethod\" 2022-10-20-04:18:26 microservice.do-cluster \" return self._stub.InvokeMethod(self, info, args)\" 2022-10-20-04:18:26 microservice.do-cluster \" File \"\"/usr/local/venv/lib64/python3.6/site-packages/pyVmomi/SoapAdapter.py\"\", line 1350, in InvokeMethod\" 2022-10-20-04:18:26 microservice.do-cluster \" conn.request('POST', self.path, req, headers)\" 2022-10-20-04:18:26 microservice.do-cluster \" File \"\"/usr/lib64/python3.6/http/client.py\"\", line 1291, in request\" 2022-10-20-04:18:26 microservice.do-cluster \" self._send_request(method, url, body, headers, encode_chunked)\" 2022-10-20-04:18:26 microservice.do-cluster \" File \"\"/usr/lib64/python3.6/http/client.py\"\", line 1337, in _send_request\" 2022-10-20-04:18:26 microservice.do-cluster \" self.endheaders(body, encode_chunked=encode_chunked)\" 2022-10-20-04:18:26 microservice.do-cluster \" File \"\"/usr/lib64/python3.6/http/client.py\"\", line 1286, in endheaders\" 2022-10-20-04:18:26 microservice.do-cluster \" self._send_output(message_body, encode_chunked=encode_chunked)\" 2022-10-20-04:18:26 microservice.do-cluster \" File \"\"/usr/lib64/python3.6/http/client.py\"\", line 1046, in _send_output\" 2022-10-20-04:18:26 microservice.do-cluster \" self.send(msg)\" 2022-10-20-04:18:26 microservice.do-cluster \" File \"\"/usr/lib64/python3.6/http/client.py\"\", line 984, in send\" 2022-10-20-04:18:26 microservice.do-cluster \" self.connect()\" 2022-10-20-04:18:26 microservice.do-cluster \" File \"\"/usr/local/venv/lib64/python3.6/site-packages/pyVmomi/SoapAdapter.py\"\", line 1039, in connect\" 2022-10-20-04:18:26 microservice.do-cluster \" http_client.HTTPSConnection.connect(self)\" 2022-10-20-04:18:26 microservice.do-cluster \" File \"\"/usr/lib64/python3.6/http/client.py\"\", line 1452, in connect\" 2022-10-20-04:18:26 microservice.do-cluster \" server_hostname=server_hostname)\" 2022-10-20-04:18:26 microservice.do-cluster \" File \"\"/usr/local/venv/lib64/python3.6/site-packages/gevent/_ssl3.py\"\", line 120, in wrap_socket\" 2022-10-20-04:18:26 microservice.do-cluster \" _session=session)\" 2022-10-20-04:18:26 microservice.do-cluster \" File \"\"/usr/local/venv/lib64/python3.6/site-packages/gevent/_ssl3.py\"\", line 308, in __init__\" 2022-10-20-04:18:26 microservice.do-cluster \" self.do_handshake()\" 2022-10-20-04:18:26 microservice.do-cluster \" File \"\"/usr/local/venv/lib64/python3.6/site-packages/gevent/_ssl3.py\"\", line 667, in do_handshake\" 2022-10-20-04:18:26 microservice.do-cluster \" self._sslobj.do_handshake()\" 2022-10-20-04:18:26 microservice.do-cluster \" File \"\"/usr/lib64/python3.6/ssl.py\"\", line 694, in do_handshake\" 2022-10-20-04:18:26 microservice.do-cluster \" match_hostname(self.getpeercert(), self.server_hostname)\" 2022-10-20-04:18:26 microservice.do-cluster \" File \"\"/usr/lib64/python3.6/ssl.py\"\", line 331, in match_hostname\" 2022-10-20-04:18:26 microservice.do-cluster \" % (hostname, dnsnames[0]))\" 2022-10-20-04:18:26 microservice.do-cluster \"ssl.CertificateError: hostname '192.168.1.20' doesn't match 'vcluster202-vcsa.demo.local'\" 2022-10-20-04:18:26 microservice.do-cluster \"[2022-10-20 04:18:26,816: INFO/MainProcess] Task worker.celery_task.rest_cluster.login_by_session_cookie[90e54bf5-a9d3-4d28-a76e-70fcbbe062cb] succeeded in 0.03339903799860622s: {'result': 'LOGIN_BY_SESSION_C OOKIE_FAIL'}\" On the Vxrail manager, confirm that in /etc/hosts you have an entry for your vCenter. If not, add it. On vCenter as root run mkdir -p /var/tmp/vmware && chmod 755 /var/tmp/vmware Next run /usr/lib/vmware-vmca/bin/certificate-manager . Select option 8 Do you wish to generate all certs using configuration file: n Enter your credentials You can leave everything to default until you get to Hostname. Here I used the FQDN for VCSA. For VMCA I used the shortname of VCSA When asked if you want to continue select yes. The script will run. This takes a while to run. For me it stopped at 85% for a long time. This appears to be normal. Grab cert_util.py and upload it to VxRail Manager. Run the script. If it fails saying No host configured run curl -k -H \"Content-Type: application/json\" -X PUT --unix-socket /var/lib/vxrail/nginx/socket/nginx.sock -d '{\"value\": \"false\"}' http://127.0.0.1/rest/vxm/internal/configservice/v1/configuration/keys/certificate_verification_enable . This will disable cert checking for it. After you run that command, rerun the script and it should complete successfully.","title":"Redeploying VxRail Manager"},{"location":"VMWare/VxRail%20Architecture%20and%20Troubleshooting/#logs","text":"","title":"Logs"},{"location":"VMWare/VxRail%20Architecture%20and%20Troubleshooting/#generating-logs","text":"If you want to get a full dump of all the logs associated with VxRail you can generate a dump by going to the VxRail plugin on vCenter, navigating to the support tab, and then clicking the generate log button.","title":"Generating Logs"},{"location":"VMWare/VxRail%20Architecture%20and%20Troubleshooting/#vxrail-microservice-container-logs","text":"/var/log/microservice/vxrail-manager","title":"VxRail Microservice (container) Logs"},{"location":"VMWare/VxRail%20Architecture%20and%20Troubleshooting/#rasr-process","text":"The totality of the RASR process' output is stored in the images folder on the local datastore in fist.log.","title":"RASR Process"},{"location":"VMWare/VxRail%20Architecture%20and%20Troubleshooting/#tomcat-logs","text":"/var/log/marvin/tomcat/logs /var/log/mystic/web.log","title":"Tomcat Logs"},{"location":"VMWare/VxRail%20Architecture%20and%20Troubleshooting/#upgrade-logs","text":"The LCM process handles upgrades. It's logs are all in /var/log/mystic/lcm*. You can also try checking the PSC log (haven't personally done this)","title":"Upgrade Logs"},{"location":"VMWare/VxRail%20Architecture%20and%20Troubleshooting/#api-logs","text":"These are on vCenter. See VMWARE API Logs","title":"API Logs"},{"location":"VMWare/VxRail%20Architecture%20and%20Troubleshooting/#helpful-commands","text":"See Helpful Commands","title":"Helpful Commands"},{"location":"VMWare/VxRail%20Architecture%20and%20Troubleshooting/#helpful-kb-articles","text":"https://www.dell.com/support/kbdoc/en-uk/000021742/dell-emc-vxrail-troubleshooting-guide-for-vxrail-micro-services-infrastructure https://www.dell.com/support/kbdoc/en-uk/000157667/dell-emc-vxrail-how-to-query-vxrail-cluster-configuration-data-in-7-0-010-or-higher https://www.dell.com/support/kbdoc/en-us/000157715/dell-emc-vxrail-troubleshooting-guide-for-upgrading-vxrail-manager-to-7-0-010-or-higher https://www.dell.com/support/kbdoc/en-us/000157662/dell-emc-vxrail-how-to-get-or-update-management-account-in-vxrail-7-0-010-or-higher https://www.dell.com/support/kbdoc/en-us/000181759/dell-emc-vxrail-upgrading-vxrail-manager-to-7-0-x-release-failed-vcenter-plugin-no-loading https://www.dell.com/support/kbdoc/en-us/000181712/dell-emc-vxrail-how-to-update-vxrail-manager-without-lcm","title":"Helpful KB Articles"},{"location":"VMWare/VxRail%20Architecture%20and%20Troubleshooting/#vxrail-api-info","text":"See VMWare APIs","title":"VxRail API Info"},{"location":"VMWare/vRealize%20Setup%20%28Incomplete%29/","text":"Setting Up vRealize Setting Credentials I had to get on the Linux command line to set the credentials. I did the following: When you hit the grub menu, add init=/bin/bash to drop to a command line. Once you get to the command line run sudo passwd root and set that password. Run sudo passwd admin to reset the local admin account. Note: This is not the admin account you need to get to the web frontend. Run $VMWARE_PYTHON_BIN $VCOPS_BASE/../vmware-vcopssuite/utilities/sliceConfiguration/bin/vcopsSetAdminPassword.py --reset to reset the admin account for the web GUI. Note: Despite what the documentation says, the username is not admin@local, it is admin. There are also password complexity requirements.","title":"Setting Up vRealize"},{"location":"VMWare/vRealize%20Setup%20%28Incomplete%29/#setting-up-vrealize","text":"","title":"Setting Up vRealize"},{"location":"VMWare/vRealize%20Setup%20%28Incomplete%29/#setting-credentials","text":"I had to get on the Linux command line to set the credentials. I did the following: When you hit the grub menu, add init=/bin/bash to drop to a command line. Once you get to the command line run sudo passwd root and set that password. Run sudo passwd admin to reset the local admin account. Note: This is not the admin account you need to get to the web frontend. Run $VMWARE_PYTHON_BIN $VCOPS_BASE/../vmware-vcopssuite/utilities/sliceConfiguration/bin/vcopsSetAdminPassword.py --reset to reset the admin account for the web GUI. Note: Despite what the documentation says, the username is not admin@local, it is admin. There are also password complexity requirements.","title":"Setting Credentials"},{"location":"Web%20Traffic%20Generator/","text":"Web Traffic Generator See: https://github.com/grantcurell/generatewebtraffic","title":"Web Traffic Generator"},{"location":"Web%20Traffic%20Generator/#web-traffic-generator","text":"See: https://github.com/grantcurell/generatewebtraffic","title":"Web Traffic Generator"},{"location":"Writing%20udev%20Rules%20for%20Dell%20PERC%20H755/","text":"Writing udev Rules for Dell PERC H755 Writing udev Rules for Dell PERC H755 Operating System Test 1 /etc/udev/rules.d/99-abj.nr.rules udevadm test --action=\"add\" /sys/block/sdc with custom rules Test Without Custom Rules perccli64 /c0 show /opt/MegaRAID/perccli/perccli64 /c1 show lsblk Test 2 99-abj.nr.rules /opt/MegaRAID/perccli/perccli64 /c1 show udevadm test --action=\"add\" /sys/block/sda nr_requests value Post Reboot Operating System [root@r7525 ~]# cat /etc/*-release NAME=\"Red Hat Enterprise Linux\" VERSION=\"8.6 (Ootpa)\" ID=\"rhel\" ID_LIKE=\"fedora\" VERSION_ID=\"8.6\" PLATFORM_ID=\"platform:el8\" PRETTY_NAME=\"Red Hat Enterprise Linux 8.6 (Ootpa)\" ANSI_COLOR=\"0;31\" CPE_NAME=\"cpe:/o:redhat:enterprise_linux:8::baseos\" HOME_URL=\"https://www.redhat.com/\" DOCUMENTATION_URL=\"https://access.redhat.com/documentation/red_hat_enterprise_linux/8/\" BUG_REPORT_URL=\"https://bugzilla.redhat.com/\" REDHAT_BUGZILLA_PRODUCT=\"Red Hat Enterprise Linux 8\" REDHAT_BUGZILLA_PRODUCT_VERSION=8.6 REDHAT_SUPPORT_PRODUCT=\"Red Hat Enterprise Linux\" REDHAT_SUPPORT_PRODUCT_VERSION=\"8.6\" Red Hat Enterprise Linux release 8.6 (Ootpa) Red Hat Enterprise Linux release 8.6 (Ootpa) Test 1 /etc/udev/rules.d/99-abj.nr.rules KERNEL==\"sd*\",ACTION==\"add|change\",ATTRS{model}==\"PERC_H755N_Front\",\\ ATTR{queue/nomerges}=\"2\",\\ ATTR{queue/nr_requests}=\"1023\",\\ ATTR{queue/rotational}=\"0\",\\ ATTR{queue/rq_affinity}=\"2\",\\ ATTR{queue/scheduler}=\"none\",\\ ATTR{queue/add_random}=\"0\",ATTR{queue/max_sectors_kb}=\"4096\" KERNEL==\"sd*\",ACTION==\"add|change\",ATTRS{model}==\"Dell Ent NVMe v2\",\\ ATTR{queue/nomerges}=\"2\",\\ ATTR{queue/nr_requests}=\"1023\",\\ ATTR{queue/rotational}=\"0\",\\ ATTR{queue/rq_affinity}=\"2\",\\ ATTR{queue/scheduler}=\"none\",\\ ATTR{queue/add_random}=\"0\",ATTR{queue/max_sectors_kb}=\"4096\" SUBSYSTEM==\"block\",ACTION==\"add|change\",KERNEL==\"nvme*[0-9]n*[0-9]\",ATTRS{model}==\"Dell Ent NVMe v2 AGN RI U.2 1.92TB\",\\ ATTR{queue/nomerges}=\"2\",\\ ATTR{queue/nr_requests}=\"1023\",\\ ATTR{queue/rotational}=\"0\",\\ ATTR{queue/rq_affinity}=\"2\",\\ ATTR{queue/scheduler}=\"none\",\\ ATTR{queue/add_random}=\"0\",\\ ATTR{queue/max_sectors_kb}=\"4096\" SUBSYSTEM==\"block\",ACTION==\"add|change\",KERNEL==\"md*\",\\ ATTR{md/sync_speed_max}=\"2000000\",\\ ATTR{md/group_thread_cnt}=\"64\",\\ ATTR{md/stripe_cache_size}=\"8192\",\\ ATTR{queue/nomerges}=\"2\",\\ ATTR{queue/nr_requests}=\"1023\",\\ ATTR{queue/rotational}=\"0\",\\ ATTR{queue/rq_affinity}=\"2\",\\ ATTR{queue/scheduler}=\"none\",\\ ATTR{queue/add_random}=\"0\", ATTR{queue/max_sectors_kb}=\"4096\" udevadm test --action=\"add\" /sys/block/sdc with custom rules [root@r7525 rules.d]# udevadm test --action=\"add\" /sys/block/sdc calling: test version 239 (239-58.el8) This program is for debugging only, it does not run any program specified by a RUN key. It may show incorrect results, because some values may be different, or not available at a simulation run. Load module index Parsed configuration file /usr/lib/systemd/network/99-default.link Created link configuration context. ...SNIP... Reading rules file: /etc/udev/rules.d/99-abj.nr.rules Reading rules file: /usr/lib/udev/rules.d/99-qemu-guest-agent.rules Reading rules file: /usr/lib/udev/rules.d/99-systemd.rules Reading rules file: /usr/lib/udev/rules.d/99-vmware-scsi-udev.rules rules contain 393216 bytes tokens (32768 * 12 bytes), 48613 bytes strings 47970 strings (396524 bytes), 43723 de-duplicated (352159 bytes), 4248 trie nodes used GROUP 6 /usr/lib/udev/rules.d/50-udev-default.rules:59 IMPORT 'scsi_id --export --whitelisted -d /dev/sdc' /usr/lib/udev/rules.d/60-persistent-storage.rules:50 starting 'scsi_id --export --whitelisted -d /dev/sdc' 'scsi_id --export --whitelisted -d /dev/sdc'(out) 'ID_SCSI=1' 'scsi_id --export --whitelisted -d /dev/sdc'(out) 'ID_VENDOR=NVMe' 'scsi_id --export --whitelisted -d /dev/sdc'(out) 'ID_VENDOR_ENC=NVMe\\x20\\x20\\x20\\x20' 'scsi_id --export --whitelisted -d /dev/sdc'(out) 'ID_MODEL=Dell_Ent_NVMe_v2' 'scsi_id --export --whitelisted -d /dev/sdc'(out) 'ID_MODEL_ENC=Dell\\x20Ent\\x20NVMe\\x20v2' 'scsi_id --export --whitelisted -d /dev/sdc'(out) 'ID_REVISION=.2.0' 'scsi_id --export --whitelisted -d /dev/sdc'(out) 'ID_TYPE=disk' 'scsi_id --export --whitelisted -d /dev/sdc'(out) 'ID_SERIAL=236435330529024150025384100000002' 'scsi_id --export --whitelisted -d /dev/sdc'(out) 'ID_SERIAL_SHORT=36435330529024150025384100000002' 'scsi_id --export --whitelisted -d /dev/sdc'(out) 'ID_SCSI_SERIAL=S6CSNA0R902415 ' Process 'scsi_id --export --whitelisted -d /dev/sdc' succeeded. LINK 'disk/by-id/scsi-236435330529024150025384100000002' /usr/lib/udev/rules.d/60-persistent-storage.rules:52 IMPORT builtin 'path_id' /usr/lib/udev/rules.d/60-persistent-storage.rules:73 LINK 'disk/by-path/pci-0000:01:00.0-scsi-0:2:8:0' /usr/lib/udev/rules.d/60-persistent-storage.rules:75 IMPORT builtin 'blkid' /usr/lib/udev/rules.d/60-persistent-storage.rules:90 probe /dev/sdc raid offset=0 IMPORT '/usr/bin/sg_inq --export --inhex=/sys/block/sdc/device/inquiry --raw' /usr/lib/udev/rules.d/61-scsi-sg3_id.rules:17 starting '/usr/bin/sg_inq --export --inhex=/sys/block/sdc/device/inquiry --raw' '/usr/bin/sg_inq --export --inhex=/sys/block/sdc/device/inquiry --raw'(out) 'SCSI_TPGS=0' '/usr/bin/sg_inq --export --inhex=/sys/block/sdc/device/inquiry --raw'(out) 'SCSI_TYPE=disk' '/usr/bin/sg_inq --export --inhex=/sys/block/sdc/device/inquiry --raw'(out) 'SCSI_VENDOR=NVMe' '/usr/bin/sg_inq --export --inhex=/sys/block/sdc/device/inquiry --raw'(out) 'SCSI_VENDOR_ENC=NVMe\\x20\\x20\\x20\\x20' '/usr/bin/sg_inq --export --inhex=/sys/block/sdc/device/inquiry --raw'(out) 'SCSI_MODEL=Dell_Ent_NVMe_v2' '/usr/bin/sg_inq --export --inhex=/sys/block/sdc/device/inquiry --raw'(out) 'SCSI_MODEL_ENC=Dell\\x20Ent\\x20NVMe\\x20v2' '/usr/bin/sg_inq --export --inhex=/sys/block/sdc/device/inquiry --raw'(out) 'SCSI_REVISION=.2.0' Process '/usr/bin/sg_inq --export --inhex=/sys/block/sdc/device/inquiry --raw' succeeded. IMPORT '/usr/bin/sg_inq --export --inhex=/sys/block/sdc/device/vpd_pg80 --raw' /usr/lib/udev/rules.d/61-scsi-sg3_id.rules:27 starting '/usr/bin/sg_inq --export --inhex=/sys/block/sdc/device/vpd_pg80 --raw' '/usr/bin/sg_inq --export --inhex=/sys/block/sdc/device/vpd_pg80 --raw'(out) 'SCSI_IDENT_SERIAL=S6CSNA0R902415' Process '/usr/bin/sg_inq --export --inhex=/sys/block/sdc/device/vpd_pg80 --raw' succeeded. IMPORT '/usr/bin/sg_inq --export --inhex=/sys/block/sdc/device/vpd_pg83 --raw' /usr/lib/udev/rules.d/61-scsi-sg3_id.rules:30 starting '/usr/bin/sg_inq --export --inhex=/sys/block/sdc/device/vpd_pg83 --raw' '/usr/bin/sg_inq --export --inhex=/sys/block/sdc/device/vpd_pg83 --raw'(out) 'SCSI_IDENT_LUN_EUI64=36435330529024150025384100000002' '/usr/bin/sg_inq --export --inhex=/sys/block/sdc/device/vpd_pg83 --raw'(out) 'SCSI_IDENT_PORT_NAA_LOCAL=3bd790c2eff1ebd9' Process '/usr/bin/sg_inq --export --inhex=/sys/block/sdc/device/vpd_pg83 --raw' succeeded. IMPORT 'fc_wwpn_id /devices/pci0000:00/0000:00:01.1/0000:01:00.0/host0/target0:2:8/0:2:8:0/block/sdc' /usr/lib/udev/rules.d/63-fc-wwpn-id.rules:8 starting 'fc_wwpn_id /devices/pci0000:00/0000:00:01.1/0000:01:00.0/host0/target0:2:8/0:2:8:0/block/sdc' Process 'fc_wwpn_id /devices/pci0000:00/0000:00:01.1/0000:01:00.0/host0/target0:2:8/0:2:8:0/block/sdc' succeeded. LINK 'disk/by-id/scsi-SNVMe_Dell_Ent_NVMe_v2_S6CSNA0R902415' /usr/lib/udev/rules.d/63-scsi-sg3_symlink.rules:12 LINK 'disk/by-id/scsi-236435330529024150025384100000002' /usr/lib/udev/rules.d/63-scsi-sg3_symlink.rules:25 ATTR '/sys/devices/pci0000:00/0000:00:01.1/0000:01:00.0/host0/target0:2:8/0:2:8:0/block/sdc/queue/nomerges' writing '2' /etc/udev/rules.d/99-abj.nr.rules:14 ATTR '/sys/devices/pci0000:00/0000:00:01.1/0000:01:00.0/host0/target0:2:8/0:2:8:0/block/sdc/queue/nr_requests' writing '1023' /etc/udev/rules.d/99-abj.nr.rules:14 ATTR '/sys/devices/pci0000:00/0000:00:01.1/0000:01:00.0/host0/target0:2:8/0:2:8:0/block/sdc/queue/rotational' writing '0' /etc/udev/rules.d/99-abj.nr.rules:14 ATTR '/sys/devices/pci0000:00/0000:00:01.1/0000:01:00.0/host0/target0:2:8/0:2:8:0/block/sdc/queue/rq_affinity' writing '2' /etc/udev/rules.d/99-abj.nr.rules:14 ATTR '/sys/devices/pci0000:00/0000:00:01.1/0000:01:00.0/host0/target0:2:8/0:2:8:0/block/sdc/queue/scheduler' writing 'none' /etc/udev/rules.d/99-abj.nr.rules:14 ATTR '/sys/devices/pci0000:00/0000:00:01.1/0000:01:00.0/host0/target0:2:8/0:2:8:0/block/sdc/queue/add_random' writing '0' /etc/udev/rules.d/99-abj.nr.rules:14 ATTR '/sys/devices/pci0000:00/0000:00:01.1/0000:01:00.0/host0/target0:2:8/0:2:8:0/block/sdc/queue/max_sectors_kb' writing '4096' /etc/udev/rules.d/99-abj.nr.rules:14 handling device node '/dev/sdc', devnum=b8:32, mode=0660, uid=0, gid=6 preserve permissions /dev/sdc, 060660, uid=0, gid=6 preserve already existing symlink '/dev/block/8:32' to '../sdc' found 'b8:32' claiming '/run/udev/links/\\x2fdisk\\x2fby-id\\x2fscsi-236435330529024150025384100000002' creating link '/dev/disk/by-id/scsi-236435330529024150025384100000002' to '/dev/sdc' preserve already existing symlink '/dev/disk/by-id/scsi-236435330529024150025384100000002' to '../../sdc' found 'b8:32' claiming '/run/udev/links/\\x2fdisk\\x2fby-id\\x2fscsi-SNVMe_Dell_Ent_NVMe_v2_S6CSNA0R902415' creating link '/dev/disk/by-id/scsi-SNVMe_Dell_Ent_NVMe_v2_S6CSNA0R902415' to '/dev/sdc' preserve already existing symlink '/dev/disk/by-id/scsi-SNVMe_Dell_Ent_NVMe_v2_S6CSNA0R902415' to '../../sdc' found 'b8:32' claiming '/run/udev/links/\\x2fdisk\\x2fby-path\\x2fpci-0000:01:00.0-scsi-0:2:8:0' creating link '/dev/disk/by-path/pci-0000:01:00.0-scsi-0:2:8:0' to '/dev/sdc' preserve already existing symlink '/dev/disk/by-path/pci-0000:01:00.0-scsi-0:2:8:0' to '../../sdc' .ID_FS_TYPE_NEW= ACTION=add DEVLINKS=/dev/disk/by-id/scsi-SNVMe_Dell_Ent_NVMe_v2_S6CSNA0R902415 /dev/disk/by-id/scsi-236435330529024150025384100000002 /dev/disk/by-path/pci-0000:01:00.0-scsi-0:2:8:0 DEVNAME=/dev/sdc DEVPATH=/devices/pci0000:00/0000:00:01.1/0000:01:00.0/host0/target0:2:8/0:2:8:0/block/sdc DEVTYPE=disk ID_BUS=scsi ID_FS_TYPE= ID_MODEL=Dell_Ent_NVMe_v2 ID_MODEL_ENC=Dell\\x20Ent\\x20NVMe\\x20v2 ID_PATH=pci-0000:01:00.0-scsi-0:2:8:0 ID_PATH_TAG=pci-0000_01_00_0-scsi-0_2_8_0 ID_REVISION=.2.0 ID_SCSI=1 ID_SCSI_INQUIRY=1 ID_SCSI_SERIAL=S6CSNA0R902415 ID_SERIAL=236435330529024150025384100000002 ID_SERIAL_SHORT=36435330529024150025384100000002 ID_TYPE=disk ID_VENDOR=NVMe ID_VENDOR_ENC=NVMe\\x20\\x20\\x20\\x20 MAJOR=8 MINOR=32 SCSI_IDENT_LUN_EUI64=36435330529024150025384100000002 SCSI_IDENT_PORT_NAA_LOCAL=3bd790c2eff1ebd9 SCSI_IDENT_SERIAL=S6CSNA0R902415 SCSI_MODEL=Dell_Ent_NVMe_v2 SCSI_MODEL_ENC=Dell\\x20Ent\\x20NVMe\\x20v2 SCSI_REVISION=.2.0 SCSI_TPGS=0 SCSI_TYPE=disk SCSI_VENDOR=NVMe SCSI_VENDOR_ENC=NVMe\\x20\\x20\\x20\\x20 SUBSYSTEM=block TAGS=:systemd: USEC_INITIALIZED=76984220 Unload module index Unloaded link configuration context. Test Without Custom Rules [root@r7525 rules.d]# !udev udevadm test --action=\"add\" /sys/block/sdc calling: test version 239 (239-58.el8) This program is for debugging only, it does not run any program specified by a RUN key. It may show incorrect results, because some values may be different, or not available at a simulation run. Load module index Parsed configuration file /usr/lib/systemd/network/99-default.link Created link configuration context. Reading rules file: /usr/lib/udev/rules.d/01-md-raid-creating.rules Reading rules file: /usr/lib/udev/rules.d/10-dm.rules Reading rules file: /usr/lib/udev/rules.d/11-dm-lvm.rules Reading rules file: /usr/lib/udev/rules.d/11-dm-mpath.rules Reading rules file: /usr/lib/udev/rules.d/11-dm-parts.rules Reading rules file: /usr/lib/udev/rules.d/13-dm-disk.rules Reading rules file: /usr/lib/udev/rules.d/39-usbmuxd.rules Reading rules file: /usr/lib/udev/rules.d/40-elevator.rules Reading rules file: /usr/lib/udev/rules.d/40-libgphoto2.rules /usr/lib/udev/rules.d/40-libgphoto2.rules:11: IMPORT found builtin 'usb_id --export %%p', replacing Reading rules file: /usr/lib/udev/rules.d/40-redhat.rules Reading rules file: /usr/lib/udev/rules.d/40-usb-blacklist.rules Reading rules file: /usr/lib/udev/rules.d/40-usb_modeswitch.rules Reading rules file: /usr/lib/udev/rules.d/50-udev-default.rules Reading rules file: /usr/lib/udev/rules.d/60-alias-kmsg.rules Reading rules file: /usr/lib/udev/rules.d/60-block.rules Reading rules file: /usr/lib/udev/rules.d/60-cdrom_id.rules Reading rules file: /usr/lib/udev/rules.d/60-drm.rules Reading rules file: /usr/lib/udev/rules.d/60-evdev.rules Reading rules file: /usr/lib/udev/rules.d/60-fido-id.rules Reading rules file: /usr/lib/udev/rules.d/60-input-id.rules Reading rules file: /usr/lib/udev/rules.d/60-libfprint-2-autosuspend.rules Reading rules file: /usr/lib/udev/rules.d/60-mdevctl.rules Reading rules file: /usr/lib/udev/rules.d/60-net.rules Reading rules file: /usr/lib/udev/rules.d/60-persistent-alsa.rules Reading rules file: /usr/lib/udev/rules.d/60-persistent-input.rules Reading rules file: /usr/lib/udev/rules.d/60-persistent-storage-tape.rules Reading rules file: /usr/lib/udev/rules.d/60-persistent-storage.rules Reading rules file: /usr/lib/udev/rules.d/60-persistent-v4l.rules Reading rules file: /usr/lib/udev/rules.d/60-raw.rules Reading rules file: /usr/lib/udev/rules.d/60-rdma-ndd.rules Reading rules file: /usr/lib/udev/rules.d/60-rdma-persistent-naming.rules Reading rules file: /usr/lib/udev/rules.d/60-sensor.rules Reading rules file: /usr/lib/udev/rules.d/60-serial.rules Reading rules file: /usr/lib/udev/rules.d/60-tpm-udev.rules Reading rules file: /usr/lib/udev/rules.d/61-gdm.rules Reading rules file: /usr/lib/udev/rules.d/61-gnome-bluetooth-rfkill.rules Reading rules file: /usr/lib/udev/rules.d/61-gnome-settings-daemon-rfkill.rules Reading rules file: /usr/lib/udev/rules.d/61-scsi-sg3_id.rules Reading rules file: /usr/lib/udev/rules.d/62-multipath.rules Reading rules file: /usr/lib/udev/rules.d/63-fc-wwpn-id.rules Reading rules file: /usr/lib/udev/rules.d/63-md-raid-arrays.rules Reading rules file: /usr/lib/udev/rules.d/63-scsi-sg3_symlink.rules Reading rules file: /usr/lib/udev/rules.d/64-btrfs.rules Reading rules file: /usr/lib/udev/rules.d/64-md-raid-assembly.rules Reading rules file: /usr/lib/udev/rules.d/65-libwacom.rules Reading rules file: /usr/lib/udev/rules.d/65-md-incremental.rules Reading rules file: /usr/lib/udev/rules.d/65-sane-backends.rules Reading rules file: /usr/lib/udev/rules.d/66-kpartx.rules Reading rules file: /usr/lib/udev/rules.d/68-del-part-nodes.rules Reading rules file: /usr/lib/udev/rules.d/69-btattach-bcm.rules Reading rules file: /usr/lib/udev/rules.d/69-cd-sensors.rules Reading rules file: /usr/lib/udev/rules.d/69-dm-lvm-metad.rules Reading rules file: /usr/lib/udev/rules.d/69-libmtp.rules Reading rules file: /usr/lib/udev/rules.d/69-md-clustered-confirm-device.rules Reading rules file: /etc/udev/rules.d/69-vdo-start-by-dev.rules Reading rules file: /usr/lib/udev/rules.d/70-hypervfcopy.rules Reading rules file: /usr/lib/udev/rules.d/70-hypervkvp.rules Reading rules file: /usr/lib/udev/rules.d/70-hypervvss.rules Reading rules file: /usr/lib/udev/rules.d/70-joystick.rules Reading rules file: /usr/lib/udev/rules.d/70-mouse.rules Reading rules file: /usr/lib/udev/rules.d/70-nvmf-autoconnect.rules Reading rules file: /etc/udev/rules.d/70-persistent-ipoib.rules Reading rules file: /usr/lib/udev/rules.d/70-power-switch.rules Reading rules file: /usr/lib/udev/rules.d/70-printers.rules Reading rules file: /usr/lib/udev/rules.d/70-spice-vdagentd.rules Reading rules file: /usr/lib/udev/rules.d/70-touchpad.rules Reading rules file: /usr/lib/udev/rules.d/70-uaccess.rules Reading rules file: /usr/lib/udev/rules.d/70-wacom.rules Reading rules file: /usr/lib/udev/rules.d/71-biosdevname.rules Reading rules file: /usr/lib/udev/rules.d/71-nvmf-iopolicy-netapp.rules Reading rules file: /usr/lib/udev/rules.d/71-prefixdevname.rules Reading rules file: /usr/lib/udev/rules.d/71-seat.rules Reading rules file: /usr/lib/udev/rules.d/73-idrac.rules Reading rules file: /usr/lib/udev/rules.d/73-seat-late.rules Reading rules file: /usr/lib/udev/rules.d/75-net-description.rules Reading rules file: /usr/lib/udev/rules.d/75-probe_mtd.rules Reading rules file: /usr/lib/udev/rules.d/75-rdma-description.rules Reading rules file: /usr/lib/udev/rules.d/77-mm-broadmobi-port-types.rules Reading rules file: /usr/lib/udev/rules.d/77-mm-cinterion-port-types.rules Reading rules file: /usr/lib/udev/rules.d/77-mm-dell-port-types.rules Reading rules file: /usr/lib/udev/rules.d/77-mm-dlink-port-types.rules Reading rules file: /usr/lib/udev/rules.d/77-mm-ericsson-mbm.rules Reading rules file: /usr/lib/udev/rules.d/77-mm-fibocom-port-types.rules Reading rules file: /usr/lib/udev/rules.d/77-mm-foxconn-port-types.rules Reading rules file: /usr/lib/udev/rules.d/77-mm-gosuncn-port-types.rules Reading rules file: /usr/lib/udev/rules.d/77-mm-haier-port-types.rules Reading rules file: /usr/lib/udev/rules.d/77-mm-huawei-net-port-types.rules Reading rules file: /usr/lib/udev/rules.d/77-mm-longcheer-port-types.rules Reading rules file: /usr/lib/udev/rules.d/77-mm-mtk-port-types.rules Reading rules file: /usr/lib/udev/rules.d/77-mm-nokia-port-types.rules Reading rules file: /usr/lib/udev/rules.d/77-mm-quectel-port-types.rules Reading rules file: /usr/lib/udev/rules.d/77-mm-sierra.rules Reading rules file: /usr/lib/udev/rules.d/77-mm-simtech-port-types.rules Reading rules file: /usr/lib/udev/rules.d/77-mm-telit-port-types.rules Reading rules file: /usr/lib/udev/rules.d/77-mm-tplink-port-types.rules Reading rules file: /usr/lib/udev/rules.d/77-mm-ublox-port-types.rules Reading rules file: /usr/lib/udev/rules.d/77-mm-x22x-port-types.rules Reading rules file: /usr/lib/udev/rules.d/77-mm-zte-port-types.rules Reading rules file: /usr/lib/udev/rules.d/78-sound-card.rules Reading rules file: /usr/lib/udev/rules.d/80-drivers.rules Reading rules file: /usr/lib/udev/rules.d/80-iio-sensor-proxy.rules Reading rules file: /usr/lib/udev/rules.d/80-libinput-device-groups.rules Reading rules file: /usr/lib/udev/rules.d/80-mm-candidate.rules Reading rules file: /usr/lib/udev/rules.d/80-net-setup-link.rules Reading rules file: /usr/lib/udev/rules.d/80-udisks2.rules Reading rules file: /usr/lib/udev/rules.d/81-kvm-rhel.rules Reading rules file: /usr/lib/udev/rules.d/84-nm-drivers.rules Reading rules file: /usr/lib/udev/rules.d/85-nm-unmanaged.rules Reading rules file: /usr/lib/udev/rules.d/85-regulatory.rules Reading rules file: /usr/lib/udev/rules.d/90-alsa-restore.rules Reading rules file: /usr/lib/udev/rules.d/90-bolt.rules Reading rules file: /usr/lib/udev/rules.d/90-fwupd-devices.rules Reading rules file: /usr/lib/udev/rules.d/90-iprutils.rules Reading rules file: /usr/lib/udev/rules.d/90-libinput-fuzz-override.rules Reading rules file: /usr/lib/udev/rules.d/90-nm-thunderbolt.rules Reading rules file: /usr/lib/udev/rules.d/90-pulseaudio.rules Reading rules file: /usr/lib/udev/rules.d/90-rdma-hw-modules.rules Reading rules file: /usr/lib/udev/rules.d/90-rdma-ulp-modules.rules Reading rules file: /usr/lib/udev/rules.d/90-rdma-umad.rules Reading rules file: /usr/lib/udev/rules.d/90-vconsole.rules Reading rules file: /usr/lib/udev/rules.d/91-drm-modeset.rules Reading rules file: /usr/lib/udev/rules.d/95-cd-devices.rules Reading rules file: /usr/lib/udev/rules.d/95-dm-notify.rules Reading rules file: /usr/lib/udev/rules.d/95-iSM-usbnic-systemd.rules Reading rules file: /usr/lib/udev/rules.d/95-upower-csr.rules Reading rules file: /usr/lib/udev/rules.d/95-upower-hid.rules Reading rules file: /usr/lib/udev/rules.d/95-upower-wup.rules Reading rules file: /usr/lib/udev/rules.d/98-kexec.rules Reading rules file: /usr/lib/udev/rules.d/98-trace-cmd.rules Reading rules file: /usr/lib/udev/rules.d/99-qemu-guest-agent.rules Reading rules file: /usr/lib/udev/rules.d/99-systemd.rules Reading rules file: /usr/lib/udev/rules.d/99-vmware-scsi-udev.rules rules contain 393216 bytes tokens (32768 * 12 bytes), 48327 bytes strings 47890 strings (395708 bytes), 43659 de-duplicated (351613 bytes), 4232 trie nodes used GROUP 6 /usr/lib/udev/rules.d/50-udev-default.rules:59 IMPORT 'scsi_id --export --whitelisted -d /dev/sdc' /usr/lib/udev/rules.d/60-persistent-storage.rules:50 starting 'scsi_id --export --whitelisted -d /dev/sdc' 'scsi_id --export --whitelisted -d /dev/sdc'(out) 'ID_SCSI=1' 'scsi_id --export --whitelisted -d /dev/sdc'(out) 'ID_VENDOR=NVMe' 'scsi_id --export --whitelisted -d /dev/sdc'(out) 'ID_VENDOR_ENC=NVMe\\x20\\x20\\x20\\x20' 'scsi_id --export --whitelisted -d /dev/sdc'(out) 'ID_MODEL=Dell_Ent_NVMe_v2' 'scsi_id --export --whitelisted -d /dev/sdc'(out) 'ID_MODEL_ENC=Dell\\x20Ent\\x20NVMe\\x20v2' 'scsi_id --export --whitelisted -d /dev/sdc'(out) 'ID_REVISION=.2.0' 'scsi_id --export --whitelisted -d /dev/sdc'(out) 'ID_TYPE=disk' 'scsi_id --export --whitelisted -d /dev/sdc'(out) 'ID_SERIAL=236435330529024150025384100000002' 'scsi_id --export --whitelisted -d /dev/sdc'(out) 'ID_SERIAL_SHORT=36435330529024150025384100000002' 'scsi_id --export --whitelisted -d /dev/sdc'(out) 'ID_SCSI_SERIAL=S6CSNA0R902415 ' Process 'scsi_id --export --whitelisted -d /dev/sdc' succeeded. LINK 'disk/by-id/scsi-236435330529024150025384100000002' /usr/lib/udev/rules.d/60-persistent-storage.rules:52 IMPORT builtin 'path_id' /usr/lib/udev/rules.d/60-persistent-storage.rules:73 LINK 'disk/by-path/pci-0000:01:00.0-scsi-0:2:8:0' /usr/lib/udev/rules.d/60-persistent-storage.rules:75 IMPORT builtin 'blkid' /usr/lib/udev/rules.d/60-persistent-storage.rules:90 probe /dev/sdc raid offset=0 IMPORT '/usr/bin/sg_inq --export --inhex=/sys/block/sdc/device/inquiry --raw' /usr/lib/udev/rules.d/61-scsi-sg3_id.rules:17 starting '/usr/bin/sg_inq --export --inhex=/sys/block/sdc/device/inquiry --raw' '/usr/bin/sg_inq --export --inhex=/sys/block/sdc/device/inquiry --raw'(out) 'SCSI_TPGS=0' '/usr/bin/sg_inq --export --inhex=/sys/block/sdc/device/inquiry --raw'(out) 'SCSI_TYPE=disk' '/usr/bin/sg_inq --export --inhex=/sys/block/sdc/device/inquiry --raw'(out) 'SCSI_VENDOR=NVMe' '/usr/bin/sg_inq --export --inhex=/sys/block/sdc/device/inquiry --raw'(out) 'SCSI_VENDOR_ENC=NVMe\\x20\\x20\\x20\\x20' '/usr/bin/sg_inq --export --inhex=/sys/block/sdc/device/inquiry --raw'(out) 'SCSI_MODEL=Dell_Ent_NVMe_v2' '/usr/bin/sg_inq --export --inhex=/sys/block/sdc/device/inquiry --raw'(out) 'SCSI_MODEL_ENC=Dell\\x20Ent\\x20NVMe\\x20v2' '/usr/bin/sg_inq --export --inhex=/sys/block/sdc/device/inquiry --raw'(out) 'SCSI_REVISION=.2.0' Process '/usr/bin/sg_inq --export --inhex=/sys/block/sdc/device/inquiry --raw' succeeded. IMPORT '/usr/bin/sg_inq --export --inhex=/sys/block/sdc/device/vpd_pg80 --raw' /usr/lib/udev/rules.d/61-scsi-sg3_id.rules:27 starting '/usr/bin/sg_inq --export --inhex=/sys/block/sdc/device/vpd_pg80 --raw' '/usr/bin/sg_inq --export --inhex=/sys/block/sdc/device/vpd_pg80 --raw'(out) 'SCSI_IDENT_SERIAL=S6CSNA0R902415' Process '/usr/bin/sg_inq --export --inhex=/sys/block/sdc/device/vpd_pg80 --raw' succeeded. IMPORT '/usr/bin/sg_inq --export --inhex=/sys/block/sdc/device/vpd_pg83 --raw' /usr/lib/udev/rules.d/61-scsi-sg3_id.rules:30 starting '/usr/bin/sg_inq --export --inhex=/sys/block/sdc/device/vpd_pg83 --raw' '/usr/bin/sg_inq --export --inhex=/sys/block/sdc/device/vpd_pg83 --raw'(out) 'SCSI_IDENT_LUN_EUI64=36435330529024150025384100000002' '/usr/bin/sg_inq --export --inhex=/sys/block/sdc/device/vpd_pg83 --raw'(out) 'SCSI_IDENT_PORT_NAA_LOCAL=3bd790c2eff1ebd9' Process '/usr/bin/sg_inq --export --inhex=/sys/block/sdc/device/vpd_pg83 --raw' succeeded. IMPORT 'fc_wwpn_id /devices/pci0000:00/0000:00:01.1/0000:01:00.0/host0/target0:2:8/0:2:8:0/block/sdc' /usr/lib/udev/rules.d/63-fc-wwpn-id.rules:8 starting 'fc_wwpn_id /devices/pci0000:00/0000:00:01.1/0000:01:00.0/host0/target0:2:8/0:2:8:0/block/sdc' Process 'fc_wwpn_id /devices/pci0000:00/0000:00:01.1/0000:01:00.0/host0/target0:2:8/0:2:8:0/block/sdc' succeeded. LINK 'disk/by-id/scsi-SNVMe_Dell_Ent_NVMe_v2_S6CSNA0R902415' /usr/lib/udev/rules.d/63-scsi-sg3_symlink.rules:12 LINK 'disk/by-id/scsi-236435330529024150025384100000002' /usr/lib/udev/rules.d/63-scsi-sg3_symlink.rules:25 handling device node '/dev/sdc', devnum=b8:32, mode=0660, uid=0, gid=6 preserve permissions /dev/sdc, 060660, uid=0, gid=6 preserve already existing symlink '/dev/block/8:32' to '../sdc' found 'b8:32' claiming '/run/udev/links/\\x2fdisk\\x2fby-id\\x2fscsi-236435330529024150025384100000002' creating link '/dev/disk/by-id/scsi-236435330529024150025384100000002' to '/dev/sdc' preserve already existing symlink '/dev/disk/by-id/scsi-236435330529024150025384100000002' to '../../sdc' found 'b8:32' claiming '/run/udev/links/\\x2fdisk\\x2fby-id\\x2fscsi-SNVMe_Dell_Ent_NVMe_v2_S6CSNA0R902415' creating link '/dev/disk/by-id/scsi-SNVMe_Dell_Ent_NVMe_v2_S6CSNA0R902415' to '/dev/sdc' preserve already existing symlink '/dev/disk/by-id/scsi-SNVMe_Dell_Ent_NVMe_v2_S6CSNA0R902415' to '../../sdc' found 'b8:32' claiming '/run/udev/links/\\x2fdisk\\x2fby-path\\x2fpci-0000:01:00.0-scsi-0:2:8:0' creating link '/dev/disk/by-path/pci-0000:01:00.0-scsi-0:2:8:0' to '/dev/sdc' preserve already existing symlink '/dev/disk/by-path/pci-0000:01:00.0-scsi-0:2:8:0' to '../../sdc' .ID_FS_TYPE_NEW= ACTION=add DEVLINKS=/dev/disk/by-path/pci-0000:01:00.0-scsi-0:2:8:0 /dev/disk/by-id/scsi-SNVMe_Dell_Ent_NVMe_v2_S6CSNA0R902415 /dev/disk/by-id/scsi-236435330529024150025384100000002 DEVNAME=/dev/sdc DEVPATH=/devices/pci0000:00/0000:00:01.1/0000:01:00.0/host0/target0:2:8/0:2:8:0/block/sdc DEVTYPE=disk ID_BUS=scsi ID_FS_TYPE= ID_MODEL=Dell_Ent_NVMe_v2 ID_MODEL_ENC=Dell\\x20Ent\\x20NVMe\\x20v2 ID_PATH=pci-0000:01:00.0-scsi-0:2:8:0 ID_PATH_TAG=pci-0000_01_00_0-scsi-0_2_8_0 ID_REVISION=.2.0 ID_SCSI=1 ID_SCSI_INQUIRY=1 ID_SCSI_SERIAL=S6CSNA0R902415 ID_SERIAL=236435330529024150025384100000002 ID_SERIAL_SHORT=36435330529024150025384100000002 ID_TYPE=disk ID_VENDOR=NVMe ID_VENDOR_ENC=NVMe\\x20\\x20\\x20\\x20 MAJOR=8 MINOR=32 SCSI_IDENT_LUN_EUI64=36435330529024150025384100000002 SCSI_IDENT_PORT_NAA_LOCAL=3bd790c2eff1ebd9 SCSI_IDENT_SERIAL=S6CSNA0R902415 SCSI_MODEL=Dell_Ent_NVMe_v2 SCSI_MODEL_ENC=Dell\\x20Ent\\x20NVMe\\x20v2 SCSI_REVISION=.2.0 SCSI_TPGS=0 SCSI_TYPE=disk SCSI_VENDOR=NVMe SCSI_VENDOR_ENC=NVMe\\x20\\x20\\x20\\x20 SUBSYSTEM=block TAGS=:systemd: USEC_INITIALIZED=76984220 Unload module index Unloaded link configuration context. perccli64 /c0 show [root@r7525 rules.d]# /opt/MegaRAID/perccli/perccli64 /c0 show Generating detailed summary of the adapter, it may take a while to complete. CLI Version = 007.1623.0000.0000 May 17, 2021 Operating system = Linux 4.18.0-372.9.1.el8.x86_64 Controller = 0 Status = Success Description = None Product Name = PERC H755N Front Serial Number = 07R001E SAS Address = 5f4ee0801601c700 PCI Address = 00:01:00:00 System Time = 09/15/2022 14:05:38 Mfg. Date = 12/10/21 Controller Time = 09/15/2022 18:05:38 FW Package Build = 52.16.1-4405 BIOS Version = 7.16.00.0_0x07100501 FW Version = 5.160.02-3552 Driver Name = megaraid_sas Driver Version = 07.719.03.00-rh1 Current Personality = RAID-Mode Vendor Id = 0x1000 Device Id = 0x10E2 SubVendor Id = 0x1028 SubDevice Id = 0x1AE2 Host Interface = PCI-E Bus Number = 1 Device Number = 0 Function Number = 0 Domain ID = 0 Security Protocol = None JBOD Drives = 2 JBOD LIST : ========= ---------------------------------------------------------------------------------------------------- ID EID:Slt DID State Intf Med Size SeSz Model Vendor Port ---------------------------------------------------------------------------------------------------- 8 252:8 1 Onln NVMe SSD 1.746 TB 512B Dell Ent NVMe v2 AGN RI U.2 1.92TB NVMe 00 x2 9 252:9 0 Onln NVMe SSD 1.746 TB 512B Dell Ent NVMe v2 AGN RI U.2 1.92TB NVMe 00 x2 ---------------------------------------------------------------------------------------------------- ID=JBOD Target ID|EID=Enclosure Device ID|Slt=Slot No|DID=Device ID|Onln=Online Offln=Offline|Intf=Interface|Med=Media Type|SeSz=Sector Size SED=Self Encryptive Drive|PI=Protection Info|Sp=Spun|U=Up|D=Down Physical Drives = 8 PD LIST : ======= ----------------------------------------------------------------------------------------------------- EID:Slt DID State DG Size Intf Med SED PI SeSz Model Sp Type ----------------------------------------------------------------------------------------------------- 252:8 1 Onln - 1.746 TB NVMe SSD N N 512B Dell Ent NVMe v2 AGN RI U.2 1.92TB U JBOD 252:9 0 Onln - 1.746 TB NVMe SSD N N 512B Dell Ent NVMe v2 AGN RI U.2 1.92TB U JBOD 252:10 6 UGood - 1.454 TB NVMe SSD N N 512B Dell Express Flash PM1725b 1.6TB SFF U - 252:11 7 UGood - 13.971 TB NVMe SSD N N 512B Micron_9300_MTFDHAL15T3TDP U - 252:12 5 UGood - 1.454 TB NVMe SSD N N 512B Dell Express Flash PM1725b 1.6TB SFF U - 252:13 4 UGood - 2.910 TB NVMe SSD N N 512B Dell Express Flash NVMe P4610 3.2TB SFF U - 252:14 9 UGood - 2.910 TB NVMe SSD N N 512B Dell Express Flash NVMe P4610 3.2TB SFF U - 252:15 8 UGood - 2.910 TB NVMe SSD N N 512B Dell Express Flash NVMe P4610 3.2TB SFF U - ----------------------------------------------------------------------------------------------------- EID=Enclosure Device ID|Slt=Slot No|DID=Device ID|DG=DriveGroup DHS=Dedicated Hot Spare|UGood=Unconfigured Good|GHS=Global Hotspare UBad=Unconfigured Bad|Sntze=Sanitize|Onln=Online|Offln=Offline|Intf=Interface Med=Media Type|SED=Self Encryptive Drive|PI=Protection Info SeSz=Sector Size|Sp=Spun|U=Up|D=Down|T=Transition|F=Foreign UGUnsp=UGood Unsupported|UGShld=UGood shielded|HSPShld=Hotspare shielded CFShld=Configured shielded|Cpybck=CopyBack|CBShld=Copyback Shielded UBUnsp=UBad Unsupported|Rbld=Rebuild Enclosures = 1 Enclosure LIST : ============== -------------------------------------------------------------------- EID State Slots PD PS Fans TSs Alms SIM Port# ProdID VendorSpecific -------------------------------------------------------------------- 252 OK 8 8 0 0 0 0 0 - BP15G+ -------------------------------------------------------------------- EID=Enclosure Device ID | PD=Physical drive count | PS=Power Supply count TSs=Temperature sensor count | Alms=Alarm count | SIM=SIM Count | ProdID=Product ID BBU_Info : ======== ---------------------------------------------- Model State RetentionTime Temp Mode MfgDate ---------------------------------------------- BBU Optimal 0 hour(s) 27C - 0/00/00 ---------------------------------------------- /opt/MegaRAID/perccli/perccli64 /c1 show [root@r7525 rules.d]# /opt/MegaRAID/perccli/perccli64 /c1 show Generating detailed summary of the adapter, it may take a while to complete. CLI Version = 007.1623.0000.0000 May 17, 2021 Operating system = Linux 4.18.0-372.9.1.el8.x86_64 Controller = 1 Status = Success Description = None Product Name = PERC H755N Front Serial Number = 07R00K2 SAS Address = 5f4ee080160bd500 PCI Address = 00:c1:00:00 System Time = 09/15/2022 14:17:35 Mfg. Date = 12/10/21 Controller Time = 09/15/2022 18:17:35 FW Package Build = 52.16.1-4405 BIOS Version = 7.16.00.0_0x07100501 FW Version = 5.160.02-3552 Driver Name = megaraid_sas Driver Version = 07.719.03.00-rh1 Current Personality = RAID-Mode Vendor Id = 0x1000 Device Id = 0x10E2 SubVendor Id = 0x1028 SubDevice Id = 0x1AE2 Host Interface = PCI-E Bus Number = 193 Device Number = 0 Function Number = 0 Domain ID = 0 Security Protocol = None JBOD Drives = 2 JBOD LIST : ========= ---------------------------------------------------------------------------------------------------- ID EID:Slt DID State Intf Med Size SeSz Model Vendor Port ---------------------------------------------------------------------------------------------------- 0 252:0 1 Onln NVMe SSD 1.746 TB 512B Dell Ent NVMe v2 AGN RI U.2 1.92TB NVMe 00 x2 1 252:1 0 Onln NVMe SSD 1.746 TB 512B Dell Ent NVMe v2 AGN RI U.2 1.92TB NVMe 00 x2 ---------------------------------------------------------------------------------------------------- ID=JBOD Target ID|EID=Enclosure Device ID|Slt=Slot No|DID=Device ID|Onln=Online Offln=Offline|Intf=Interface|Med=Media Type|SeSz=Sector Size SED=Self Encryptive Drive|PI=Protection Info|Sp=Spun|U=Up|D=Down Physical Drives = 8 PD LIST : ======= ---------------------------------------------------------------------------------------------------- EID:Slt DID State DG Size Intf Med SED PI SeSz Model Sp Type ---------------------------------------------------------------------------------------------------- 252:0 1 Onln - 1.746 TB NVMe SSD N N 512B Dell Ent NVMe v2 AGN RI U.2 1.92TB U JBOD 252:1 0 Onln - 1.746 TB NVMe SSD N N 512B Dell Ent NVMe v2 AGN RI U.2 1.92TB U JBOD 252:2 5 UGood - 2.910 TB NVMe SSD N N 512B Dell Express Flash NVMe P4610 3.2TB SFF U - 252:3 8 UGood - 2.910 TB NVMe SSD N N 512B Dell Express Flash NVMe P4610 3.2TB SFF U - 252:4 4 UGood - 2.910 TB NVMe SSD N N 512B Dell Express Flash NVMe P4610 3.2TB SFF U - 252:5 7 UGood - 2.910 TB NVMe SSD N N 512B Dell Express Flash NVMe P4610 3.2TB SFF U - 252:6 6 UGood - 2.910 TB NVMe SSD N N 512B Dell Express Flash NVMe P4610 3.2TB SFF U - 252:7 3 UGood - 2.910 TB NVMe SSD N N 512B Dell Express Flash NVMe P4610 3.2TB SFF U - ---------------------------------------------------------------------------------------------------- EID=Enclosure Device ID|Slt=Slot No|DID=Device ID|DG=DriveGroup DHS=Dedicated Hot Spare|UGood=Unconfigured Good|GHS=Global Hotspare UBad=Unconfigured Bad|Sntze=Sanitize|Onln=Online|Offln=Offline|Intf=Interface Med=Media Type|SED=Self Encryptive Drive|PI=Protection Info SeSz=Sector Size|Sp=Spun|U=Up|D=Down|T=Transition|F=Foreign UGUnsp=UGood Unsupported|UGShld=UGood shielded|HSPShld=Hotspare shielded CFShld=Configured shielded|Cpybck=CopyBack|CBShld=Copyback Shielded UBUnsp=UBad Unsupported|Rbld=Rebuild Enclosures = 1 Enclosure LIST : ============== -------------------------------------------------------------------- EID State Slots PD PS Fans TSs Alms SIM Port# ProdID VendorSpecific -------------------------------------------------------------------- 252 OK 8 8 0 0 0 0 0 - BP15G+ -------------------------------------------------------------------- EID=Enclosure Device ID | PD=Physical drive count | PS=Power Supply count TSs=Temperature sensor count | Alms=Alarm count | SIM=SIM Count | ProdID=Product ID BBU_Info : ======== ---------------------------------------------- Model State RetentionTime Temp Mode MfgDate ---------------------------------------------- BBU Optimal 0 hour(s) 27C - 0/00/00 ---------------------------------------------- lsblk [root@r7525 rules.d]# lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT sda 8:0 0 223.6G 0 disk \u251c\u2500sda1 8:1 0 600M 0 part /boot/efi \u251c\u2500sda2 8:2 0 1G 0 part /boot \u2514\u2500sda3 8:3 0 222G 0 part \u2514\u2500md127 9:127 0 221.9G 0 raid1 \u251c\u2500boss_drives-root 253:0 0 217.9G 0 lvm / \u2514\u2500boss_drives-swap 253:1 0 4G 0 lvm [SWAP] sdb 8:16 0 223.6G 0 disk \u2514\u2500sdb1 8:17 0 222G 0 part \u2514\u2500md127 9:127 0 221.9G 0 raid1 \u251c\u2500boss_drives-root 253:0 0 217.9G 0 lvm / \u2514\u2500boss_drives-swap 253:1 0 4G 0 lvm [SWAP] sdc 8:32 0 1.8T 0 disk sdd 8:48 0 1.8T 0 disk sde 8:64 0 1.8T 0 disk sdf 8:80 0 1.8T 0 disk Test 2 99-abj.nr.rules [root@r7525 rules.d]# cat 99-abj.nr.rules KERNEL==\"sd*\",ACTION==\"add|change\",ATTRS{model}==\"PERC_H755N_Front\",\\ ATTR{queue/nomerges}=\"2\",\\ ATTR{queue/nr_requests}=\"1023\",\\ ATTR{queue/rotational}=\"0\",\\ ATTR{queue/rq_affinity}=\"2\",\\ ATTR{queue/scheduler}=\"none\",\\ ATTR{queue/add_random}=\"0\",ATTR{queue/max_sectors_kb}=\"4096\" KERNEL==\"sd*\",ACTION==\"add|change\",ATTRS{model}==\"Dell Ent NVMe v2\",\\ ATTR{queue/nomerges}=\"2\",\\ ATTR{queue/nr_requests}=\"1023\",\\ ATTR{queue/rotational}=\"0\",\\ ATTR{queue/rq_affinity}=\"2\",\\ ATTR{queue/scheduler}=\"none\",\\ ATTR{queue/add_random}=\"0\",ATTR{queue/max_sectors_kb}=\"4096\" SUBSYSTEM==\"block\",ACTION==\"add|change\",KERNEL==\"nvme*[0-9]n*[0-9]\",ATTRS{model}==\"Dell Ent NVMe v2 AGN RI U.2 1.92TB\",\\ ATTR{queue/nomerges}=\"2\",\\ ATTR{queue/nr_requests}=\"1023\",\\ ATTR{queue/rotational}=\"0\",\\ ATTR{queue/rq_affinity}=\"2\",\\ ATTR{queue/scheduler}=\"none\",\\ ATTR{queue/add_random}=\"0\",\\ ATTR{queue/max_sectors_kb}=\"4096\" SUBSYSTEM==\"block\",ACTION==\"add|change\",KERNEL==\"md*\",\\ ATTR{md/sync_speed_max}=\"2000000\",\\ ATTR{md/group_thread_cnt}=\"64\",\\ ATTR{md/stripe_cache_size}=\"8192\",\\ ATTR{queue/nomerges}=\"2\",\\ ATTR{queue/nr_requests}=\"1023\",\\ ATTR{queue/rotational}=\"0\",\\ ATTR{queue/rq_affinity}=\"2\",\\ ATTR{queue/scheduler}=\"none\",\\ ATTR{queue/add_random}=\"0\", ATTR{queue/max_sectors_kb}=\"4096\" /opt/MegaRAID/perccli/perccli64 /c1 show [root@r7525 rules.d]# /opt/MegaRAID/perccli/perccli64 /c1 show Generating detailed summary of the adapter, it may take a while to complete. CLI Version = 007.1623.0000.0000 May 17, 2021 Operating system = Linux 4.18.0-372.9.1.el8.x86_64 Controller = 1 Status = Success Description = None Product Name = PERC H755N Front Serial Number = 07R00K2 SAS Address = 5f4ee080160bd500 PCI Address = 00:c1:00:00 System Time = 09/16/2022 12:58:02 Mfg. Date = 12/10/21 Controller Time = 09/16/2022 16:58:02 FW Package Build = 52.16.1-4405 BIOS Version = 7.16.00.0_0x07100501 FW Version = 5.160.02-3552 Driver Name = megaraid_sas Driver Version = 07.719.03.00-rh1 Current Personality = RAID-Mode Vendor Id = 0x1000 Device Id = 0x10E2 SubVendor Id = 0x1028 SubDevice Id = 0x1AE2 Host Interface = PCI-E Bus Number = 193 Device Number = 0 Function Number = 0 Domain ID = 0 Security Protocol = None Drive Groups = 1 TOPOLOGY : ======== --------------------------------------------------------------------------- DG Arr Row EID:Slot DID Type State BT Size PDC PI SED DS3 FSpace TR --------------------------------------------------------------------------- 0 - - - - RAID5 Optl Y 5.820 TB dflt N N dflt N N 0 0 - - - RAID5 Optl Y 5.820 TB dflt N N dflt N N 0 0 0 252:2 5 DRIVE Onln N 2.910 TB dflt N N dflt - N 0 0 1 252:3 8 DRIVE Onln N 2.910 TB dflt N N dflt - N 0 0 2 252:4 4 DRIVE Onln N 2.910 TB dflt N N dflt - N --------------------------------------------------------------------------- DG=Disk Group Index|Arr=Array Index|Row=Row Index|EID=Enclosure Device ID DID=Device ID|Type=Drive Type|Onln=Online|Rbld=Rebuild|Optl=Optimal|Dgrd=Degraded Pdgd=Partially degraded|Offln=Offline|BT=Background Task Active PDC=PD Cache|PI=Protection Info|SED=Self Encrypting Drive|Frgn=Foreign DS3=Dimmer Switch 3|dflt=Default|Msng=Missing|FSpace=Free Space Present TR=Transport Ready Virtual Drives = 1 VD LIST : ======= ------------------------------------------------------------- DG/VD TYPE State Access Consist Cache Cac sCC Size Name ------------------------------------------------------------- 0/239 RAID5 Optl RW No RWTD - OFF 5.820 TB ------------------------------------------------------------- VD=Virtual Drive| DG=Drive Group|Rec=Recovery Cac=CacheCade|OfLn=OffLine|Pdgd=Partially Degraded|Dgrd=Degraded Optl=Optimal|dflt=Default|RO=Read Only|RW=Read Write|HD=Hidden|TRANS=TransportReady B=Blocked|Consist=Consistent|R=Read Ahead Always|NR=No Read Ahead|WB=WriteBack AWB=Always WriteBack|WT=WriteThrough|C=Cached IO|D=Direct IO|sCC=Scheduled Check Consistency JBOD Drives = 2 JBOD LIST : ========= ---------------------------------------------------------------------------------------------------- ID EID:Slt DID State Intf Med Size SeSz Model Vendor Port ---------------------------------------------------------------------------------------------------- 0 252:0 1 Onln NVMe SSD 1.746 TB 512B Dell Ent NVMe v2 AGN RI U.2 1.92TB NVMe 00 x2 1 252:1 0 Onln NVMe SSD 1.746 TB 512B Dell Ent NVMe v2 AGN RI U.2 1.92TB NVMe 00 x2 ---------------------------------------------------------------------------------------------------- ID=JBOD Target ID|EID=Enclosure Device ID|Slt=Slot No|DID=Device ID|Onln=Online Offln=Offline|Intf=Interface|Med=Media Type|SeSz=Sector Size SED=Self Encryptive Drive|PI=Protection Info|Sp=Spun|U=Up|D=Down Physical Drives = 8 PD LIST : ======= ---------------------------------------------------------------------------------------------------- EID:Slt DID State DG Size Intf Med SED PI SeSz Model Sp Type ---------------------------------------------------------------------------------------------------- 252:0 1 Onln - 1.746 TB NVMe SSD N N 512B Dell Ent NVMe v2 AGN RI U.2 1.92TB U JBOD 252:1 0 Onln - 1.746 TB NVMe SSD N N 512B Dell Ent NVMe v2 AGN RI U.2 1.92TB U JBOD 252:2 5 Onln 0 2.910 TB NVMe SSD N N 512B Dell Express Flash NVMe P4610 3.2TB SFF U - 252:3 8 Onln 0 2.910 TB NVMe SSD N N 512B Dell Express Flash NVMe P4610 3.2TB SFF U - 252:4 4 Onln 0 2.910 TB NVMe SSD N N 512B Dell Express Flash NVMe P4610 3.2TB SFF U - 252:5 7 UGood - 2.910 TB NVMe SSD N N 512B Dell Express Flash NVMe P4610 3.2TB SFF U - 252:6 6 UGood - 2.910 TB NVMe SSD N N 512B Dell Express Flash NVMe P4610 3.2TB SFF U - 252:7 3 UGood - 2.910 TB NVMe SSD N N 512B Dell Express Flash NVMe P4610 3.2TB SFF U - ---------------------------------------------------------------------------------------------------- EID=Enclosure Device ID|Slt=Slot No|DID=Device ID|DG=DriveGroup DHS=Dedicated Hot Spare|UGood=Unconfigured Good|GHS=Global Hotspare UBad=Unconfigured Bad|Sntze=Sanitize|Onln=Online|Offln=Offline|Intf=Interface Med=Media Type|SED=Self Encryptive Drive|PI=Protection Info SeSz=Sector Size|Sp=Spun|U=Up|D=Down|T=Transition|F=Foreign UGUnsp=UGood Unsupported|UGShld=UGood shielded|HSPShld=Hotspare shielded CFShld=Configured shielded|Cpybck=CopyBack|CBShld=Copyback Shielded UBUnsp=UBad Unsupported|Rbld=Rebuild Enclosures = 1 Enclosure LIST : ============== -------------------------------------------------------------------- EID State Slots PD PS Fans TSs Alms SIM Port# ProdID VendorSpecific -------------------------------------------------------------------- 252 OK 8 8 0 0 0 0 0 - BP15G+ -------------------------------------------------------------------- EID=Enclosure Device ID | PD=Physical drive count | PS=Power Supply count TSs=Temperature sensor count | Alms=Alarm count | SIM=SIM Count | ProdID=Product ID BBU_Info : ======== ---------------------------------------------- Model State RetentionTime Temp Mode MfgDate ---------------------------------------------- BBU Optimal 0 hour(s) 26C - 0/00/00 ---------------------------------------------- udevadm test --action=\"add\" /sys/block/sda [root@r7525 rules.d]# udevadm test --action=\"add\" /sys/block/sda calling: test version 239 (239-58.el8) This program is for debugging only, it does not run any program specified by a RUN key. It may show incorrect results, because some values may be different, or not available at a simulation run. Load module index Parsed configuration file /usr/lib/systemd/network/99-default.link Created link configuration context. Reading rules file: /usr/lib/udev/rules.d/01-md-raid-creating.rules Reading rules file: /usr/lib/udev/rules.d/10-dm.rules Reading rules file: /usr/lib/udev/rules.d/11-dm-lvm.rules Reading rules file: /usr/lib/udev/rules.d/11-dm-mpath.rules Reading rules file: /usr/lib/udev/rules.d/11-dm-parts.rules Reading rules file: /usr/lib/udev/rules.d/13-dm-disk.rules Reading rules file: /usr/lib/udev/rules.d/39-usbmuxd.rules Reading rules file: /usr/lib/udev/rules.d/40-elevator.rules Reading rules file: /usr/lib/udev/rules.d/40-libgphoto2.rules /usr/lib/udev/rules.d/40-libgphoto2.rules:11: IMPORT found builtin 'usb_id --export %%p', replacing Reading rules file: /usr/lib/udev/rules.d/40-redhat.rules Reading rules file: /usr/lib/udev/rules.d/40-usb-blacklist.rules Reading rules file: /usr/lib/udev/rules.d/40-usb_modeswitch.rules Reading rules file: /usr/lib/udev/rules.d/50-udev-default.rules Reading rules file: /usr/lib/udev/rules.d/60-alias-kmsg.rules Reading rules file: /usr/lib/udev/rules.d/60-block.rules Reading rules file: /usr/lib/udev/rules.d/60-cdrom_id.rules Reading rules file: /usr/lib/udev/rules.d/60-drm.rules Reading rules file: /usr/lib/udev/rules.d/60-evdev.rules Reading rules file: /usr/lib/udev/rules.d/60-fido-id.rules Reading rules file: /usr/lib/udev/rules.d/60-input-id.rules Reading rules file: /usr/lib/udev/rules.d/60-libfprint-2-autosuspend.rules Reading rules file: /usr/lib/udev/rules.d/60-mdevctl.rules Reading rules file: /usr/lib/udev/rules.d/60-net.rules Reading rules file: /usr/lib/udev/rules.d/60-persistent-alsa.rules Reading rules file: /usr/lib/udev/rules.d/60-persistent-input.rules Reading rules file: /usr/lib/udev/rules.d/60-persistent-storage-tape.rules Reading rules file: /usr/lib/udev/rules.d/60-persistent-storage.rules Reading rules file: /usr/lib/udev/rules.d/60-persistent-v4l.rules Reading rules file: /usr/lib/udev/rules.d/60-raw.rules Reading rules file: /usr/lib/udev/rules.d/60-rdma-ndd.rules Reading rules file: /usr/lib/udev/rules.d/60-rdma-persistent-naming.rules Reading rules file: /usr/lib/udev/rules.d/60-sensor.rules Reading rules file: /usr/lib/udev/rules.d/60-serial.rules Reading rules file: /usr/lib/udev/rules.d/60-tpm-udev.rules Reading rules file: /usr/lib/udev/rules.d/61-gdm.rules Reading rules file: /usr/lib/udev/rules.d/61-gnome-bluetooth-rfkill.rules Reading rules file: /usr/lib/udev/rules.d/61-gnome-settings-daemon-rfkill.rules Reading rules file: /usr/lib/udev/rules.d/61-scsi-sg3_id.rules Reading rules file: /usr/lib/udev/rules.d/62-multipath.rules Reading rules file: /usr/lib/udev/rules.d/63-fc-wwpn-id.rules Reading rules file: /usr/lib/udev/rules.d/63-md-raid-arrays.rules Reading rules file: /usr/lib/udev/rules.d/63-scsi-sg3_symlink.rules Reading rules file: /usr/lib/udev/rules.d/64-btrfs.rules Reading rules file: /usr/lib/udev/rules.d/64-md-raid-assembly.rules Reading rules file: /usr/lib/udev/rules.d/65-libwacom.rules Reading rules file: /usr/lib/udev/rules.d/65-md-incremental.rules Reading rules file: /usr/lib/udev/rules.d/65-sane-backends.rules Reading rules file: /usr/lib/udev/rules.d/66-kpartx.rules Reading rules file: /usr/lib/udev/rules.d/68-del-part-nodes.rules Reading rules file: /usr/lib/udev/rules.d/69-btattach-bcm.rules Reading rules file: /usr/lib/udev/rules.d/69-cd-sensors.rules Reading rules file: /usr/lib/udev/rules.d/69-dm-lvm-metad.rules Reading rules file: /usr/lib/udev/rules.d/69-libmtp.rules Reading rules file: /usr/lib/udev/rules.d/69-md-clustered-confirm-device.rules Reading rules file: /etc/udev/rules.d/69-vdo-start-by-dev.rules Reading rules file: /usr/lib/udev/rules.d/70-hypervfcopy.rules Reading rules file: /usr/lib/udev/rules.d/70-hypervkvp.rules Reading rules file: /usr/lib/udev/rules.d/70-hypervvss.rules Reading rules file: /usr/lib/udev/rules.d/70-joystick.rules Reading rules file: /usr/lib/udev/rules.d/70-mouse.rules Reading rules file: /usr/lib/udev/rules.d/70-nvmf-autoconnect.rules Reading rules file: /etc/udev/rules.d/70-persistent-ipoib.rules Reading rules file: /usr/lib/udev/rules.d/70-power-switch.rules Reading rules file: /usr/lib/udev/rules.d/70-printers.rules Reading rules file: /usr/lib/udev/rules.d/70-spice-vdagentd.rules Reading rules file: /usr/lib/udev/rules.d/70-touchpad.rules Reading rules file: /usr/lib/udev/rules.d/70-uaccess.rules Reading rules file: /usr/lib/udev/rules.d/70-wacom.rules Reading rules file: /usr/lib/udev/rules.d/71-biosdevname.rules Reading rules file: /usr/lib/udev/rules.d/71-nvmf-iopolicy-netapp.rules Reading rules file: /usr/lib/udev/rules.d/71-prefixdevname.rules Reading rules file: /usr/lib/udev/rules.d/71-seat.rules Reading rules file: /usr/lib/udev/rules.d/73-idrac.rules Reading rules file: /usr/lib/udev/rules.d/73-seat-late.rules Reading rules file: /usr/lib/udev/rules.d/75-net-description.rules Reading rules file: /usr/lib/udev/rules.d/75-probe_mtd.rules Reading rules file: /usr/lib/udev/rules.d/75-rdma-description.rules Reading rules file: /usr/lib/udev/rules.d/77-mm-broadmobi-port-types.rules Reading rules file: /usr/lib/udev/rules.d/77-mm-cinterion-port-types.rules Reading rules file: /usr/lib/udev/rules.d/77-mm-dell-port-types.rules Reading rules file: /usr/lib/udev/rules.d/77-mm-dlink-port-types.rules Reading rules file: /usr/lib/udev/rules.d/77-mm-ericsson-mbm.rules Reading rules file: /usr/lib/udev/rules.d/77-mm-fibocom-port-types.rules Reading rules file: /usr/lib/udev/rules.d/77-mm-foxconn-port-types.rules Reading rules file: /usr/lib/udev/rules.d/77-mm-gosuncn-port-types.rules Reading rules file: /usr/lib/udev/rules.d/77-mm-haier-port-types.rules Reading rules file: /usr/lib/udev/rules.d/77-mm-huawei-net-port-types.rules Reading rules file: /usr/lib/udev/rules.d/77-mm-longcheer-port-types.rules Reading rules file: /usr/lib/udev/rules.d/77-mm-mtk-port-types.rules Reading rules file: /usr/lib/udev/rules.d/77-mm-nokia-port-types.rules Reading rules file: /usr/lib/udev/rules.d/77-mm-quectel-port-types.rules Reading rules file: /usr/lib/udev/rules.d/77-mm-sierra.rules Reading rules file: /usr/lib/udev/rules.d/77-mm-simtech-port-types.rules Reading rules file: /usr/lib/udev/rules.d/77-mm-telit-port-types.rules Reading rules file: /usr/lib/udev/rules.d/77-mm-tplink-port-types.rules Reading rules file: /usr/lib/udev/rules.d/77-mm-ublox-port-types.rules Reading rules file: /usr/lib/udev/rules.d/77-mm-x22x-port-types.rules Reading rules file: /usr/lib/udev/rules.d/77-mm-zte-port-types.rules Reading rules file: /usr/lib/udev/rules.d/78-sound-card.rules Reading rules file: /usr/lib/udev/rules.d/80-drivers.rules Reading rules file: /usr/lib/udev/rules.d/80-iio-sensor-proxy.rules Reading rules file: /usr/lib/udev/rules.d/80-libinput-device-groups.rules Reading rules file: /usr/lib/udev/rules.d/80-mm-candidate.rules Reading rules file: /usr/lib/udev/rules.d/80-net-setup-link.rules Reading rules file: /usr/lib/udev/rules.d/80-udisks2.rules Reading rules file: /usr/lib/udev/rules.d/81-kvm-rhel.rules Reading rules file: /usr/lib/udev/rules.d/84-nm-drivers.rules Reading rules file: /usr/lib/udev/rules.d/85-nm-unmanaged.rules Reading rules file: /usr/lib/udev/rules.d/85-regulatory.rules Reading rules file: /usr/lib/udev/rules.d/90-alsa-restore.rules Reading rules file: /usr/lib/udev/rules.d/90-bolt.rules Reading rules file: /usr/lib/udev/rules.d/90-fwupd-devices.rules Reading rules file: /usr/lib/udev/rules.d/90-iprutils.rules Reading rules file: /usr/lib/udev/rules.d/90-libinput-fuzz-override.rules Reading rules file: /usr/lib/udev/rules.d/90-nm-thunderbolt.rules Reading rules file: /usr/lib/udev/rules.d/90-pulseaudio.rules Reading rules file: /usr/lib/udev/rules.d/90-rdma-hw-modules.rules Reading rules file: /usr/lib/udev/rules.d/90-rdma-ulp-modules.rules Reading rules file: /usr/lib/udev/rules.d/90-rdma-umad.rules Reading rules file: /usr/lib/udev/rules.d/90-vconsole.rules Reading rules file: /usr/lib/udev/rules.d/91-drm-modeset.rules Reading rules file: /usr/lib/udev/rules.d/95-cd-devices.rules Reading rules file: /usr/lib/udev/rules.d/95-dm-notify.rules Reading rules file: /usr/lib/udev/rules.d/95-iSM-usbnic-systemd.rules Reading rules file: /usr/lib/udev/rules.d/95-upower-csr.rules Reading rules file: /usr/lib/udev/rules.d/95-upower-hid.rules Reading rules file: /usr/lib/udev/rules.d/95-upower-wup.rules Reading rules file: /usr/lib/udev/rules.d/98-kexec.rules Reading rules file: /usr/lib/udev/rules.d/98-trace-cmd.rules Reading rules file: /etc/udev/rules.d/99-abj.nr.rules Reading rules file: /usr/lib/udev/rules.d/99-qemu-guest-agent.rules Reading rules file: /usr/lib/udev/rules.d/99-systemd.rules Reading rules file: /usr/lib/udev/rules.d/99-vmware-scsi-udev.rules rules contain 393216 bytes tokens (32768 * 12 bytes), 48613 bytes strings 47970 strings (396524 bytes), 43723 de-duplicated (352159 bytes), 4248 trie nodes used GROUP 6 /usr/lib/udev/rules.d/50-udev-default.rules:59 IMPORT 'scsi_id --export --whitelisted -d /dev/sda' /usr/lib/udev/rules.d/60-persistent-storage.rules:50 starting 'scsi_id --export --whitelisted -d /dev/sda' 'scsi_id --export --whitelisted -d /dev/sda'(out) 'ID_SCSI=1' 'scsi_id --export --whitelisted -d /dev/sda'(out) 'ID_VENDOR=DELL' 'scsi_id --export --whitelisted -d /dev/sda'(out) 'ID_VENDOR_ENC=DELL\\x20\\x20\\x20\\x20' 'scsi_id --export --whitelisted -d /dev/sda'(out) 'ID_MODEL=PERC_H755N_Front' 'scsi_id --export --whitelisted -d /dev/sda'(out) 'ID_MODEL_ENC=PERC\\x20H755N\\x20Front' 'scsi_id --export --whitelisted -d /dev/sda'(out) 'ID_REVISION=5.16' 'scsi_id --export --whitelisted -d /dev/sda'(out) 'ID_TYPE=disk' 'scsi_id --export --whitelisted -d /dev/sda'(out) 'ID_SERIAL=36f4ee080160bd5002ab7652100a1691a' 'scsi_id --export --whitelisted -d /dev/sda'(out) 'ID_SERIAL_SHORT=6f4ee080160bd5002ab7652100a1691a' 'scsi_id --export --whitelisted -d /dev/sda'(out) 'ID_WWN=0x6f4ee080160bd500' 'scsi_id --export --whitelisted -d /dev/sda'(out) 'ID_WWN_VENDOR_EXTENSION=0x2ab7652100a1691a' 'scsi_id --export --whitelisted -d /dev/sda'(out) 'ID_WWN_WITH_EXTENSION=0x6f4ee080160bd5002ab7652100a1691a' 'scsi_id --export --whitelisted -d /dev/sda'(out) 'ID_SCSI_SERIAL=001a69a1002165b72a00d50b1680e04e' Process 'scsi_id --export --whitelisted -d /dev/sda' succeeded. LINK 'disk/by-id/scsi-36f4ee080160bd5002ab7652100a1691a' /usr/lib/udev/rules.d/60-persistent-storage.rules:52 IMPORT builtin 'path_id' /usr/lib/udev/rules.d/60-persistent-storage.rules:73 LINK 'disk/by-path/pci-0000:c1:00.0-scsi-0:3:111:0' /usr/lib/udev/rules.d/60-persistent-storage.rules:75 IMPORT builtin 'blkid' /usr/lib/udev/rules.d/60-persistent-storage.rules:90 probe /dev/sda raid offset=0 LINK 'disk/by-id/wwn-0x6f4ee080160bd5002ab7652100a1691a' /usr/lib/udev/rules.d/60-persistent-storage.rules:97 IMPORT '/usr/bin/sg_inq --export --inhex=/sys/block/sda/device/inquiry --raw' /usr/lib/udev/rules.d/61-scsi-sg3_id.rules:17 starting '/usr/bin/sg_inq --export --inhex=/sys/block/sda/device/inquiry --raw' '/usr/bin/sg_inq --export --inhex=/sys/block/sda/device/inquiry --raw'(out) 'SCSI_TPGS=0' '/usr/bin/sg_inq --export --inhex=/sys/block/sda/device/inquiry --raw'(out) 'SCSI_TYPE=disk' '/usr/bin/sg_inq --export --inhex=/sys/block/sda/device/inquiry --raw'(out) 'SCSI_VENDOR=DELL' '/usr/bin/sg_inq --export --inhex=/sys/block/sda/device/inquiry --raw'(out) 'SCSI_VENDOR_ENC=DELL\\x20\\x20\\x20\\x20' '/usr/bin/sg_inq --export --inhex=/sys/block/sda/device/inquiry --raw'(out) 'SCSI_MODEL=PERC_H755N_Front' '/usr/bin/sg_inq --export --inhex=/sys/block/sda/device/inquiry --raw'(out) 'SCSI_MODEL_ENC=PERC\\x20H755N\\x20Front' '/usr/bin/sg_inq --export --inhex=/sys/block/sda/device/inquiry --raw'(out) 'SCSI_REVISION=5.16' Process '/usr/bin/sg_inq --export --inhex=/sys/block/sda/device/inquiry --raw' succeeded. IMPORT '/usr/bin/sg_inq --export --inhex=/sys/block/sda/device/vpd_pg80 --raw' /usr/lib/udev/rules.d/61-scsi-sg3_id.rules:27 starting '/usr/bin/sg_inq --export --inhex=/sys/block/sda/device/vpd_pg80 --raw' '/usr/bin/sg_inq --export --inhex=/sys/block/sda/device/vpd_pg80 --raw'(out) 'SCSI_IDENT_SERIAL=001a69a1002165b72a00d50b1680e04e' Process '/usr/bin/sg_inq --export --inhex=/sys/block/sda/device/vpd_pg80 --raw' succeeded. IMPORT '/usr/bin/sg_inq --export --inhex=/sys/block/sda/device/vpd_pg83 --raw' /usr/lib/udev/rules.d/61-scsi-sg3_id.rules:30 starting '/usr/bin/sg_inq --export --inhex=/sys/block/sda/device/vpd_pg83 --raw' '/usr/bin/sg_inq --export --inhex=/sys/block/sda/device/vpd_pg83 --raw'(out) 'SCSI_IDENT_LUN_NAA_REGEXT=6f4ee080160bd5002ab7652100a1691a' Process '/usr/bin/sg_inq --export --inhex=/sys/block/sda/device/vpd_pg83 --raw' succeeded. IMPORT 'fc_wwpn_id /devices/pci0000:c0/0000:c0:01.1/0000:c1:00.0/host12/target12:3:111/12:3:111:0/block/sda' /usr/lib/udev/rules.d/63-fc-wwpn-id.rules:8 starting 'fc_wwpn_id /devices/pci0000:c0/0000:c0:01.1/0000:c1:00.0/host12/target12:3:111/12:3:111:0/block/sda' Process 'fc_wwpn_id /devices/pci0000:c0/0000:c0:01.1/0000:c1:00.0/host12/target12:3:111/12:3:111:0/block/sda' succeeded. LINK 'disk/by-id/scsi-SDELL_PERC_H755N_Front_001a69a1002165b72a00d50b1680e04e' /usr/lib/udev/rules.d/63-scsi-sg3_symlink.rules:12 LINK 'disk/by-id/scsi-36f4ee080160bd5002ab7652100a1691a' /usr/lib/udev/rules.d/63-scsi-sg3_symlink.rules:16 handling device node '/dev/sda', devnum=b8:0, mode=0660, uid=0, gid=6 preserve permissions /dev/sda, 060660, uid=0, gid=6 preserve already existing symlink '/dev/block/8:0' to '../sda' found 'b8:0' claiming '/run/udev/links/\\x2fdisk\\x2fby-id\\x2fscsi-36f4ee080160bd5002ab7652100a1691a' creating link '/dev/disk/by-id/scsi-36f4ee080160bd5002ab7652100a1691a' to '/dev/sda' preserve already existing symlink '/dev/disk/by-id/scsi-36f4ee080160bd5002ab7652100a1691a' to '../../sda' found 'b8:0' claiming '/run/udev/links/\\x2fdisk\\x2fby-id\\x2fscsi-SDELL_PERC_H755N_Front_001a69a1002165b72a00d50b1680e04e' creating link '/dev/disk/by-id/scsi-SDELL_PERC_H755N_Front_001a69a1002165b72a00d50b1680e04e' to '/dev/sda' preserve already existing symlink '/dev/disk/by-id/scsi-SDELL_PERC_H755N_Front_001a69a1002165b72a00d50b1680e04e' to '../../sda' found 'b8:0' claiming '/run/udev/links/\\x2fdisk\\x2fby-id\\x2fwwn-0x6f4ee080160bd5002ab7652100a1691a' creating link '/dev/disk/by-id/wwn-0x6f4ee080160bd5002ab7652100a1691a' to '/dev/sda' preserve already existing symlink '/dev/disk/by-id/wwn-0x6f4ee080160bd5002ab7652100a1691a' to '../../sda' found 'b8:0' claiming '/run/udev/links/\\x2fdisk\\x2fby-path\\x2fpci-0000:c1:00.0-scsi-0:3:111:0' creating link '/dev/disk/by-path/pci-0000:c1:00.0-scsi-0:3:111:0' to '/dev/sda' preserve already existing symlink '/dev/disk/by-path/pci-0000:c1:00.0-scsi-0:3:111:0' to '../../sda' .ID_FS_TYPE_NEW= ACTION=add DEVLINKS=/dev/disk/by-id/scsi-SDELL_PERC_H755N_Front_001a69a1002165b72a00d50b1680e04e /dev/disk/by-id/scsi-36f4ee080160bd5002ab7652100a1691a /dev/disk/by-path/pci-0000:c1:00.0-scsi-0:3:111:0 /dev/disk/by-id/wwn-0x6f4ee080160bd5002ab7652100a1691a DEVNAME=/dev/sda DEVPATH=/devices/pci0000:c0/0000:c0:01.1/0000:c1:00.0/host12/target12:3:111/12:3:111:0/block/sda DEVTYPE=disk ID_BUS=scsi ID_FS_TYPE= ID_MODEL=PERC_H755N_Front ID_MODEL_ENC=PERC\\x20H755N\\x20Front ID_PATH=pci-0000:c1:00.0-scsi-0:3:111:0 ID_PATH_TAG=pci-0000_c1_00_0-scsi-0_3_111_0 ID_REVISION=5.16 ID_SCSI=1 ID_SCSI_INQUIRY=1 ID_SCSI_SERIAL=001a69a1002165b72a00d50b1680e04e ID_SERIAL=36f4ee080160bd5002ab7652100a1691a ID_SERIAL_SHORT=6f4ee080160bd5002ab7652100a1691a ID_TYPE=disk ID_VENDOR=DELL ID_VENDOR_ENC=DELL\\x20\\x20\\x20\\x20 ID_WWN=0x6f4ee080160bd500 ID_WWN_VENDOR_EXTENSION=0x2ab7652100a1691a ID_WWN_WITH_EXTENSION=0x6f4ee080160bd5002ab7652100a1691a MAJOR=8 MINOR=0 SCSI_IDENT_LUN_NAA_REGEXT=6f4ee080160bd5002ab7652100a1691a SCSI_IDENT_SERIAL=001a69a1002165b72a00d50b1680e04e SCSI_MODEL=PERC_H755N_Front SCSI_MODEL_ENC=PERC\\x20H755N\\x20Front SCSI_REVISION=5.16 SCSI_TPGS=0 SCSI_TYPE=disk SCSI_VENDOR=DELL SCSI_VENDOR_ENC=DELL\\x20\\x20\\x20\\x20 SUBSYSTEM=block TAGS=:systemd: USEC_INITIALIZED=3263961497 Unload module index Unloaded link configuration context. nr_requests value [root@r7525 rules.d]# udevadm control --reload-rules && udevadm trigger [root@r7525 rules.d]# !cat cat 99-abj.nr.rules KERNEL==\"sd*\",ACTION==\"add|change\",ATTRS{model}==\"PERC_H755N_Front\",\\ ATTR{queue/nomerges}=\"2\",\\ ATTR{queue/nr_requests}=\"1023\",\\ ATTR{queue/rotational}=\"0\",\\ ATTR{queue/rq_affinity}=\"2\",\\ ATTR{queue/scheduler}=\"none\",\\ ATTR{queue/add_random}=\"0\",ATTR{queue/max_sectors_kb}=\"4096\" KERNEL==\"sd*\",ACTION==\"add|change\",ATTRS{model}==\"Dell Ent NVMe v2\",\\ ATTR{queue/nomerges}=\"2\",\\ ATTR{queue/nr_requests}=\"1023\",\\ ATTR{queue/rotational}=\"0\",\\ ATTR{queue/rq_affinity}=\"2\",\\ ATTR{queue/scheduler}=\"none\",\\ ATTR{queue/add_random}=\"0\",ATTR{queue/max_sectors_kb}=\"4096\" SUBSYSTEM==\"block\",ACTION==\"add|change\",KERNEL==\"nvme*[0-9]n*[0-9]\",ATTRS{model}==\"Dell Ent NVMe v2 AGN RI U.2 1.92TB\",\\ ATTR{queue/nomerges}=\"2\",\\ ATTR{queue/nr_requests}=\"1023\",\\ ATTR{queue/rotational}=\"0\",\\ ATTR{queue/rq_affinity}=\"2\",\\ ATTR{queue/scheduler}=\"none\",\\ ATTR{queue/add_random}=\"0\",\\ ATTR{queue/max_sectors_kb}=\"4096\" SUBSYSTEM==\"block\",ACTION==\"add|change\",KERNEL==\"md*\",\\ ATTR{md/sync_speed_max}=\"2000000\",\\ ATTR{md/group_thread_cnt}=\"64\",\\ ATTR{md/stripe_cache_size}=\"8192\",\\ ATTR{queue/nomerges}=\"2\",\\ ATTR{queue/nr_requests}=\"1023\",\\ ATTR{queue/rotational}=\"0\",\\ ATTR{queue/rq_affinity}=\"2\",\\ ATTR{queue/scheduler}=\"none\",\\ ATTR{queue/add_random}=\"0\", ATTR{queue/max_sectors_kb}=\"4096\" [root@r7525 rules.d]# cat /sys/block/sda/queue/nr_requests 256 Post Reboot [root@r7525 ~]# reboot Using username \"root\". root@192.168.1.60's password: Activate the web console with: systemctl enable --now cockpit.socket Register this system with Red Hat Insights: insights-client --register Create an account or view all your systems at https://red.ht/insights-dashboard Last login: Fri Sep 16 13:06:28 2022 from 10.8.0.6 [root@r7525 ~]# cat /sys/block/sda/queue/nr_requests 5089 [root@r7525 ~]# cat /sys/block/sda/queue/nomerges 2 [root@r7525 ~]# cat /sys/block/sda/queue/rotational 0 [root@r7525 ~]# cat /sys/block/sda/queue/rq_affinity 2 [root@r7525 ~]# cat /sys/block/sda/queue/scheduler [none] mq-deadline kyber bfq [root@r7525 ~]# cat /sys/block/sda/queue/add_random 0 [root@r7525 ~]# udevadm control --reload-rules && udevadm trigger [root@r7525 ~]# cat /sys/block/sda/queue/nr_requests 1023 [root@r7525 ~]# mv /etc/udev/rules.d/99-abj.nr.rules /root [root@r7525 ~]# reboot Using username \"root\". root@192.168.1.60's password: Activate the web console with: systemctl enable --now cockpit.socket Register this system with Red Hat Insights: insights-client --register Create an account or view all your systems at https://red.ht/insights-dashboard Last login: Fri Sep 16 13:41:47 2022 from 10.8.0.6 [root@r7525 ~]# cat /sys/block/sda/queue/nr_requests 256 [root@r7525 ~]# cat /sys/block/sda/queue/nomerges 2 [root@r7525 ~]# cat /sys/block/sda/queue/rq_affinity 1 [root@r7525 ~]# cat /sys/block/sda/queue/scheduler [mq-deadline] kyber bfq none [root@r7525 ~]# cat /sys/block/sda/queue/add_random 0 [root@r7525 ~]# mv /root/99-abj.nr.rules /etc/udev/rules.d/ [root@r7525 ~]# udevadm control --reload-rules && udevadm trigger [root@r7525 ~]# cat /sys/block/sda/queue/nr_requests 1023 [root@r7525 ~]# cat /sys/block/sda/queue/nomerges 2 [root@r7525 ~]# cat /sys/block/sda/queue/rq_affinity 2 [root@r7525 ~]# cat /sys/block/sda/queue/scheduler [none] mq-deadline kyber bfq [root@r7525 ~]# cat /sys/block/sda/queue/add_random 0 [root@r7525 ~]# reboot Using username \"root\". root@192.168.1.60's password: Activate the web console with: systemctl enable --now cockpit.socket Register this system with Red Hat Insights: insights-client --register Create an account or view all your systems at https://red.ht/insights-dashboard Last login: Fri Sep 16 13:47:53 2022 from 10.8.0.6 [root@r7525 ~]# cat /sys/block/sda/queue/nr_requests 5089 [root@r7525 ~]# cat /sys/block/sda/queue/nomerges 2 [root@r7525 ~]# cat /sys/block/sda/queue/rq_affinity 2 [root@r7525 ~]# cat /sys/block/sda/queue/scheduler [none] mq-deadline kyber bfq [root@r7525 ~]# cat /sys/block/sda/queue/add_random 0 [root@r7525 ~]# cat /etc/udev/rules.d/99-abj.nr.rules KERNEL==\"sd*\",ACTION==\"add|change\",ATTRS{model}==\"PERC_H755N_Front\",\\ ATTR{queue/nomerges}=\"2\",\\ ATTR{queue/nr_requests}=\"1023\",\\ ATTR{queue/rotational}=\"0\",\\ ATTR{queue/rq_affinity}=\"2\",\\ ATTR{queue/scheduler}=\"none\",\\ ATTR{queue/add_random}=\"0\",ATTR{queue/max_sectors_kb}=\"4096\" KERNEL==\"sd*\",ACTION==\"add|change\",ATTRS{model}==\"Dell Ent NVMe v2\",\\ ATTR{queue/nomerges}=\"2\",\\ ATTR{queue/nr_requests}=\"1023\",\\ ATTR{queue/rotational}=\"0\",\\ ATTR{queue/rq_affinity}=\"2\",\\ ATTR{queue/scheduler}=\"none\",\\ ATTR{queue/add_random}=\"0\",ATTR{queue/max_sectors_kb}=\"4096\" SUBSYSTEM==\"block\",ACTION==\"add|change\",KERNEL==\"nvme*[0-9]n*[0-9]\",ATTRS{model}==\"Dell Ent NVMe v2 AGN RI U.2 1.92TB\",\\ ATTR{queue/nomerges}=\"2\",\\ ATTR{queue/nr_requests}=\"1023\",\\ ATTR{queue/rotational}=\"0\",\\ ATTR{queue/rq_affinity}=\"2\",\\ ATTR{queue/scheduler}=\"none\",\\ ATTR{queue/add_random}=\"0\",\\ ATTR{queue/max_sectors_kb}=\"4096\" SUBSYSTEM==\"block\",ACTION==\"add|change\",KERNEL==\"md*\",\\ ATTR{md/sync_speed_max}=\"2000000\",\\ ATTR{md/group_thread_cnt}=\"64\",\\ ATTR{md/stripe_cache_size}=\"8192\",\\ ATTR{queue/nomerges}=\"2\",\\ ATTR{queue/nr_requests}=\"1023\",\\ ATTR{queue/rotational}=\"0\",\\ ATTR{queue/rq_affinity}=\"2\",\\ ATTR{queue/scheduler}=\"none\",\\ ATTR{queue/add_random}=\"0\", ATTR{queue/max_sectors_kb}=\"4096\" Post reboot the rules did not take affect nor did they appear to work when I created them and then attempted to force them with udevadm control --reload-rules && udevadm trigger before rebooting. What I had to do: Create the rules Reboot After reboot run udevadm control --reload-rules && udevadm trigger","title":"Writing udev Rules for Dell PERC H755"},{"location":"Writing%20udev%20Rules%20for%20Dell%20PERC%20H755/#writing-udev-rules-for-dell-perc-h755","text":"Writing udev Rules for Dell PERC H755 Operating System Test 1 /etc/udev/rules.d/99-abj.nr.rules udevadm test --action=\"add\" /sys/block/sdc with custom rules Test Without Custom Rules perccli64 /c0 show /opt/MegaRAID/perccli/perccli64 /c1 show lsblk Test 2 99-abj.nr.rules /opt/MegaRAID/perccli/perccli64 /c1 show udevadm test --action=\"add\" /sys/block/sda nr_requests value Post Reboot","title":"Writing udev Rules for Dell PERC H755"},{"location":"Writing%20udev%20Rules%20for%20Dell%20PERC%20H755/#operating-system","text":"[root@r7525 ~]# cat /etc/*-release NAME=\"Red Hat Enterprise Linux\" VERSION=\"8.6 (Ootpa)\" ID=\"rhel\" ID_LIKE=\"fedora\" VERSION_ID=\"8.6\" PLATFORM_ID=\"platform:el8\" PRETTY_NAME=\"Red Hat Enterprise Linux 8.6 (Ootpa)\" ANSI_COLOR=\"0;31\" CPE_NAME=\"cpe:/o:redhat:enterprise_linux:8::baseos\" HOME_URL=\"https://www.redhat.com/\" DOCUMENTATION_URL=\"https://access.redhat.com/documentation/red_hat_enterprise_linux/8/\" BUG_REPORT_URL=\"https://bugzilla.redhat.com/\" REDHAT_BUGZILLA_PRODUCT=\"Red Hat Enterprise Linux 8\" REDHAT_BUGZILLA_PRODUCT_VERSION=8.6 REDHAT_SUPPORT_PRODUCT=\"Red Hat Enterprise Linux\" REDHAT_SUPPORT_PRODUCT_VERSION=\"8.6\" Red Hat Enterprise Linux release 8.6 (Ootpa) Red Hat Enterprise Linux release 8.6 (Ootpa)","title":"Operating System"},{"location":"Writing%20udev%20Rules%20for%20Dell%20PERC%20H755/#test-1","text":"","title":"Test 1"},{"location":"Writing%20udev%20Rules%20for%20Dell%20PERC%20H755/#etcudevrulesd99-abjnrrules","text":"KERNEL==\"sd*\",ACTION==\"add|change\",ATTRS{model}==\"PERC_H755N_Front\",\\ ATTR{queue/nomerges}=\"2\",\\ ATTR{queue/nr_requests}=\"1023\",\\ ATTR{queue/rotational}=\"0\",\\ ATTR{queue/rq_affinity}=\"2\",\\ ATTR{queue/scheduler}=\"none\",\\ ATTR{queue/add_random}=\"0\",ATTR{queue/max_sectors_kb}=\"4096\" KERNEL==\"sd*\",ACTION==\"add|change\",ATTRS{model}==\"Dell Ent NVMe v2\",\\ ATTR{queue/nomerges}=\"2\",\\ ATTR{queue/nr_requests}=\"1023\",\\ ATTR{queue/rotational}=\"0\",\\ ATTR{queue/rq_affinity}=\"2\",\\ ATTR{queue/scheduler}=\"none\",\\ ATTR{queue/add_random}=\"0\",ATTR{queue/max_sectors_kb}=\"4096\" SUBSYSTEM==\"block\",ACTION==\"add|change\",KERNEL==\"nvme*[0-9]n*[0-9]\",ATTRS{model}==\"Dell Ent NVMe v2 AGN RI U.2 1.92TB\",\\ ATTR{queue/nomerges}=\"2\",\\ ATTR{queue/nr_requests}=\"1023\",\\ ATTR{queue/rotational}=\"0\",\\ ATTR{queue/rq_affinity}=\"2\",\\ ATTR{queue/scheduler}=\"none\",\\ ATTR{queue/add_random}=\"0\",\\ ATTR{queue/max_sectors_kb}=\"4096\" SUBSYSTEM==\"block\",ACTION==\"add|change\",KERNEL==\"md*\",\\ ATTR{md/sync_speed_max}=\"2000000\",\\ ATTR{md/group_thread_cnt}=\"64\",\\ ATTR{md/stripe_cache_size}=\"8192\",\\ ATTR{queue/nomerges}=\"2\",\\ ATTR{queue/nr_requests}=\"1023\",\\ ATTR{queue/rotational}=\"0\",\\ ATTR{queue/rq_affinity}=\"2\",\\ ATTR{queue/scheduler}=\"none\",\\ ATTR{queue/add_random}=\"0\", ATTR{queue/max_sectors_kb}=\"4096\"","title":"/etc/udev/rules.d/99-abj.nr.rules"},{"location":"Writing%20udev%20Rules%20for%20Dell%20PERC%20H755/#udevadm-test-actionadd-sysblocksdc-with-custom-rules","text":"[root@r7525 rules.d]# udevadm test --action=\"add\" /sys/block/sdc calling: test version 239 (239-58.el8) This program is for debugging only, it does not run any program specified by a RUN key. It may show incorrect results, because some values may be different, or not available at a simulation run. Load module index Parsed configuration file /usr/lib/systemd/network/99-default.link Created link configuration context. ...SNIP... Reading rules file: /etc/udev/rules.d/99-abj.nr.rules Reading rules file: /usr/lib/udev/rules.d/99-qemu-guest-agent.rules Reading rules file: /usr/lib/udev/rules.d/99-systemd.rules Reading rules file: /usr/lib/udev/rules.d/99-vmware-scsi-udev.rules rules contain 393216 bytes tokens (32768 * 12 bytes), 48613 bytes strings 47970 strings (396524 bytes), 43723 de-duplicated (352159 bytes), 4248 trie nodes used GROUP 6 /usr/lib/udev/rules.d/50-udev-default.rules:59 IMPORT 'scsi_id --export --whitelisted -d /dev/sdc' /usr/lib/udev/rules.d/60-persistent-storage.rules:50 starting 'scsi_id --export --whitelisted -d /dev/sdc' 'scsi_id --export --whitelisted -d /dev/sdc'(out) 'ID_SCSI=1' 'scsi_id --export --whitelisted -d /dev/sdc'(out) 'ID_VENDOR=NVMe' 'scsi_id --export --whitelisted -d /dev/sdc'(out) 'ID_VENDOR_ENC=NVMe\\x20\\x20\\x20\\x20' 'scsi_id --export --whitelisted -d /dev/sdc'(out) 'ID_MODEL=Dell_Ent_NVMe_v2' 'scsi_id --export --whitelisted -d /dev/sdc'(out) 'ID_MODEL_ENC=Dell\\x20Ent\\x20NVMe\\x20v2' 'scsi_id --export --whitelisted -d /dev/sdc'(out) 'ID_REVISION=.2.0' 'scsi_id --export --whitelisted -d /dev/sdc'(out) 'ID_TYPE=disk' 'scsi_id --export --whitelisted -d /dev/sdc'(out) 'ID_SERIAL=236435330529024150025384100000002' 'scsi_id --export --whitelisted -d /dev/sdc'(out) 'ID_SERIAL_SHORT=36435330529024150025384100000002' 'scsi_id --export --whitelisted -d /dev/sdc'(out) 'ID_SCSI_SERIAL=S6CSNA0R902415 ' Process 'scsi_id --export --whitelisted -d /dev/sdc' succeeded. LINK 'disk/by-id/scsi-236435330529024150025384100000002' /usr/lib/udev/rules.d/60-persistent-storage.rules:52 IMPORT builtin 'path_id' /usr/lib/udev/rules.d/60-persistent-storage.rules:73 LINK 'disk/by-path/pci-0000:01:00.0-scsi-0:2:8:0' /usr/lib/udev/rules.d/60-persistent-storage.rules:75 IMPORT builtin 'blkid' /usr/lib/udev/rules.d/60-persistent-storage.rules:90 probe /dev/sdc raid offset=0 IMPORT '/usr/bin/sg_inq --export --inhex=/sys/block/sdc/device/inquiry --raw' /usr/lib/udev/rules.d/61-scsi-sg3_id.rules:17 starting '/usr/bin/sg_inq --export --inhex=/sys/block/sdc/device/inquiry --raw' '/usr/bin/sg_inq --export --inhex=/sys/block/sdc/device/inquiry --raw'(out) 'SCSI_TPGS=0' '/usr/bin/sg_inq --export --inhex=/sys/block/sdc/device/inquiry --raw'(out) 'SCSI_TYPE=disk' '/usr/bin/sg_inq --export --inhex=/sys/block/sdc/device/inquiry --raw'(out) 'SCSI_VENDOR=NVMe' '/usr/bin/sg_inq --export --inhex=/sys/block/sdc/device/inquiry --raw'(out) 'SCSI_VENDOR_ENC=NVMe\\x20\\x20\\x20\\x20' '/usr/bin/sg_inq --export --inhex=/sys/block/sdc/device/inquiry --raw'(out) 'SCSI_MODEL=Dell_Ent_NVMe_v2' '/usr/bin/sg_inq --export --inhex=/sys/block/sdc/device/inquiry --raw'(out) 'SCSI_MODEL_ENC=Dell\\x20Ent\\x20NVMe\\x20v2' '/usr/bin/sg_inq --export --inhex=/sys/block/sdc/device/inquiry --raw'(out) 'SCSI_REVISION=.2.0' Process '/usr/bin/sg_inq --export --inhex=/sys/block/sdc/device/inquiry --raw' succeeded. IMPORT '/usr/bin/sg_inq --export --inhex=/sys/block/sdc/device/vpd_pg80 --raw' /usr/lib/udev/rules.d/61-scsi-sg3_id.rules:27 starting '/usr/bin/sg_inq --export --inhex=/sys/block/sdc/device/vpd_pg80 --raw' '/usr/bin/sg_inq --export --inhex=/sys/block/sdc/device/vpd_pg80 --raw'(out) 'SCSI_IDENT_SERIAL=S6CSNA0R902415' Process '/usr/bin/sg_inq --export --inhex=/sys/block/sdc/device/vpd_pg80 --raw' succeeded. IMPORT '/usr/bin/sg_inq --export --inhex=/sys/block/sdc/device/vpd_pg83 --raw' /usr/lib/udev/rules.d/61-scsi-sg3_id.rules:30 starting '/usr/bin/sg_inq --export --inhex=/sys/block/sdc/device/vpd_pg83 --raw' '/usr/bin/sg_inq --export --inhex=/sys/block/sdc/device/vpd_pg83 --raw'(out) 'SCSI_IDENT_LUN_EUI64=36435330529024150025384100000002' '/usr/bin/sg_inq --export --inhex=/sys/block/sdc/device/vpd_pg83 --raw'(out) 'SCSI_IDENT_PORT_NAA_LOCAL=3bd790c2eff1ebd9' Process '/usr/bin/sg_inq --export --inhex=/sys/block/sdc/device/vpd_pg83 --raw' succeeded. IMPORT 'fc_wwpn_id /devices/pci0000:00/0000:00:01.1/0000:01:00.0/host0/target0:2:8/0:2:8:0/block/sdc' /usr/lib/udev/rules.d/63-fc-wwpn-id.rules:8 starting 'fc_wwpn_id /devices/pci0000:00/0000:00:01.1/0000:01:00.0/host0/target0:2:8/0:2:8:0/block/sdc' Process 'fc_wwpn_id /devices/pci0000:00/0000:00:01.1/0000:01:00.0/host0/target0:2:8/0:2:8:0/block/sdc' succeeded. LINK 'disk/by-id/scsi-SNVMe_Dell_Ent_NVMe_v2_S6CSNA0R902415' /usr/lib/udev/rules.d/63-scsi-sg3_symlink.rules:12 LINK 'disk/by-id/scsi-236435330529024150025384100000002' /usr/lib/udev/rules.d/63-scsi-sg3_symlink.rules:25 ATTR '/sys/devices/pci0000:00/0000:00:01.1/0000:01:00.0/host0/target0:2:8/0:2:8:0/block/sdc/queue/nomerges' writing '2' /etc/udev/rules.d/99-abj.nr.rules:14 ATTR '/sys/devices/pci0000:00/0000:00:01.1/0000:01:00.0/host0/target0:2:8/0:2:8:0/block/sdc/queue/nr_requests' writing '1023' /etc/udev/rules.d/99-abj.nr.rules:14 ATTR '/sys/devices/pci0000:00/0000:00:01.1/0000:01:00.0/host0/target0:2:8/0:2:8:0/block/sdc/queue/rotational' writing '0' /etc/udev/rules.d/99-abj.nr.rules:14 ATTR '/sys/devices/pci0000:00/0000:00:01.1/0000:01:00.0/host0/target0:2:8/0:2:8:0/block/sdc/queue/rq_affinity' writing '2' /etc/udev/rules.d/99-abj.nr.rules:14 ATTR '/sys/devices/pci0000:00/0000:00:01.1/0000:01:00.0/host0/target0:2:8/0:2:8:0/block/sdc/queue/scheduler' writing 'none' /etc/udev/rules.d/99-abj.nr.rules:14 ATTR '/sys/devices/pci0000:00/0000:00:01.1/0000:01:00.0/host0/target0:2:8/0:2:8:0/block/sdc/queue/add_random' writing '0' /etc/udev/rules.d/99-abj.nr.rules:14 ATTR '/sys/devices/pci0000:00/0000:00:01.1/0000:01:00.0/host0/target0:2:8/0:2:8:0/block/sdc/queue/max_sectors_kb' writing '4096' /etc/udev/rules.d/99-abj.nr.rules:14 handling device node '/dev/sdc', devnum=b8:32, mode=0660, uid=0, gid=6 preserve permissions /dev/sdc, 060660, uid=0, gid=6 preserve already existing symlink '/dev/block/8:32' to '../sdc' found 'b8:32' claiming '/run/udev/links/\\x2fdisk\\x2fby-id\\x2fscsi-236435330529024150025384100000002' creating link '/dev/disk/by-id/scsi-236435330529024150025384100000002' to '/dev/sdc' preserve already existing symlink '/dev/disk/by-id/scsi-236435330529024150025384100000002' to '../../sdc' found 'b8:32' claiming '/run/udev/links/\\x2fdisk\\x2fby-id\\x2fscsi-SNVMe_Dell_Ent_NVMe_v2_S6CSNA0R902415' creating link '/dev/disk/by-id/scsi-SNVMe_Dell_Ent_NVMe_v2_S6CSNA0R902415' to '/dev/sdc' preserve already existing symlink '/dev/disk/by-id/scsi-SNVMe_Dell_Ent_NVMe_v2_S6CSNA0R902415' to '../../sdc' found 'b8:32' claiming '/run/udev/links/\\x2fdisk\\x2fby-path\\x2fpci-0000:01:00.0-scsi-0:2:8:0' creating link '/dev/disk/by-path/pci-0000:01:00.0-scsi-0:2:8:0' to '/dev/sdc' preserve already existing symlink '/dev/disk/by-path/pci-0000:01:00.0-scsi-0:2:8:0' to '../../sdc' .ID_FS_TYPE_NEW= ACTION=add DEVLINKS=/dev/disk/by-id/scsi-SNVMe_Dell_Ent_NVMe_v2_S6CSNA0R902415 /dev/disk/by-id/scsi-236435330529024150025384100000002 /dev/disk/by-path/pci-0000:01:00.0-scsi-0:2:8:0 DEVNAME=/dev/sdc DEVPATH=/devices/pci0000:00/0000:00:01.1/0000:01:00.0/host0/target0:2:8/0:2:8:0/block/sdc DEVTYPE=disk ID_BUS=scsi ID_FS_TYPE= ID_MODEL=Dell_Ent_NVMe_v2 ID_MODEL_ENC=Dell\\x20Ent\\x20NVMe\\x20v2 ID_PATH=pci-0000:01:00.0-scsi-0:2:8:0 ID_PATH_TAG=pci-0000_01_00_0-scsi-0_2_8_0 ID_REVISION=.2.0 ID_SCSI=1 ID_SCSI_INQUIRY=1 ID_SCSI_SERIAL=S6CSNA0R902415 ID_SERIAL=236435330529024150025384100000002 ID_SERIAL_SHORT=36435330529024150025384100000002 ID_TYPE=disk ID_VENDOR=NVMe ID_VENDOR_ENC=NVMe\\x20\\x20\\x20\\x20 MAJOR=8 MINOR=32 SCSI_IDENT_LUN_EUI64=36435330529024150025384100000002 SCSI_IDENT_PORT_NAA_LOCAL=3bd790c2eff1ebd9 SCSI_IDENT_SERIAL=S6CSNA0R902415 SCSI_MODEL=Dell_Ent_NVMe_v2 SCSI_MODEL_ENC=Dell\\x20Ent\\x20NVMe\\x20v2 SCSI_REVISION=.2.0 SCSI_TPGS=0 SCSI_TYPE=disk SCSI_VENDOR=NVMe SCSI_VENDOR_ENC=NVMe\\x20\\x20\\x20\\x20 SUBSYSTEM=block TAGS=:systemd: USEC_INITIALIZED=76984220 Unload module index Unloaded link configuration context.","title":"udevadm test --action=\"add\" /sys/block/sdc with custom rules"},{"location":"Writing%20udev%20Rules%20for%20Dell%20PERC%20H755/#test-without-custom-rules","text":"[root@r7525 rules.d]# !udev udevadm test --action=\"add\" /sys/block/sdc calling: test version 239 (239-58.el8) This program is for debugging only, it does not run any program specified by a RUN key. It may show incorrect results, because some values may be different, or not available at a simulation run. Load module index Parsed configuration file /usr/lib/systemd/network/99-default.link Created link configuration context. Reading rules file: /usr/lib/udev/rules.d/01-md-raid-creating.rules Reading rules file: /usr/lib/udev/rules.d/10-dm.rules Reading rules file: /usr/lib/udev/rules.d/11-dm-lvm.rules Reading rules file: /usr/lib/udev/rules.d/11-dm-mpath.rules Reading rules file: /usr/lib/udev/rules.d/11-dm-parts.rules Reading rules file: /usr/lib/udev/rules.d/13-dm-disk.rules Reading rules file: /usr/lib/udev/rules.d/39-usbmuxd.rules Reading rules file: /usr/lib/udev/rules.d/40-elevator.rules Reading rules file: /usr/lib/udev/rules.d/40-libgphoto2.rules /usr/lib/udev/rules.d/40-libgphoto2.rules:11: IMPORT found builtin 'usb_id --export %%p', replacing Reading rules file: /usr/lib/udev/rules.d/40-redhat.rules Reading rules file: /usr/lib/udev/rules.d/40-usb-blacklist.rules Reading rules file: /usr/lib/udev/rules.d/40-usb_modeswitch.rules Reading rules file: /usr/lib/udev/rules.d/50-udev-default.rules Reading rules file: /usr/lib/udev/rules.d/60-alias-kmsg.rules Reading rules file: /usr/lib/udev/rules.d/60-block.rules Reading rules file: /usr/lib/udev/rules.d/60-cdrom_id.rules Reading rules file: /usr/lib/udev/rules.d/60-drm.rules Reading rules file: /usr/lib/udev/rules.d/60-evdev.rules Reading rules file: /usr/lib/udev/rules.d/60-fido-id.rules Reading rules file: /usr/lib/udev/rules.d/60-input-id.rules Reading rules file: /usr/lib/udev/rules.d/60-libfprint-2-autosuspend.rules Reading rules file: /usr/lib/udev/rules.d/60-mdevctl.rules Reading rules file: /usr/lib/udev/rules.d/60-net.rules Reading rules file: /usr/lib/udev/rules.d/60-persistent-alsa.rules Reading rules file: /usr/lib/udev/rules.d/60-persistent-input.rules Reading rules file: /usr/lib/udev/rules.d/60-persistent-storage-tape.rules Reading rules file: /usr/lib/udev/rules.d/60-persistent-storage.rules Reading rules file: /usr/lib/udev/rules.d/60-persistent-v4l.rules Reading rules file: /usr/lib/udev/rules.d/60-raw.rules Reading rules file: /usr/lib/udev/rules.d/60-rdma-ndd.rules Reading rules file: /usr/lib/udev/rules.d/60-rdma-persistent-naming.rules Reading rules file: /usr/lib/udev/rules.d/60-sensor.rules Reading rules file: /usr/lib/udev/rules.d/60-serial.rules Reading rules file: /usr/lib/udev/rules.d/60-tpm-udev.rules Reading rules file: /usr/lib/udev/rules.d/61-gdm.rules Reading rules file: /usr/lib/udev/rules.d/61-gnome-bluetooth-rfkill.rules Reading rules file: /usr/lib/udev/rules.d/61-gnome-settings-daemon-rfkill.rules Reading rules file: /usr/lib/udev/rules.d/61-scsi-sg3_id.rules Reading rules file: /usr/lib/udev/rules.d/62-multipath.rules Reading rules file: /usr/lib/udev/rules.d/63-fc-wwpn-id.rules Reading rules file: /usr/lib/udev/rules.d/63-md-raid-arrays.rules Reading rules file: /usr/lib/udev/rules.d/63-scsi-sg3_symlink.rules Reading rules file: /usr/lib/udev/rules.d/64-btrfs.rules Reading rules file: /usr/lib/udev/rules.d/64-md-raid-assembly.rules Reading rules file: /usr/lib/udev/rules.d/65-libwacom.rules Reading rules file: /usr/lib/udev/rules.d/65-md-incremental.rules Reading rules file: /usr/lib/udev/rules.d/65-sane-backends.rules Reading rules file: /usr/lib/udev/rules.d/66-kpartx.rules Reading rules file: /usr/lib/udev/rules.d/68-del-part-nodes.rules Reading rules file: /usr/lib/udev/rules.d/69-btattach-bcm.rules Reading rules file: /usr/lib/udev/rules.d/69-cd-sensors.rules Reading rules file: /usr/lib/udev/rules.d/69-dm-lvm-metad.rules Reading rules file: /usr/lib/udev/rules.d/69-libmtp.rules Reading rules file: /usr/lib/udev/rules.d/69-md-clustered-confirm-device.rules Reading rules file: /etc/udev/rules.d/69-vdo-start-by-dev.rules Reading rules file: /usr/lib/udev/rules.d/70-hypervfcopy.rules Reading rules file: /usr/lib/udev/rules.d/70-hypervkvp.rules Reading rules file: /usr/lib/udev/rules.d/70-hypervvss.rules Reading rules file: /usr/lib/udev/rules.d/70-joystick.rules Reading rules file: /usr/lib/udev/rules.d/70-mouse.rules Reading rules file: /usr/lib/udev/rules.d/70-nvmf-autoconnect.rules Reading rules file: /etc/udev/rules.d/70-persistent-ipoib.rules Reading rules file: /usr/lib/udev/rules.d/70-power-switch.rules Reading rules file: /usr/lib/udev/rules.d/70-printers.rules Reading rules file: /usr/lib/udev/rules.d/70-spice-vdagentd.rules Reading rules file: /usr/lib/udev/rules.d/70-touchpad.rules Reading rules file: /usr/lib/udev/rules.d/70-uaccess.rules Reading rules file: /usr/lib/udev/rules.d/70-wacom.rules Reading rules file: /usr/lib/udev/rules.d/71-biosdevname.rules Reading rules file: /usr/lib/udev/rules.d/71-nvmf-iopolicy-netapp.rules Reading rules file: /usr/lib/udev/rules.d/71-prefixdevname.rules Reading rules file: /usr/lib/udev/rules.d/71-seat.rules Reading rules file: /usr/lib/udev/rules.d/73-idrac.rules Reading rules file: /usr/lib/udev/rules.d/73-seat-late.rules Reading rules file: /usr/lib/udev/rules.d/75-net-description.rules Reading rules file: /usr/lib/udev/rules.d/75-probe_mtd.rules Reading rules file: /usr/lib/udev/rules.d/75-rdma-description.rules Reading rules file: /usr/lib/udev/rules.d/77-mm-broadmobi-port-types.rules Reading rules file: /usr/lib/udev/rules.d/77-mm-cinterion-port-types.rules Reading rules file: /usr/lib/udev/rules.d/77-mm-dell-port-types.rules Reading rules file: /usr/lib/udev/rules.d/77-mm-dlink-port-types.rules Reading rules file: /usr/lib/udev/rules.d/77-mm-ericsson-mbm.rules Reading rules file: /usr/lib/udev/rules.d/77-mm-fibocom-port-types.rules Reading rules file: /usr/lib/udev/rules.d/77-mm-foxconn-port-types.rules Reading rules file: /usr/lib/udev/rules.d/77-mm-gosuncn-port-types.rules Reading rules file: /usr/lib/udev/rules.d/77-mm-haier-port-types.rules Reading rules file: /usr/lib/udev/rules.d/77-mm-huawei-net-port-types.rules Reading rules file: /usr/lib/udev/rules.d/77-mm-longcheer-port-types.rules Reading rules file: /usr/lib/udev/rules.d/77-mm-mtk-port-types.rules Reading rules file: /usr/lib/udev/rules.d/77-mm-nokia-port-types.rules Reading rules file: /usr/lib/udev/rules.d/77-mm-quectel-port-types.rules Reading rules file: /usr/lib/udev/rules.d/77-mm-sierra.rules Reading rules file: /usr/lib/udev/rules.d/77-mm-simtech-port-types.rules Reading rules file: /usr/lib/udev/rules.d/77-mm-telit-port-types.rules Reading rules file: /usr/lib/udev/rules.d/77-mm-tplink-port-types.rules Reading rules file: /usr/lib/udev/rules.d/77-mm-ublox-port-types.rules Reading rules file: /usr/lib/udev/rules.d/77-mm-x22x-port-types.rules Reading rules file: /usr/lib/udev/rules.d/77-mm-zte-port-types.rules Reading rules file: /usr/lib/udev/rules.d/78-sound-card.rules Reading rules file: /usr/lib/udev/rules.d/80-drivers.rules Reading rules file: /usr/lib/udev/rules.d/80-iio-sensor-proxy.rules Reading rules file: /usr/lib/udev/rules.d/80-libinput-device-groups.rules Reading rules file: /usr/lib/udev/rules.d/80-mm-candidate.rules Reading rules file: /usr/lib/udev/rules.d/80-net-setup-link.rules Reading rules file: /usr/lib/udev/rules.d/80-udisks2.rules Reading rules file: /usr/lib/udev/rules.d/81-kvm-rhel.rules Reading rules file: /usr/lib/udev/rules.d/84-nm-drivers.rules Reading rules file: /usr/lib/udev/rules.d/85-nm-unmanaged.rules Reading rules file: /usr/lib/udev/rules.d/85-regulatory.rules Reading rules file: /usr/lib/udev/rules.d/90-alsa-restore.rules Reading rules file: /usr/lib/udev/rules.d/90-bolt.rules Reading rules file: /usr/lib/udev/rules.d/90-fwupd-devices.rules Reading rules file: /usr/lib/udev/rules.d/90-iprutils.rules Reading rules file: /usr/lib/udev/rules.d/90-libinput-fuzz-override.rules Reading rules file: /usr/lib/udev/rules.d/90-nm-thunderbolt.rules Reading rules file: /usr/lib/udev/rules.d/90-pulseaudio.rules Reading rules file: /usr/lib/udev/rules.d/90-rdma-hw-modules.rules Reading rules file: /usr/lib/udev/rules.d/90-rdma-ulp-modules.rules Reading rules file: /usr/lib/udev/rules.d/90-rdma-umad.rules Reading rules file: /usr/lib/udev/rules.d/90-vconsole.rules Reading rules file: /usr/lib/udev/rules.d/91-drm-modeset.rules Reading rules file: /usr/lib/udev/rules.d/95-cd-devices.rules Reading rules file: /usr/lib/udev/rules.d/95-dm-notify.rules Reading rules file: /usr/lib/udev/rules.d/95-iSM-usbnic-systemd.rules Reading rules file: /usr/lib/udev/rules.d/95-upower-csr.rules Reading rules file: /usr/lib/udev/rules.d/95-upower-hid.rules Reading rules file: /usr/lib/udev/rules.d/95-upower-wup.rules Reading rules file: /usr/lib/udev/rules.d/98-kexec.rules Reading rules file: /usr/lib/udev/rules.d/98-trace-cmd.rules Reading rules file: /usr/lib/udev/rules.d/99-qemu-guest-agent.rules Reading rules file: /usr/lib/udev/rules.d/99-systemd.rules Reading rules file: /usr/lib/udev/rules.d/99-vmware-scsi-udev.rules rules contain 393216 bytes tokens (32768 * 12 bytes), 48327 bytes strings 47890 strings (395708 bytes), 43659 de-duplicated (351613 bytes), 4232 trie nodes used GROUP 6 /usr/lib/udev/rules.d/50-udev-default.rules:59 IMPORT 'scsi_id --export --whitelisted -d /dev/sdc' /usr/lib/udev/rules.d/60-persistent-storage.rules:50 starting 'scsi_id --export --whitelisted -d /dev/sdc' 'scsi_id --export --whitelisted -d /dev/sdc'(out) 'ID_SCSI=1' 'scsi_id --export --whitelisted -d /dev/sdc'(out) 'ID_VENDOR=NVMe' 'scsi_id --export --whitelisted -d /dev/sdc'(out) 'ID_VENDOR_ENC=NVMe\\x20\\x20\\x20\\x20' 'scsi_id --export --whitelisted -d /dev/sdc'(out) 'ID_MODEL=Dell_Ent_NVMe_v2' 'scsi_id --export --whitelisted -d /dev/sdc'(out) 'ID_MODEL_ENC=Dell\\x20Ent\\x20NVMe\\x20v2' 'scsi_id --export --whitelisted -d /dev/sdc'(out) 'ID_REVISION=.2.0' 'scsi_id --export --whitelisted -d /dev/sdc'(out) 'ID_TYPE=disk' 'scsi_id --export --whitelisted -d /dev/sdc'(out) 'ID_SERIAL=236435330529024150025384100000002' 'scsi_id --export --whitelisted -d /dev/sdc'(out) 'ID_SERIAL_SHORT=36435330529024150025384100000002' 'scsi_id --export --whitelisted -d /dev/sdc'(out) 'ID_SCSI_SERIAL=S6CSNA0R902415 ' Process 'scsi_id --export --whitelisted -d /dev/sdc' succeeded. LINK 'disk/by-id/scsi-236435330529024150025384100000002' /usr/lib/udev/rules.d/60-persistent-storage.rules:52 IMPORT builtin 'path_id' /usr/lib/udev/rules.d/60-persistent-storage.rules:73 LINK 'disk/by-path/pci-0000:01:00.0-scsi-0:2:8:0' /usr/lib/udev/rules.d/60-persistent-storage.rules:75 IMPORT builtin 'blkid' /usr/lib/udev/rules.d/60-persistent-storage.rules:90 probe /dev/sdc raid offset=0 IMPORT '/usr/bin/sg_inq --export --inhex=/sys/block/sdc/device/inquiry --raw' /usr/lib/udev/rules.d/61-scsi-sg3_id.rules:17 starting '/usr/bin/sg_inq --export --inhex=/sys/block/sdc/device/inquiry --raw' '/usr/bin/sg_inq --export --inhex=/sys/block/sdc/device/inquiry --raw'(out) 'SCSI_TPGS=0' '/usr/bin/sg_inq --export --inhex=/sys/block/sdc/device/inquiry --raw'(out) 'SCSI_TYPE=disk' '/usr/bin/sg_inq --export --inhex=/sys/block/sdc/device/inquiry --raw'(out) 'SCSI_VENDOR=NVMe' '/usr/bin/sg_inq --export --inhex=/sys/block/sdc/device/inquiry --raw'(out) 'SCSI_VENDOR_ENC=NVMe\\x20\\x20\\x20\\x20' '/usr/bin/sg_inq --export --inhex=/sys/block/sdc/device/inquiry --raw'(out) 'SCSI_MODEL=Dell_Ent_NVMe_v2' '/usr/bin/sg_inq --export --inhex=/sys/block/sdc/device/inquiry --raw'(out) 'SCSI_MODEL_ENC=Dell\\x20Ent\\x20NVMe\\x20v2' '/usr/bin/sg_inq --export --inhex=/sys/block/sdc/device/inquiry --raw'(out) 'SCSI_REVISION=.2.0' Process '/usr/bin/sg_inq --export --inhex=/sys/block/sdc/device/inquiry --raw' succeeded. IMPORT '/usr/bin/sg_inq --export --inhex=/sys/block/sdc/device/vpd_pg80 --raw' /usr/lib/udev/rules.d/61-scsi-sg3_id.rules:27 starting '/usr/bin/sg_inq --export --inhex=/sys/block/sdc/device/vpd_pg80 --raw' '/usr/bin/sg_inq --export --inhex=/sys/block/sdc/device/vpd_pg80 --raw'(out) 'SCSI_IDENT_SERIAL=S6CSNA0R902415' Process '/usr/bin/sg_inq --export --inhex=/sys/block/sdc/device/vpd_pg80 --raw' succeeded. IMPORT '/usr/bin/sg_inq --export --inhex=/sys/block/sdc/device/vpd_pg83 --raw' /usr/lib/udev/rules.d/61-scsi-sg3_id.rules:30 starting '/usr/bin/sg_inq --export --inhex=/sys/block/sdc/device/vpd_pg83 --raw' '/usr/bin/sg_inq --export --inhex=/sys/block/sdc/device/vpd_pg83 --raw'(out) 'SCSI_IDENT_LUN_EUI64=36435330529024150025384100000002' '/usr/bin/sg_inq --export --inhex=/sys/block/sdc/device/vpd_pg83 --raw'(out) 'SCSI_IDENT_PORT_NAA_LOCAL=3bd790c2eff1ebd9' Process '/usr/bin/sg_inq --export --inhex=/sys/block/sdc/device/vpd_pg83 --raw' succeeded. IMPORT 'fc_wwpn_id /devices/pci0000:00/0000:00:01.1/0000:01:00.0/host0/target0:2:8/0:2:8:0/block/sdc' /usr/lib/udev/rules.d/63-fc-wwpn-id.rules:8 starting 'fc_wwpn_id /devices/pci0000:00/0000:00:01.1/0000:01:00.0/host0/target0:2:8/0:2:8:0/block/sdc' Process 'fc_wwpn_id /devices/pci0000:00/0000:00:01.1/0000:01:00.0/host0/target0:2:8/0:2:8:0/block/sdc' succeeded. LINK 'disk/by-id/scsi-SNVMe_Dell_Ent_NVMe_v2_S6CSNA0R902415' /usr/lib/udev/rules.d/63-scsi-sg3_symlink.rules:12 LINK 'disk/by-id/scsi-236435330529024150025384100000002' /usr/lib/udev/rules.d/63-scsi-sg3_symlink.rules:25 handling device node '/dev/sdc', devnum=b8:32, mode=0660, uid=0, gid=6 preserve permissions /dev/sdc, 060660, uid=0, gid=6 preserve already existing symlink '/dev/block/8:32' to '../sdc' found 'b8:32' claiming '/run/udev/links/\\x2fdisk\\x2fby-id\\x2fscsi-236435330529024150025384100000002' creating link '/dev/disk/by-id/scsi-236435330529024150025384100000002' to '/dev/sdc' preserve already existing symlink '/dev/disk/by-id/scsi-236435330529024150025384100000002' to '../../sdc' found 'b8:32' claiming '/run/udev/links/\\x2fdisk\\x2fby-id\\x2fscsi-SNVMe_Dell_Ent_NVMe_v2_S6CSNA0R902415' creating link '/dev/disk/by-id/scsi-SNVMe_Dell_Ent_NVMe_v2_S6CSNA0R902415' to '/dev/sdc' preserve already existing symlink '/dev/disk/by-id/scsi-SNVMe_Dell_Ent_NVMe_v2_S6CSNA0R902415' to '../../sdc' found 'b8:32' claiming '/run/udev/links/\\x2fdisk\\x2fby-path\\x2fpci-0000:01:00.0-scsi-0:2:8:0' creating link '/dev/disk/by-path/pci-0000:01:00.0-scsi-0:2:8:0' to '/dev/sdc' preserve already existing symlink '/dev/disk/by-path/pci-0000:01:00.0-scsi-0:2:8:0' to '../../sdc' .ID_FS_TYPE_NEW= ACTION=add DEVLINKS=/dev/disk/by-path/pci-0000:01:00.0-scsi-0:2:8:0 /dev/disk/by-id/scsi-SNVMe_Dell_Ent_NVMe_v2_S6CSNA0R902415 /dev/disk/by-id/scsi-236435330529024150025384100000002 DEVNAME=/dev/sdc DEVPATH=/devices/pci0000:00/0000:00:01.1/0000:01:00.0/host0/target0:2:8/0:2:8:0/block/sdc DEVTYPE=disk ID_BUS=scsi ID_FS_TYPE= ID_MODEL=Dell_Ent_NVMe_v2 ID_MODEL_ENC=Dell\\x20Ent\\x20NVMe\\x20v2 ID_PATH=pci-0000:01:00.0-scsi-0:2:8:0 ID_PATH_TAG=pci-0000_01_00_0-scsi-0_2_8_0 ID_REVISION=.2.0 ID_SCSI=1 ID_SCSI_INQUIRY=1 ID_SCSI_SERIAL=S6CSNA0R902415 ID_SERIAL=236435330529024150025384100000002 ID_SERIAL_SHORT=36435330529024150025384100000002 ID_TYPE=disk ID_VENDOR=NVMe ID_VENDOR_ENC=NVMe\\x20\\x20\\x20\\x20 MAJOR=8 MINOR=32 SCSI_IDENT_LUN_EUI64=36435330529024150025384100000002 SCSI_IDENT_PORT_NAA_LOCAL=3bd790c2eff1ebd9 SCSI_IDENT_SERIAL=S6CSNA0R902415 SCSI_MODEL=Dell_Ent_NVMe_v2 SCSI_MODEL_ENC=Dell\\x20Ent\\x20NVMe\\x20v2 SCSI_REVISION=.2.0 SCSI_TPGS=0 SCSI_TYPE=disk SCSI_VENDOR=NVMe SCSI_VENDOR_ENC=NVMe\\x20\\x20\\x20\\x20 SUBSYSTEM=block TAGS=:systemd: USEC_INITIALIZED=76984220 Unload module index Unloaded link configuration context.","title":"Test Without Custom Rules"},{"location":"Writing%20udev%20Rules%20for%20Dell%20PERC%20H755/#perccli64-c0-show","text":"[root@r7525 rules.d]# /opt/MegaRAID/perccli/perccli64 /c0 show Generating detailed summary of the adapter, it may take a while to complete. CLI Version = 007.1623.0000.0000 May 17, 2021 Operating system = Linux 4.18.0-372.9.1.el8.x86_64 Controller = 0 Status = Success Description = None Product Name = PERC H755N Front Serial Number = 07R001E SAS Address = 5f4ee0801601c700 PCI Address = 00:01:00:00 System Time = 09/15/2022 14:05:38 Mfg. Date = 12/10/21 Controller Time = 09/15/2022 18:05:38 FW Package Build = 52.16.1-4405 BIOS Version = 7.16.00.0_0x07100501 FW Version = 5.160.02-3552 Driver Name = megaraid_sas Driver Version = 07.719.03.00-rh1 Current Personality = RAID-Mode Vendor Id = 0x1000 Device Id = 0x10E2 SubVendor Id = 0x1028 SubDevice Id = 0x1AE2 Host Interface = PCI-E Bus Number = 1 Device Number = 0 Function Number = 0 Domain ID = 0 Security Protocol = None JBOD Drives = 2 JBOD LIST : ========= ---------------------------------------------------------------------------------------------------- ID EID:Slt DID State Intf Med Size SeSz Model Vendor Port ---------------------------------------------------------------------------------------------------- 8 252:8 1 Onln NVMe SSD 1.746 TB 512B Dell Ent NVMe v2 AGN RI U.2 1.92TB NVMe 00 x2 9 252:9 0 Onln NVMe SSD 1.746 TB 512B Dell Ent NVMe v2 AGN RI U.2 1.92TB NVMe 00 x2 ---------------------------------------------------------------------------------------------------- ID=JBOD Target ID|EID=Enclosure Device ID|Slt=Slot No|DID=Device ID|Onln=Online Offln=Offline|Intf=Interface|Med=Media Type|SeSz=Sector Size SED=Self Encryptive Drive|PI=Protection Info|Sp=Spun|U=Up|D=Down Physical Drives = 8 PD LIST : ======= ----------------------------------------------------------------------------------------------------- EID:Slt DID State DG Size Intf Med SED PI SeSz Model Sp Type ----------------------------------------------------------------------------------------------------- 252:8 1 Onln - 1.746 TB NVMe SSD N N 512B Dell Ent NVMe v2 AGN RI U.2 1.92TB U JBOD 252:9 0 Onln - 1.746 TB NVMe SSD N N 512B Dell Ent NVMe v2 AGN RI U.2 1.92TB U JBOD 252:10 6 UGood - 1.454 TB NVMe SSD N N 512B Dell Express Flash PM1725b 1.6TB SFF U - 252:11 7 UGood - 13.971 TB NVMe SSD N N 512B Micron_9300_MTFDHAL15T3TDP U - 252:12 5 UGood - 1.454 TB NVMe SSD N N 512B Dell Express Flash PM1725b 1.6TB SFF U - 252:13 4 UGood - 2.910 TB NVMe SSD N N 512B Dell Express Flash NVMe P4610 3.2TB SFF U - 252:14 9 UGood - 2.910 TB NVMe SSD N N 512B Dell Express Flash NVMe P4610 3.2TB SFF U - 252:15 8 UGood - 2.910 TB NVMe SSD N N 512B Dell Express Flash NVMe P4610 3.2TB SFF U - ----------------------------------------------------------------------------------------------------- EID=Enclosure Device ID|Slt=Slot No|DID=Device ID|DG=DriveGroup DHS=Dedicated Hot Spare|UGood=Unconfigured Good|GHS=Global Hotspare UBad=Unconfigured Bad|Sntze=Sanitize|Onln=Online|Offln=Offline|Intf=Interface Med=Media Type|SED=Self Encryptive Drive|PI=Protection Info SeSz=Sector Size|Sp=Spun|U=Up|D=Down|T=Transition|F=Foreign UGUnsp=UGood Unsupported|UGShld=UGood shielded|HSPShld=Hotspare shielded CFShld=Configured shielded|Cpybck=CopyBack|CBShld=Copyback Shielded UBUnsp=UBad Unsupported|Rbld=Rebuild Enclosures = 1 Enclosure LIST : ============== -------------------------------------------------------------------- EID State Slots PD PS Fans TSs Alms SIM Port# ProdID VendorSpecific -------------------------------------------------------------------- 252 OK 8 8 0 0 0 0 0 - BP15G+ -------------------------------------------------------------------- EID=Enclosure Device ID | PD=Physical drive count | PS=Power Supply count TSs=Temperature sensor count | Alms=Alarm count | SIM=SIM Count | ProdID=Product ID BBU_Info : ======== ---------------------------------------------- Model State RetentionTime Temp Mode MfgDate ---------------------------------------------- BBU Optimal 0 hour(s) 27C - 0/00/00 ----------------------------------------------","title":"perccli64 /c0 show"},{"location":"Writing%20udev%20Rules%20for%20Dell%20PERC%20H755/#optmegaraidperccliperccli64-c1-show","text":"[root@r7525 rules.d]# /opt/MegaRAID/perccli/perccli64 /c1 show Generating detailed summary of the adapter, it may take a while to complete. CLI Version = 007.1623.0000.0000 May 17, 2021 Operating system = Linux 4.18.0-372.9.1.el8.x86_64 Controller = 1 Status = Success Description = None Product Name = PERC H755N Front Serial Number = 07R00K2 SAS Address = 5f4ee080160bd500 PCI Address = 00:c1:00:00 System Time = 09/15/2022 14:17:35 Mfg. Date = 12/10/21 Controller Time = 09/15/2022 18:17:35 FW Package Build = 52.16.1-4405 BIOS Version = 7.16.00.0_0x07100501 FW Version = 5.160.02-3552 Driver Name = megaraid_sas Driver Version = 07.719.03.00-rh1 Current Personality = RAID-Mode Vendor Id = 0x1000 Device Id = 0x10E2 SubVendor Id = 0x1028 SubDevice Id = 0x1AE2 Host Interface = PCI-E Bus Number = 193 Device Number = 0 Function Number = 0 Domain ID = 0 Security Protocol = None JBOD Drives = 2 JBOD LIST : ========= ---------------------------------------------------------------------------------------------------- ID EID:Slt DID State Intf Med Size SeSz Model Vendor Port ---------------------------------------------------------------------------------------------------- 0 252:0 1 Onln NVMe SSD 1.746 TB 512B Dell Ent NVMe v2 AGN RI U.2 1.92TB NVMe 00 x2 1 252:1 0 Onln NVMe SSD 1.746 TB 512B Dell Ent NVMe v2 AGN RI U.2 1.92TB NVMe 00 x2 ---------------------------------------------------------------------------------------------------- ID=JBOD Target ID|EID=Enclosure Device ID|Slt=Slot No|DID=Device ID|Onln=Online Offln=Offline|Intf=Interface|Med=Media Type|SeSz=Sector Size SED=Self Encryptive Drive|PI=Protection Info|Sp=Spun|U=Up|D=Down Physical Drives = 8 PD LIST : ======= ---------------------------------------------------------------------------------------------------- EID:Slt DID State DG Size Intf Med SED PI SeSz Model Sp Type ---------------------------------------------------------------------------------------------------- 252:0 1 Onln - 1.746 TB NVMe SSD N N 512B Dell Ent NVMe v2 AGN RI U.2 1.92TB U JBOD 252:1 0 Onln - 1.746 TB NVMe SSD N N 512B Dell Ent NVMe v2 AGN RI U.2 1.92TB U JBOD 252:2 5 UGood - 2.910 TB NVMe SSD N N 512B Dell Express Flash NVMe P4610 3.2TB SFF U - 252:3 8 UGood - 2.910 TB NVMe SSD N N 512B Dell Express Flash NVMe P4610 3.2TB SFF U - 252:4 4 UGood - 2.910 TB NVMe SSD N N 512B Dell Express Flash NVMe P4610 3.2TB SFF U - 252:5 7 UGood - 2.910 TB NVMe SSD N N 512B Dell Express Flash NVMe P4610 3.2TB SFF U - 252:6 6 UGood - 2.910 TB NVMe SSD N N 512B Dell Express Flash NVMe P4610 3.2TB SFF U - 252:7 3 UGood - 2.910 TB NVMe SSD N N 512B Dell Express Flash NVMe P4610 3.2TB SFF U - ---------------------------------------------------------------------------------------------------- EID=Enclosure Device ID|Slt=Slot No|DID=Device ID|DG=DriveGroup DHS=Dedicated Hot Spare|UGood=Unconfigured Good|GHS=Global Hotspare UBad=Unconfigured Bad|Sntze=Sanitize|Onln=Online|Offln=Offline|Intf=Interface Med=Media Type|SED=Self Encryptive Drive|PI=Protection Info SeSz=Sector Size|Sp=Spun|U=Up|D=Down|T=Transition|F=Foreign UGUnsp=UGood Unsupported|UGShld=UGood shielded|HSPShld=Hotspare shielded CFShld=Configured shielded|Cpybck=CopyBack|CBShld=Copyback Shielded UBUnsp=UBad Unsupported|Rbld=Rebuild Enclosures = 1 Enclosure LIST : ============== -------------------------------------------------------------------- EID State Slots PD PS Fans TSs Alms SIM Port# ProdID VendorSpecific -------------------------------------------------------------------- 252 OK 8 8 0 0 0 0 0 - BP15G+ -------------------------------------------------------------------- EID=Enclosure Device ID | PD=Physical drive count | PS=Power Supply count TSs=Temperature sensor count | Alms=Alarm count | SIM=SIM Count | ProdID=Product ID BBU_Info : ======== ---------------------------------------------- Model State RetentionTime Temp Mode MfgDate ---------------------------------------------- BBU Optimal 0 hour(s) 27C - 0/00/00 ----------------------------------------------","title":"/opt/MegaRAID/perccli/perccli64 /c1 show"},{"location":"Writing%20udev%20Rules%20for%20Dell%20PERC%20H755/#lsblk","text":"[root@r7525 rules.d]# lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT sda 8:0 0 223.6G 0 disk \u251c\u2500sda1 8:1 0 600M 0 part /boot/efi \u251c\u2500sda2 8:2 0 1G 0 part /boot \u2514\u2500sda3 8:3 0 222G 0 part \u2514\u2500md127 9:127 0 221.9G 0 raid1 \u251c\u2500boss_drives-root 253:0 0 217.9G 0 lvm / \u2514\u2500boss_drives-swap 253:1 0 4G 0 lvm [SWAP] sdb 8:16 0 223.6G 0 disk \u2514\u2500sdb1 8:17 0 222G 0 part \u2514\u2500md127 9:127 0 221.9G 0 raid1 \u251c\u2500boss_drives-root 253:0 0 217.9G 0 lvm / \u2514\u2500boss_drives-swap 253:1 0 4G 0 lvm [SWAP] sdc 8:32 0 1.8T 0 disk sdd 8:48 0 1.8T 0 disk sde 8:64 0 1.8T 0 disk sdf 8:80 0 1.8T 0 disk","title":"lsblk"},{"location":"Writing%20udev%20Rules%20for%20Dell%20PERC%20H755/#test-2","text":"","title":"Test 2"},{"location":"Writing%20udev%20Rules%20for%20Dell%20PERC%20H755/#99-abjnrrules","text":"[root@r7525 rules.d]# cat 99-abj.nr.rules KERNEL==\"sd*\",ACTION==\"add|change\",ATTRS{model}==\"PERC_H755N_Front\",\\ ATTR{queue/nomerges}=\"2\",\\ ATTR{queue/nr_requests}=\"1023\",\\ ATTR{queue/rotational}=\"0\",\\ ATTR{queue/rq_affinity}=\"2\",\\ ATTR{queue/scheduler}=\"none\",\\ ATTR{queue/add_random}=\"0\",ATTR{queue/max_sectors_kb}=\"4096\" KERNEL==\"sd*\",ACTION==\"add|change\",ATTRS{model}==\"Dell Ent NVMe v2\",\\ ATTR{queue/nomerges}=\"2\",\\ ATTR{queue/nr_requests}=\"1023\",\\ ATTR{queue/rotational}=\"0\",\\ ATTR{queue/rq_affinity}=\"2\",\\ ATTR{queue/scheduler}=\"none\",\\ ATTR{queue/add_random}=\"0\",ATTR{queue/max_sectors_kb}=\"4096\" SUBSYSTEM==\"block\",ACTION==\"add|change\",KERNEL==\"nvme*[0-9]n*[0-9]\",ATTRS{model}==\"Dell Ent NVMe v2 AGN RI U.2 1.92TB\",\\ ATTR{queue/nomerges}=\"2\",\\ ATTR{queue/nr_requests}=\"1023\",\\ ATTR{queue/rotational}=\"0\",\\ ATTR{queue/rq_affinity}=\"2\",\\ ATTR{queue/scheduler}=\"none\",\\ ATTR{queue/add_random}=\"0\",\\ ATTR{queue/max_sectors_kb}=\"4096\" SUBSYSTEM==\"block\",ACTION==\"add|change\",KERNEL==\"md*\",\\ ATTR{md/sync_speed_max}=\"2000000\",\\ ATTR{md/group_thread_cnt}=\"64\",\\ ATTR{md/stripe_cache_size}=\"8192\",\\ ATTR{queue/nomerges}=\"2\",\\ ATTR{queue/nr_requests}=\"1023\",\\ ATTR{queue/rotational}=\"0\",\\ ATTR{queue/rq_affinity}=\"2\",\\ ATTR{queue/scheduler}=\"none\",\\ ATTR{queue/add_random}=\"0\", ATTR{queue/max_sectors_kb}=\"4096\"","title":"99-abj.nr.rules"},{"location":"Writing%20udev%20Rules%20for%20Dell%20PERC%20H755/#optmegaraidperccliperccli64-c1-show_1","text":"[root@r7525 rules.d]# /opt/MegaRAID/perccli/perccli64 /c1 show Generating detailed summary of the adapter, it may take a while to complete. CLI Version = 007.1623.0000.0000 May 17, 2021 Operating system = Linux 4.18.0-372.9.1.el8.x86_64 Controller = 1 Status = Success Description = None Product Name = PERC H755N Front Serial Number = 07R00K2 SAS Address = 5f4ee080160bd500 PCI Address = 00:c1:00:00 System Time = 09/16/2022 12:58:02 Mfg. Date = 12/10/21 Controller Time = 09/16/2022 16:58:02 FW Package Build = 52.16.1-4405 BIOS Version = 7.16.00.0_0x07100501 FW Version = 5.160.02-3552 Driver Name = megaraid_sas Driver Version = 07.719.03.00-rh1 Current Personality = RAID-Mode Vendor Id = 0x1000 Device Id = 0x10E2 SubVendor Id = 0x1028 SubDevice Id = 0x1AE2 Host Interface = PCI-E Bus Number = 193 Device Number = 0 Function Number = 0 Domain ID = 0 Security Protocol = None Drive Groups = 1 TOPOLOGY : ======== --------------------------------------------------------------------------- DG Arr Row EID:Slot DID Type State BT Size PDC PI SED DS3 FSpace TR --------------------------------------------------------------------------- 0 - - - - RAID5 Optl Y 5.820 TB dflt N N dflt N N 0 0 - - - RAID5 Optl Y 5.820 TB dflt N N dflt N N 0 0 0 252:2 5 DRIVE Onln N 2.910 TB dflt N N dflt - N 0 0 1 252:3 8 DRIVE Onln N 2.910 TB dflt N N dflt - N 0 0 2 252:4 4 DRIVE Onln N 2.910 TB dflt N N dflt - N --------------------------------------------------------------------------- DG=Disk Group Index|Arr=Array Index|Row=Row Index|EID=Enclosure Device ID DID=Device ID|Type=Drive Type|Onln=Online|Rbld=Rebuild|Optl=Optimal|Dgrd=Degraded Pdgd=Partially degraded|Offln=Offline|BT=Background Task Active PDC=PD Cache|PI=Protection Info|SED=Self Encrypting Drive|Frgn=Foreign DS3=Dimmer Switch 3|dflt=Default|Msng=Missing|FSpace=Free Space Present TR=Transport Ready Virtual Drives = 1 VD LIST : ======= ------------------------------------------------------------- DG/VD TYPE State Access Consist Cache Cac sCC Size Name ------------------------------------------------------------- 0/239 RAID5 Optl RW No RWTD - OFF 5.820 TB ------------------------------------------------------------- VD=Virtual Drive| DG=Drive Group|Rec=Recovery Cac=CacheCade|OfLn=OffLine|Pdgd=Partially Degraded|Dgrd=Degraded Optl=Optimal|dflt=Default|RO=Read Only|RW=Read Write|HD=Hidden|TRANS=TransportReady B=Blocked|Consist=Consistent|R=Read Ahead Always|NR=No Read Ahead|WB=WriteBack AWB=Always WriteBack|WT=WriteThrough|C=Cached IO|D=Direct IO|sCC=Scheduled Check Consistency JBOD Drives = 2 JBOD LIST : ========= ---------------------------------------------------------------------------------------------------- ID EID:Slt DID State Intf Med Size SeSz Model Vendor Port ---------------------------------------------------------------------------------------------------- 0 252:0 1 Onln NVMe SSD 1.746 TB 512B Dell Ent NVMe v2 AGN RI U.2 1.92TB NVMe 00 x2 1 252:1 0 Onln NVMe SSD 1.746 TB 512B Dell Ent NVMe v2 AGN RI U.2 1.92TB NVMe 00 x2 ---------------------------------------------------------------------------------------------------- ID=JBOD Target ID|EID=Enclosure Device ID|Slt=Slot No|DID=Device ID|Onln=Online Offln=Offline|Intf=Interface|Med=Media Type|SeSz=Sector Size SED=Self Encryptive Drive|PI=Protection Info|Sp=Spun|U=Up|D=Down Physical Drives = 8 PD LIST : ======= ---------------------------------------------------------------------------------------------------- EID:Slt DID State DG Size Intf Med SED PI SeSz Model Sp Type ---------------------------------------------------------------------------------------------------- 252:0 1 Onln - 1.746 TB NVMe SSD N N 512B Dell Ent NVMe v2 AGN RI U.2 1.92TB U JBOD 252:1 0 Onln - 1.746 TB NVMe SSD N N 512B Dell Ent NVMe v2 AGN RI U.2 1.92TB U JBOD 252:2 5 Onln 0 2.910 TB NVMe SSD N N 512B Dell Express Flash NVMe P4610 3.2TB SFF U - 252:3 8 Onln 0 2.910 TB NVMe SSD N N 512B Dell Express Flash NVMe P4610 3.2TB SFF U - 252:4 4 Onln 0 2.910 TB NVMe SSD N N 512B Dell Express Flash NVMe P4610 3.2TB SFF U - 252:5 7 UGood - 2.910 TB NVMe SSD N N 512B Dell Express Flash NVMe P4610 3.2TB SFF U - 252:6 6 UGood - 2.910 TB NVMe SSD N N 512B Dell Express Flash NVMe P4610 3.2TB SFF U - 252:7 3 UGood - 2.910 TB NVMe SSD N N 512B Dell Express Flash NVMe P4610 3.2TB SFF U - ---------------------------------------------------------------------------------------------------- EID=Enclosure Device ID|Slt=Slot No|DID=Device ID|DG=DriveGroup DHS=Dedicated Hot Spare|UGood=Unconfigured Good|GHS=Global Hotspare UBad=Unconfigured Bad|Sntze=Sanitize|Onln=Online|Offln=Offline|Intf=Interface Med=Media Type|SED=Self Encryptive Drive|PI=Protection Info SeSz=Sector Size|Sp=Spun|U=Up|D=Down|T=Transition|F=Foreign UGUnsp=UGood Unsupported|UGShld=UGood shielded|HSPShld=Hotspare shielded CFShld=Configured shielded|Cpybck=CopyBack|CBShld=Copyback Shielded UBUnsp=UBad Unsupported|Rbld=Rebuild Enclosures = 1 Enclosure LIST : ============== -------------------------------------------------------------------- EID State Slots PD PS Fans TSs Alms SIM Port# ProdID VendorSpecific -------------------------------------------------------------------- 252 OK 8 8 0 0 0 0 0 - BP15G+ -------------------------------------------------------------------- EID=Enclosure Device ID | PD=Physical drive count | PS=Power Supply count TSs=Temperature sensor count | Alms=Alarm count | SIM=SIM Count | ProdID=Product ID BBU_Info : ======== ---------------------------------------------- Model State RetentionTime Temp Mode MfgDate ---------------------------------------------- BBU Optimal 0 hour(s) 26C - 0/00/00 ----------------------------------------------","title":"/opt/MegaRAID/perccli/perccli64 /c1 show"},{"location":"Writing%20udev%20Rules%20for%20Dell%20PERC%20H755/#udevadm-test-actionadd-sysblocksda","text":"[root@r7525 rules.d]# udevadm test --action=\"add\" /sys/block/sda calling: test version 239 (239-58.el8) This program is for debugging only, it does not run any program specified by a RUN key. It may show incorrect results, because some values may be different, or not available at a simulation run. Load module index Parsed configuration file /usr/lib/systemd/network/99-default.link Created link configuration context. Reading rules file: /usr/lib/udev/rules.d/01-md-raid-creating.rules Reading rules file: /usr/lib/udev/rules.d/10-dm.rules Reading rules file: /usr/lib/udev/rules.d/11-dm-lvm.rules Reading rules file: /usr/lib/udev/rules.d/11-dm-mpath.rules Reading rules file: /usr/lib/udev/rules.d/11-dm-parts.rules Reading rules file: /usr/lib/udev/rules.d/13-dm-disk.rules Reading rules file: /usr/lib/udev/rules.d/39-usbmuxd.rules Reading rules file: /usr/lib/udev/rules.d/40-elevator.rules Reading rules file: /usr/lib/udev/rules.d/40-libgphoto2.rules /usr/lib/udev/rules.d/40-libgphoto2.rules:11: IMPORT found builtin 'usb_id --export %%p', replacing Reading rules file: /usr/lib/udev/rules.d/40-redhat.rules Reading rules file: /usr/lib/udev/rules.d/40-usb-blacklist.rules Reading rules file: /usr/lib/udev/rules.d/40-usb_modeswitch.rules Reading rules file: /usr/lib/udev/rules.d/50-udev-default.rules Reading rules file: /usr/lib/udev/rules.d/60-alias-kmsg.rules Reading rules file: /usr/lib/udev/rules.d/60-block.rules Reading rules file: /usr/lib/udev/rules.d/60-cdrom_id.rules Reading rules file: /usr/lib/udev/rules.d/60-drm.rules Reading rules file: /usr/lib/udev/rules.d/60-evdev.rules Reading rules file: /usr/lib/udev/rules.d/60-fido-id.rules Reading rules file: /usr/lib/udev/rules.d/60-input-id.rules Reading rules file: /usr/lib/udev/rules.d/60-libfprint-2-autosuspend.rules Reading rules file: /usr/lib/udev/rules.d/60-mdevctl.rules Reading rules file: /usr/lib/udev/rules.d/60-net.rules Reading rules file: /usr/lib/udev/rules.d/60-persistent-alsa.rules Reading rules file: /usr/lib/udev/rules.d/60-persistent-input.rules Reading rules file: /usr/lib/udev/rules.d/60-persistent-storage-tape.rules Reading rules file: /usr/lib/udev/rules.d/60-persistent-storage.rules Reading rules file: /usr/lib/udev/rules.d/60-persistent-v4l.rules Reading rules file: /usr/lib/udev/rules.d/60-raw.rules Reading rules file: /usr/lib/udev/rules.d/60-rdma-ndd.rules Reading rules file: /usr/lib/udev/rules.d/60-rdma-persistent-naming.rules Reading rules file: /usr/lib/udev/rules.d/60-sensor.rules Reading rules file: /usr/lib/udev/rules.d/60-serial.rules Reading rules file: /usr/lib/udev/rules.d/60-tpm-udev.rules Reading rules file: /usr/lib/udev/rules.d/61-gdm.rules Reading rules file: /usr/lib/udev/rules.d/61-gnome-bluetooth-rfkill.rules Reading rules file: /usr/lib/udev/rules.d/61-gnome-settings-daemon-rfkill.rules Reading rules file: /usr/lib/udev/rules.d/61-scsi-sg3_id.rules Reading rules file: /usr/lib/udev/rules.d/62-multipath.rules Reading rules file: /usr/lib/udev/rules.d/63-fc-wwpn-id.rules Reading rules file: /usr/lib/udev/rules.d/63-md-raid-arrays.rules Reading rules file: /usr/lib/udev/rules.d/63-scsi-sg3_symlink.rules Reading rules file: /usr/lib/udev/rules.d/64-btrfs.rules Reading rules file: /usr/lib/udev/rules.d/64-md-raid-assembly.rules Reading rules file: /usr/lib/udev/rules.d/65-libwacom.rules Reading rules file: /usr/lib/udev/rules.d/65-md-incremental.rules Reading rules file: /usr/lib/udev/rules.d/65-sane-backends.rules Reading rules file: /usr/lib/udev/rules.d/66-kpartx.rules Reading rules file: /usr/lib/udev/rules.d/68-del-part-nodes.rules Reading rules file: /usr/lib/udev/rules.d/69-btattach-bcm.rules Reading rules file: /usr/lib/udev/rules.d/69-cd-sensors.rules Reading rules file: /usr/lib/udev/rules.d/69-dm-lvm-metad.rules Reading rules file: /usr/lib/udev/rules.d/69-libmtp.rules Reading rules file: /usr/lib/udev/rules.d/69-md-clustered-confirm-device.rules Reading rules file: /etc/udev/rules.d/69-vdo-start-by-dev.rules Reading rules file: /usr/lib/udev/rules.d/70-hypervfcopy.rules Reading rules file: /usr/lib/udev/rules.d/70-hypervkvp.rules Reading rules file: /usr/lib/udev/rules.d/70-hypervvss.rules Reading rules file: /usr/lib/udev/rules.d/70-joystick.rules Reading rules file: /usr/lib/udev/rules.d/70-mouse.rules Reading rules file: /usr/lib/udev/rules.d/70-nvmf-autoconnect.rules Reading rules file: /etc/udev/rules.d/70-persistent-ipoib.rules Reading rules file: /usr/lib/udev/rules.d/70-power-switch.rules Reading rules file: /usr/lib/udev/rules.d/70-printers.rules Reading rules file: /usr/lib/udev/rules.d/70-spice-vdagentd.rules Reading rules file: /usr/lib/udev/rules.d/70-touchpad.rules Reading rules file: /usr/lib/udev/rules.d/70-uaccess.rules Reading rules file: /usr/lib/udev/rules.d/70-wacom.rules Reading rules file: /usr/lib/udev/rules.d/71-biosdevname.rules Reading rules file: /usr/lib/udev/rules.d/71-nvmf-iopolicy-netapp.rules Reading rules file: /usr/lib/udev/rules.d/71-prefixdevname.rules Reading rules file: /usr/lib/udev/rules.d/71-seat.rules Reading rules file: /usr/lib/udev/rules.d/73-idrac.rules Reading rules file: /usr/lib/udev/rules.d/73-seat-late.rules Reading rules file: /usr/lib/udev/rules.d/75-net-description.rules Reading rules file: /usr/lib/udev/rules.d/75-probe_mtd.rules Reading rules file: /usr/lib/udev/rules.d/75-rdma-description.rules Reading rules file: /usr/lib/udev/rules.d/77-mm-broadmobi-port-types.rules Reading rules file: /usr/lib/udev/rules.d/77-mm-cinterion-port-types.rules Reading rules file: /usr/lib/udev/rules.d/77-mm-dell-port-types.rules Reading rules file: /usr/lib/udev/rules.d/77-mm-dlink-port-types.rules Reading rules file: /usr/lib/udev/rules.d/77-mm-ericsson-mbm.rules Reading rules file: /usr/lib/udev/rules.d/77-mm-fibocom-port-types.rules Reading rules file: /usr/lib/udev/rules.d/77-mm-foxconn-port-types.rules Reading rules file: /usr/lib/udev/rules.d/77-mm-gosuncn-port-types.rules Reading rules file: /usr/lib/udev/rules.d/77-mm-haier-port-types.rules Reading rules file: /usr/lib/udev/rules.d/77-mm-huawei-net-port-types.rules Reading rules file: /usr/lib/udev/rules.d/77-mm-longcheer-port-types.rules Reading rules file: /usr/lib/udev/rules.d/77-mm-mtk-port-types.rules Reading rules file: /usr/lib/udev/rules.d/77-mm-nokia-port-types.rules Reading rules file: /usr/lib/udev/rules.d/77-mm-quectel-port-types.rules Reading rules file: /usr/lib/udev/rules.d/77-mm-sierra.rules Reading rules file: /usr/lib/udev/rules.d/77-mm-simtech-port-types.rules Reading rules file: /usr/lib/udev/rules.d/77-mm-telit-port-types.rules Reading rules file: /usr/lib/udev/rules.d/77-mm-tplink-port-types.rules Reading rules file: /usr/lib/udev/rules.d/77-mm-ublox-port-types.rules Reading rules file: /usr/lib/udev/rules.d/77-mm-x22x-port-types.rules Reading rules file: /usr/lib/udev/rules.d/77-mm-zte-port-types.rules Reading rules file: /usr/lib/udev/rules.d/78-sound-card.rules Reading rules file: /usr/lib/udev/rules.d/80-drivers.rules Reading rules file: /usr/lib/udev/rules.d/80-iio-sensor-proxy.rules Reading rules file: /usr/lib/udev/rules.d/80-libinput-device-groups.rules Reading rules file: /usr/lib/udev/rules.d/80-mm-candidate.rules Reading rules file: /usr/lib/udev/rules.d/80-net-setup-link.rules Reading rules file: /usr/lib/udev/rules.d/80-udisks2.rules Reading rules file: /usr/lib/udev/rules.d/81-kvm-rhel.rules Reading rules file: /usr/lib/udev/rules.d/84-nm-drivers.rules Reading rules file: /usr/lib/udev/rules.d/85-nm-unmanaged.rules Reading rules file: /usr/lib/udev/rules.d/85-regulatory.rules Reading rules file: /usr/lib/udev/rules.d/90-alsa-restore.rules Reading rules file: /usr/lib/udev/rules.d/90-bolt.rules Reading rules file: /usr/lib/udev/rules.d/90-fwupd-devices.rules Reading rules file: /usr/lib/udev/rules.d/90-iprutils.rules Reading rules file: /usr/lib/udev/rules.d/90-libinput-fuzz-override.rules Reading rules file: /usr/lib/udev/rules.d/90-nm-thunderbolt.rules Reading rules file: /usr/lib/udev/rules.d/90-pulseaudio.rules Reading rules file: /usr/lib/udev/rules.d/90-rdma-hw-modules.rules Reading rules file: /usr/lib/udev/rules.d/90-rdma-ulp-modules.rules Reading rules file: /usr/lib/udev/rules.d/90-rdma-umad.rules Reading rules file: /usr/lib/udev/rules.d/90-vconsole.rules Reading rules file: /usr/lib/udev/rules.d/91-drm-modeset.rules Reading rules file: /usr/lib/udev/rules.d/95-cd-devices.rules Reading rules file: /usr/lib/udev/rules.d/95-dm-notify.rules Reading rules file: /usr/lib/udev/rules.d/95-iSM-usbnic-systemd.rules Reading rules file: /usr/lib/udev/rules.d/95-upower-csr.rules Reading rules file: /usr/lib/udev/rules.d/95-upower-hid.rules Reading rules file: /usr/lib/udev/rules.d/95-upower-wup.rules Reading rules file: /usr/lib/udev/rules.d/98-kexec.rules Reading rules file: /usr/lib/udev/rules.d/98-trace-cmd.rules Reading rules file: /etc/udev/rules.d/99-abj.nr.rules Reading rules file: /usr/lib/udev/rules.d/99-qemu-guest-agent.rules Reading rules file: /usr/lib/udev/rules.d/99-systemd.rules Reading rules file: /usr/lib/udev/rules.d/99-vmware-scsi-udev.rules rules contain 393216 bytes tokens (32768 * 12 bytes), 48613 bytes strings 47970 strings (396524 bytes), 43723 de-duplicated (352159 bytes), 4248 trie nodes used GROUP 6 /usr/lib/udev/rules.d/50-udev-default.rules:59 IMPORT 'scsi_id --export --whitelisted -d /dev/sda' /usr/lib/udev/rules.d/60-persistent-storage.rules:50 starting 'scsi_id --export --whitelisted -d /dev/sda' 'scsi_id --export --whitelisted -d /dev/sda'(out) 'ID_SCSI=1' 'scsi_id --export --whitelisted -d /dev/sda'(out) 'ID_VENDOR=DELL' 'scsi_id --export --whitelisted -d /dev/sda'(out) 'ID_VENDOR_ENC=DELL\\x20\\x20\\x20\\x20' 'scsi_id --export --whitelisted -d /dev/sda'(out) 'ID_MODEL=PERC_H755N_Front' 'scsi_id --export --whitelisted -d /dev/sda'(out) 'ID_MODEL_ENC=PERC\\x20H755N\\x20Front' 'scsi_id --export --whitelisted -d /dev/sda'(out) 'ID_REVISION=5.16' 'scsi_id --export --whitelisted -d /dev/sda'(out) 'ID_TYPE=disk' 'scsi_id --export --whitelisted -d /dev/sda'(out) 'ID_SERIAL=36f4ee080160bd5002ab7652100a1691a' 'scsi_id --export --whitelisted -d /dev/sda'(out) 'ID_SERIAL_SHORT=6f4ee080160bd5002ab7652100a1691a' 'scsi_id --export --whitelisted -d /dev/sda'(out) 'ID_WWN=0x6f4ee080160bd500' 'scsi_id --export --whitelisted -d /dev/sda'(out) 'ID_WWN_VENDOR_EXTENSION=0x2ab7652100a1691a' 'scsi_id --export --whitelisted -d /dev/sda'(out) 'ID_WWN_WITH_EXTENSION=0x6f4ee080160bd5002ab7652100a1691a' 'scsi_id --export --whitelisted -d /dev/sda'(out) 'ID_SCSI_SERIAL=001a69a1002165b72a00d50b1680e04e' Process 'scsi_id --export --whitelisted -d /dev/sda' succeeded. LINK 'disk/by-id/scsi-36f4ee080160bd5002ab7652100a1691a' /usr/lib/udev/rules.d/60-persistent-storage.rules:52 IMPORT builtin 'path_id' /usr/lib/udev/rules.d/60-persistent-storage.rules:73 LINK 'disk/by-path/pci-0000:c1:00.0-scsi-0:3:111:0' /usr/lib/udev/rules.d/60-persistent-storage.rules:75 IMPORT builtin 'blkid' /usr/lib/udev/rules.d/60-persistent-storage.rules:90 probe /dev/sda raid offset=0 LINK 'disk/by-id/wwn-0x6f4ee080160bd5002ab7652100a1691a' /usr/lib/udev/rules.d/60-persistent-storage.rules:97 IMPORT '/usr/bin/sg_inq --export --inhex=/sys/block/sda/device/inquiry --raw' /usr/lib/udev/rules.d/61-scsi-sg3_id.rules:17 starting '/usr/bin/sg_inq --export --inhex=/sys/block/sda/device/inquiry --raw' '/usr/bin/sg_inq --export --inhex=/sys/block/sda/device/inquiry --raw'(out) 'SCSI_TPGS=0' '/usr/bin/sg_inq --export --inhex=/sys/block/sda/device/inquiry --raw'(out) 'SCSI_TYPE=disk' '/usr/bin/sg_inq --export --inhex=/sys/block/sda/device/inquiry --raw'(out) 'SCSI_VENDOR=DELL' '/usr/bin/sg_inq --export --inhex=/sys/block/sda/device/inquiry --raw'(out) 'SCSI_VENDOR_ENC=DELL\\x20\\x20\\x20\\x20' '/usr/bin/sg_inq --export --inhex=/sys/block/sda/device/inquiry --raw'(out) 'SCSI_MODEL=PERC_H755N_Front' '/usr/bin/sg_inq --export --inhex=/sys/block/sda/device/inquiry --raw'(out) 'SCSI_MODEL_ENC=PERC\\x20H755N\\x20Front' '/usr/bin/sg_inq --export --inhex=/sys/block/sda/device/inquiry --raw'(out) 'SCSI_REVISION=5.16' Process '/usr/bin/sg_inq --export --inhex=/sys/block/sda/device/inquiry --raw' succeeded. IMPORT '/usr/bin/sg_inq --export --inhex=/sys/block/sda/device/vpd_pg80 --raw' /usr/lib/udev/rules.d/61-scsi-sg3_id.rules:27 starting '/usr/bin/sg_inq --export --inhex=/sys/block/sda/device/vpd_pg80 --raw' '/usr/bin/sg_inq --export --inhex=/sys/block/sda/device/vpd_pg80 --raw'(out) 'SCSI_IDENT_SERIAL=001a69a1002165b72a00d50b1680e04e' Process '/usr/bin/sg_inq --export --inhex=/sys/block/sda/device/vpd_pg80 --raw' succeeded. IMPORT '/usr/bin/sg_inq --export --inhex=/sys/block/sda/device/vpd_pg83 --raw' /usr/lib/udev/rules.d/61-scsi-sg3_id.rules:30 starting '/usr/bin/sg_inq --export --inhex=/sys/block/sda/device/vpd_pg83 --raw' '/usr/bin/sg_inq --export --inhex=/sys/block/sda/device/vpd_pg83 --raw'(out) 'SCSI_IDENT_LUN_NAA_REGEXT=6f4ee080160bd5002ab7652100a1691a' Process '/usr/bin/sg_inq --export --inhex=/sys/block/sda/device/vpd_pg83 --raw' succeeded. IMPORT 'fc_wwpn_id /devices/pci0000:c0/0000:c0:01.1/0000:c1:00.0/host12/target12:3:111/12:3:111:0/block/sda' /usr/lib/udev/rules.d/63-fc-wwpn-id.rules:8 starting 'fc_wwpn_id /devices/pci0000:c0/0000:c0:01.1/0000:c1:00.0/host12/target12:3:111/12:3:111:0/block/sda' Process 'fc_wwpn_id /devices/pci0000:c0/0000:c0:01.1/0000:c1:00.0/host12/target12:3:111/12:3:111:0/block/sda' succeeded. LINK 'disk/by-id/scsi-SDELL_PERC_H755N_Front_001a69a1002165b72a00d50b1680e04e' /usr/lib/udev/rules.d/63-scsi-sg3_symlink.rules:12 LINK 'disk/by-id/scsi-36f4ee080160bd5002ab7652100a1691a' /usr/lib/udev/rules.d/63-scsi-sg3_symlink.rules:16 handling device node '/dev/sda', devnum=b8:0, mode=0660, uid=0, gid=6 preserve permissions /dev/sda, 060660, uid=0, gid=6 preserve already existing symlink '/dev/block/8:0' to '../sda' found 'b8:0' claiming '/run/udev/links/\\x2fdisk\\x2fby-id\\x2fscsi-36f4ee080160bd5002ab7652100a1691a' creating link '/dev/disk/by-id/scsi-36f4ee080160bd5002ab7652100a1691a' to '/dev/sda' preserve already existing symlink '/dev/disk/by-id/scsi-36f4ee080160bd5002ab7652100a1691a' to '../../sda' found 'b8:0' claiming '/run/udev/links/\\x2fdisk\\x2fby-id\\x2fscsi-SDELL_PERC_H755N_Front_001a69a1002165b72a00d50b1680e04e' creating link '/dev/disk/by-id/scsi-SDELL_PERC_H755N_Front_001a69a1002165b72a00d50b1680e04e' to '/dev/sda' preserve already existing symlink '/dev/disk/by-id/scsi-SDELL_PERC_H755N_Front_001a69a1002165b72a00d50b1680e04e' to '../../sda' found 'b8:0' claiming '/run/udev/links/\\x2fdisk\\x2fby-id\\x2fwwn-0x6f4ee080160bd5002ab7652100a1691a' creating link '/dev/disk/by-id/wwn-0x6f4ee080160bd5002ab7652100a1691a' to '/dev/sda' preserve already existing symlink '/dev/disk/by-id/wwn-0x6f4ee080160bd5002ab7652100a1691a' to '../../sda' found 'b8:0' claiming '/run/udev/links/\\x2fdisk\\x2fby-path\\x2fpci-0000:c1:00.0-scsi-0:3:111:0' creating link '/dev/disk/by-path/pci-0000:c1:00.0-scsi-0:3:111:0' to '/dev/sda' preserve already existing symlink '/dev/disk/by-path/pci-0000:c1:00.0-scsi-0:3:111:0' to '../../sda' .ID_FS_TYPE_NEW= ACTION=add DEVLINKS=/dev/disk/by-id/scsi-SDELL_PERC_H755N_Front_001a69a1002165b72a00d50b1680e04e /dev/disk/by-id/scsi-36f4ee080160bd5002ab7652100a1691a /dev/disk/by-path/pci-0000:c1:00.0-scsi-0:3:111:0 /dev/disk/by-id/wwn-0x6f4ee080160bd5002ab7652100a1691a DEVNAME=/dev/sda DEVPATH=/devices/pci0000:c0/0000:c0:01.1/0000:c1:00.0/host12/target12:3:111/12:3:111:0/block/sda DEVTYPE=disk ID_BUS=scsi ID_FS_TYPE= ID_MODEL=PERC_H755N_Front ID_MODEL_ENC=PERC\\x20H755N\\x20Front ID_PATH=pci-0000:c1:00.0-scsi-0:3:111:0 ID_PATH_TAG=pci-0000_c1_00_0-scsi-0_3_111_0 ID_REVISION=5.16 ID_SCSI=1 ID_SCSI_INQUIRY=1 ID_SCSI_SERIAL=001a69a1002165b72a00d50b1680e04e ID_SERIAL=36f4ee080160bd5002ab7652100a1691a ID_SERIAL_SHORT=6f4ee080160bd5002ab7652100a1691a ID_TYPE=disk ID_VENDOR=DELL ID_VENDOR_ENC=DELL\\x20\\x20\\x20\\x20 ID_WWN=0x6f4ee080160bd500 ID_WWN_VENDOR_EXTENSION=0x2ab7652100a1691a ID_WWN_WITH_EXTENSION=0x6f4ee080160bd5002ab7652100a1691a MAJOR=8 MINOR=0 SCSI_IDENT_LUN_NAA_REGEXT=6f4ee080160bd5002ab7652100a1691a SCSI_IDENT_SERIAL=001a69a1002165b72a00d50b1680e04e SCSI_MODEL=PERC_H755N_Front SCSI_MODEL_ENC=PERC\\x20H755N\\x20Front SCSI_REVISION=5.16 SCSI_TPGS=0 SCSI_TYPE=disk SCSI_VENDOR=DELL SCSI_VENDOR_ENC=DELL\\x20\\x20\\x20\\x20 SUBSYSTEM=block TAGS=:systemd: USEC_INITIALIZED=3263961497 Unload module index Unloaded link configuration context.","title":"udevadm test --action=\"add\" /sys/block/sda"},{"location":"Writing%20udev%20Rules%20for%20Dell%20PERC%20H755/#nr_requests-value","text":"[root@r7525 rules.d]# udevadm control --reload-rules && udevadm trigger [root@r7525 rules.d]# !cat cat 99-abj.nr.rules KERNEL==\"sd*\",ACTION==\"add|change\",ATTRS{model}==\"PERC_H755N_Front\",\\ ATTR{queue/nomerges}=\"2\",\\ ATTR{queue/nr_requests}=\"1023\",\\ ATTR{queue/rotational}=\"0\",\\ ATTR{queue/rq_affinity}=\"2\",\\ ATTR{queue/scheduler}=\"none\",\\ ATTR{queue/add_random}=\"0\",ATTR{queue/max_sectors_kb}=\"4096\" KERNEL==\"sd*\",ACTION==\"add|change\",ATTRS{model}==\"Dell Ent NVMe v2\",\\ ATTR{queue/nomerges}=\"2\",\\ ATTR{queue/nr_requests}=\"1023\",\\ ATTR{queue/rotational}=\"0\",\\ ATTR{queue/rq_affinity}=\"2\",\\ ATTR{queue/scheduler}=\"none\",\\ ATTR{queue/add_random}=\"0\",ATTR{queue/max_sectors_kb}=\"4096\" SUBSYSTEM==\"block\",ACTION==\"add|change\",KERNEL==\"nvme*[0-9]n*[0-9]\",ATTRS{model}==\"Dell Ent NVMe v2 AGN RI U.2 1.92TB\",\\ ATTR{queue/nomerges}=\"2\",\\ ATTR{queue/nr_requests}=\"1023\",\\ ATTR{queue/rotational}=\"0\",\\ ATTR{queue/rq_affinity}=\"2\",\\ ATTR{queue/scheduler}=\"none\",\\ ATTR{queue/add_random}=\"0\",\\ ATTR{queue/max_sectors_kb}=\"4096\" SUBSYSTEM==\"block\",ACTION==\"add|change\",KERNEL==\"md*\",\\ ATTR{md/sync_speed_max}=\"2000000\",\\ ATTR{md/group_thread_cnt}=\"64\",\\ ATTR{md/stripe_cache_size}=\"8192\",\\ ATTR{queue/nomerges}=\"2\",\\ ATTR{queue/nr_requests}=\"1023\",\\ ATTR{queue/rotational}=\"0\",\\ ATTR{queue/rq_affinity}=\"2\",\\ ATTR{queue/scheduler}=\"none\",\\ ATTR{queue/add_random}=\"0\", ATTR{queue/max_sectors_kb}=\"4096\" [root@r7525 rules.d]# cat /sys/block/sda/queue/nr_requests 256","title":"nr_requests value"},{"location":"Writing%20udev%20Rules%20for%20Dell%20PERC%20H755/#post-reboot","text":"[root@r7525 ~]# reboot Using username \"root\". root@192.168.1.60's password: Activate the web console with: systemctl enable --now cockpit.socket Register this system with Red Hat Insights: insights-client --register Create an account or view all your systems at https://red.ht/insights-dashboard Last login: Fri Sep 16 13:06:28 2022 from 10.8.0.6 [root@r7525 ~]# cat /sys/block/sda/queue/nr_requests 5089 [root@r7525 ~]# cat /sys/block/sda/queue/nomerges 2 [root@r7525 ~]# cat /sys/block/sda/queue/rotational 0 [root@r7525 ~]# cat /sys/block/sda/queue/rq_affinity 2 [root@r7525 ~]# cat /sys/block/sda/queue/scheduler [none] mq-deadline kyber bfq [root@r7525 ~]# cat /sys/block/sda/queue/add_random 0 [root@r7525 ~]# udevadm control --reload-rules && udevadm trigger [root@r7525 ~]# cat /sys/block/sda/queue/nr_requests 1023 [root@r7525 ~]# mv /etc/udev/rules.d/99-abj.nr.rules /root [root@r7525 ~]# reboot Using username \"root\". root@192.168.1.60's password: Activate the web console with: systemctl enable --now cockpit.socket Register this system with Red Hat Insights: insights-client --register Create an account or view all your systems at https://red.ht/insights-dashboard Last login: Fri Sep 16 13:41:47 2022 from 10.8.0.6 [root@r7525 ~]# cat /sys/block/sda/queue/nr_requests 256 [root@r7525 ~]# cat /sys/block/sda/queue/nomerges 2 [root@r7525 ~]# cat /sys/block/sda/queue/rq_affinity 1 [root@r7525 ~]# cat /sys/block/sda/queue/scheduler [mq-deadline] kyber bfq none [root@r7525 ~]# cat /sys/block/sda/queue/add_random 0 [root@r7525 ~]# mv /root/99-abj.nr.rules /etc/udev/rules.d/ [root@r7525 ~]# udevadm control --reload-rules && udevadm trigger [root@r7525 ~]# cat /sys/block/sda/queue/nr_requests 1023 [root@r7525 ~]# cat /sys/block/sda/queue/nomerges 2 [root@r7525 ~]# cat /sys/block/sda/queue/rq_affinity 2 [root@r7525 ~]# cat /sys/block/sda/queue/scheduler [none] mq-deadline kyber bfq [root@r7525 ~]# cat /sys/block/sda/queue/add_random 0 [root@r7525 ~]# reboot Using username \"root\". root@192.168.1.60's password: Activate the web console with: systemctl enable --now cockpit.socket Register this system with Red Hat Insights: insights-client --register Create an account or view all your systems at https://red.ht/insights-dashboard Last login: Fri Sep 16 13:47:53 2022 from 10.8.0.6 [root@r7525 ~]# cat /sys/block/sda/queue/nr_requests 5089 [root@r7525 ~]# cat /sys/block/sda/queue/nomerges 2 [root@r7525 ~]# cat /sys/block/sda/queue/rq_affinity 2 [root@r7525 ~]# cat /sys/block/sda/queue/scheduler [none] mq-deadline kyber bfq [root@r7525 ~]# cat /sys/block/sda/queue/add_random 0 [root@r7525 ~]# cat /etc/udev/rules.d/99-abj.nr.rules KERNEL==\"sd*\",ACTION==\"add|change\",ATTRS{model}==\"PERC_H755N_Front\",\\ ATTR{queue/nomerges}=\"2\",\\ ATTR{queue/nr_requests}=\"1023\",\\ ATTR{queue/rotational}=\"0\",\\ ATTR{queue/rq_affinity}=\"2\",\\ ATTR{queue/scheduler}=\"none\",\\ ATTR{queue/add_random}=\"0\",ATTR{queue/max_sectors_kb}=\"4096\" KERNEL==\"sd*\",ACTION==\"add|change\",ATTRS{model}==\"Dell Ent NVMe v2\",\\ ATTR{queue/nomerges}=\"2\",\\ ATTR{queue/nr_requests}=\"1023\",\\ ATTR{queue/rotational}=\"0\",\\ ATTR{queue/rq_affinity}=\"2\",\\ ATTR{queue/scheduler}=\"none\",\\ ATTR{queue/add_random}=\"0\",ATTR{queue/max_sectors_kb}=\"4096\" SUBSYSTEM==\"block\",ACTION==\"add|change\",KERNEL==\"nvme*[0-9]n*[0-9]\",ATTRS{model}==\"Dell Ent NVMe v2 AGN RI U.2 1.92TB\",\\ ATTR{queue/nomerges}=\"2\",\\ ATTR{queue/nr_requests}=\"1023\",\\ ATTR{queue/rotational}=\"0\",\\ ATTR{queue/rq_affinity}=\"2\",\\ ATTR{queue/scheduler}=\"none\",\\ ATTR{queue/add_random}=\"0\",\\ ATTR{queue/max_sectors_kb}=\"4096\" SUBSYSTEM==\"block\",ACTION==\"add|change\",KERNEL==\"md*\",\\ ATTR{md/sync_speed_max}=\"2000000\",\\ ATTR{md/group_thread_cnt}=\"64\",\\ ATTR{md/stripe_cache_size}=\"8192\",\\ ATTR{queue/nomerges}=\"2\",\\ ATTR{queue/nr_requests}=\"1023\",\\ ATTR{queue/rotational}=\"0\",\\ ATTR{queue/rq_affinity}=\"2\",\\ ATTR{queue/scheduler}=\"none\",\\ ATTR{queue/add_random}=\"0\", ATTR{queue/max_sectors_kb}=\"4096\" Post reboot the rules did not take affect nor did they appear to work when I created them and then attempted to force them with udevadm control --reload-rules && udevadm trigger before rebooting. What I had to do: Create the rules Reboot After reboot run udevadm control --reload-rules && udevadm trigger","title":"Post Reboot"},{"location":"esrally%20%28INCOMPLETE%29/","text":"esrally (INCOMPLETE) My Environment CentOS Version Info CentOS Linux release 7.7.1908 (Core) NAME=\"CentOS Linux\" VERSION=\"7 (Core)\" ID=\"centos\" ID_LIKE=\"rhel fedora\" VERSION_ID=\"7\" PRETTY_NAME=\"CentOS Linux 7 (Core)\" ANSI_COLOR=\"0;31\" CPE_NAME=\"cpe:/o:centos:centos:7\" HOME_URL=\"https://www.centos.org/\" BUG_REPORT_URL=\"https://bugs.centos.org/\" CENTOS_MANTISBT_PROJECT=\"CentOS-7\" CENTOS_MANTISBT_PROJECT_VERSION=\"7\" REDHAT_SUPPORT_PRODUCT=\"centos\" REDHAT_SUPPORT_PRODUCT_VERSION=\"7\" CentOS Linux release 7.7.1908 (Core) CentOS Linux release 7.7.1908 (Core) Kernel Version Info [root@elk ~]# uname -a Linux hostname 3.10.0-1062.9.1.el7.x86_64 #1 SMP Fri Dec 6 15:49:49 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux Elasticsearch/Kibana Version Info I used 6.8.5 to set this up. I only used a single node for this setup. Installation Run install yum install -y python3 Download Java 12 from the Java archive . You will unfortunately have to make an account. Add the wandisco repo to install a current version of git. vim /etc/yum.repos.d/wandisco-git.repo . Add the below to the file. [wandisco-git] name=Wandisco GIT Repository baseurl=http://opensource.wandisco.com/centos/7/git/$basearch/ enabled=1 gpgcheck=1 gpgkey=http://opensource.wandisco.com/RPM-GPG-KEY-WANdisco Add their GPG keys with rpm --import http://opensource.wandisco.com/RPM-GPG-KEY-WANdisco Run yum install -y git gcc python3-devel Run pip3 install esrally to install Rally.","title":"esrally (INCOMPLETE)"},{"location":"esrally%20%28INCOMPLETE%29/#esrally-incomplete","text":"","title":"esrally (INCOMPLETE)"},{"location":"esrally%20%28INCOMPLETE%29/#my-environment","text":"","title":"My Environment"},{"location":"esrally%20%28INCOMPLETE%29/#centos-version-info","text":"CentOS Linux release 7.7.1908 (Core) NAME=\"CentOS Linux\" VERSION=\"7 (Core)\" ID=\"centos\" ID_LIKE=\"rhel fedora\" VERSION_ID=\"7\" PRETTY_NAME=\"CentOS Linux 7 (Core)\" ANSI_COLOR=\"0;31\" CPE_NAME=\"cpe:/o:centos:centos:7\" HOME_URL=\"https://www.centos.org/\" BUG_REPORT_URL=\"https://bugs.centos.org/\" CENTOS_MANTISBT_PROJECT=\"CentOS-7\" CENTOS_MANTISBT_PROJECT_VERSION=\"7\" REDHAT_SUPPORT_PRODUCT=\"centos\" REDHAT_SUPPORT_PRODUCT_VERSION=\"7\" CentOS Linux release 7.7.1908 (Core) CentOS Linux release 7.7.1908 (Core)","title":"CentOS Version Info"},{"location":"esrally%20%28INCOMPLETE%29/#kernel-version-info","text":"[root@elk ~]# uname -a Linux hostname 3.10.0-1062.9.1.el7.x86_64 #1 SMP Fri Dec 6 15:49:49 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux","title":"Kernel Version Info"},{"location":"esrally%20%28INCOMPLETE%29/#elasticsearchkibana-version-info","text":"I used 6.8.5 to set this up. I only used a single node for this setup.","title":"Elasticsearch/Kibana Version Info"},{"location":"esrally%20%28INCOMPLETE%29/#installation","text":"Run install yum install -y python3 Download Java 12 from the Java archive . You will unfortunately have to make an account. Add the wandisco repo to install a current version of git. vim /etc/yum.repos.d/wandisco-git.repo . Add the below to the file. [wandisco-git] name=Wandisco GIT Repository baseurl=http://opensource.wandisco.com/centos/7/git/$basearch/ enabled=1 gpgcheck=1 gpgkey=http://opensource.wandisco.com/RPM-GPG-KEY-WANdisco Add their GPG keys with rpm --import http://opensource.wandisco.com/RPM-GPG-KEY-WANdisco Run yum install -y git gcc python3-devel Run pip3 install esrally to install Rally.","title":"Installation"},{"location":"idrac%20with%20LDAP/","text":"idrac with LDAP idrac with LDAP Version Info Setup FreeIPA Helpful Commands Setup idrac YouTube Video of Login Version Info Fedora release 33 (Thirty Three) NAME=Fedora VERSION=\"33 (Workstation Edition)\" ID=fedora VERSION_ID=33 VERSION_CODENAME=\"\" PLATFORM_ID=\"platform:f33\" PRETTY_NAME=\"Fedora 33 (Workstation Edition)\" ANSI_COLOR=\"0;38;2;60;110;180\" LOGO=fedora-logo-icon CPE_NAME=\"cpe:/o:fedoraproject:fedora:33\" HOME_URL=\"https://fedoraproject.org/\" DOCUMENTATION_URL=\"https://docs.fedoraproject.org/en-US/fedora/f33/system-administrators-guide/\" SUPPORT_URL=\"https://fedoraproject.org/wiki/Communicating_and_getting_help\" BUG_REPORT_URL=\"https://bugzilla.redhat.com/\" REDHAT_BUGZILLA_PRODUCT=\"Fedora\" REDHAT_BUGZILLA_PRODUCT_VERSION=33 REDHAT_SUPPORT_PRODUCT=\"Fedora\" REDHAT_SUPPORT_PRODUCT_VERSION=33 PRIVACY_POLICY_URL=\"https://fedoraproject.org/wiki/Legal:PrivacyPolicy\" VARIANT=\"Workstation Edition\" VARIANT_ID=workstation Fedora release 33 (Thirty Three) Fedora release 33 (Thirty Three) Setup FreeIPA Install Fedora Change hostname 1. hostname freeipa.grant.lan && hostnamectl set-hostname freeipa.grant.lan 2.Change in /etc/hostname 3.Configure DNS to return for this hostname. Double check with dig +short freeipa.grant.lan A && dig +short -x 192.168.1.95 Follow RHEL's instructions 1.I used Chapter 5 for primary installation 2.Make sure you add the requested DNS entries at the end Run firewall-cmd --permanent --add-port={80/tcp,443/tcp,389/tcp,636/tcp,88/tcp,464/tcp,88/udp,464/udp,123/udp} && firewall-cmd --reload to allow the appropriate ports Run kinit admin - this allows you to use the command line tools otherwise they'll complain about kerberos. Log into FreeIPA server at https://<your_hostname> . In my case, Windows popped up a username and password prompt. That prompt didn't work - I had to exit it and then log into the webGUI. Add a user other than administrator. Create a new user and new group in the UI and assign the new user to the new group. WARNING I had to actually add the user to a new group. The group could not be admins or it wouldn't work. When I dumped ldapsearch -x -H ldap://localhost -b \"cn=admins,cn=groups,cn=accounts,dc=grant,dc=lan\" -D \"uid=grant,cn=users,cn=accounts,dc=grant,dc=lan\" -w PASSWORD | less my user actually didn't show in the admins group. When I created my own group and checked it worked fine. Helpful Commands To start the IPA service use ipactl start|stop|restart . You can check the status with ipactl status . Test LDAP credentials: ldapwhoami -vvv -h 192.168.1.95 -p 389 -D 'uid=grant,cn=users,cn=accounts,dc=grant,dc=lan' -x -w <PASSWORD> Dump the structure of FreeIPA: ldapsearch -x -H ldap://localhost -b \"cn=accounts,dc=grant,dc=lan\" -D \"uid=grant,cn=users,cn=accounts,dc=grant,dc=lan\" -w <YOUR_USERS_PASSWORD> | less You can also use this to check to see if a user is a member of a group by changing the base path to search (-b). Change it to what you think the FQDN of the group is. The user in question should show up. Setup idrac Go into the idrac and click the directories prompt. I ran without certs I used the following values: Generic LDAP: Enabled Use Distinguished Name to Search Group Membership: Enabled LDAP Server Address: freeipa.grant.lan LDAP Server Port: 636 Bind DN: uid=grant,cn=users,cn=accounts,dc=grant,dc=lan Bind Password: Your password Base DN to Search: cn=accounts,dc=grant,dc=lan Attribute of User Login: uid Attribute of Group Membership: member Click next and for Group DN I used cn=grantgroup,cn=groups,cn=accounts,dc=grant,dc=lan where grantgroup was the name of the user group I added I ran the test with the following results: # Test Results Ping Directory Server Not Run Directory Server DNS Name Passed LDAP connection to the Directory Server Passed User DN existence Passed Certificate Validation Disabled User Authentication Passed User Authorization Passed # Test Log 16:49:14 Initiating Directory Services Settings Diagnostics: 16:49:14 trying LDAP server freeipa.grant.lan:636 16:49:14 Server Address freeipa.grant.lan resolved to 192.168.1.95 16:49:14 connect to 192.168.1.95:636 passed 16:49:14 Connecting to ldaps://[freeipa.grant.lan]:636... 16:49:14 Test user authenticated user=uid=grant,cn=users,cn=accounts,dc=grant,dc=lan host=freeipa.grant.lan 16:49:14 Search command: Bind DN: uid=grant,cn=users,cn=accounts,dc=grant,dc=lan Scope: subtree Base DN: cn=accounts,dc=grant,dc=lan Search filter: (uid=grant) Attribute list: objectClass memberOf dn uid objectCategory defaultNamingContext namingContexts ldapServiceName supportedControl supportedExtension 16:49:14 Connecting to ldaps://[freeipa.grant.lan]:636... 16:49:14 Test user authenticated user=uid=grant,cn=users,cn=accounts,dc=grant,dc=lan host=freeipa.grant.lan 16:49:15 Connecting to ldaps://[freeipa.grant.lan]:636... 16:49:15 Test user authenticated user=uid=grant,cn=users,cn=accounts,dc=grant,dc=lan host=freeipa.grant.lan 16:49:15 Search command: Bind DN: uid=grant,cn=users,cn=accounts,dc=grant,dc=lan Scope: base Base DN: cn=grantgroup,cn=groups,cn=accounts,dc=grant,dc=lan Search filter: (member=uid=grant,cn=users,cn=accounts,dc=grant,dc=lan) Attribute list: objectClass memberOf dn uid objectCategory defaultNamingContext namingContexts ldapServiceName supportedControl supportedExtension 16:49:15 Privileges gained from role group 'cn=grantgroup,cn=groups,cn=accounts,dc=grant,dc=lan': Login Config iDRAC Config User Clear Logs Server Control Virtual Console Virtual Media Test Alerts Diagnostic Command 16:49:15 Test user grant authorized 16:49:15 Cumulative privileges gained: Login Config iDRAC Config User Clear Logs Server Control Virtual Console Virtual Media Test Alerts Diagnostic Command YouTube Video of Login See: https://youtu.be/-fHlHhF9vH4","title":"idrac with LDAP"},{"location":"idrac%20with%20LDAP/#idrac-with-ldap","text":"idrac with LDAP Version Info Setup FreeIPA Helpful Commands Setup idrac YouTube Video of Login","title":"idrac with LDAP"},{"location":"idrac%20with%20LDAP/#version-info","text":"Fedora release 33 (Thirty Three) NAME=Fedora VERSION=\"33 (Workstation Edition)\" ID=fedora VERSION_ID=33 VERSION_CODENAME=\"\" PLATFORM_ID=\"platform:f33\" PRETTY_NAME=\"Fedora 33 (Workstation Edition)\" ANSI_COLOR=\"0;38;2;60;110;180\" LOGO=fedora-logo-icon CPE_NAME=\"cpe:/o:fedoraproject:fedora:33\" HOME_URL=\"https://fedoraproject.org/\" DOCUMENTATION_URL=\"https://docs.fedoraproject.org/en-US/fedora/f33/system-administrators-guide/\" SUPPORT_URL=\"https://fedoraproject.org/wiki/Communicating_and_getting_help\" BUG_REPORT_URL=\"https://bugzilla.redhat.com/\" REDHAT_BUGZILLA_PRODUCT=\"Fedora\" REDHAT_BUGZILLA_PRODUCT_VERSION=33 REDHAT_SUPPORT_PRODUCT=\"Fedora\" REDHAT_SUPPORT_PRODUCT_VERSION=33 PRIVACY_POLICY_URL=\"https://fedoraproject.org/wiki/Legal:PrivacyPolicy\" VARIANT=\"Workstation Edition\" VARIANT_ID=workstation Fedora release 33 (Thirty Three) Fedora release 33 (Thirty Three)","title":"Version Info"},{"location":"idrac%20with%20LDAP/#setup-freeipa","text":"Install Fedora Change hostname 1. hostname freeipa.grant.lan && hostnamectl set-hostname freeipa.grant.lan 2.Change in /etc/hostname 3.Configure DNS to return for this hostname. Double check with dig +short freeipa.grant.lan A && dig +short -x 192.168.1.95 Follow RHEL's instructions 1.I used Chapter 5 for primary installation 2.Make sure you add the requested DNS entries at the end Run firewall-cmd --permanent --add-port={80/tcp,443/tcp,389/tcp,636/tcp,88/tcp,464/tcp,88/udp,464/udp,123/udp} && firewall-cmd --reload to allow the appropriate ports Run kinit admin - this allows you to use the command line tools otherwise they'll complain about kerberos. Log into FreeIPA server at https://<your_hostname> . In my case, Windows popped up a username and password prompt. That prompt didn't work - I had to exit it and then log into the webGUI. Add a user other than administrator. Create a new user and new group in the UI and assign the new user to the new group. WARNING I had to actually add the user to a new group. The group could not be admins or it wouldn't work. When I dumped ldapsearch -x -H ldap://localhost -b \"cn=admins,cn=groups,cn=accounts,dc=grant,dc=lan\" -D \"uid=grant,cn=users,cn=accounts,dc=grant,dc=lan\" -w PASSWORD | less my user actually didn't show in the admins group. When I created my own group and checked it worked fine.","title":"Setup FreeIPA"},{"location":"idrac%20with%20LDAP/#helpful-commands","text":"To start the IPA service use ipactl start|stop|restart . You can check the status with ipactl status . Test LDAP credentials: ldapwhoami -vvv -h 192.168.1.95 -p 389 -D 'uid=grant,cn=users,cn=accounts,dc=grant,dc=lan' -x -w <PASSWORD> Dump the structure of FreeIPA: ldapsearch -x -H ldap://localhost -b \"cn=accounts,dc=grant,dc=lan\" -D \"uid=grant,cn=users,cn=accounts,dc=grant,dc=lan\" -w <YOUR_USERS_PASSWORD> | less You can also use this to check to see if a user is a member of a group by changing the base path to search (-b). Change it to what you think the FQDN of the group is. The user in question should show up.","title":"Helpful Commands"},{"location":"idrac%20with%20LDAP/#setup-idrac","text":"Go into the idrac and click the directories prompt. I ran without certs I used the following values: Generic LDAP: Enabled Use Distinguished Name to Search Group Membership: Enabled LDAP Server Address: freeipa.grant.lan LDAP Server Port: 636 Bind DN: uid=grant,cn=users,cn=accounts,dc=grant,dc=lan Bind Password: Your password Base DN to Search: cn=accounts,dc=grant,dc=lan Attribute of User Login: uid Attribute of Group Membership: member Click next and for Group DN I used cn=grantgroup,cn=groups,cn=accounts,dc=grant,dc=lan where grantgroup was the name of the user group I added I ran the test with the following results: # Test Results Ping Directory Server Not Run Directory Server DNS Name Passed LDAP connection to the Directory Server Passed User DN existence Passed Certificate Validation Disabled User Authentication Passed User Authorization Passed # Test Log 16:49:14 Initiating Directory Services Settings Diagnostics: 16:49:14 trying LDAP server freeipa.grant.lan:636 16:49:14 Server Address freeipa.grant.lan resolved to 192.168.1.95 16:49:14 connect to 192.168.1.95:636 passed 16:49:14 Connecting to ldaps://[freeipa.grant.lan]:636... 16:49:14 Test user authenticated user=uid=grant,cn=users,cn=accounts,dc=grant,dc=lan host=freeipa.grant.lan 16:49:14 Search command: Bind DN: uid=grant,cn=users,cn=accounts,dc=grant,dc=lan Scope: subtree Base DN: cn=accounts,dc=grant,dc=lan Search filter: (uid=grant) Attribute list: objectClass memberOf dn uid objectCategory defaultNamingContext namingContexts ldapServiceName supportedControl supportedExtension 16:49:14 Connecting to ldaps://[freeipa.grant.lan]:636... 16:49:14 Test user authenticated user=uid=grant,cn=users,cn=accounts,dc=grant,dc=lan host=freeipa.grant.lan 16:49:15 Connecting to ldaps://[freeipa.grant.lan]:636... 16:49:15 Test user authenticated user=uid=grant,cn=users,cn=accounts,dc=grant,dc=lan host=freeipa.grant.lan 16:49:15 Search command: Bind DN: uid=grant,cn=users,cn=accounts,dc=grant,dc=lan Scope: base Base DN: cn=grantgroup,cn=groups,cn=accounts,dc=grant,dc=lan Search filter: (member=uid=grant,cn=users,cn=accounts,dc=grant,dc=lan) Attribute list: objectClass memberOf dn uid objectCategory defaultNamingContext namingContexts ldapServiceName supportedControl supportedExtension 16:49:15 Privileges gained from role group 'cn=grantgroup,cn=groups,cn=accounts,dc=grant,dc=lan': Login Config iDRAC Config User Clear Logs Server Control Virtual Console Virtual Media Test Alerts Diagnostic Command 16:49:15 Test user grant authorized 16:49:15 Cumulative privileges gained: Login Config iDRAC Config User Clear Logs Server Control Virtual Console Virtual Media Test Alerts Diagnostic Command","title":"Setup idrac"},{"location":"idrac%20with%20LDAP/#youtube-video-of-login","text":"See: https://youtu.be/-fHlHhF9vH4","title":"YouTube Video of Login"},{"location":"idrac%20with%20LDAP/open-LDAP-failed/","text":"Configuring idrac with OpenLDAP Configuring idrac with OpenLDAP Setup OpenLDAP Debugging Configure idrac Stopped Setup OpenLDAP I used osixia's openldap container for testing. I used osixia's phpLDAPadmin container for administration. Add an entry to your DNS server for ldap.granttest.lan podman run -p 389:389 -p 636:636 --name my-openldap-container --env LDAP_TLS=false --env LDAP_LOG_LEVEL=8 --env LDAP_ORGANISATION=\"Grant Test\" --env LDAP_DOMAIN=\"granttest.lan\" --env LDAP_ADMIN_PASSWORD=\"admin\" --detach osixia/openldap:1.5.0 --loglevel debug && podman run -p 6443:443 --env PHPLDAPADMIN_LDAP_HOSTS=ldap.granttest.lan --detach osixia/phpldapadmin:0.9.0 1.Test the container: podman exec my-openldap-container ldapsearch -x -H ldap://localhost -b dc=granttest,dc=lan -D \"cn=admin,dc=granttest,dc=lan\" -w admin . That should output: # extended LDIF # # LDAPv3 # base <dc=example,dc=org> with scope subtree # filter: (objectclass=*) # requesting: ALL # [...] # numResponses: 3 # numEntries: 2 Configure firewall: firewall-cmd --add-port=389/tcp --permanent && firewall-cmd --add-port=636/tcp --permanent && firewall-cmd --add-port=6443/tcp --permanent && firewall-cmd --reload Make sure you can log into https://<YOUR_IP_ADDRESS>:6443 with username cn=admin,dc=granttest,dc=lan and password admin Debugging You can use podman inspect <container_name> and then search for Log to find the location of the logs. Configure idrac I disabled certificates (yes I was lazy) Stopped For whatever reason the container networking never quite cooperated. Even though DNS entries were present for the external IP when either OME or idrac would try to hit it they would both say the LDAP server was unavailable. phpldapadmin worked without issue. I know it's a probably with the networking but decided it wasn't worth diving into so I abandonded this approach and went with a baremetal FreeIPA instance.","title":"Configuring idrac with OpenLDAP"},{"location":"idrac%20with%20LDAP/open-LDAP-failed/#configuring-idrac-with-openldap","text":"Configuring idrac with OpenLDAP Setup OpenLDAP Debugging Configure idrac Stopped","title":"Configuring idrac with OpenLDAP"},{"location":"idrac%20with%20LDAP/open-LDAP-failed/#setup-openldap","text":"I used osixia's openldap container for testing. I used osixia's phpLDAPadmin container for administration. Add an entry to your DNS server for ldap.granttest.lan podman run -p 389:389 -p 636:636 --name my-openldap-container --env LDAP_TLS=false --env LDAP_LOG_LEVEL=8 --env LDAP_ORGANISATION=\"Grant Test\" --env LDAP_DOMAIN=\"granttest.lan\" --env LDAP_ADMIN_PASSWORD=\"admin\" --detach osixia/openldap:1.5.0 --loglevel debug && podman run -p 6443:443 --env PHPLDAPADMIN_LDAP_HOSTS=ldap.granttest.lan --detach osixia/phpldapadmin:0.9.0 1.Test the container: podman exec my-openldap-container ldapsearch -x -H ldap://localhost -b dc=granttest,dc=lan -D \"cn=admin,dc=granttest,dc=lan\" -w admin . That should output: # extended LDIF # # LDAPv3 # base <dc=example,dc=org> with scope subtree # filter: (objectclass=*) # requesting: ALL # [...] # numResponses: 3 # numEntries: 2 Configure firewall: firewall-cmd --add-port=389/tcp --permanent && firewall-cmd --add-port=636/tcp --permanent && firewall-cmd --add-port=6443/tcp --permanent && firewall-cmd --reload Make sure you can log into https://<YOUR_IP_ADDRESS>:6443 with username cn=admin,dc=granttest,dc=lan and password admin","title":"Setup OpenLDAP"},{"location":"idrac%20with%20LDAP/open-LDAP-failed/#debugging","text":"You can use podman inspect <container_name> and then search for Log to find the location of the logs.","title":"Debugging"},{"location":"idrac%20with%20LDAP/open-LDAP-failed/#configure-idrac","text":"I disabled certificates (yes I was lazy)","title":"Configure idrac"},{"location":"idrac%20with%20LDAP/open-LDAP-failed/#stopped","text":"For whatever reason the container networking never quite cooperated. Even though DNS entries were present for the external IP when either OME or idrac would try to hit it they would both say the LDAP server was unavailable. phpldapadmin worked without issue. I know it's a probably with the networking but decided it wasn't worth diving into so I abandonded this approach and went with a baremetal FreeIPA instance.","title":"Stopped"}]}