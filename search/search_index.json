{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Grant Curell's Dell Projects Every time Dell asks me to figure something out, I write it down. Mostly because more often than not someone asks me about it a year later and I have to remember what I did. Questions If you have any questions about something I did, open an issue on the repo at grantcurell.github.io and I'll respond. Additional Files Most of the files I use in the projects are linked from the markdown documentation but often there might be additional files not visible for this site. It may be easier to browse to the actual docs folder and look at the appropriate folder. Disclaimer These are just my personal notes for the random things I test. I'm generally thorough, but there is no guarentee on completeness :-D How to Configure ONIE I ran the network version of the ONIE installation using a web server. Below is what I did to get things installed. We will host the OS installer on our web server and then we will use ONIE to grab it. We will use a DNS record to control the ONIE server location. Install Apache on RHEL or your favorite Linux distro. Make sure you allow HTTP traffic through the firewall Download your operating system of choice and untar it. Upload <YOUR INSTALLER>.bin to the root of your web server. Create a symlink to the installer with ln -s <YOUR INSTALLER>.bin onie-installer . The file must have this name for the installation to work. The switch will use DHCP to acquire an IP address. On the DNS server pointed to by your DHCP configuration, add a record for onie-server and point it at the host running Apache. On a test box, run a DHCP request, ensure you pull the correct DNS server and that the host can resolve onie-server . After you confirm DNS is working, browse to the onie-installer file on your Apache server and make sure you can download it without issue. Warning: It must be able to resolve the hostname onie-server with the FQDN. If onie-server is not immediately resolvable, the install process will not work. ONIE Boot the Switch Connect to the switch over the console port. My configuration was: Baud Rate: 115200 Data Bits: 8 Stop Bits: 1 Parity: None Flow Control: None Connect the ethernet management port of the switch to the same network containing your web server. ONIE will use the management port to establish a connection to the ONIE server. Once the grub menu appears, select ONIE Installer. In my case this was the top option. At this point the ONIE discovery process will commence. It will print each location it attempts to search. It should find the onie-server DNS record and the installation should begin automatically. If this doesn't happen it means there is an issue with the preconfiguration above. Try swapping out the ethernet management cable with a host. Make sure that host pulls DNS/DHCP correctly and is able to download the onie-installer file. Wait for the installation to finish and the switch to reboot. Login with admin/admin. If after logging in you are told you can't enter configuration mode because \"% Error: ZTD is in progress(configuration is locked).\" run ztd cancel Configure Managment Interface on Dell OS10 Do the following to configure a management interface on Dell OS10 OS10# configure terminal OS10(config)# interface mgmt 1/1/1 OS10(conf-if-ma-1/1/1)# ip address 192.168.1.20/24 OS10(conf-if-ma-1/1/1)# <165>1 2019-10-28T19:04:39.385196+00:00 OS10 dn_alm 669 - - Node.1-Unit.1:PRI [event], Dell EMC (OS10) %IP_ADDRESS_ADD: IP Address add is successful. IP 192.168.1.20/24 in VRF:default added successfully OS10(conf-if-ma-1/1/1)# do write memory","title":"Grant Curell's Dell Projects"},{"location":"#grant-curells-dell-projects","text":"Every time Dell asks me to figure something out, I write it down. Mostly because more often than not someone asks me about it a year later and I have to remember what I did.","title":"Grant Curell's Dell Projects"},{"location":"#questions","text":"If you have any questions about something I did, open an issue on the repo at grantcurell.github.io and I'll respond.","title":"Questions"},{"location":"#additional-files","text":"Most of the files I use in the projects are linked from the markdown documentation but often there might be additional files not visible for this site. It may be easier to browse to the actual docs folder and look at the appropriate folder.","title":"Additional Files"},{"location":"#disclaimer","text":"These are just my personal notes for the random things I test. I'm generally thorough, but there is no guarentee on completeness :-D","title":"Disclaimer"},{"location":"#how-to-configure-onie","text":"I ran the network version of the ONIE installation using a web server. Below is what I did to get things installed. We will host the OS installer on our web server and then we will use ONIE to grab it. We will use a DNS record to control the ONIE server location. Install Apache on RHEL or your favorite Linux distro. Make sure you allow HTTP traffic through the firewall Download your operating system of choice and untar it. Upload <YOUR INSTALLER>.bin to the root of your web server. Create a symlink to the installer with ln -s <YOUR INSTALLER>.bin onie-installer . The file must have this name for the installation to work. The switch will use DHCP to acquire an IP address. On the DNS server pointed to by your DHCP configuration, add a record for onie-server and point it at the host running Apache. On a test box, run a DHCP request, ensure you pull the correct DNS server and that the host can resolve onie-server . After you confirm DNS is working, browse to the onie-installer file on your Apache server and make sure you can download it without issue. Warning: It must be able to resolve the hostname onie-server with the FQDN. If onie-server is not immediately resolvable, the install process will not work.","title":"How to Configure ONIE"},{"location":"#onie-boot-the-switch","text":"Connect to the switch over the console port. My configuration was: Baud Rate: 115200 Data Bits: 8 Stop Bits: 1 Parity: None Flow Control: None Connect the ethernet management port of the switch to the same network containing your web server. ONIE will use the management port to establish a connection to the ONIE server. Once the grub menu appears, select ONIE Installer. In my case this was the top option. At this point the ONIE discovery process will commence. It will print each location it attempts to search. It should find the onie-server DNS record and the installation should begin automatically. If this doesn't happen it means there is an issue with the preconfiguration above. Try swapping out the ethernet management cable with a host. Make sure that host pulls DNS/DHCP correctly and is able to download the onie-installer file. Wait for the installation to finish and the switch to reboot. Login with admin/admin. If after logging in you are told you can't enter configuration mode because \"% Error: ZTD is in progress(configuration is locked).\" run ztd cancel","title":"ONIE Boot the Switch"},{"location":"#configure-managment-interface-on-dell-os10","text":"Do the following to configure a management interface on Dell OS10 OS10# configure terminal OS10(config)# interface mgmt 1/1/1 OS10(conf-if-ma-1/1/1)# ip address 192.168.1.20/24 OS10(conf-if-ma-1/1/1)# <165>1 2019-10-28T19:04:39.385196+00:00 OS10 dn_alm 669 - - Node.1-Unit.1:PRI [event], Dell EMC (OS10) %IP_ADDRESS_ADD: IP Address add is successful. IP 192.168.1.20/24 in VRF:default added successfully OS10(conf-if-ma-1/1/1)# do write memory","title":"Configure Managment Interface on Dell OS10"},{"location":"Adding%20Hashing%20to%20OpenSwitch/","text":"Adding Hashing to OpenSwitch This is the first problem: https://github.com/open-switch/opx-tools/issues/27 He says the code should be somewhere here: https://github.com/open-switch/opx-base-model/blob/master/yang-models/dell-base-hash.yang Helpful docs on how yang is processed Helpful forum communication about header files Installing header files for OPX base Run apt install -y libopx-base-model-dev . The headers will be in /usr/include/opx . The metadata files are in ls /usr/lib/x86_64-linux-gnu/ Adding Space / Remote Desktop I wanted more space on my installation so I added a usb drive. Do the following to clear the USB driver and then add space: Configure USB fdisk /dev/sdb d d d w Extend Partition pvcreate /deb/sdb vgextend OPX /dev/sdb lvextend -l +100%FREE /dev/OPX/SYSROOT1 resize2fs /dev/mapper/OPX-SYSROOT1 Remote Desktop sudo apt update sudo apt install xfce4 xfce4-goodies xorg dbus-x11 x11-xserver-utils sudo apt install xrdp sudo adduser xrdp ssl-cert xfce4-session-logout --halt On CentOS 7 yum install -y epel-release yum groupinstall -y \"Xfce\" echo \"xfce4-session\" > ~/.Xclients chmod a+x ~/.Xclients Set Up VM Failed, but should have worked wget http://archive.openswitch.net/vm-tools/lvm chmod +x lvm wget https://dell-networking.bintray.com/opx-images/opx-onie-installer_1.1_amd64.bin wget https://dell-networking.bintray.com/opx-images/onie-kvm_x86_64-r0.iso wget https://archive.openswitch.net/installers/3.2.1/Dell-EMC/PKGS_OPX-3.2.1-installer-x86_64.bin ./lvm create openswitch --iso onie-kvm_x86_64-r0.iso --bin PKGS_OPX-3.2.1-installer-x86_64.bin Building from source Docs git clone https://github.com/opencomputeproject/onie.git cd onie/contrib/build-env/ docker build -t debian:build-env . mkdir --mode=0777 -p opt/src docker run -it -v /opt/src:/home/build/src --privileged --name onie debian:build-env In the container: ./clone-onie cd src/onie/build-config make -j4 MACHINE=kvm_x86_64 all Build an installer See available distrubitions: opx-build/scripts/opx_run opx_rel_pkgasm.py --help After running a build you can run opx-build/scripts/opx_run opx_rel_pkgasm.py --dist unstable -b opx-onie-installer/release_bp/OPX_dell_base.xml to build an installer This looks like it should be the docker build command docker run --rm --name root_root_32994 --privileged -e LOCAL_UID=0 -e LOCAL_GID=0 -v /root:/mnt -v /root/.gitconfig:/home/opx/.gitconfig -v /etc/localtime:/etc/localtime:ro -e ARCH -e DIST -e OPX_RELEASE -e OPX_GIT_TAG -e CUSTOM_SOURCES opxhub/build:latest Investigation into the opx-config-global-switch problem cps_get_oid.py -qua target base-switch/switching-entities/switching-entity The question to ask is - what is the server application for the CPS object? The problem is that in opx-config-global-switch the value lag-hash-fields isn't present in target_attrs on line 214 of opx-config-global-switch The question is what would populate that? I don't think any of what I care about is in NAS L2. NAS L2 handles This repository contains the Layer 2 (L2) component of the network abstraction service (NAS). This handles media access control (MAC) learning, programming spanning-tree protocol (STP) state, mirroring, sFlow, and other switch configurations. I think what I care about is in opx-nas-interface because the operating system is handling the LAG. Description is This repository contains the interface portion of the network abstraction service (NAS). This creates interfaces in the Linux kernel corresponding to the network processor unit (NPU) front panel ports, manages VLAN and LAG configurations, statistics management and control packet handling. Logically there are three components including the LAG: LAG DS, NAS LAG, NDI LAG. See picture. NAS Daemon: The NAS daemon integrates standard Linux network APIs with NPU hardware functionality, and registers and listens to networking (netlink) events. What enum values in the model of dell-base-hash align with what's in opx-config-global-switch in hash fields map. What are all these actions in opx-config-global-switch? Ok - so now we know the thing I want is in base-traffic-hash/entry, but opx-config-global-switch is looking at base-switch/switching-entities/switching-entity . The next question is what populates that? Why are the hashes not in it? The definition for switching-entity is at: https://github.com/open-switch/opx-base-model/blob/abdf66f813b48a3c8e7682361cdacccd0271866d/yang-models/dell-base-switch-element.yang Say what? hash-fields is also defined in base-switch/switching-entities/switching-entity The next question to ask is who has ownership of base-traffic-hash? What about base-switch/switching-entities/switching-entity? The problem seems like it should be there? Why? base-traffic-hash is owned by opx_nas_daemon base-switch/switching-entities/switching-entity is also owned by opx_nas_daemon CPS has a REST service according to: https://github.com/open-switch/opx-cps The connection between YANG and the applications seems to be the CPS API? Applications define objects through (optionally YANG-based) object models. These object models are converted into binary (C accessible) object keys and object attributes that can be used in conjunction with the C-based CPS APIs. Description of cps_model_info: This tool is useful to get all information about CPS objects on the target. It is used to get the attributes of a specific CPS object, or first-level contents of a given YANG path of the CPS object (as defined in the YANG model). IT GIVES THE PROCESS OWNER! Investigating ops-nas-daemon There is a file called base_nas_default_init which defines the mirror port and the flow behaviors. I haven't found anything about other stuff yet. opx-config-global-switch --lag-hash-alg crc works and is owned by opx_nas_daemon - there must be other things it owns beside this. There is a file called hald_init.c . I think what is happening is all the other services fall under the NAS daemon. The code I'm looking for is somewhere else. After following that around for a while it looks like the file I'm really interested in is here https://github.com/open-switch/opx-nas-l2/blob/7e80d3952786f219b8072f1666ff1f16ba353d86/src/switch/nas_hash_cps.cpp . This bubbles up to the L2 init function, which bubbles back up to hald_init.c . dell-base-hash.h gets included in this thing. The YANG for the regular hash algorithm is at https://github.com/open-switch/opx-base-model/blob/abdf66f813b48a3c8e7682361cdacccd0271866d/yang-models/dell-base-switch-element.yang Initial investigation finished opened: https://github.com/open-switch/opx-nas-l2/issues/34 Investigation 2 - Compilation problems https://android.googlesource.com/platform/hardware/broadcom/wlan/+/master/bcmdhd/config/config-bcm.mk - examlpe config-bcm file. Seems to have something to do with broadcom's config. Better example configuration from Broadcom: http://broadcom-switch.github.io/OpenNSL/doc/html/OPENNSL_CUSTOMIZING_OPENNSL.html#OPENNSL_CONFIG_EXAMPLE Configuration properties: http://broadcom-switch.github.io/OpenNSL/doc/html/OPENNSL_CUSTOMIZING_OPENNSL.html#OPENNSL_CONFIG_PROPERTIES More info on config.bcm https://docs.broadcom.com/doc/12378910 Investigation 3 - post compilation value still missing The hash variables seem to be passed by pointer from nas_hash_cps.cpp to nas_ndi_hash.c in line 222. The SAI portion of the hash creation seems to be here which could prove helpful later. In the bug at https://github.com/open-switch/opx-tools/issues/27 where he says obj-type that is referring to the YANG model. There is a leaf called obj-type . Traffic refers to: typedef traffic { type enumeration { enum \"ECMP_NON_IP\" { value 1; description \"ECMP routing: flow of non-IP ethernet frames\"; } enum \"LAG_NON_IP\" { value 2; description \"LAG routing: flow of non-IP ethernet frames\"; } enum \"ECMP_IPV4\" { value 3; description \"ECMP routing: flow of IPv4 packets\"; } enum \"ECMP_IPV4_IN_IPV4\" { value 4; description \"ECMP routing: flow of IPv4 packets with IPv4 packets tunnelled inside them\"; } enum \"ECMP_IPV6\" { value 5; description \"ECMP routing: flow of IPv6 packets\"; } enum \"LAG_IPV4\" { value 6; description \"LAG routing: flow of IPv4 packets\"; } enum \"LAG_IPV4_IN_IPV4\" { value 7; description \"ECMP routing: flow of IPv4 packets with IPv4 packets tunnelled inside them\"; } enum \"LAG_IPV6\" { value 8; description \"LAG routing: traffic flow is identified as IPv6 packets\"; } } description \"Enumeration of different types of traffic routing categories\"; } std-hash-field is a leaf-list with a type of field . field is defined as: typedef field { type enumeration { enum \"src-ip\" { value 1; description \"Traffic flow is identified by the source IP\"; } enum \"dest-ip\" { value 2; description \"Traffic flow is identified by the destination IP\"; } enum \"inner-src-ip\" { value 3; description \"Traffic flow is identified by the source IP of the tunnelled packet\"; } enum \"inner-dst-ip\" { value 4; description \"Traffic flow is identified by the destination IP of the tunnelled packet\"; } enum \"vlan-id\" { value 5; description \"Traffic flow is identified by the VLAN ID in the packet\"; } enum \"ip-protocol\" { value 6; description \"Traffic flow is identified by the IP protocol type(v4/v6)\"; } enum \"ethertype\" { value 7; description \"Traffic flow is identified by the IP protocol ether-type\"; } enum \"l4-src-port\" { value 8; description \"Traffic flow is identified by the source port in the packet\"; } enum \"l4-dest-port\" { value 9; description \"Traffic flow is identified by the destination port in the packet\"; } enum \"src-mac\" { value 10; description \"Traffic flow is identified by the source MAC address\"; } enum \"dest-mac\" { value 11; description \"Traffic flow is identified by the destination MAC address\"; } enum \"in-port\" { value 12; description \"Traffic flow is identified by the front-panel port the packet is received at\"; } } description \"Enumeration of different types of packet fields to check for routing\"; } - cps_get definition: def cps_get(q, obj, attrs={}): resp = [] return resp if cps.get([cps_object.CPSObject(obj, qual=q, data=attrs ).get() ], resp ) else None def cps_set(obj, qual, data): return (cps_utils.CPSTransaction([('set', cps_object.CPSObject(obj, qual=qual, data=data).get())]).commit() ) - cps_set('base-pas/led', 'target', {'entity-type': 3, 'slot': 1, 'name': 'Beacon', 'on': args.state}) - Set syntax: root@OPX:~# cps_set_oid.py -qua target -oper set -attr base-switch/entry/std-hash-field=1,2,8,9,6,5 base-switch/entry base-switch/entry/obj-type=6 - cps_get python syntax: cps_get('target', 'base-traffic-hash/entry', {'base-traffic-hash/entry/obj-type': '2'}) . That would get the object target from qualifier base-switch/entry which has attribute of base-switch/entry/obj-type (which is the key in entry) and we specifically want key 1 which is a type of traffic correspeonding to ECMP_NON_IP pp.pprint(cps_get('target', 'base-traffic-hash/entry', {'base-traffic-hash/entry/obj-type': '6'})) [ { 'data': { 'base-traffic-hash/entry/std-hash-field': [ bytearray(b'\\x01\\x00\\x00\\x00'), bytearray(b'\\x02\\x00\\x00\\x00'), bytearray(b'\\x08\\x00\\x00\\x00'), bytearray(b'\\t\\x00\\x00\\x00'), bytearray(b'\\x06\\x00\\x00\\x00')]}, 'key': '1.257.16842755.16842753.16842754.'}] CORRECT SYNTAX FOR CPS_SET: cps_set('base-traffic-hash/entry', 'target', {'base-traffic-hash/entry/obj-type': '6', 'base-traffic-hash/entry/std-hash-field': [1,2,8,9,6,5]}) Investigation 4 - Where are the hash values? Part of the hash seems to be set in nas_ndi_switch.cpp There is also a unit test for it in sai_hash_unit_test.cpp This code seems to imply that the hashes themselves are implemented in the SAI: static _enum_map _algo_stoy = { {SAI_HASH_ALGORITHM_XOR, BASE_SWITCH_HASH_ALGORITHM_XOR }, {SAI_HASH_ALGORITHM_CRC, BASE_SWITCH_HASH_ALGORITHM_CRC }, {SAI_HASH_ALGORITHM_RANDOM, BASE_SWITCH_HASH_ALGORITHM_RANDOM }, {SAI_HASH_ALGORITHM_CRC_CCITT, BASE_SWITCH_HASH_ALGORITHM_CRC16CC }, {SAI_HASH_ALGORITHM_CRC_32LO, BASE_SWITCH_HASH_ALGORITHM_CRC32LSB }, {SAI_HASH_ALGORITHM_CRC_32HI, BASE_SWITCH_HASH_ALGORITHM_CRC32MSB }, {SAI_HASH_ALGORITHM_CRC_XOR8, BASE_SWITCH_HASH_ALGORITHM_XOR8 }, {SAI_HASH_ALGORITHM_CRC_XOR4, BASE_SWITCH_HASH_ALGORITHM_XOR4 }, {SAI_HASH_ALGORITHM_CRC_XOR2, BASE_SWITCH_HASH_ALGORITHM_XOR2 }, {SAI_HASH_ALGORITHM_CRC_XOR1, BASE_SWITCH_HASH_ALGORITHM_XOR1 }, }; static bool to_sai_type_hash_algo(sai_attribute_t *param ) { return to_sai_type(_algo_stoy,param); } static bool from_sai_type_hash_algo(sai_attribute_t *param ) { return from_sai_type(_algo_stoy,param); } There is a reference to each of these hash algorithms in https://github.com/open-switch/opx-base-model/blob/abdf66f813b48a3c8e7682361cdacccd0271866d/history/dell-base-switch-element.yhist From the SAI unit test I found: set_attr.value.s32 = SAI_HASH_ALGORITHM_XOR; status = switch_api_tbl_get()->set_switch_attribute (switch_id,&set_attr); EXPECT_EQ (SAI_STATUS_SUCCESS, status); This must be where they are setting the hash algorithm and how to set it. But where is this API? How do I find out what the options are? Is symmetric hashing exposed? The definition for the function is (it happens inside one of the unit tests): static inline sai_switch_api_t* switch_api_tbl_get (void) { return p_sai_switch_api_tbl; } This is a pointer to a table. I'm guessing it's a struct. sai_switch_api_t is short for SAI switch API table. It is a pointer to the table with all the API calls. From this I got a hint - it looks like there is a thing called switch_api - the same name is used in OpenSwitch. Based on the inputs from nas_ndi_hash.c there must be a series of SAI files with the functionality I want. #include \"std_error_codes.h\" #include \"std_assert.h\" #include \"nas_ndi_event_logs.h\" #include \"nas_ndi_utils.h\" #include \"dell-base-hash.h\" #include \"sai.h\" #include \"saiswitch.h\" #include \"saihash.h\" #include <stdio.h> #include <stdlib.h> #include <string.h> #include <inttypes.h> Some related resources ECMP Hashing explained Broadcom Paper on Hashing Can I do this with openvswitch This didn't pan out. Broadcom docs Descriptions NAS The NAS manages the high-level NPU abstraction and adaptation, and abstracts and aggregates the core functionality required for networking access at Layer 1 (physical), Layer 2 (VLAN, link aggregation), Layer 3 (routing), ACL, QoS, and network monitoring. The NAS provides adaptation of the low-level switch abstraction provided by the SAI for standard Linux networking APIs and interfaces, and CPS API functionality. The NAS is also responsible for providing packet I/O services using the Linux kernel IP stack (see Network adaptation service for complete information).","title":"Adding Hashing to OpenSwitch"},{"location":"Adding%20Hashing%20to%20OpenSwitch/#adding-hashing-to-openswitch","text":"This is the first problem: https://github.com/open-switch/opx-tools/issues/27 He says the code should be somewhere here: https://github.com/open-switch/opx-base-model/blob/master/yang-models/dell-base-hash.yang Helpful docs on how yang is processed Helpful forum communication about header files","title":"Adding Hashing to OpenSwitch"},{"location":"Adding%20Hashing%20to%20OpenSwitch/#installing-header-files-for-opx-base","text":"Run apt install -y libopx-base-model-dev . The headers will be in /usr/include/opx . The metadata files are in ls /usr/lib/x86_64-linux-gnu/","title":"Installing header files for OPX base"},{"location":"Adding%20Hashing%20to%20OpenSwitch/#adding-space-remote-desktop","text":"I wanted more space on my installation so I added a usb drive. Do the following to clear the USB driver and then add space:","title":"Adding Space / Remote Desktop"},{"location":"Adding%20Hashing%20to%20OpenSwitch/#configure-usb","text":"fdisk /dev/sdb d d d w","title":"Configure USB"},{"location":"Adding%20Hashing%20to%20OpenSwitch/#extend-partition","text":"pvcreate /deb/sdb vgextend OPX /dev/sdb lvextend -l +100%FREE /dev/OPX/SYSROOT1 resize2fs /dev/mapper/OPX-SYSROOT1","title":"Extend Partition"},{"location":"Adding%20Hashing%20to%20OpenSwitch/#remote-desktop","text":"sudo apt update sudo apt install xfce4 xfce4-goodies xorg dbus-x11 x11-xserver-utils sudo apt install xrdp sudo adduser xrdp ssl-cert xfce4-session-logout --halt","title":"Remote Desktop"},{"location":"Adding%20Hashing%20to%20OpenSwitch/#on-centos-7","text":"yum install -y epel-release yum groupinstall -y \"Xfce\" echo \"xfce4-session\" > ~/.Xclients chmod a+x ~/.Xclients","title":"On CentOS 7"},{"location":"Adding%20Hashing%20to%20OpenSwitch/#set-up-vm","text":"","title":"Set Up VM"},{"location":"Adding%20Hashing%20to%20OpenSwitch/#failed-but-should-have-worked","text":"wget http://archive.openswitch.net/vm-tools/lvm chmod +x lvm wget https://dell-networking.bintray.com/opx-images/opx-onie-installer_1.1_amd64.bin wget https://dell-networking.bintray.com/opx-images/onie-kvm_x86_64-r0.iso wget https://archive.openswitch.net/installers/3.2.1/Dell-EMC/PKGS_OPX-3.2.1-installer-x86_64.bin ./lvm create openswitch --iso onie-kvm_x86_64-r0.iso --bin PKGS_OPX-3.2.1-installer-x86_64.bin","title":"Failed, but should have worked"},{"location":"Adding%20Hashing%20to%20OpenSwitch/#building-from-source","text":"Docs git clone https://github.com/opencomputeproject/onie.git cd onie/contrib/build-env/ docker build -t debian:build-env . mkdir --mode=0777 -p opt/src docker run -it -v /opt/src:/home/build/src --privileged --name onie debian:build-env In the container: ./clone-onie cd src/onie/build-config make -j4 MACHINE=kvm_x86_64 all","title":"Building from source"},{"location":"Adding%20Hashing%20to%20OpenSwitch/#build-an-installer","text":"See available distrubitions: opx-build/scripts/opx_run opx_rel_pkgasm.py --help After running a build you can run opx-build/scripts/opx_run opx_rel_pkgasm.py --dist unstable -b opx-onie-installer/release_bp/OPX_dell_base.xml to build an installer This looks like it should be the docker build command docker run --rm --name root_root_32994 --privileged -e LOCAL_UID=0 -e LOCAL_GID=0 -v /root:/mnt -v /root/.gitconfig:/home/opx/.gitconfig -v /etc/localtime:/etc/localtime:ro -e ARCH -e DIST -e OPX_RELEASE -e OPX_GIT_TAG -e CUSTOM_SOURCES opxhub/build:latest","title":"Build an installer"},{"location":"Adding%20Hashing%20to%20OpenSwitch/#investigation-into-the-opx-config-global-switch-problem","text":"cps_get_oid.py -qua target base-switch/switching-entities/switching-entity The question to ask is - what is the server application for the CPS object? The problem is that in opx-config-global-switch the value lag-hash-fields isn't present in target_attrs on line 214 of opx-config-global-switch The question is what would populate that? I don't think any of what I care about is in NAS L2. NAS L2 handles This repository contains the Layer 2 (L2) component of the network abstraction service (NAS). This handles media access control (MAC) learning, programming spanning-tree protocol (STP) state, mirroring, sFlow, and other switch configurations. I think what I care about is in opx-nas-interface because the operating system is handling the LAG. Description is This repository contains the interface portion of the network abstraction service (NAS). This creates interfaces in the Linux kernel corresponding to the network processor unit (NPU) front panel ports, manages VLAN and LAG configurations, statistics management and control packet handling. Logically there are three components including the LAG: LAG DS, NAS LAG, NDI LAG. See picture. NAS Daemon: The NAS daemon integrates standard Linux network APIs with NPU hardware functionality, and registers and listens to networking (netlink) events. What enum values in the model of dell-base-hash align with what's in opx-config-global-switch in hash fields map. What are all these actions in opx-config-global-switch? Ok - so now we know the thing I want is in base-traffic-hash/entry, but opx-config-global-switch is looking at base-switch/switching-entities/switching-entity . The next question is what populates that? Why are the hashes not in it? The definition for switching-entity is at: https://github.com/open-switch/opx-base-model/blob/abdf66f813b48a3c8e7682361cdacccd0271866d/yang-models/dell-base-switch-element.yang Say what? hash-fields is also defined in base-switch/switching-entities/switching-entity The next question to ask is who has ownership of base-traffic-hash? What about base-switch/switching-entities/switching-entity? The problem seems like it should be there? Why? base-traffic-hash is owned by opx_nas_daemon base-switch/switching-entities/switching-entity is also owned by opx_nas_daemon CPS has a REST service according to: https://github.com/open-switch/opx-cps The connection between YANG and the applications seems to be the CPS API? Applications define objects through (optionally YANG-based) object models. These object models are converted into binary (C accessible) object keys and object attributes that can be used in conjunction with the C-based CPS APIs. Description of cps_model_info: This tool is useful to get all information about CPS objects on the target. It is used to get the attributes of a specific CPS object, or first-level contents of a given YANG path of the CPS object (as defined in the YANG model). IT GIVES THE PROCESS OWNER! Investigating ops-nas-daemon There is a file called base_nas_default_init which defines the mirror port and the flow behaviors. I haven't found anything about other stuff yet. opx-config-global-switch --lag-hash-alg crc works and is owned by opx_nas_daemon - there must be other things it owns beside this. There is a file called hald_init.c . I think what is happening is all the other services fall under the NAS daemon. The code I'm looking for is somewhere else. After following that around for a while it looks like the file I'm really interested in is here https://github.com/open-switch/opx-nas-l2/blob/7e80d3952786f219b8072f1666ff1f16ba353d86/src/switch/nas_hash_cps.cpp . This bubbles up to the L2 init function, which bubbles back up to hald_init.c . dell-base-hash.h gets included in this thing. The YANG for the regular hash algorithm is at https://github.com/open-switch/opx-base-model/blob/abdf66f813b48a3c8e7682361cdacccd0271866d/yang-models/dell-base-switch-element.yang Initial investigation finished opened: https://github.com/open-switch/opx-nas-l2/issues/34","title":"Investigation into the opx-config-global-switch problem"},{"location":"Adding%20Hashing%20to%20OpenSwitch/#investigation-2-compilation-problems","text":"https://android.googlesource.com/platform/hardware/broadcom/wlan/+/master/bcmdhd/config/config-bcm.mk - examlpe config-bcm file. Seems to have something to do with broadcom's config. Better example configuration from Broadcom: http://broadcom-switch.github.io/OpenNSL/doc/html/OPENNSL_CUSTOMIZING_OPENNSL.html#OPENNSL_CONFIG_EXAMPLE Configuration properties: http://broadcom-switch.github.io/OpenNSL/doc/html/OPENNSL_CUSTOMIZING_OPENNSL.html#OPENNSL_CONFIG_PROPERTIES More info on config.bcm https://docs.broadcom.com/doc/12378910","title":"Investigation 2 - Compilation problems"},{"location":"Adding%20Hashing%20to%20OpenSwitch/#investigation-3-post-compilation-value-still-missing","text":"The hash variables seem to be passed by pointer from nas_hash_cps.cpp to nas_ndi_hash.c in line 222. The SAI portion of the hash creation seems to be here which could prove helpful later. In the bug at https://github.com/open-switch/opx-tools/issues/27 where he says obj-type that is referring to the YANG model. There is a leaf called obj-type . Traffic refers to: typedef traffic { type enumeration { enum \"ECMP_NON_IP\" { value 1; description \"ECMP routing: flow of non-IP ethernet frames\"; } enum \"LAG_NON_IP\" { value 2; description \"LAG routing: flow of non-IP ethernet frames\"; } enum \"ECMP_IPV4\" { value 3; description \"ECMP routing: flow of IPv4 packets\"; } enum \"ECMP_IPV4_IN_IPV4\" { value 4; description \"ECMP routing: flow of IPv4 packets with IPv4 packets tunnelled inside them\"; } enum \"ECMP_IPV6\" { value 5; description \"ECMP routing: flow of IPv6 packets\"; } enum \"LAG_IPV4\" { value 6; description \"LAG routing: flow of IPv4 packets\"; } enum \"LAG_IPV4_IN_IPV4\" { value 7; description \"ECMP routing: flow of IPv4 packets with IPv4 packets tunnelled inside them\"; } enum \"LAG_IPV6\" { value 8; description \"LAG routing: traffic flow is identified as IPv6 packets\"; } } description \"Enumeration of different types of traffic routing categories\"; } std-hash-field is a leaf-list with a type of field . field is defined as: typedef field { type enumeration { enum \"src-ip\" { value 1; description \"Traffic flow is identified by the source IP\"; } enum \"dest-ip\" { value 2; description \"Traffic flow is identified by the destination IP\"; } enum \"inner-src-ip\" { value 3; description \"Traffic flow is identified by the source IP of the tunnelled packet\"; } enum \"inner-dst-ip\" { value 4; description \"Traffic flow is identified by the destination IP of the tunnelled packet\"; } enum \"vlan-id\" { value 5; description \"Traffic flow is identified by the VLAN ID in the packet\"; } enum \"ip-protocol\" { value 6; description \"Traffic flow is identified by the IP protocol type(v4/v6)\"; } enum \"ethertype\" { value 7; description \"Traffic flow is identified by the IP protocol ether-type\"; } enum \"l4-src-port\" { value 8; description \"Traffic flow is identified by the source port in the packet\"; } enum \"l4-dest-port\" { value 9; description \"Traffic flow is identified by the destination port in the packet\"; } enum \"src-mac\" { value 10; description \"Traffic flow is identified by the source MAC address\"; } enum \"dest-mac\" { value 11; description \"Traffic flow is identified by the destination MAC address\"; } enum \"in-port\" { value 12; description \"Traffic flow is identified by the front-panel port the packet is received at\"; } } description \"Enumeration of different types of packet fields to check for routing\"; } - cps_get definition: def cps_get(q, obj, attrs={}): resp = [] return resp if cps.get([cps_object.CPSObject(obj, qual=q, data=attrs ).get() ], resp ) else None def cps_set(obj, qual, data): return (cps_utils.CPSTransaction([('set', cps_object.CPSObject(obj, qual=qual, data=data).get())]).commit() ) - cps_set('base-pas/led', 'target', {'entity-type': 3, 'slot': 1, 'name': 'Beacon', 'on': args.state}) - Set syntax: root@OPX:~# cps_set_oid.py -qua target -oper set -attr base-switch/entry/std-hash-field=1,2,8,9,6,5 base-switch/entry base-switch/entry/obj-type=6 - cps_get python syntax: cps_get('target', 'base-traffic-hash/entry', {'base-traffic-hash/entry/obj-type': '2'}) . That would get the object target from qualifier base-switch/entry which has attribute of base-switch/entry/obj-type (which is the key in entry) and we specifically want key 1 which is a type of traffic correspeonding to ECMP_NON_IP pp.pprint(cps_get('target', 'base-traffic-hash/entry', {'base-traffic-hash/entry/obj-type': '6'})) [ { 'data': { 'base-traffic-hash/entry/std-hash-field': [ bytearray(b'\\x01\\x00\\x00\\x00'), bytearray(b'\\x02\\x00\\x00\\x00'), bytearray(b'\\x08\\x00\\x00\\x00'), bytearray(b'\\t\\x00\\x00\\x00'), bytearray(b'\\x06\\x00\\x00\\x00')]}, 'key': '1.257.16842755.16842753.16842754.'}] CORRECT SYNTAX FOR CPS_SET: cps_set('base-traffic-hash/entry', 'target', {'base-traffic-hash/entry/obj-type': '6', 'base-traffic-hash/entry/std-hash-field': [1,2,8,9,6,5]})","title":"Investigation 3 - post compilation value still missing"},{"location":"Adding%20Hashing%20to%20OpenSwitch/#investigation-4-where-are-the-hash-values","text":"Part of the hash seems to be set in nas_ndi_switch.cpp There is also a unit test for it in sai_hash_unit_test.cpp This code seems to imply that the hashes themselves are implemented in the SAI: static _enum_map _algo_stoy = { {SAI_HASH_ALGORITHM_XOR, BASE_SWITCH_HASH_ALGORITHM_XOR }, {SAI_HASH_ALGORITHM_CRC, BASE_SWITCH_HASH_ALGORITHM_CRC }, {SAI_HASH_ALGORITHM_RANDOM, BASE_SWITCH_HASH_ALGORITHM_RANDOM }, {SAI_HASH_ALGORITHM_CRC_CCITT, BASE_SWITCH_HASH_ALGORITHM_CRC16CC }, {SAI_HASH_ALGORITHM_CRC_32LO, BASE_SWITCH_HASH_ALGORITHM_CRC32LSB }, {SAI_HASH_ALGORITHM_CRC_32HI, BASE_SWITCH_HASH_ALGORITHM_CRC32MSB }, {SAI_HASH_ALGORITHM_CRC_XOR8, BASE_SWITCH_HASH_ALGORITHM_XOR8 }, {SAI_HASH_ALGORITHM_CRC_XOR4, BASE_SWITCH_HASH_ALGORITHM_XOR4 }, {SAI_HASH_ALGORITHM_CRC_XOR2, BASE_SWITCH_HASH_ALGORITHM_XOR2 }, {SAI_HASH_ALGORITHM_CRC_XOR1, BASE_SWITCH_HASH_ALGORITHM_XOR1 }, }; static bool to_sai_type_hash_algo(sai_attribute_t *param ) { return to_sai_type(_algo_stoy,param); } static bool from_sai_type_hash_algo(sai_attribute_t *param ) { return from_sai_type(_algo_stoy,param); } There is a reference to each of these hash algorithms in https://github.com/open-switch/opx-base-model/blob/abdf66f813b48a3c8e7682361cdacccd0271866d/history/dell-base-switch-element.yhist From the SAI unit test I found: set_attr.value.s32 = SAI_HASH_ALGORITHM_XOR; status = switch_api_tbl_get()->set_switch_attribute (switch_id,&set_attr); EXPECT_EQ (SAI_STATUS_SUCCESS, status); This must be where they are setting the hash algorithm and how to set it. But where is this API? How do I find out what the options are? Is symmetric hashing exposed? The definition for the function is (it happens inside one of the unit tests): static inline sai_switch_api_t* switch_api_tbl_get (void) { return p_sai_switch_api_tbl; } This is a pointer to a table. I'm guessing it's a struct. sai_switch_api_t is short for SAI switch API table. It is a pointer to the table with all the API calls. From this I got a hint - it looks like there is a thing called switch_api - the same name is used in OpenSwitch. Based on the inputs from nas_ndi_hash.c there must be a series of SAI files with the functionality I want. #include \"std_error_codes.h\" #include \"std_assert.h\" #include \"nas_ndi_event_logs.h\" #include \"nas_ndi_utils.h\" #include \"dell-base-hash.h\" #include \"sai.h\" #include \"saiswitch.h\" #include \"saihash.h\" #include <stdio.h> #include <stdlib.h> #include <string.h> #include <inttypes.h>","title":"Investigation 4 - Where are the hash values?"},{"location":"Adding%20Hashing%20to%20OpenSwitch/#some-related-resources","text":"ECMP Hashing explained Broadcom Paper on Hashing Can I do this with openvswitch This didn't pan out. Broadcom docs","title":"Some related resources"},{"location":"Adding%20Hashing%20to%20OpenSwitch/#descriptions","text":"","title":"Descriptions"},{"location":"Adding%20Hashing%20to%20OpenSwitch/#nas","text":"The NAS manages the high-level NPU abstraction and adaptation, and abstracts and aggregates the core functionality required for networking access at Layer 1 (physical), Layer 2 (VLAN, link aggregation), Layer 3 (routing), ACL, QoS, and network monitoring. The NAS provides adaptation of the low-level switch abstraction provided by the SAI for standard Linux networking APIs and interfaces, and CPS API functionality. The NAS is also responsible for providing packet I/O services using the Linux kernel IP stack (see Network adaptation service for complete information).","title":"NAS"},{"location":"Adding%20Intel%20I219-LM%20%288086.0d4c%29%20Driver%20to%20ESXi/","text":"Adding Intel I219-LM (8086.0d4c) Driver to ESXi Download the following: ne1000 VIB (offline bundle) ESXi 7.0 GA Offline Bundle (VMware-ESXi-7.0.0-15843807-depot.zip ESXi 7.0b patch Offline Bundle Open vCenter and go to Menu->Auto Deploy->Image Builder Import all three offline bundles with the following names which will be referenced: ESXi 7.0 GA, ESXi 7.0b and Intel Driver I originally received \"Your software depot \"Intel Driver\" failed to import. Try importing it again or choose another software depot to import.\" . Fix action: Leave the page, come back, delete the name of the failed import, and then reimport with the same name. I had to do it three times total and the third it worked. Select the ESXi 7.0b Software Depot and make a note of the build number for the specific Image Profile you wish to use. In this case, we will be using 16324942 which is the \"full\" image which includes both bug fix + security fix along with VMware Tools. If you only want the security fix, use 16321839. In the right hand corner, click New to create a new Custom Depot called I219-8086.0d4c and then create new Image Profile and provide a name, vendor and description of your choosing. On the Depot column, filter by ESXi 7.0b initially and select the following 13 packages as shown in the screenshot below. I used the version 7.0.0-1.25.16324942 . I just sorted by version as well and then grabbed nvme-pcie and vmkusb separately. cpu-microcode crx esx-base esx-dvfilter-generic-fastpath esx-update esx-xserver loadesx native-misc-drvers nvme-pcie vdfs vmkusb vsan vsanhealth On the Depot column, filter by Intel Driver and select the ne1000-intelnuc package On the Depot column, filter by ESXi 7.0 GA and select everything, EXCEPT for the 13 packages we had already selected from Step 6. At the end I had 73 packages. You can now export and either download the Image Profile either as a bootable ISO which can then be used for fresh installation and/or upgrade as well as using vSphere Update Manager. You can also download the Image Profile which can be used to update via ESXCLI on the ESXi Shell. Resources How to make your unsupported NIC work with ESXi 5.x or 6.0 https://www.virtuallyghetto.com/2020/01/esxi-on-10th-gen-intel-nuc-frost-canyon.html Enhancements to the community ne1000 VIB for Intel NUC 10","title":"Adding Intel I219-LM (8086.0d4c) Driver to ESXi"},{"location":"Adding%20Intel%20I219-LM%20%288086.0d4c%29%20Driver%20to%20ESXi/#adding-intel-i219-lm-80860d4c-driver-to-esxi","text":"Download the following: ne1000 VIB (offline bundle) ESXi 7.0 GA Offline Bundle (VMware-ESXi-7.0.0-15843807-depot.zip ESXi 7.0b patch Offline Bundle Open vCenter and go to Menu->Auto Deploy->Image Builder Import all three offline bundles with the following names which will be referenced: ESXi 7.0 GA, ESXi 7.0b and Intel Driver I originally received \"Your software depot \"Intel Driver\" failed to import. Try importing it again or choose another software depot to import.\" . Fix action: Leave the page, come back, delete the name of the failed import, and then reimport with the same name. I had to do it three times total and the third it worked. Select the ESXi 7.0b Software Depot and make a note of the build number for the specific Image Profile you wish to use. In this case, we will be using 16324942 which is the \"full\" image which includes both bug fix + security fix along with VMware Tools. If you only want the security fix, use 16321839. In the right hand corner, click New to create a new Custom Depot called I219-8086.0d4c and then create new Image Profile and provide a name, vendor and description of your choosing. On the Depot column, filter by ESXi 7.0b initially and select the following 13 packages as shown in the screenshot below. I used the version 7.0.0-1.25.16324942 . I just sorted by version as well and then grabbed nvme-pcie and vmkusb separately. cpu-microcode crx esx-base esx-dvfilter-generic-fastpath esx-update esx-xserver loadesx native-misc-drvers nvme-pcie vdfs vmkusb vsan vsanhealth On the Depot column, filter by Intel Driver and select the ne1000-intelnuc package On the Depot column, filter by ESXi 7.0 GA and select everything, EXCEPT for the 13 packages we had already selected from Step 6. At the end I had 73 packages. You can now export and either download the Image Profile either as a bootable ISO which can then be used for fresh installation and/or upgrade as well as using vSphere Update Manager. You can also download the Image Profile which can be used to update via ESXCLI on the ESXi Shell.","title":"Adding Intel I219-LM (8086.0d4c) Driver to ESXi"},{"location":"Adding%20Intel%20I219-LM%20%288086.0d4c%29%20Driver%20to%20ESXi/#resources","text":"How to make your unsupported NIC work with ESXi 5.x or 6.0 https://www.virtuallyghetto.com/2020/01/esxi-on-10th-gen-intel-nuc-frost-canyon.html Enhancements to the community ne1000 VIB for Intel NUC 10","title":"Resources"},{"location":"Automating%20OME%20Hardware%20Reports/","text":"Automating OME Hardware Reports See https://github.com/grantcurell/omeinventory","title":"Automating OME Hardware Reports"},{"location":"Automating%20OME%20Hardware%20Reports/#automating-ome-hardware-reports","text":"See https://github.com/grantcurell/omeinventory","title":"Automating OME Hardware Reports"},{"location":"CloudLink/","text":"CloudLink How Does CloudLink Work General Functioning Components Licensing General Licenses License Types Installation Notes Connecting VMWare to CloudLink Set Up CloudLink Configure vCenter Enabling Encryption vSAN My Outstanding Questions General Functioning CloudLink provides encryption by using Microsoft BitLocker and dm-crypt ( How does dm-crypt work )for Linux to provide encryption. CloudLink's VM encryption functionality enables you to use native OS encryption features to encrypt a machine's boot and data volumes in a multi tenant cloud environment. This encryption enables you to protect the integrity of the machine itself against unauthorized modifications. CloudLink encrypts the machine boot and data volumes with unique keys that enterprise security administrators control. Neither cloud administrators nor other tenants in the cloud have access to the keys. By securing machines, you can define the security policy that must be met before passing the prestartup authorization, including verifying the integrity of the machine\u2019s boot chain. This offers protection against tampering. Components CloudLink Center\u2014The web-based interface for CloudLink that is used to manage machines that belong to the CloudLink environment (those machines on which CloudLink Agent has been installed). CloudLink Center: Communicates with machines over Transport Layer Security (TLS) Manages the encryption keys that are used to secure the boot volumes, data volumes, and devices for the machines Configures the security policies Monitors the security and operation events Collects log data CloudLink Agent - The agent that runs on individual machines. It communicates with CloudLink Center for pre-startup authorization and decryption of BitLocker or dm-crypt encryption keys. For Enterprise and PowerFlex\u2014CloudLink Center is packaged as a virtual appliance that can be deployed in the enterprise on VMware ESXi or Microsoft Hyper-V. Download CloudLink Agent from CloudLink Center. For Microsoft Azure or Azure Stack\u2014CloudLink Center can be deployed from the Azure Gallery in a simple-to-deploy, self-contained image file that enables you to quickly start your business-critical operations by using CloudLink. Search the Azure Gallery for CloudLink to locate the image. Download CloudLink Agent from CloudLink Center Licensing General Licenses Evaluation license\u2014This is a free trial license to test the CloudLink features. This license has an expiry date and is not allowed to be used in production. Use a subscription or a perpetual license that is purchased through Dell EMC for production purposes. Subscription license\u2014This license expires on a predefined date and time. The subscription license period is for one, two, or three years only. Repurchase the subscription licenses at the end of their term. Perpetual license\u2014This license never expires. License Types Encryption for Machines license\u2014Licensed per machine for volume encryption. This license defines the number of machines, virtual, or bare metal, that can be protected using CloudLink Center. Encryption for Containers license\u2014Enables data encryption for containers. A single Container license supports any number of Kubernetes clusters. Encryption for PowerFlex license\u2014Encrypted capacity for PowerFlex This license defines the total storage that can be encrypted using CloudLink Center. Key Management over KMIP license\u2014Licensed KMIP clients This license defines the number of KMIP clients that can be managed using CloudLink Center. With one Key Management over KMIP license you can create: One KMIP Client One CloudLink Center cluster NOTE: To create additional KMIP Clients or CloudLink Center clusters, purchase additional Key Management over KMIP licenses. Key Management for SED license\u2014Number of physical machines with SEDs. A single Key Management for SED license is used per physical machine regardless of the number of SEDs connected to that machine Installation Notes I followed the guide here for VMWare https://docs.delltechnologies.com/bundle/P_DG_CL_701/page/GUID-1EDFBE27-3218-43AD-9449-24374D5FE1F6.html The default user for the webui is secadmin Before I added VMs to my cluster I created a machine group to put them in Make sure you add approved networks with IP ranges before installation or adding machines will fail with IP address (192.168.1.95) not in group's approved networks Alternatively you will have to go to machines and accept the machine To install in standard mode on Linux run sudo ./clagent.sh -S 192.168.1.86 -G cf41-f71e where -S specifies the server and -G is the group key Connecting VMWare to CloudLink WARNING This option requires you to use the KMIP license. If you use another license the below indicated options will not be present. Connection to vSphere / VMWare works through a protocol called KMIP. KMIP is a protocol for communicating key information between key management servers and key management clients. Broadly speaking there are two ways which CloudLink can function in a VMWare environment: You can set up the CloudLink Center server and it can encrypt the virtual hard disks used by the various VMs. This is accomplished by running the CloudLink Agent on the individual VMs themselves. You can connect Cloudlink Center directly to vSphere and vSphere will instead use it to encrypt the VMs themselves. You can verify that CloudLink is compatible with the version of VMWare you are going to use here . See Dell EMC CloudLink Key Management for VMware vCenter Server Configuration Guide for an overview of configuring VMWare vCenter with CloudLink. Set Up CloudLink Note The use of Chinese characters is deliberate. I do this to ensure that software I test correctly handles character encodings other than ASCII. It is surprising how many modern software applications fail to handle UTF8 and other character encodings properly which is frequently an indicator of poor design quality. A KMIP partition is a container for the keys and certificates that are created by some KMIP client. Multiple clients can use the same partition but then keep in mind they will also be mutually accessible. See https://docs.delltechnologies.com/bundle/P_AG_CL/page/GUID-3A10CBF2-07DA-4951-96F3-2A7008390F55.html After that I added a KMIP client: Configure vCenter First we have to establish trust with the KMS server by make the KMS trust vCenter. After clicking the above select KMS certificate and private key as shown below. In KMS certificate upload cert.pem (which you got from setting up the client in CloudLink) and in KMS private key upload key.pem. Upload the certificate you downloaded earlier here (it should be called ca.pem). I didn't have to do this in my setup. Enabling Encryption Encryption is controlled by storage policy so you can set it as you would on any other object. Right click on a VM Click Edit Storage Policies Now you can change it to VM Encryption Policy vSAN To enable encryption on vSAN do this: https://docs.vmware.com/en/VMware-vSphere/6.7/com.vmware.vsphere.virtualsan.doc/GUID-E7CA36B7-D7EB-423A-ADD1-7E410E36F5A7.html My Outstanding Questions What is vault mode? What are these three unique manual passcodes? Can it encrypt data in motion? How does the licensing work if you have multiple clusters?","title":"CloudLink"},{"location":"CloudLink/#cloudlink","text":"How Does CloudLink Work General Functioning Components Licensing General Licenses License Types Installation Notes Connecting VMWare to CloudLink Set Up CloudLink Configure vCenter Enabling Encryption vSAN My Outstanding Questions","title":"CloudLink"},{"location":"CloudLink/#general-functioning","text":"CloudLink provides encryption by using Microsoft BitLocker and dm-crypt ( How does dm-crypt work )for Linux to provide encryption. CloudLink's VM encryption functionality enables you to use native OS encryption features to encrypt a machine's boot and data volumes in a multi tenant cloud environment. This encryption enables you to protect the integrity of the machine itself against unauthorized modifications. CloudLink encrypts the machine boot and data volumes with unique keys that enterprise security administrators control. Neither cloud administrators nor other tenants in the cloud have access to the keys. By securing machines, you can define the security policy that must be met before passing the prestartup authorization, including verifying the integrity of the machine\u2019s boot chain. This offers protection against tampering.","title":"General Functioning"},{"location":"CloudLink/#components","text":"CloudLink Center\u2014The web-based interface for CloudLink that is used to manage machines that belong to the CloudLink environment (those machines on which CloudLink Agent has been installed). CloudLink Center: Communicates with machines over Transport Layer Security (TLS) Manages the encryption keys that are used to secure the boot volumes, data volumes, and devices for the machines Configures the security policies Monitors the security and operation events Collects log data CloudLink Agent - The agent that runs on individual machines. It communicates with CloudLink Center for pre-startup authorization and decryption of BitLocker or dm-crypt encryption keys. For Enterprise and PowerFlex\u2014CloudLink Center is packaged as a virtual appliance that can be deployed in the enterprise on VMware ESXi or Microsoft Hyper-V. Download CloudLink Agent from CloudLink Center. For Microsoft Azure or Azure Stack\u2014CloudLink Center can be deployed from the Azure Gallery in a simple-to-deploy, self-contained image file that enables you to quickly start your business-critical operations by using CloudLink. Search the Azure Gallery for CloudLink to locate the image. Download CloudLink Agent from CloudLink Center","title":"Components"},{"location":"CloudLink/#licensing","text":"","title":"Licensing"},{"location":"CloudLink/#general-licenses","text":"Evaluation license\u2014This is a free trial license to test the CloudLink features. This license has an expiry date and is not allowed to be used in production. Use a subscription or a perpetual license that is purchased through Dell EMC for production purposes. Subscription license\u2014This license expires on a predefined date and time. The subscription license period is for one, two, or three years only. Repurchase the subscription licenses at the end of their term. Perpetual license\u2014This license never expires.","title":"General Licenses"},{"location":"CloudLink/#license-types","text":"Encryption for Machines license\u2014Licensed per machine for volume encryption. This license defines the number of machines, virtual, or bare metal, that can be protected using CloudLink Center. Encryption for Containers license\u2014Enables data encryption for containers. A single Container license supports any number of Kubernetes clusters. Encryption for PowerFlex license\u2014Encrypted capacity for PowerFlex This license defines the total storage that can be encrypted using CloudLink Center. Key Management over KMIP license\u2014Licensed KMIP clients This license defines the number of KMIP clients that can be managed using CloudLink Center. With one Key Management over KMIP license you can create: One KMIP Client One CloudLink Center cluster NOTE: To create additional KMIP Clients or CloudLink Center clusters, purchase additional Key Management over KMIP licenses. Key Management for SED license\u2014Number of physical machines with SEDs. A single Key Management for SED license is used per physical machine regardless of the number of SEDs connected to that machine","title":"License Types"},{"location":"CloudLink/#installation-notes","text":"I followed the guide here for VMWare https://docs.delltechnologies.com/bundle/P_DG_CL_701/page/GUID-1EDFBE27-3218-43AD-9449-24374D5FE1F6.html The default user for the webui is secadmin Before I added VMs to my cluster I created a machine group to put them in Make sure you add approved networks with IP ranges before installation or adding machines will fail with IP address (192.168.1.95) not in group's approved networks Alternatively you will have to go to machines and accept the machine To install in standard mode on Linux run sudo ./clagent.sh -S 192.168.1.86 -G cf41-f71e where -S specifies the server and -G is the group key","title":"Installation Notes"},{"location":"CloudLink/#connecting-vmware-to-cloudlink","text":"WARNING This option requires you to use the KMIP license. If you use another license the below indicated options will not be present. Connection to vSphere / VMWare works through a protocol called KMIP. KMIP is a protocol for communicating key information between key management servers and key management clients. Broadly speaking there are two ways which CloudLink can function in a VMWare environment: You can set up the CloudLink Center server and it can encrypt the virtual hard disks used by the various VMs. This is accomplished by running the CloudLink Agent on the individual VMs themselves. You can connect Cloudlink Center directly to vSphere and vSphere will instead use it to encrypt the VMs themselves. You can verify that CloudLink is compatible with the version of VMWare you are going to use here . See Dell EMC CloudLink Key Management for VMware vCenter Server Configuration Guide for an overview of configuring VMWare vCenter with CloudLink.","title":"Connecting VMWare to CloudLink"},{"location":"CloudLink/#set-up-cloudlink","text":"Note The use of Chinese characters is deliberate. I do this to ensure that software I test correctly handles character encodings other than ASCII. It is surprising how many modern software applications fail to handle UTF8 and other character encodings properly which is frequently an indicator of poor design quality. A KMIP partition is a container for the keys and certificates that are created by some KMIP client. Multiple clients can use the same partition but then keep in mind they will also be mutually accessible. See https://docs.delltechnologies.com/bundle/P_AG_CL/page/GUID-3A10CBF2-07DA-4951-96F3-2A7008390F55.html After that I added a KMIP client:","title":"Set Up CloudLink"},{"location":"CloudLink/#configure-vcenter","text":"First we have to establish trust with the KMS server by make the KMS trust vCenter. After clicking the above select KMS certificate and private key as shown below. In KMS certificate upload cert.pem (which you got from setting up the client in CloudLink) and in KMS private key upload key.pem. Upload the certificate you downloaded earlier here (it should be called ca.pem). I didn't have to do this in my setup.","title":"Configure vCenter"},{"location":"CloudLink/#enabling-encryption","text":"Encryption is controlled by storage policy so you can set it as you would on any other object. Right click on a VM Click Edit Storage Policies Now you can change it to VM Encryption Policy","title":"Enabling Encryption"},{"location":"CloudLink/#vsan","text":"To enable encryption on vSAN do this: https://docs.vmware.com/en/VMware-vSphere/6.7/com.vmware.vsphere.virtualsan.doc/GUID-E7CA36B7-D7EB-423A-ADD1-7E410E36F5A7.html","title":"vSAN"},{"location":"CloudLink/#my-outstanding-questions","text":"What is vault mode? What are these three unique manual passcodes? Can it encrypt data in motion? How does the licensing work if you have multiple clusters?","title":"My Outstanding Questions"},{"location":"Configure%20FN410%20as%20a%20Switch/","text":"Configure FN410 as a Switch Scenario Overview You are running a TFX2, you have an FN410, and that FN410 is connected back to some sort of top of rack switch. In my case I am using an 4112F-ON. In our scenario, we want to create a LAG between the Top of Rack (TOR) switch and two of the front ports of the FN410. On the back of the FN410, connected to the blades, you will have a trunk going to an ESXi server. Ports The ports are configured in the following way: 4112F-ON ethernet 1/1/1 - port 1 of LAG ethernet 1/1/2 - port 2 of LAG port-channel 128 - LAG port In my setup ethernet 1/1/9 went to the CMC and ethernet 1/1/12 went to the rest of my network. There were not other relevant ports. FN410 tengigabitethernet 0/9 - port 1 of LAG tengigabitethernet 0/10 - port 2 of LAG tengigabitethernet 0/1 - port 1 going to ESXi tengigabitethernet 0/2 - port 2 going to ESXi Useful Notes How the FN410 Management Interface Works From the manual The IOM management interface has both a public IP and private IP address on the internal fabric D interface. The public IP address is exposed to the outside world for Web GUI configurations/WSMAN and other proprietary traffic. You can statically configure the public IP address or obtain the IP address dynamically using the dynamic host configuration protocol (DHCP). What is a fabric? A fabric is just a series of electrical connections that in this case make up a L2 switching domain. Fabric D is the internal management fabric. The CMC basically acts as a switch on this fabric and all of the chassis management related services are connected to it - idrac, CMC, and FN410 management functions. Example Configs 4112F-ON Example Configuration FN410 Example Configuration For the keen eyed I was too lazy to remove the password hashes. Spoiler it's admin/admin on the 4112 and root/calvin on the FN410. Configuring the FN410 Configure FN410 as a Switch via GUI Go to CMC -> Click I/O Module Overview Click on your FN410 Go to setup and configure networking. The address you sit here is tied to the CMC's physical port. That is to say, you do not need to be plugged into the FN410 to reach this address. It should be on the same subnet and VLAN as the CMC. Click Launch I/O Module GUI Once in the GUI, in mode settings, select Full Switch Mode Set your network settings as needed Credentials set as needed SNMP set as needed Disable Uplink Failure Detection On time I set my time zone and used 216.239.35.0 (Google's time servers - the zero isn't a type-o) At the end you will be asked to reboot. Say yes. A window will appear that says rebooting. Wait for it to go away. At the end the page will refresh and you will get an error message on the web site saying it isn't available. This is because you put it in switch mode. This process took ~3 minutes for me. Configure FN410 as a Swich via Command Line From the manual You can connect to the FN410 using one of the following: Internal RS-232 using the chassis management controller (CMC). Telnet into CMC and do a connect -b switch-id to get console access to corresponding IOM. External serial port with a universal serial bus (USB) connector (front panel): connect using the IOM front panel USB serial line to get console access (Labeled as USB B). Telnet/others using the public IP interface on the fabric D interface. CMC through the private IP interface on the fabric D interface. You will need to connect through the CMC: SSH to the CMC's IP address. Use the CMC username/password to log in. Run the command connect -m switch-1 to connect to the switch. My switch was in BMP mode. I had to hit A to cancel it. Run stack-unit 0 iom-mode full-switch to change the switch to full switch mode Run write mem and reboot the IOM Configuring Switch Management You may have already done this in the GUI, but if not, you can do the following: Configure Default Route management route 0.0.0.0/0 192.168.1.1 Configure NTP ntp server 216.239.35.0 Configure Management IP Address Dell(conf)#interface managementethernet 0/0 Dell(conf-if-ma-0/0)#ip address 192.168.1.114/24 Configure SNMP snmp-server community public ro snmp-server enable traps snmp linkdown linkup snmp-server enable traps stack Upgrading the firmware You can download the firmware from the force10 website Upgrade the Dell Networking OS in flash partition A: or B:. Run upgrade system tftp://10.16.127.149/dell-FN-B B: Verify that the Dell Networking OS has been upgraded correctly in the upgraded flash partition. show boot system stack-unit [0-5 | all] Upgrade the FN I/O Module Boot Flash and Boot Selector image (if needed). upgrade boot [all | bootflash-image | bootselector-image] stack-unit [0-5 | all] [booted | flash: | ftp: | scp: | tftp: | usbflash:] [A: | B:] Change the Primary Boot Parameter of the FN I/O Module to the upgraded partition A: or B:. boot system stack-unit [0-5 | all] primary [system A: | system B: | tftp://] Save the config: write memory . Reload the switch so the config takes effect. reload After boot, use show version and show system stack-unit [0-5] to check that versions have updated correctly. Configure Interfaces Begin by configuring the port-channel interface: Note: Hybrid mode allows the interface to pass tagged and untagged traffic. Dell(conf)#interface port-channel 128 Dell(conf-if-po-128)#portmode hybrid Dell(conf-if-po-128)#switchport Dell(conf-if-po-128)#no shutdown Next you need to configure the individual interfaces: Dell(conf)#interface range tengigabitethernet 0/9-10 Dell(conf-if-range-te-0/9-10)#port-channel-protocol LACP Dell(conf-if-range-te-0/9-10-lacp)#port-channel 128 mode active Now you need to configure any VLANs you want to run: Dell(conf)#interface range vlan 32-37 Dell(conf-if-range-vl-32-37)#tagged tengigabitethernet 0/1-2 Dell(conf-if-range-vl-32-37)#tagged tengigabitethernet 0/12 If desired at this point you can enter into each VLAN interface and run the ip address command to give it an IP address. Run write memory Configure 4112F-ON Log into the 4112F-ON via its management interfaces. Move to configuration mode. Next, configure the port channel interface: OS10(config)# interface port-channel 128 OS10(conf-if-po-128)# switchport mode trunk OS10(conf-if-po-128)# no switchport access vlan OS10(conf-if-po-128)# switchport trunk allowed vlan 32 OS10(conf-if-po-128)# no shutdown Now configure the interfaces to join the port channel. OS10(config)# interface range ethernet 1/1/1-1/1/2 OS10(conf-range-eth1/1/1-1/1/2)# channel-group 128 mode active OS10(conf-range-eth1/1/1-1/1/2)# no switchport OS10(conf-range-eth1/1/1-1/1/2)# switchport mode trunk Run write memory Troubleshooting VLANs On 4112F-ON Make sure the VLANs using are all shown as active on the correct ports: OS10# show vlan Codes: * - Default VLAN, M - Management VLAN, R - Remote Port Mirroring VLANs, @ \u2013 Attached to Virtual Network Q: A - Access (Untagged), T - Tagged NUM Status Description Q Ports * 1 Inactive A Eth1/1/4-1/1/8,1/1/10,1/1/13-1/1/15 32 Active T Eth1/1/11 T Po128 A Eth1/1/9,1/1/12 On FN410 Dell#show vlan Codes: * - Default VLAN, G - GVRP VLANs, R - Remote Port Mirroring VLANs, P - Primary, C - Community, I - Isolated O - Openflow, Vx - Vxlan Q: U - Untagged, T - Tagged x - Dot1x untagged, X - Dot1x tagged o - OpenFlow untagged, O - OpenFlow tagged G - GVRP tagged, M - Vlan-stack, H - VSN tagged i - Internal untagged, I - Internal tagged, v - VLT untagged, V - VLT tagged NUM Status Description Q Ports * 1 Active U Po128(Te 0/9-10) U Te 0/1-2,12 32 Active T Po128(Te 0/9-10) T Te 0/1-2,12","title":"Configure FN410 as a Switch"},{"location":"Configure%20FN410%20as%20a%20Switch/#configure-fn410-as-a-switch","text":"","title":"Configure FN410 as a Switch"},{"location":"Configure%20FN410%20as%20a%20Switch/#scenario","text":"","title":"Scenario"},{"location":"Configure%20FN410%20as%20a%20Switch/#overview","text":"You are running a TFX2, you have an FN410, and that FN410 is connected back to some sort of top of rack switch. In my case I am using an 4112F-ON. In our scenario, we want to create a LAG between the Top of Rack (TOR) switch and two of the front ports of the FN410. On the back of the FN410, connected to the blades, you will have a trunk going to an ESXi server.","title":"Overview"},{"location":"Configure%20FN410%20as%20a%20Switch/#ports","text":"The ports are configured in the following way:","title":"Ports"},{"location":"Configure%20FN410%20as%20a%20Switch/#4112f-on","text":"ethernet 1/1/1 - port 1 of LAG ethernet 1/1/2 - port 2 of LAG port-channel 128 - LAG port In my setup ethernet 1/1/9 went to the CMC and ethernet 1/1/12 went to the rest of my network. There were not other relevant ports.","title":"4112F-ON"},{"location":"Configure%20FN410%20as%20a%20Switch/#fn410","text":"tengigabitethernet 0/9 - port 1 of LAG tengigabitethernet 0/10 - port 2 of LAG tengigabitethernet 0/1 - port 1 going to ESXi tengigabitethernet 0/2 - port 2 going to ESXi","title":"FN410"},{"location":"Configure%20FN410%20as%20a%20Switch/#useful-notes","text":"","title":"Useful Notes"},{"location":"Configure%20FN410%20as%20a%20Switch/#how-the-fn410-management-interface-works","text":"From the manual The IOM management interface has both a public IP and private IP address on the internal fabric D interface. The public IP address is exposed to the outside world for Web GUI configurations/WSMAN and other proprietary traffic. You can statically configure the public IP address or obtain the IP address dynamically using the dynamic host configuration protocol (DHCP).","title":"How the FN410 Management Interface Works"},{"location":"Configure%20FN410%20as%20a%20Switch/#what-is-a-fabric","text":"A fabric is just a series of electrical connections that in this case make up a L2 switching domain. Fabric D is the internal management fabric. The CMC basically acts as a switch on this fabric and all of the chassis management related services are connected to it - idrac, CMC, and FN410 management functions.","title":"What is a fabric?"},{"location":"Configure%20FN410%20as%20a%20Switch/#example-configs","text":"4112F-ON Example Configuration FN410 Example Configuration For the keen eyed I was too lazy to remove the password hashes. Spoiler it's admin/admin on the 4112 and root/calvin on the FN410.","title":"Example Configs"},{"location":"Configure%20FN410%20as%20a%20Switch/#configuring-the-fn410","text":"","title":"Configuring the FN410"},{"location":"Configure%20FN410%20as%20a%20Switch/#configure-fn410-as-a-switch-via-gui","text":"Go to CMC -> Click I/O Module Overview Click on your FN410 Go to setup and configure networking. The address you sit here is tied to the CMC's physical port. That is to say, you do not need to be plugged into the FN410 to reach this address. It should be on the same subnet and VLAN as the CMC. Click Launch I/O Module GUI Once in the GUI, in mode settings, select Full Switch Mode Set your network settings as needed Credentials set as needed SNMP set as needed Disable Uplink Failure Detection On time I set my time zone and used 216.239.35.0 (Google's time servers - the zero isn't a type-o) At the end you will be asked to reboot. Say yes. A window will appear that says rebooting. Wait for it to go away. At the end the page will refresh and you will get an error message on the web site saying it isn't available. This is because you put it in switch mode. This process took ~3 minutes for me.","title":"Configure FN410 as a Switch via GUI"},{"location":"Configure%20FN410%20as%20a%20Switch/#configure-fn410-as-a-swich-via-command-line","text":"From the manual You can connect to the FN410 using one of the following: Internal RS-232 using the chassis management controller (CMC). Telnet into CMC and do a connect -b switch-id to get console access to corresponding IOM. External serial port with a universal serial bus (USB) connector (front panel): connect using the IOM front panel USB serial line to get console access (Labeled as USB B). Telnet/others using the public IP interface on the fabric D interface. CMC through the private IP interface on the fabric D interface. You will need to connect through the CMC: SSH to the CMC's IP address. Use the CMC username/password to log in. Run the command connect -m switch-1 to connect to the switch. My switch was in BMP mode. I had to hit A to cancel it. Run stack-unit 0 iom-mode full-switch to change the switch to full switch mode Run write mem and reboot the IOM","title":"Configure FN410 as a Swich via Command Line"},{"location":"Configure%20FN410%20as%20a%20Switch/#configuring-switch-management","text":"You may have already done this in the GUI, but if not, you can do the following:","title":"Configuring Switch Management"},{"location":"Configure%20FN410%20as%20a%20Switch/#configure-default-route","text":"management route 0.0.0.0/0 192.168.1.1","title":"Configure Default Route"},{"location":"Configure%20FN410%20as%20a%20Switch/#configure-ntp","text":"ntp server 216.239.35.0","title":"Configure NTP"},{"location":"Configure%20FN410%20as%20a%20Switch/#configure-management-ip-address","text":"Dell(conf)#interface managementethernet 0/0 Dell(conf-if-ma-0/0)#ip address 192.168.1.114/24","title":"Configure Management IP Address"},{"location":"Configure%20FN410%20as%20a%20Switch/#configure-snmp","text":"snmp-server community public ro snmp-server enable traps snmp linkdown linkup snmp-server enable traps stack","title":"Configure SNMP"},{"location":"Configure%20FN410%20as%20a%20Switch/#upgrading-the-firmware","text":"You can download the firmware from the force10 website Upgrade the Dell Networking OS in flash partition A: or B:. Run upgrade system tftp://10.16.127.149/dell-FN-B B: Verify that the Dell Networking OS has been upgraded correctly in the upgraded flash partition. show boot system stack-unit [0-5 | all] Upgrade the FN I/O Module Boot Flash and Boot Selector image (if needed). upgrade boot [all | bootflash-image | bootselector-image] stack-unit [0-5 | all] [booted | flash: | ftp: | scp: | tftp: | usbflash:] [A: | B:] Change the Primary Boot Parameter of the FN I/O Module to the upgraded partition A: or B:. boot system stack-unit [0-5 | all] primary [system A: | system B: | tftp://] Save the config: write memory . Reload the switch so the config takes effect. reload After boot, use show version and show system stack-unit [0-5] to check that versions have updated correctly.","title":"Upgrading the firmware"},{"location":"Configure%20FN410%20as%20a%20Switch/#configure-interfaces","text":"Begin by configuring the port-channel interface: Note: Hybrid mode allows the interface to pass tagged and untagged traffic. Dell(conf)#interface port-channel 128 Dell(conf-if-po-128)#portmode hybrid Dell(conf-if-po-128)#switchport Dell(conf-if-po-128)#no shutdown Next you need to configure the individual interfaces: Dell(conf)#interface range tengigabitethernet 0/9-10 Dell(conf-if-range-te-0/9-10)#port-channel-protocol LACP Dell(conf-if-range-te-0/9-10-lacp)#port-channel 128 mode active Now you need to configure any VLANs you want to run: Dell(conf)#interface range vlan 32-37 Dell(conf-if-range-vl-32-37)#tagged tengigabitethernet 0/1-2 Dell(conf-if-range-vl-32-37)#tagged tengigabitethernet 0/12 If desired at this point you can enter into each VLAN interface and run the ip address command to give it an IP address. Run write memory","title":"Configure Interfaces"},{"location":"Configure%20FN410%20as%20a%20Switch/#configure-4112f-on","text":"Log into the 4112F-ON via its management interfaces. Move to configuration mode. Next, configure the port channel interface: OS10(config)# interface port-channel 128 OS10(conf-if-po-128)# switchport mode trunk OS10(conf-if-po-128)# no switchport access vlan OS10(conf-if-po-128)# switchport trunk allowed vlan 32 OS10(conf-if-po-128)# no shutdown Now configure the interfaces to join the port channel. OS10(config)# interface range ethernet 1/1/1-1/1/2 OS10(conf-range-eth1/1/1-1/1/2)# channel-group 128 mode active OS10(conf-range-eth1/1/1-1/1/2)# no switchport OS10(conf-range-eth1/1/1-1/1/2)# switchport mode trunk Run write memory","title":"Configure 4112F-ON"},{"location":"Configure%20FN410%20as%20a%20Switch/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"Configure%20FN410%20as%20a%20Switch/#vlans","text":"","title":"VLANs"},{"location":"Configure%20FN410%20as%20a%20Switch/#on-4112f-on","text":"Make sure the VLANs using are all shown as active on the correct ports: OS10# show vlan Codes: * - Default VLAN, M - Management VLAN, R - Remote Port Mirroring VLANs, @ \u2013 Attached to Virtual Network Q: A - Access (Untagged), T - Tagged NUM Status Description Q Ports * 1 Inactive A Eth1/1/4-1/1/8,1/1/10,1/1/13-1/1/15 32 Active T Eth1/1/11 T Po128 A Eth1/1/9,1/1/12","title":"On 4112F-ON"},{"location":"Configure%20FN410%20as%20a%20Switch/#on-fn410","text":"Dell#show vlan Codes: * - Default VLAN, G - GVRP VLANs, R - Remote Port Mirroring VLANs, P - Primary, C - Community, I - Isolated O - Openflow, Vx - Vxlan Q: U - Untagged, T - Tagged x - Dot1x untagged, X - Dot1x tagged o - OpenFlow untagged, O - OpenFlow tagged G - GVRP tagged, M - Vlan-stack, H - VSN tagged i - Internal untagged, I - Internal tagged, v - VLT untagged, V - VLT tagged NUM Status Description Q Ports * 1 Active U Po128(Te 0/9-10) U Te 0/1-2,12 32 Active T Po128(Te 0/9-10) T Te 0/1-2,12","title":"On FN410"},{"location":"Configure%20Gigamon%20Tap/","text":"Configure Gigamon Tap Getting Help You have to register with a valid serial number on Gigamon's support site . For it to work your name either has to be attached to the account or you have to have a valid .mil address. My Setup Gigamon Model Number / Version GTP-ASF01 Hardware Revision: G-TAP A2/SF G-Tap> show version SW Version : 2.3.03 Filename=gtb_ASF_IMAGE_2.3.03_20161118.bin SFPs Tested Front Panel Configuration Traffic Generation Traffic was generated on a laptop going into port network 1 and load balanced across ports toolA and toolB Configuring the Device Console Settings I used the following console settings: On Windows with Putty my serial line was COM3 Speed: 115200 Data bits: 8 Stop bits: 1 Parity: None Flow Control: None Logging In Default password is root123 . There is no username. Configure Management config ipaddr 192.168.1.105 subnetmask 255.255.255.0 WARNING For reasons unknown, telnet is enabled by default on the management interface. If you are using this outside a lab you'll probably want to disable it with telnet 0 . At this juncture, you can telnet to the management IP if you want to. If you want to change the default password this can be done with passwd . If you need to set the date or time you can do so with the commands time <hh:mm:ss> or date <mm-dd-yy> respectively. Configure TAP Capability According to the instructions no configuration is required. Test Results Run 1 - Pass Scenario: Get nominal load balancing working. For testing I added a second Dell 1Gb/s copper SFP. Distant end was a USB 1Gb/s copper adapter on a laptop. Result: Worked as expected. Input went into the network port and load balanced out the toolA port. Run 2 - Fail Scenario: Test to see if the Intel 10Gb/s NIC will run. Distant end was another Intel 10Gb/s SFP in an Intel x710 attached to ESXi. Result: Failure. The tap detected the insert and removal of the SFP: Monitor Port B: \"SFP\" Module Removed Monitor Port B: NOTE: I2C device respond successfully! \"SFP+ SR\" Inserted However, the distant end, which I confirmed to be working using a Dell 4112F-ON, would not come up. Note: As long as the SFP was seated in the Gigamon the green light remained on. There was no correlation between plugging in the cable and the green light. Run 3 - Fail Scenario: Same as 2 except I used the 3rd part Gtek SFP+. Results: Same as Run 2. Run 4 - Fail Scenario: Same as 2 except with a 10Gb/s Dell adapter. Results: Same as Run 2. Run 5 - Fail Scenario: I tried with the 10Gb/s Dell SFP again - this time I made the distant end a RHEL box. Results: RHEL saw nothing. ethtool showed both the duplex and speed as unknown. Nothing came up. Run 6 - Fail Scenario: I tried with the 10Gb/s Dell SFP again - this time I made the distant end a Dell 4112F-ON. Results: Even after manually setting speed, duplex, autonegotiation, and double checking the interface types it still didn't come up. Other Helpful Commands If you need to close or open the tap you can do so using taptx <active|passive> where active pushes traffic to the tool ports and passive only pushes traffic through the network ports. show system gives you a nice overview of the status of the tap and what is plugged in where. G-Tap> show system ======================================================================== System Information ======================================================================== System Name : GTP-A2/SF S/N=*******, rev=A1, HW Built=10/24/2019 SW Version : 2.3.03 Filename=gtb_ASF_IMAGE_2.3.03_20161118.bin App Loader Ver : 2.04 Current : Mon Jun 22, 2020 20:50:26 System Boot : Mon Jun 22, 2020 17:02:39 ------------------------------------------------------------------------ Eth Mgmt Port : DHCP=DISABLE, MAC=00:1D:AC:1B:09:DE : 192.168.1.105/255.255.255.0 gateway=255.255.255.255 : Autoneg=ON, Link=Up Speed=100 Duplex=Full ------------------------------------------------------------------------ Telnet Access : Enabled, Timeout=300 seconds Console baud : 115200 bps ------------------------------------------------------------------------ Power Supply : AC adapter=[ON] DC -48V=[OFF] PoE=[OFF] ------------------------------------------------------------------------ Battery fuel gauge busy. Please Try again... ------------------------------------------------------------------------ Temperatures : Board 38 C, Battery 2 C Fan : OFF ------------------------------------------------------------------------ Weird Behaviors Temperature I continuously saw a warning for temperature: Battery I saw a warning for the battery: NOTE: Battery fuel gauge busy! Will try again later It looks like you can buy a battery separately so I believe this to be expected. Where is SSH? I was unable to figure out how to configure SSH. In the manual they only had telnet and console listed. I feel like I must be missing something. Telnet Console Didn't Display Correctly When I typed in the telnet console with Putty the text would only appear after I hit enter. Commands Straight from the Manual Do Not Work Using the arrow keys in the command line deletes text instead of moving cursor Why? Manual is incorrect about display output Ex: G-Tap> show port-params NETWORK MONITOR Parameter Port A Port B Port A Port B ================= ========== ========== ========== ========== Admin: 1 1 1 1 Signal Detect: 1 0 1 1 Tx Power(dBm): n/a n/a n/a -2.36 Rx Power(dBm): n/a n/a n/a -2.50 SFP Module Type: SFP Copper -- SFP Copper SFP+ SR Cable Length(m): n/a n/a n/a n/a The speed, duplex and autonegotiation settings are not listed anywhere in the output. Bizzare Compatibility Errors with SFPs Even after confirming all setting, fixing the speed at 10Gb/s, duplex at full, and turning off autonegation, the fiber interface still didn't come up. I went and cross referenced the manual to see what type of SFPs they support and confirmed that the wavelength and type matched.","title":"Configure Gigamon Tap"},{"location":"Configure%20Gigamon%20Tap/#configure-gigamon-tap","text":"","title":"Configure Gigamon Tap"},{"location":"Configure%20Gigamon%20Tap/#getting-help","text":"You have to register with a valid serial number on Gigamon's support site . For it to work your name either has to be attached to the account or you have to have a valid .mil address.","title":"Getting Help"},{"location":"Configure%20Gigamon%20Tap/#my-setup","text":"","title":"My Setup"},{"location":"Configure%20Gigamon%20Tap/#gigamon-model-number-version","text":"GTP-ASF01 Hardware Revision: G-TAP A2/SF G-Tap> show version SW Version : 2.3.03 Filename=gtb_ASF_IMAGE_2.3.03_20161118.bin","title":"Gigamon Model Number / Version"},{"location":"Configure%20Gigamon%20Tap/#sfps-tested","text":"","title":"SFPs Tested"},{"location":"Configure%20Gigamon%20Tap/#front-panel-configuration","text":"","title":"Front Panel Configuration"},{"location":"Configure%20Gigamon%20Tap/#traffic-generation","text":"Traffic was generated on a laptop going into port network 1 and load balanced across ports toolA and toolB","title":"Traffic Generation"},{"location":"Configure%20Gigamon%20Tap/#configuring-the-device","text":"","title":"Configuring the Device"},{"location":"Configure%20Gigamon%20Tap/#console-settings","text":"I used the following console settings: On Windows with Putty my serial line was COM3 Speed: 115200 Data bits: 8 Stop bits: 1 Parity: None Flow Control: None","title":"Console Settings"},{"location":"Configure%20Gigamon%20Tap/#logging-in","text":"Default password is root123 . There is no username.","title":"Logging In"},{"location":"Configure%20Gigamon%20Tap/#configure-management","text":"config ipaddr 192.168.1.105 subnetmask 255.255.255.0 WARNING For reasons unknown, telnet is enabled by default on the management interface. If you are using this outside a lab you'll probably want to disable it with telnet 0 . At this juncture, you can telnet to the management IP if you want to. If you want to change the default password this can be done with passwd . If you need to set the date or time you can do so with the commands time <hh:mm:ss> or date <mm-dd-yy> respectively.","title":"Configure Management"},{"location":"Configure%20Gigamon%20Tap/#configure-tap-capability","text":"According to the instructions no configuration is required.","title":"Configure TAP Capability"},{"location":"Configure%20Gigamon%20Tap/#test-results","text":"","title":"Test Results"},{"location":"Configure%20Gigamon%20Tap/#run-1-pass","text":"Scenario: Get nominal load balancing working. For testing I added a second Dell 1Gb/s copper SFP. Distant end was a USB 1Gb/s copper adapter on a laptop. Result: Worked as expected. Input went into the network port and load balanced out the toolA port.","title":"Run 1 - Pass"},{"location":"Configure%20Gigamon%20Tap/#run-2-fail","text":"Scenario: Test to see if the Intel 10Gb/s NIC will run. Distant end was another Intel 10Gb/s SFP in an Intel x710 attached to ESXi. Result: Failure. The tap detected the insert and removal of the SFP: Monitor Port B: \"SFP\" Module Removed Monitor Port B: NOTE: I2C device respond successfully! \"SFP+ SR\" Inserted However, the distant end, which I confirmed to be working using a Dell 4112F-ON, would not come up. Note: As long as the SFP was seated in the Gigamon the green light remained on. There was no correlation between plugging in the cable and the green light.","title":"Run 2 - Fail"},{"location":"Configure%20Gigamon%20Tap/#run-3-fail","text":"Scenario: Same as 2 except I used the 3rd part Gtek SFP+. Results: Same as Run 2.","title":"Run 3 - Fail"},{"location":"Configure%20Gigamon%20Tap/#run-4-fail","text":"Scenario: Same as 2 except with a 10Gb/s Dell adapter. Results: Same as Run 2.","title":"Run 4 - Fail"},{"location":"Configure%20Gigamon%20Tap/#run-5-fail","text":"Scenario: I tried with the 10Gb/s Dell SFP again - this time I made the distant end a RHEL box. Results: RHEL saw nothing. ethtool showed both the duplex and speed as unknown. Nothing came up.","title":"Run 5 - Fail"},{"location":"Configure%20Gigamon%20Tap/#run-6-fail","text":"Scenario: I tried with the 10Gb/s Dell SFP again - this time I made the distant end a Dell 4112F-ON. Results: Even after manually setting speed, duplex, autonegotiation, and double checking the interface types it still didn't come up.","title":"Run 6 - Fail"},{"location":"Configure%20Gigamon%20Tap/#other-helpful-commands","text":"If you need to close or open the tap you can do so using taptx <active|passive> where active pushes traffic to the tool ports and passive only pushes traffic through the network ports. show system gives you a nice overview of the status of the tap and what is plugged in where. G-Tap> show system ======================================================================== System Information ======================================================================== System Name : GTP-A2/SF S/N=*******, rev=A1, HW Built=10/24/2019 SW Version : 2.3.03 Filename=gtb_ASF_IMAGE_2.3.03_20161118.bin App Loader Ver : 2.04 Current : Mon Jun 22, 2020 20:50:26 System Boot : Mon Jun 22, 2020 17:02:39 ------------------------------------------------------------------------ Eth Mgmt Port : DHCP=DISABLE, MAC=00:1D:AC:1B:09:DE : 192.168.1.105/255.255.255.0 gateway=255.255.255.255 : Autoneg=ON, Link=Up Speed=100 Duplex=Full ------------------------------------------------------------------------ Telnet Access : Enabled, Timeout=300 seconds Console baud : 115200 bps ------------------------------------------------------------------------ Power Supply : AC adapter=[ON] DC -48V=[OFF] PoE=[OFF] ------------------------------------------------------------------------ Battery fuel gauge busy. Please Try again... ------------------------------------------------------------------------ Temperatures : Board 38 C, Battery 2 C Fan : OFF ------------------------------------------------------------------------","title":"Other Helpful Commands"},{"location":"Configure%20Gigamon%20Tap/#weird-behaviors","text":"","title":"Weird Behaviors"},{"location":"Configure%20Gigamon%20Tap/#temperature","text":"I continuously saw a warning for temperature:","title":"Temperature"},{"location":"Configure%20Gigamon%20Tap/#battery","text":"I saw a warning for the battery: NOTE: Battery fuel gauge busy! Will try again later It looks like you can buy a battery separately so I believe this to be expected.","title":"Battery"},{"location":"Configure%20Gigamon%20Tap/#where-is-ssh","text":"I was unable to figure out how to configure SSH. In the manual they only had telnet and console listed. I feel like I must be missing something.","title":"Where is SSH?"},{"location":"Configure%20Gigamon%20Tap/#telnet-console-didnt-display-correctly","text":"When I typed in the telnet console with Putty the text would only appear after I hit enter.","title":"Telnet Console Didn't Display Correctly"},{"location":"Configure%20Gigamon%20Tap/#commands-straight-from-the-manual-do-not-work","text":"","title":"Commands Straight from the Manual Do Not Work"},{"location":"Configure%20Gigamon%20Tap/#using-the-arrow-keys-in-the-command-line-deletes-text-instead-of-moving-cursor","text":"Why?","title":"Using the arrow keys in the command line deletes text instead of moving cursor"},{"location":"Configure%20Gigamon%20Tap/#manual-is-incorrect-about-display-output","text":"Ex: G-Tap> show port-params NETWORK MONITOR Parameter Port A Port B Port A Port B ================= ========== ========== ========== ========== Admin: 1 1 1 1 Signal Detect: 1 0 1 1 Tx Power(dBm): n/a n/a n/a -2.36 Rx Power(dBm): n/a n/a n/a -2.50 SFP Module Type: SFP Copper -- SFP Copper SFP+ SR Cable Length(m): n/a n/a n/a n/a The speed, duplex and autonegotiation settings are not listed anywhere in the output.","title":"Manual is incorrect about display output"},{"location":"Configure%20Gigamon%20Tap/#bizzare-compatibility-errors-with-sfps","text":"Even after confirming all setting, fixing the speed at 10Gb/s, duplex at full, and turning off autonegation, the fiber interface still didn't come up. I went and cross referenced the manual to see what type of SFPs they support and confirmed that the wavelength and type matched.","title":"Bizzare Compatibility Errors with SFPs"},{"location":"Configuring%20VLT%20on%20OS10/","text":"Configuring VLT on OS10 Configuring VLT on OS10 My Test Platform Configuration of VLT Device 1 Device 2 Configuration of VLANs for Test Test Scenario Objective Useful Commands Spanning Tree and VLT My Test Platform OS10# show version Dell EMC Networking OS10 Enterprise Copyright (c) 1999-2020 by Dell Inc. All Rights Reserved. OS Version: 10.5.1.3 Build Version: 10.5.1.3.190 Build Time: 2020-06-19T21:48:07+0000 System Type: S4112F-ON Architecture: x86_64 Up Time: 00:19:57 Configuration of VLT Device 1 # Configure management configure terminal interface mgmt 1/1/1 no ip address dhcp ip address 192.168.1.24/24 exit # Configure spanning tree spanning-tree mode rstp interface range ethernet 1/1/13-1/1/14 # Create a VLT Domain and Configure the VLT interconnect (VLTi) no switchport exit vlt-domain 1 discovery-interface ethernet 1/1/13 discovery-interface ethernet 1/1/14 # Configure the VLT Priority, VLT MAC Address, and VLT Backup Link primary-priority 4096 vlt-mac 00:11:22:33:44:55 backup destination 192.168.1.25 end Device 2 # Configure management configure terminal interface mgmt 1/1/1 no ip address dhcp ip address 192.168.1.25/24 exit # Configure spanning tree spanning-tree mode rstp interface range ethernet 1/1/13-1/1/14 # Create a VLT Domain and Configure the VLT interconnect (VLTi) no switchport exit vlt-domain 1 discovery-interface ethernet 1/1/13 discovery-interface ethernet 1/1/14 # Configure the VLT Priority, VLT MAC Address, and VLT Backup Link primary-priority 8192 vlt-mac 00:11:22:33:44:55 backup destination 192.168.1.24 end Configuration of VLANs for Test (On both devices) configure terminal interface vlan 9 no shut exit interface ethernet 1/1/1 switchport mode trunk switchport trunk allowed vlan 9 end On ESXi I used two separate virtual switches each with a port group. Each port group was assigned VLAN 9. Test Scenario Objective Ping from VM1 to VM2 to show that communication will flow over the VLT and vice versa. Works as expected. Useful Commands show vlt 1 show vlt 1 mismatch show running-configuration vlt Spanning Tree and VLT I created the below scenario to see what would happen if you created a situation where RSTP can forward from S2 across both port 11 and port 12 to get to interface VLAN 1 on switch 3. In this scenario I gave the VLAN 1 SVI on S2 10.0.0.2 and on S# 10.0.0.3. The first thing I investigated were the effects on STP on S2. Under the hood you can drop to the Linux command line and inspect the bridge associated with the VLT setup to see its root path cost (here my VLT interface is bo1000 ). It is worth noting that VLT interfaces are represented as bonds to the Linux kernel. Furthermore I also noticed the updated values, if updated from the OS10 shell, are not reflected here: root@OS10:~# brctl showstp br1 br1 bridge id 8000.886fd498b7b1 designated root 8000.886fd498b7b1 root port 0 path cost 0 max age 20.00 bridge max age 20.00 hello time 2.00 bridge hello time 2.00 forward delay 15.00 bridge forward delay 15.00 ageing time 5400.00 hello timer 0.00 tcn timer 0.00 topology change timer 0.00 gc timer 910.24 flags bo1000 (16) port id 8010 state forwarding designated root 8000.886fd498b7b1 path cost 100 designated bridge 8000.886fd498b7b1 message age timer 0.00 designated port 8010 forward delay timer 0.00 designated cost 0 hold timer 0.00 flags ... e101-012-0 (13) port id 800d state forwarding designated root 8000.886fd498b7b1 path cost 100 designated bridge 8000.886fd498b7b1 message age timer 0.00 designated port 800d forward delay timer 0.00 designated cost 0 hold timer 0.00 flags You can see which interfaces are in the VLT bond with ip a s | grep <the interface use used for vlt> . In my case I only used interface 11 so I did: root@OS10:~# ip a s | grep e101-011-0 23: e101-011-0: <BROADCAST,MULTICAST,ALLMULTI,SLAVE,UP,LOWER_UP> mtu 9184 qdisc multiq master bo1000 state UP group default qlen 1000 so we can see here that the VLT bond is bo1000. I then performed a ping 10.0.0.3 from S2 and checked the MAC address table. OS10(conf-if-vl-1)# do show mac address-table VlanId Mac Address Type Interface ... 1 88:6f:d4:98:a7:b1 static port-channel1000 We can see that the switch learned the MAC address for switch 3 over the VLT interface. I had a sneaky suspicion that the switch was biasing the VLT link so I reset the experiment except instead of having the VLT between S2 and S3 I put the VLT between S1 and S2. This has the effect of making it so that instead of a single hop across a port channel to get to S3 it would have two hops using the VLT: I then repeated my VLAN 1 ping test from S2 to S3 and as expected - S2 learned S3's mac address of 88:6f:d4:98:a7:b1 on the VLT port not the port channel: On S2: OS10# show mac address-table VlanId Mac Address Type Interface ... 1 88:6f:d4:98:a7:b1 dynamic port-channel1000 What's going on here is that under the covers it looks like, all things being equal, spanning tree will favor the VLT link. However, if you go into the interface and set the port priority and cost with spanning-tree rstp cost 1 and spanning-tree rstp priority 0 , you can force it to use the port channel instead. I repeated my ping test and then checked the mac address table on S2 and as expected got: OS10(config)# show mac address-table VlanId Mac Address Type Interface ... 1 88:6f:d4:98:a7:b1 dynamic ethernet1/1/11 Then to confirm my results, on S2, I went back and deprioritized port 11 with: OS10(config)# interface ethernet 1/1/11 OS10(conf-if-eth1/1/11)# spanning-tree rstp cost 200000000 OS10(conf-if-eth1/1/11)# spanning-tree rstp priority 240 and then redid my ping and as expected got: OS10(config)# ping 10.0.0.3 PING 10.0.0.3 (10.0.0.3) 56(84) bytes of data. 64 bytes from 10.0.0.3: icmp_seq=1 ttl=64 time=1.16 ms 64 bytes from 10.0.0.3: icmp_seq=2 ttl=64 time=1.04 ms ^C --- 10.0.0.3 ping statistics --- 2 packets transmitted, 2 received, 0% packet loss, time 1001ms rtt min/avg/max/mdev = 1.047/1.106/1.166/0.068 ms OS10(config)# show mac address-table VlanId Mac Address Type Interface ... 1 88:6f:d4:98:a7:b1 dynamic port-channel1000","title":"Configuring VLT on OS10"},{"location":"Configuring%20VLT%20on%20OS10/#configuring-vlt-on-os10","text":"Configuring VLT on OS10 My Test Platform Configuration of VLT Device 1 Device 2 Configuration of VLANs for Test Test Scenario Objective Useful Commands Spanning Tree and VLT","title":"Configuring VLT on OS10"},{"location":"Configuring%20VLT%20on%20OS10/#my-test-platform","text":"OS10# show version Dell EMC Networking OS10 Enterprise Copyright (c) 1999-2020 by Dell Inc. All Rights Reserved. OS Version: 10.5.1.3 Build Version: 10.5.1.3.190 Build Time: 2020-06-19T21:48:07+0000 System Type: S4112F-ON Architecture: x86_64 Up Time: 00:19:57","title":"My Test Platform"},{"location":"Configuring%20VLT%20on%20OS10/#configuration-of-vlt","text":"","title":"Configuration of VLT"},{"location":"Configuring%20VLT%20on%20OS10/#device-1","text":"# Configure management configure terminal interface mgmt 1/1/1 no ip address dhcp ip address 192.168.1.24/24 exit # Configure spanning tree spanning-tree mode rstp interface range ethernet 1/1/13-1/1/14 # Create a VLT Domain and Configure the VLT interconnect (VLTi) no switchport exit vlt-domain 1 discovery-interface ethernet 1/1/13 discovery-interface ethernet 1/1/14 # Configure the VLT Priority, VLT MAC Address, and VLT Backup Link primary-priority 4096 vlt-mac 00:11:22:33:44:55 backup destination 192.168.1.25 end","title":"Device 1"},{"location":"Configuring%20VLT%20on%20OS10/#device-2","text":"# Configure management configure terminal interface mgmt 1/1/1 no ip address dhcp ip address 192.168.1.25/24 exit # Configure spanning tree spanning-tree mode rstp interface range ethernet 1/1/13-1/1/14 # Create a VLT Domain and Configure the VLT interconnect (VLTi) no switchport exit vlt-domain 1 discovery-interface ethernet 1/1/13 discovery-interface ethernet 1/1/14 # Configure the VLT Priority, VLT MAC Address, and VLT Backup Link primary-priority 8192 vlt-mac 00:11:22:33:44:55 backup destination 192.168.1.24 end","title":"Device 2"},{"location":"Configuring%20VLT%20on%20OS10/#configuration-of-vlans-for-test","text":"(On both devices) configure terminal interface vlan 9 no shut exit interface ethernet 1/1/1 switchport mode trunk switchport trunk allowed vlan 9 end On ESXi I used two separate virtual switches each with a port group. Each port group was assigned VLAN 9.","title":"Configuration of VLANs for Test"},{"location":"Configuring%20VLT%20on%20OS10/#test-scenario","text":"","title":"Test Scenario"},{"location":"Configuring%20VLT%20on%20OS10/#objective","text":"Ping from VM1 to VM2 to show that communication will flow over the VLT and vice versa. Works as expected.","title":"Objective"},{"location":"Configuring%20VLT%20on%20OS10/#useful-commands","text":"show vlt 1 show vlt 1 mismatch show running-configuration vlt","title":"Useful Commands"},{"location":"Configuring%20VLT%20on%20OS10/#spanning-tree-and-vlt","text":"I created the below scenario to see what would happen if you created a situation where RSTP can forward from S2 across both port 11 and port 12 to get to interface VLAN 1 on switch 3. In this scenario I gave the VLAN 1 SVI on S2 10.0.0.2 and on S# 10.0.0.3. The first thing I investigated were the effects on STP on S2. Under the hood you can drop to the Linux command line and inspect the bridge associated with the VLT setup to see its root path cost (here my VLT interface is bo1000 ). It is worth noting that VLT interfaces are represented as bonds to the Linux kernel. Furthermore I also noticed the updated values, if updated from the OS10 shell, are not reflected here: root@OS10:~# brctl showstp br1 br1 bridge id 8000.886fd498b7b1 designated root 8000.886fd498b7b1 root port 0 path cost 0 max age 20.00 bridge max age 20.00 hello time 2.00 bridge hello time 2.00 forward delay 15.00 bridge forward delay 15.00 ageing time 5400.00 hello timer 0.00 tcn timer 0.00 topology change timer 0.00 gc timer 910.24 flags bo1000 (16) port id 8010 state forwarding designated root 8000.886fd498b7b1 path cost 100 designated bridge 8000.886fd498b7b1 message age timer 0.00 designated port 8010 forward delay timer 0.00 designated cost 0 hold timer 0.00 flags ... e101-012-0 (13) port id 800d state forwarding designated root 8000.886fd498b7b1 path cost 100 designated bridge 8000.886fd498b7b1 message age timer 0.00 designated port 800d forward delay timer 0.00 designated cost 0 hold timer 0.00 flags You can see which interfaces are in the VLT bond with ip a s | grep <the interface use used for vlt> . In my case I only used interface 11 so I did: root@OS10:~# ip a s | grep e101-011-0 23: e101-011-0: <BROADCAST,MULTICAST,ALLMULTI,SLAVE,UP,LOWER_UP> mtu 9184 qdisc multiq master bo1000 state UP group default qlen 1000 so we can see here that the VLT bond is bo1000. I then performed a ping 10.0.0.3 from S2 and checked the MAC address table. OS10(conf-if-vl-1)# do show mac address-table VlanId Mac Address Type Interface ... 1 88:6f:d4:98:a7:b1 static port-channel1000 We can see that the switch learned the MAC address for switch 3 over the VLT interface. I had a sneaky suspicion that the switch was biasing the VLT link so I reset the experiment except instead of having the VLT between S2 and S3 I put the VLT between S1 and S2. This has the effect of making it so that instead of a single hop across a port channel to get to S3 it would have two hops using the VLT: I then repeated my VLAN 1 ping test from S2 to S3 and as expected - S2 learned S3's mac address of 88:6f:d4:98:a7:b1 on the VLT port not the port channel: On S2: OS10# show mac address-table VlanId Mac Address Type Interface ... 1 88:6f:d4:98:a7:b1 dynamic port-channel1000 What's going on here is that under the covers it looks like, all things being equal, spanning tree will favor the VLT link. However, if you go into the interface and set the port priority and cost with spanning-tree rstp cost 1 and spanning-tree rstp priority 0 , you can force it to use the port channel instead. I repeated my ping test and then checked the mac address table on S2 and as expected got: OS10(config)# show mac address-table VlanId Mac Address Type Interface ... 1 88:6f:d4:98:a7:b1 dynamic ethernet1/1/11 Then to confirm my results, on S2, I went back and deprioritized port 11 with: OS10(config)# interface ethernet 1/1/11 OS10(conf-if-eth1/1/11)# spanning-tree rstp cost 200000000 OS10(conf-if-eth1/1/11)# spanning-tree rstp priority 240 and then redid my ping and as expected got: OS10(config)# ping 10.0.0.3 PING 10.0.0.3 (10.0.0.3) 56(84) bytes of data. 64 bytes from 10.0.0.3: icmp_seq=1 ttl=64 time=1.16 ms 64 bytes from 10.0.0.3: icmp_seq=2 ttl=64 time=1.04 ms ^C --- 10.0.0.3 ping statistics --- 2 packets transmitted, 2 received, 0% packet loss, time 1001ms rtt min/avg/max/mdev = 1.047/1.106/1.166/0.068 ms OS10(config)# show mac address-table VlanId Mac Address Type Interface ... 1 88:6f:d4:98:a7:b1 dynamic port-channel1000","title":"Spanning Tree and VLT"},{"location":"Create%20Kickstart%20Server%20on%20Fedora/","text":"Create Kickstart Server on Fedora Files All files housed here Intro This Ansible playbook runs on CentOS. It will create a server which allows you to install Fedora automatically via PXE boot and Kickstart. For more information on Fedora's Kickstart capabilities see this link Useful Information How to set up Fedora Kickstart The Fedora Mirror I used https://gist.github.com/andrewwippler/b636cdb68249ab5ffb67b4d8693a780b Prerequisites Note: If you use a VM it will need at least 90GB of space. Install CentOS You can download and install CentOS here The installation services used by this program are very lightweight so you can install on just about anything. A 12GB hard drive, 2GB RAM, and a single processor are enough for what we are doing. Go ahead and install CentOS on whatever you like, the only requirement is that it must be reachable at layer 2 by the Fedora hosts you want to install. Install Ansible and git Before continuing you will need to install Ansible on your host by running yum install -y ansible git . Disable SELinux Or set proper policies. Pick your poison. Since I only stood this thing up for as long as it took to kickstart I was lazy and just disabled it. Sue me. setenforce 0 I also updated this in /etc/selniux/config . Grab Image Files You need to grab the image files for vmlinuz and initrd.img in order for Linux to boot properly. For Fedora grab them from the below. wget https://download.fedoraproject.org/pub/fedora/linux/releases/31/Server/x86_64/os/images/pxeboot/vmlinuz -O /var/lib/tftpboot/vmlinuz wget https://download.fedoraproject.org/pub/fedora/linux/releases/31/Server/x86_64/os/images/pxeboot/initrd.img -O /var/lib/tftpboot/initrd.img wget http://fedora.mirrors.pair.com/linux/releases/31/Server/x86_64/os/images/install.img -O /var/www/html/iso/images/install.img Clone the Fedora Repositories On a host of your choosing, run the following (update the 31 to the appropriate Fedora version): rsync http://ftp.muug.mb.ca/pub/fedora/linux/releases/31/Everything/x86_64/os/Packages/ /var/www/html/iso/Packages Note: You will probably have to create the above directory. You'll need 73 GB free to sync the packages. You will also need to grab the file in the repodata directory called ending with comps.xml. You can see its exact name in repomd.xml. Set it and forget it because it will be a bit while this all downloads. After you are downloading run: createrepo -g <THE COMPS FILE YOU DOWNLOADED> /var/www/html/iso/Packages Clone the Repo I typically use opt for optional programs. You may install wherever you like, but in this guide I will use opt. Clone the repo with: git clone https://github.com/grantcurell/Fedora-autoinstall.git Move into the Fedora-autoinstall directory with cd /opt/Fedora-autoinstall Configure the Inventory File Ansible is controlled by what is called an inventory file. This inventory file contains all the configuration settings which will be used by Ansible. In our case, we will need to populate this with the settings we wish Fedora to have after installation. The inventory file is called inventory.yml You will need to fill in the following values: dns dhcp_start dhcp_end gateway netmask domain root_password server_ip iso_fedora_pth iso_fedora_checksum Finally you will also need to fill in the host information. These are the hosts on which you would like to install Fedora. Boot Drives Most of the values are self explanatory however, boot_drive and data_drives may be a bit confusing. These options allow you to tell Ansible where to install Fedora and on which disks to configure datastores. The official documentation on how Fedora names drives is here The disks are in the order in which Fedora detects them. To get the order you may have to manually install Fedora on a system. You can make an educated guess from the BIOS menu or if you have something like iDrac you can look at the order of the disks on the RAID controller (assuming you have one). Running the Code Once you are finished editing the inventory.yml file, cd to the root of the Fedora-autoinstall directory and run make . This will run the makefile in the directory. You can see what it is doing by examining the file called Makefile CRITICAL: Change BIOS Settings If you have NVMe drives and are trying to install Fedora, make sure you go into your BIOS and disable Intel Rapid Storage Technology. If you do not, the NVMe drive will not appear and Anaconda will report that there are no disks available. (Optional) Make it so that all hosts receive dhcp I intentionally left it set up so that only configured hosts would receive dhcpd to make sure no one nukes their network. That said, if you want to make it so that all hosts receive dhcp edit /etc/dhcp/dhcpd.conf remove the line deny unknown-clients and then add the line filename \"uefi/BOOTX64.EFI\"; in the subnet {} directive. You will have to repeat this if you run make again. After you are done, run systemctl restart dhcpd Make the Repository Available to Your Clients Add a file called local.repo in /etc/yum.repos.d/local.repo with the following contents: [local] name=Local baseurl=http://192.168.1.121/iso/Packages enabled=1 gpgcheck=0 Advanced Importing Packages from Another System Minimally Method 2 - Under Development Say you want to create a new offline installer. What you'll have to do is go to a system that has all of the packages you might want to use in your new image. You will then want to scrape all those packages so that you can then use them on your new kickstart server. Do the following. On the system that already has the packages run: dnf list installed | cut -d ' ' -f 1 | tr '\\r\\n' ' ' | sed s/Installed//g . The above command gets a list of all packages on the system and outputs them space separated. You could pipe that into xargs, but I prefer to do this next part manually in case there are any packages that don't work. Copy the output of the above and paste it as an argument to dnf download --resolve <YOUR_STUFF> . That will download all the packages on the server and their dependencies. If any packages do not resolve you can run dnf list installed | cut -d ' ' -f 1 | tr '\\r\\n' ' ' | sed s/Installed//g | sed s/<PACKAGE_NAME_THAT_DIDNT_WORK>//g to omit it from the list. After you run make on the Kickstart server to run the above command, go to /var/www/html/iso/packages/ and copy all of the packages to that directory. You can use something like scp * root@192.168.1.121:/var/www/html/iso/packages/ to do this. When you are done, browse to your server using Chrome or something and make sure all the packages are available. Ex: http://192.168.1.121/iso/packages/ Info on the Kickstart Process Kickstart has a lot of black magic going on. So below I give a high level overview of what's happening. Server boots and you tell it to boot with PXE. PXE will then send out a DHCP request. The DHCP server that responds should be your kickstart server (typically) and it will have a few extra options set. Namely there should be a line that looks like the following: # This line tell sth server to reach out over TFTP and grab the uefi/BOOTX64.EFI file. This file is a binary # file which tells the server how to boot. It will be specific to your distro and you should generally get # it from the ISO which you want to install. The path is a relative path against the TFTP root directory. filename \"uefi/BOOTX64.EFI\"; Next, if you are doing UEFI, the server will search for a series of file names in the TFTP server's UEFI directory until it finds a boot menu profile that matches. I couldn't find documentation on it, but if you watch /var/log/messages you can actually see it try different as show below. It progresses through listed host MAC addresses first, then the IP address in hex format, and at the very end tries the default grub.cfg. To get maximum output you can add server_args = -s /var/lib/tftpboot --verbose to /etc/xinetd.d/tftp assuming you are using xinetd to get this output. Jan 21 15:45:00 controller xinetd[10044]: START: tftp pid=10048 from=172.16.71.98 Jan 21 15:45:00 controller in.tftpd[10049]: RRQ from 172.16.71.98 filename uefi/BOOTX64.EFI Jan 21 15:45:00 controller in.tftpd[10049]: Error code 8: User aborted the transfer Jan 21 15:45:00 controller in.tftpd[10050]: RRQ from 172.16.71.98 filename uefi/BOOTX64.EFI Jan 21 15:45:00 controller in.tftpd[10050]: Client 172.16.71.98 finished uefi/BOOTX64.EFI Jan 21 15:45:00 controller in.tftpd[10051]: RRQ from 172.16.71.98 filename uefi/grubx64.efi Jan 21 15:45:00 controller in.tftpd[10051]: Client 172.16.71.98 finished uefi/grubx64.efi Jan 21 15:45:00 controller in.tftpd[10052]: RRQ from 172.16.71.98 filename /uefi/grub.cfg-01-00-50-56-86-d4-06 Jan 21 15:45:00 controller in.tftpd[10052]: Client 172.16.71.98 File not found /uefi/grub.cfg-01-00-50-56-86-d4-06 Jan 21 15:45:00 controller in.tftpd[10053]: RRQ from 172.16.71.98 filename /uefi/grub.cfg-AC104762 Jan 21 15:45:00 controller in.tftpd[10053]: Client 172.16.71.98 File not found /uefi/grub.cfg-AC104762 Jan 21 15:45:00 controller in.tftpd[10054]: RRQ from 172.16.71.98 filename /uefi/grub.cfg-AC10476 Jan 21 15:45:00 controller in.tftpd[10054]: Client 172.16.71.98 File not found /uefi/grub.cfg-AC10476 Jan 21 15:45:00 controller in.tftpd[10055]: RRQ from 172.16.71.98 filename /uefi/grub.cfg-AC1047 Jan 21 15:45:00 controller in.tftpd[10055]: Client 172.16.71.98 File not found /uefi/grub.cfg-AC1047 Jan 21 15:45:00 controller in.tftpd[10056]: RRQ from 172.16.71.98 filename /uefi/grub.cfg-AC104 Jan 21 15:45:00 controller in.tftpd[10056]: Client 172.16.71.98 File not found /uefi/grub.cfg-AC104 Jan 21 15:45:00 controller in.tftpd[10057]: RRQ from 172.16.71.98 filename /uefi/grub.cfg-AC10 Jan 21 15:45:00 controller in.tftpd[10057]: Client 172.16.71.98 File not found /uefi/grub.cfg-AC10 Jan 21 15:45:00 controller in.tftpd[10058]: RRQ from 172.16.71.98 filename /uefi/grub.cfg-AC1 Jan 21 15:45:00 controller in.tftpd[10058]: Client 172.16.71.98 File not found /uefi/grub.cfg-AC1 Jan 21 15:45:00 controller in.tftpd[10059]: RRQ from 172.16.71.98 filename /uefi/grub.cfg-AC Jan 21 15:45:00 controller in.tftpd[10059]: Client 172.16.71.98 File not found /uefi/grub.cfg-AC Jan 21 15:45:00 controller in.tftpd[10060]: RRQ from 172.16.71.98 filename /uefi/grub.cfg-A Jan 21 15:45:00 controller in.tftpd[10060]: Client 172.16.71.98 File not found /uefi/grub.cfg-A Jan 21 15:45:00 controller in.tftpd[10061]: RRQ from 172.16.71.98 filename /uefi/grub.cfg Jan 21 15:45:00 controller in.tftpd[10061]: Client 172.16.71.98 finished /uefi/grub.cfg Jan 21 15:45:00 controller in.tftpd[10062]: RRQ from 172.16.71.98 filename /EFI/BOOT/x86_64-efi/command.lst Jan 21 15:45:00 controller in.tftpd[10062]: Client 172.16.71.98 File not found /EFI/BOOT/x86_64-efi/command.lst Jan 21 15:45:00 controller in.tftpd[10063]: RRQ from 172.16.71.98 filename /EFI/BOOT/x86_64-efi/fs.lst Jan 21 15:45:00 controller in.tftpd[10063]: Client 172.16.71.98 File not found /EFI/BOOT/x86_64-efi/fs.lst Jan 21 15:45:00 controller in.tftpd[10064]: RRQ from 172.16.71.98 filename /EFI/BOOT/x86_64-efi/crypto.lst Jan 21 15:45:00 controller in.tftpd[10064]: Client 172.16.71.98 File not found /EFI/BOOT/x86_64-efi/crypto.lst Jan 21 15:45:00 controller in.tftpd[10065]: RRQ from 172.16.71.98 filename /EFI/BOOT/x86_64-efi/terminal.lst Jan 21 15:45:00 controller in.tftpd[10065]: Client 172.16.71.98 File not found /EFI/BOOT/x86_64-efi/terminal.lst Jan 21 15:45:00 controller in.tftpd[10066]: RRQ from 172.16.71.98 filename /uefi/grub.cfg Jan 21 15:45:00 controller in.tftpd[10066]: Client 172.16.71.98 finished /uefi/grub.cfg Jan 21 15:45:02 controller in.tftpd[10067]: RRQ from 172.16.71.98 filename /vmlinuz Jan 21 15:45:03 controller in.tftpd[10067]: Client 172.16.71.98 finished /vmlinuz Jan 21 15:45:03 controller in.tftpd[10068]: RRQ from 172.16.71.98 filename /initrd.img Jan 21 15:45:08 controller in.tftpd[10068]: Client 172.16.71.98 finished /initrd.img The boot menu profile (grub.cfg) looks like the below. The most important line is the linuxefi line. This line points to where your installation media should be hosted. Typically using httpd. set default=\"1\" function load_video { insmod efi_gop insmod efi_uga insmod video_bochs insmod video_cirrus insmod all_video } load_video set gfxpayload=keep insmod gzio insmod part_gpt insmod ext2 set timeout=1 ### END /etc/grub.d/00_header ### ### BEGIN /etc/grub.d/10_linux ### menuentry 'Install Super Legit Fedora' --class fedora --class gnu-linux --class gnu --class os { echo \"Loading vmlinuz\" linuxefi vmlinuz inst.ks=http://192.168.2.101/ks/uefi/fedora.cfg inst.repo=http://192.168.1.121/iso/ echo \"Loading initrd.img\" initrdefi initrd.img } The host in question will reach out and in this configuration grab a file called install.img located at /var/www/html/iso/images/ in this build. This file allows the host to boot and perform its other functions. TODO - finish this.","title":"Create Kickstart Server on Fedora"},{"location":"Create%20Kickstart%20Server%20on%20Fedora/#create-kickstart-server-on-fedora","text":"","title":"Create Kickstart Server on Fedora"},{"location":"Create%20Kickstart%20Server%20on%20Fedora/#files","text":"All files housed here","title":"Files"},{"location":"Create%20Kickstart%20Server%20on%20Fedora/#intro","text":"This Ansible playbook runs on CentOS. It will create a server which allows you to install Fedora automatically via PXE boot and Kickstart. For more information on Fedora's Kickstart capabilities see this link","title":"Intro"},{"location":"Create%20Kickstart%20Server%20on%20Fedora/#useful-information","text":"How to set up Fedora Kickstart The Fedora Mirror I used https://gist.github.com/andrewwippler/b636cdb68249ab5ffb67b4d8693a780b","title":"Useful Information"},{"location":"Create%20Kickstart%20Server%20on%20Fedora/#prerequisites","text":"Note: If you use a VM it will need at least 90GB of space.","title":"Prerequisites"},{"location":"Create%20Kickstart%20Server%20on%20Fedora/#install-centos","text":"You can download and install CentOS here The installation services used by this program are very lightweight so you can install on just about anything. A 12GB hard drive, 2GB RAM, and a single processor are enough for what we are doing. Go ahead and install CentOS on whatever you like, the only requirement is that it must be reachable at layer 2 by the Fedora hosts you want to install.","title":"Install CentOS"},{"location":"Create%20Kickstart%20Server%20on%20Fedora/#install-ansible-and-git","text":"Before continuing you will need to install Ansible on your host by running yum install -y ansible git .","title":"Install Ansible and git"},{"location":"Create%20Kickstart%20Server%20on%20Fedora/#disable-selinux","text":"Or set proper policies. Pick your poison. Since I only stood this thing up for as long as it took to kickstart I was lazy and just disabled it. Sue me. setenforce 0 I also updated this in /etc/selniux/config .","title":"Disable SELinux"},{"location":"Create%20Kickstart%20Server%20on%20Fedora/#grab-image-files","text":"You need to grab the image files for vmlinuz and initrd.img in order for Linux to boot properly. For Fedora grab them from the below. wget https://download.fedoraproject.org/pub/fedora/linux/releases/31/Server/x86_64/os/images/pxeboot/vmlinuz -O /var/lib/tftpboot/vmlinuz wget https://download.fedoraproject.org/pub/fedora/linux/releases/31/Server/x86_64/os/images/pxeboot/initrd.img -O /var/lib/tftpboot/initrd.img wget http://fedora.mirrors.pair.com/linux/releases/31/Server/x86_64/os/images/install.img -O /var/www/html/iso/images/install.img","title":"Grab Image Files"},{"location":"Create%20Kickstart%20Server%20on%20Fedora/#clone-the-fedora-repositories","text":"On a host of your choosing, run the following (update the 31 to the appropriate Fedora version): rsync http://ftp.muug.mb.ca/pub/fedora/linux/releases/31/Everything/x86_64/os/Packages/ /var/www/html/iso/Packages Note: You will probably have to create the above directory. You'll need 73 GB free to sync the packages. You will also need to grab the file in the repodata directory called ending with comps.xml. You can see its exact name in repomd.xml. Set it and forget it because it will be a bit while this all downloads. After you are downloading run: createrepo -g <THE COMPS FILE YOU DOWNLOADED> /var/www/html/iso/Packages","title":"Clone the Fedora Repositories"},{"location":"Create%20Kickstart%20Server%20on%20Fedora/#clone-the-repo","text":"I typically use opt for optional programs. You may install wherever you like, but in this guide I will use opt. Clone the repo with: git clone https://github.com/grantcurell/Fedora-autoinstall.git Move into the Fedora-autoinstall directory with cd /opt/Fedora-autoinstall","title":"Clone the Repo"},{"location":"Create%20Kickstart%20Server%20on%20Fedora/#configure-the-inventory-file","text":"Ansible is controlled by what is called an inventory file. This inventory file contains all the configuration settings which will be used by Ansible. In our case, we will need to populate this with the settings we wish Fedora to have after installation. The inventory file is called inventory.yml You will need to fill in the following values: dns dhcp_start dhcp_end gateway netmask domain root_password server_ip iso_fedora_pth iso_fedora_checksum Finally you will also need to fill in the host information. These are the hosts on which you would like to install Fedora.","title":"Configure the Inventory File"},{"location":"Create%20Kickstart%20Server%20on%20Fedora/#boot-drives","text":"Most of the values are self explanatory however, boot_drive and data_drives may be a bit confusing. These options allow you to tell Ansible where to install Fedora and on which disks to configure datastores. The official documentation on how Fedora names drives is here The disks are in the order in which Fedora detects them. To get the order you may have to manually install Fedora on a system. You can make an educated guess from the BIOS menu or if you have something like iDrac you can look at the order of the disks on the RAID controller (assuming you have one).","title":"Boot Drives"},{"location":"Create%20Kickstart%20Server%20on%20Fedora/#running-the-code","text":"Once you are finished editing the inventory.yml file, cd to the root of the Fedora-autoinstall directory and run make . This will run the makefile in the directory. You can see what it is doing by examining the file called Makefile","title":"Running the Code"},{"location":"Create%20Kickstart%20Server%20on%20Fedora/#critical-change-bios-settings","text":"If you have NVMe drives and are trying to install Fedora, make sure you go into your BIOS and disable Intel Rapid Storage Technology. If you do not, the NVMe drive will not appear and Anaconda will report that there are no disks available.","title":"CRITICAL: Change BIOS Settings"},{"location":"Create%20Kickstart%20Server%20on%20Fedora/#optional-make-it-so-that-all-hosts-receive-dhcp","text":"I intentionally left it set up so that only configured hosts would receive dhcpd to make sure no one nukes their network. That said, if you want to make it so that all hosts receive dhcp edit /etc/dhcp/dhcpd.conf remove the line deny unknown-clients and then add the line filename \"uefi/BOOTX64.EFI\"; in the subnet {} directive. You will have to repeat this if you run make again. After you are done, run systemctl restart dhcpd","title":"(Optional) Make it so that all hosts receive dhcp"},{"location":"Create%20Kickstart%20Server%20on%20Fedora/#make-the-repository-available-to-your-clients","text":"Add a file called local.repo in /etc/yum.repos.d/local.repo with the following contents: [local] name=Local baseurl=http://192.168.1.121/iso/Packages enabled=1 gpgcheck=0","title":"Make the Repository Available to Your Clients"},{"location":"Create%20Kickstart%20Server%20on%20Fedora/#advanced","text":"","title":"Advanced"},{"location":"Create%20Kickstart%20Server%20on%20Fedora/#importing-packages-from-another-system-minimally","text":"","title":"Importing Packages from Another System Minimally"},{"location":"Create%20Kickstart%20Server%20on%20Fedora/#method-2-under-development","text":"Say you want to create a new offline installer. What you'll have to do is go to a system that has all of the packages you might want to use in your new image. You will then want to scrape all those packages so that you can then use them on your new kickstart server. Do the following. On the system that already has the packages run: dnf list installed | cut -d ' ' -f 1 | tr '\\r\\n' ' ' | sed s/Installed//g . The above command gets a list of all packages on the system and outputs them space separated. You could pipe that into xargs, but I prefer to do this next part manually in case there are any packages that don't work. Copy the output of the above and paste it as an argument to dnf download --resolve <YOUR_STUFF> . That will download all the packages on the server and their dependencies. If any packages do not resolve you can run dnf list installed | cut -d ' ' -f 1 | tr '\\r\\n' ' ' | sed s/Installed//g | sed s/<PACKAGE_NAME_THAT_DIDNT_WORK>//g to omit it from the list. After you run make on the Kickstart server to run the above command, go to /var/www/html/iso/packages/ and copy all of the packages to that directory. You can use something like scp * root@192.168.1.121:/var/www/html/iso/packages/ to do this. When you are done, browse to your server using Chrome or something and make sure all the packages are available. Ex: http://192.168.1.121/iso/packages/","title":"Method 2 - Under Development"},{"location":"Create%20Kickstart%20Server%20on%20Fedora/#info-on-the-kickstart-process","text":"Kickstart has a lot of black magic going on. So below I give a high level overview of what's happening. Server boots and you tell it to boot with PXE. PXE will then send out a DHCP request. The DHCP server that responds should be your kickstart server (typically) and it will have a few extra options set. Namely there should be a line that looks like the following: # This line tell sth server to reach out over TFTP and grab the uefi/BOOTX64.EFI file. This file is a binary # file which tells the server how to boot. It will be specific to your distro and you should generally get # it from the ISO which you want to install. The path is a relative path against the TFTP root directory. filename \"uefi/BOOTX64.EFI\"; Next, if you are doing UEFI, the server will search for a series of file names in the TFTP server's UEFI directory until it finds a boot menu profile that matches. I couldn't find documentation on it, but if you watch /var/log/messages you can actually see it try different as show below. It progresses through listed host MAC addresses first, then the IP address in hex format, and at the very end tries the default grub.cfg. To get maximum output you can add server_args = -s /var/lib/tftpboot --verbose to /etc/xinetd.d/tftp assuming you are using xinetd to get this output. Jan 21 15:45:00 controller xinetd[10044]: START: tftp pid=10048 from=172.16.71.98 Jan 21 15:45:00 controller in.tftpd[10049]: RRQ from 172.16.71.98 filename uefi/BOOTX64.EFI Jan 21 15:45:00 controller in.tftpd[10049]: Error code 8: User aborted the transfer Jan 21 15:45:00 controller in.tftpd[10050]: RRQ from 172.16.71.98 filename uefi/BOOTX64.EFI Jan 21 15:45:00 controller in.tftpd[10050]: Client 172.16.71.98 finished uefi/BOOTX64.EFI Jan 21 15:45:00 controller in.tftpd[10051]: RRQ from 172.16.71.98 filename uefi/grubx64.efi Jan 21 15:45:00 controller in.tftpd[10051]: Client 172.16.71.98 finished uefi/grubx64.efi Jan 21 15:45:00 controller in.tftpd[10052]: RRQ from 172.16.71.98 filename /uefi/grub.cfg-01-00-50-56-86-d4-06 Jan 21 15:45:00 controller in.tftpd[10052]: Client 172.16.71.98 File not found /uefi/grub.cfg-01-00-50-56-86-d4-06 Jan 21 15:45:00 controller in.tftpd[10053]: RRQ from 172.16.71.98 filename /uefi/grub.cfg-AC104762 Jan 21 15:45:00 controller in.tftpd[10053]: Client 172.16.71.98 File not found /uefi/grub.cfg-AC104762 Jan 21 15:45:00 controller in.tftpd[10054]: RRQ from 172.16.71.98 filename /uefi/grub.cfg-AC10476 Jan 21 15:45:00 controller in.tftpd[10054]: Client 172.16.71.98 File not found /uefi/grub.cfg-AC10476 Jan 21 15:45:00 controller in.tftpd[10055]: RRQ from 172.16.71.98 filename /uefi/grub.cfg-AC1047 Jan 21 15:45:00 controller in.tftpd[10055]: Client 172.16.71.98 File not found /uefi/grub.cfg-AC1047 Jan 21 15:45:00 controller in.tftpd[10056]: RRQ from 172.16.71.98 filename /uefi/grub.cfg-AC104 Jan 21 15:45:00 controller in.tftpd[10056]: Client 172.16.71.98 File not found /uefi/grub.cfg-AC104 Jan 21 15:45:00 controller in.tftpd[10057]: RRQ from 172.16.71.98 filename /uefi/grub.cfg-AC10 Jan 21 15:45:00 controller in.tftpd[10057]: Client 172.16.71.98 File not found /uefi/grub.cfg-AC10 Jan 21 15:45:00 controller in.tftpd[10058]: RRQ from 172.16.71.98 filename /uefi/grub.cfg-AC1 Jan 21 15:45:00 controller in.tftpd[10058]: Client 172.16.71.98 File not found /uefi/grub.cfg-AC1 Jan 21 15:45:00 controller in.tftpd[10059]: RRQ from 172.16.71.98 filename /uefi/grub.cfg-AC Jan 21 15:45:00 controller in.tftpd[10059]: Client 172.16.71.98 File not found /uefi/grub.cfg-AC Jan 21 15:45:00 controller in.tftpd[10060]: RRQ from 172.16.71.98 filename /uefi/grub.cfg-A Jan 21 15:45:00 controller in.tftpd[10060]: Client 172.16.71.98 File not found /uefi/grub.cfg-A Jan 21 15:45:00 controller in.tftpd[10061]: RRQ from 172.16.71.98 filename /uefi/grub.cfg Jan 21 15:45:00 controller in.tftpd[10061]: Client 172.16.71.98 finished /uefi/grub.cfg Jan 21 15:45:00 controller in.tftpd[10062]: RRQ from 172.16.71.98 filename /EFI/BOOT/x86_64-efi/command.lst Jan 21 15:45:00 controller in.tftpd[10062]: Client 172.16.71.98 File not found /EFI/BOOT/x86_64-efi/command.lst Jan 21 15:45:00 controller in.tftpd[10063]: RRQ from 172.16.71.98 filename /EFI/BOOT/x86_64-efi/fs.lst Jan 21 15:45:00 controller in.tftpd[10063]: Client 172.16.71.98 File not found /EFI/BOOT/x86_64-efi/fs.lst Jan 21 15:45:00 controller in.tftpd[10064]: RRQ from 172.16.71.98 filename /EFI/BOOT/x86_64-efi/crypto.lst Jan 21 15:45:00 controller in.tftpd[10064]: Client 172.16.71.98 File not found /EFI/BOOT/x86_64-efi/crypto.lst Jan 21 15:45:00 controller in.tftpd[10065]: RRQ from 172.16.71.98 filename /EFI/BOOT/x86_64-efi/terminal.lst Jan 21 15:45:00 controller in.tftpd[10065]: Client 172.16.71.98 File not found /EFI/BOOT/x86_64-efi/terminal.lst Jan 21 15:45:00 controller in.tftpd[10066]: RRQ from 172.16.71.98 filename /uefi/grub.cfg Jan 21 15:45:00 controller in.tftpd[10066]: Client 172.16.71.98 finished /uefi/grub.cfg Jan 21 15:45:02 controller in.tftpd[10067]: RRQ from 172.16.71.98 filename /vmlinuz Jan 21 15:45:03 controller in.tftpd[10067]: Client 172.16.71.98 finished /vmlinuz Jan 21 15:45:03 controller in.tftpd[10068]: RRQ from 172.16.71.98 filename /initrd.img Jan 21 15:45:08 controller in.tftpd[10068]: Client 172.16.71.98 finished /initrd.img The boot menu profile (grub.cfg) looks like the below. The most important line is the linuxefi line. This line points to where your installation media should be hosted. Typically using httpd. set default=\"1\" function load_video { insmod efi_gop insmod efi_uga insmod video_bochs insmod video_cirrus insmod all_video } load_video set gfxpayload=keep insmod gzio insmod part_gpt insmod ext2 set timeout=1 ### END /etc/grub.d/00_header ### ### BEGIN /etc/grub.d/10_linux ### menuentry 'Install Super Legit Fedora' --class fedora --class gnu-linux --class gnu --class os { echo \"Loading vmlinuz\" linuxefi vmlinuz inst.ks=http://192.168.2.101/ks/uefi/fedora.cfg inst.repo=http://192.168.1.121/iso/ echo \"Loading initrd.img\" initrdefi initrd.img } The host in question will reach out and in this configuration grab a file called install.img located at /var/www/html/iso/images/ in this build. This file allows the host to boot and perform its other functions. TODO - finish this.","title":"Info on the Kickstart Process"},{"location":"Create%20Kickstart%20Server%20on%20Fedora/LICENSE/","text":"GNU GENERAL PUBLIC LICENSE Version 3, 29 June 2007 Copyright (C) 2007 Free Software Foundation, Inc. http://fsf.org/ Everyone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed. Preamble The GNU General Public License is a free, copyleft license for software and other kinds of works. The licenses for most software and other practical works are designed to take away your freedom to share and change the works. By contrast, the GNU General Public License is intended to guarantee your freedom to share and change all versions of a program--to make sure it remains free software for all its users. We, the Free Software Foundation, use the GNU General Public License for most of our software; it applies also to any other work released this way by its authors. You can apply it to your programs, too. When we speak of free software, we are referring to freedom, not price. Our General Public Licenses are designed to make sure that you have the freedom to distribute copies of free software (and charge for them if you wish), that you receive source code or can get it if you want it, that you can change the software or use pieces of it in new free programs, and that you know you can do these things. To protect your rights, we need to prevent others from denying you these rights or asking you to surrender the rights. Therefore, you have certain responsibilities if you distribute copies of the software, or if you modify it: responsibilities to respect the freedom of others. For example, if you distribute copies of such a program, whether gratis or for a fee, you must pass on to the recipients the same freedoms that you received. You must make sure that they, too, receive or can get the source code. And you must show them these terms so they know their rights. Developers that use the GNU GPL protect your rights with two steps: (1) assert copyright on the software, and (2) offer you this License giving you legal permission to copy, distribute and/or modify it. For the developers' and authors' protection, the GPL clearly explains that there is no warranty for this free software. For both users' and authors' sake, the GPL requires that modified versions be marked as changed, so that their problems will not be attributed erroneously to authors of previous versions. Some devices are designed to deny users access to install or run modified versions of the software inside them, although the manufacturer can do so. This is fundamentally incompatible with the aim of protecting users' freedom to change the software. The systematic pattern of such abuse occurs in the area of products for individuals to use, which is precisely where it is most unacceptable. Therefore, we have designed this version of the GPL to prohibit the practice for those products. If such problems arise substantially in other domains, we stand ready to extend this provision to those domains in future versions of the GPL, as needed to protect the freedom of users. Finally, every program is threatened constantly by software patents. States should not allow patents to restrict development and use of software on general-purpose computers, but in those that do, we wish to avoid the special danger that patents applied to a free program could make it effectively proprietary. To prevent this, the GPL assures that patents cannot be used to render the program non-free. The precise terms and conditions for copying, distribution and modification follow. TERMS AND CONDITIONS Definitions. \"This License\" refers to version 3 of the GNU General Public License. \"Copyright\" also means copyright-like laws that apply to other kinds of works, such as semiconductor masks. \"The Program\" refers to any copyrightable work licensed under this License. Each licensee is addressed as \"you\". \"Licensees\" and \"recipients\" may be individuals or organizations. To \"modify\" a work means to copy from or adapt all or part of the work in a fashion requiring copyright permission, other than the making of an exact copy. The resulting work is called a \"modified version\" of the earlier work or a work \"based on\" the earlier work. A \"covered work\" means either the unmodified Program or a work based on the Program. To \"propagate\" a work means to do anything with it that, without permission, would make you directly or secondarily liable for infringement under applicable copyright law, except executing it on a computer or modifying a private copy. Propagation includes copying, distribution (with or without modification), making available to the public, and in some countries other activities as well. To \"convey\" a work means any kind of propagation that enables other parties to make or receive copies. Mere interaction with a user through a computer network, with no transfer of a copy, is not conveying. An interactive user interface displays \"Appropriate Legal Notices\" to the extent that it includes a convenient and prominently visible feature that (1) displays an appropriate copyright notice, and (2) tells the user that there is no warranty for the work (except to the extent that warranties are provided), that licensees may convey the work under this License, and how to view a copy of this License. If the interface presents a list of user commands or options, such as a menu, a prominent item in the list meets this criterion. Source Code. The \"source code\" for a work means the preferred form of the work for making modifications to it. \"Object code\" means any non-source form of a work. A \"Standard Interface\" means an interface that either is an official standard defined by a recognized standards body, or, in the case of interfaces specified for a particular programming language, one that is widely used among developers working in that language. The \"System Libraries\" of an executable work include anything, other than the work as a whole, that (a) is included in the normal form of packaging a Major Component, but which is not part of that Major Component, and (b) serves only to enable use of the work with that Major Component, or to implement a Standard Interface for which an implementation is available to the public in source code form. A \"Major Component\", in this context, means a major essential component (kernel, window system, and so on) of the specific operating system (if any) on which the executable work runs, or a compiler used to produce the work, or an object code interpreter used to run it. The \"Corresponding Source\" for a work in object code form means all the source code needed to generate, install, and (for an executable work) run the object code and to modify the work, including scripts to control those activities. However, it does not include the work's System Libraries, or general-purpose tools or generally available free programs which are used unmodified in performing those activities but which are not part of the work. For example, Corresponding Source includes interface definition files associated with source files for the work, and the source code for shared libraries and dynamically linked subprograms that the work is specifically designed to require, such as by intimate data communication or control flow between those subprograms and other parts of the work. The Corresponding Source need not include anything that users can regenerate automatically from other parts of the Corresponding Source. The Corresponding Source for a work in source code form is that same work. Basic Permissions. All rights granted under this License are granted for the term of copyright on the Program, and are irrevocable provided the stated conditions are met. This License explicitly affirms your unlimited permission to run the unmodified Program. The output from running a covered work is covered by this License only if the output, given its content, constitutes a covered work. This License acknowledges your rights of fair use or other equivalent, as provided by copyright law. You may make, run and propagate covered works that you do not convey, without conditions so long as your license otherwise remains in force. You may convey covered works to others for the sole purpose of having them make modifications exclusively for you, or provide you with facilities for running those works, provided that you comply with the terms of this License in conveying all material for which you do not control copyright. Those thus making or running the covered works for you must do so exclusively on your behalf, under your direction and control, on terms that prohibit them from making any copies of your copyrighted material outside their relationship with you. Conveying under any other circumstances is permitted solely under the conditions stated below. Sublicensing is not allowed; section 10 makes it unnecessary. Protecting Users' Legal Rights From Anti-Circumvention Law. No covered work shall be deemed part of an effective technological measure under any applicable law fulfilling obligations under article 11 of the WIPO copyright treaty adopted on 20 December 1996, or similar laws prohibiting or restricting circumvention of such measures. When you convey a covered work, you waive any legal power to forbid circumvention of technological measures to the extent such circumvention is effected by exercising rights under this License with respect to the covered work, and you disclaim any intention to limit operation or modification of the work as a means of enforcing, against the work's users, your or third parties' legal rights to forbid circumvention of technological measures. Conveying Verbatim Copies. You may convey verbatim copies of the Program's source code as you receive it, in any medium, provided that you conspicuously and appropriately publish on each copy an appropriate copyright notice; keep intact all notices stating that this License and any non-permissive terms added in accord with section 7 apply to the code; keep intact all notices of the absence of any warranty; and give all recipients a copy of this License along with the Program. You may charge any price or no price for each copy that you convey, and you may offer support or warranty protection for a fee. Conveying Modified Source Versions. You may convey a work based on the Program, or the modifications to produce it from the Program, in the form of source code under the terms of section 4, provided that you also meet all of these conditions: a) The work must carry prominent notices stating that you modified it, and giving a relevant date. b) The work must carry prominent notices stating that it is released under this License and any conditions added under section 7. This requirement modifies the requirement in section 4 to \"keep intact all notices\". c) You must license the entire work, as a whole, under this License to anyone who comes into possession of a copy. This License will therefore apply, along with any applicable section 7 additional terms, to the whole of the work, and all its parts, regardless of how they are packaged. This License gives no permission to license the work in any other way, but it does not invalidate such permission if you have separately received it. d) If the work has interactive user interfaces, each must display Appropriate Legal Notices; however, if the Program has interactive interfaces that do not display Appropriate Legal Notices, your work need not make them do so. A compilation of a covered work with other separate and independent works, which are not by their nature extensions of the covered work, and which are not combined with it such as to form a larger program, in or on a volume of a storage or distribution medium, is called an \"aggregate\" if the compilation and its resulting copyright are not used to limit the access or legal rights of the compilation's users beyond what the individual works permit. Inclusion of a covered work in an aggregate does not cause this License to apply to the other parts of the aggregate. Conveying Non-Source Forms. You may convey a covered work in object code form under the terms of sections 4 and 5, provided that you also convey the machine-readable Corresponding Source under the terms of this License, in one of these ways: a) Convey the object code in, or embodied in, a physical product (including a physical distribution medium), accompanied by the Corresponding Source fixed on a durable physical medium customarily used for software interchange. b) Convey the object code in, or embodied in, a physical product (including a physical distribution medium), accompanied by a written offer, valid for at least three years and valid for as long as you offer spare parts or customer support for that product model, to give anyone who possesses the object code either (1) a copy of the Corresponding Source for all the software in the product that is covered by this License, on a durable physical medium customarily used for software interchange, for a price no more than your reasonable cost of physically performing this conveying of source, or (2) access to copy the Corresponding Source from a network server at no charge. c) Convey individual copies of the object code with a copy of the written offer to provide the Corresponding Source. This alternative is allowed only occasionally and noncommercially, and only if you received the object code with such an offer, in accord with subsection 6b. d) Convey the object code by offering access from a designated place (gratis or for a charge), and offer equivalent access to the Corresponding Source in the same way through the same place at no further charge. You need not require recipients to copy the Corresponding Source along with the object code. If the place to copy the object code is a network server, the Corresponding Source may be on a different server (operated by you or a third party) that supports equivalent copying facilities, provided you maintain clear directions next to the object code saying where to find the Corresponding Source. Regardless of what server hosts the Corresponding Source, you remain obligated to ensure that it is available for as long as needed to satisfy these requirements. e) Convey the object code using peer-to-peer transmission, provided you inform other peers where the object code and Corresponding Source of the work are being offered to the general public at no charge under subsection 6d. A separable portion of the object code, whose source code is excluded from the Corresponding Source as a System Library, need not be included in conveying the object code work. A \"User Product\" is either (1) a \"consumer product\", which means any tangible personal property which is normally used for personal, family, or household purposes, or (2) anything designed or sold for incorporation into a dwelling. In determining whether a product is a consumer product, doubtful cases shall be resolved in favor of coverage. For a particular product received by a particular user, \"normally used\" refers to a typical or common use of that class of product, regardless of the status of the particular user or of the way in which the particular user actually uses, or expects or is expected to use, the product. A product is a consumer product regardless of whether the product has substantial commercial, industrial or non-consumer uses, unless such uses represent the only significant mode of use of the product. \"Installation Information\" for a User Product means any methods, procedures, authorization keys, or other information required to install and execute modified versions of a covered work in that User Product from a modified version of its Corresponding Source. The information must suffice to ensure that the continued functioning of the modified object code is in no case prevented or interfered with solely because modification has been made. If you convey an object code work under this section in, or with, or specifically for use in, a User Product, and the conveying occurs as part of a transaction in which the right of possession and use of the User Product is transferred to the recipient in perpetuity or for a fixed term (regardless of how the transaction is characterized), the Corresponding Source conveyed under this section must be accompanied by the Installation Information. But this requirement does not apply if neither you nor any third party retains the ability to install modified object code on the User Product (for example, the work has been installed in ROM). The requirement to provide Installation Information does not include a requirement to continue to provide support service, warranty, or updates for a work that has been modified or installed by the recipient, or for the User Product in which it has been modified or installed. Access to a network may be denied when the modification itself materially and adversely affects the operation of the network or violates the rules and protocols for communication across the network. Corresponding Source conveyed, and Installation Information provided, in accord with this section must be in a format that is publicly documented (and with an implementation available to the public in source code form), and must require no special password or key for unpacking, reading or copying. Additional Terms. \"Additional permissions\" are terms that supplement the terms of this License by making exceptions from one or more of its conditions. Additional permissions that are applicable to the entire Program shall be treated as though they were included in this License, to the extent that they are valid under applicable law. If additional permissions apply only to part of the Program, that part may be used separately under those permissions, but the entire Program remains governed by this License without regard to the additional permissions. When you convey a copy of a covered work, you may at your option remove any additional permissions from that copy, or from any part of it. (Additional permissions may be written to require their own removal in certain cases when you modify the work.) You may place additional permissions on material, added by you to a covered work, for which you have or can give appropriate copyright permission. Notwithstanding any other provision of this License, for material you add to a covered work, you may (if authorized by the copyright holders of that material) supplement the terms of this License with terms: a) Disclaiming warranty or limiting liability differently from the terms of sections 15 and 16 of this License; or b) Requiring preservation of specified reasonable legal notices or author attributions in that material or in the Appropriate Legal Notices displayed by works containing it; or c) Prohibiting misrepresentation of the origin of that material, or requiring that modified versions of such material be marked in reasonable ways as different from the original version; or d) Limiting the use for publicity purposes of names of licensors or authors of the material; or e) Declining to grant rights under trademark law for use of some trade names, trademarks, or service marks; or f) Requiring indemnification of licensors and authors of that material by anyone who conveys the material (or modified versions of it) with contractual assumptions of liability to the recipient, for any liability that these contractual assumptions directly impose on those licensors and authors. All other non-permissive additional terms are considered \"further restrictions\" within the meaning of section 10. If the Program as you received it, or any part of it, contains a notice stating that it is governed by this License along with a term that is a further restriction, you may remove that term. If a license document contains a further restriction but permits relicensing or conveying under this License, you may add to a covered work material governed by the terms of that license document, provided that the further restriction does not survive such relicensing or conveying. If you add terms to a covered work in accord with this section, you must place, in the relevant source files, a statement of the additional terms that apply to those files, or a notice indicating where to find the applicable terms. Additional terms, permissive or non-permissive, may be stated in the form of a separately written license, or stated as exceptions; the above requirements apply either way. Termination. You may not propagate or modify a covered work except as expressly provided under this License. Any attempt otherwise to propagate or modify it is void, and will automatically terminate your rights under this License (including any patent licenses granted under the third paragraph of section 11). However, if you cease all violation of this License, then your license from a particular copyright holder is reinstated (a) provisionally, unless and until the copyright holder explicitly and finally terminates your license, and (b) permanently, if the copyright holder fails to notify you of the violation by some reasonable means prior to 60 days after the cessation. Moreover, your license from a particular copyright holder is reinstated permanently if the copyright holder notifies you of the violation by some reasonable means, this is the first time you have received notice of violation of this License (for any work) from that copyright holder, and you cure the violation prior to 30 days after your receipt of the notice. Termination of your rights under this section does not terminate the licenses of parties who have received copies or rights from you under this License. If your rights have been terminated and not permanently reinstated, you do not qualify to receive new licenses for the same material under section 10. Acceptance Not Required for Having Copies. You are not required to accept this License in order to receive or run a copy of the Program. Ancillary propagation of a covered work occurring solely as a consequence of using peer-to-peer transmission to receive a copy likewise does not require acceptance. However, nothing other than this License grants you permission to propagate or modify any covered work. These actions infringe copyright if you do not accept this License. Therefore, by modifying or propagating a covered work, you indicate your acceptance of this License to do so. Automatic Licensing of Downstream Recipients. Each time you convey a covered work, the recipient automatically receives a license from the original licensors, to run, modify and propagate that work, subject to this License. You are not responsible for enforcing compliance by third parties with this License. An \"entity transaction\" is a transaction transferring control of an organization, or substantially all assets of one, or subdividing an organization, or merging organizations. If propagation of a covered work results from an entity transaction, each party to that transaction who receives a copy of the work also receives whatever licenses to the work the party's predecessor in interest had or could give under the previous paragraph, plus a right to possession of the Corresponding Source of the work from the predecessor in interest, if the predecessor has it or can get it with reasonable efforts. You may not impose any further restrictions on the exercise of the rights granted or affirmed under this License. For example, you may not impose a license fee, royalty, or other charge for exercise of rights granted under this License, and you may not initiate litigation (including a cross-claim or counterclaim in a lawsuit) alleging that any patent claim is infringed by making, using, selling, offering for sale, or importing the Program or any portion of it. Patents. A \"contributor\" is a copyright holder who authorizes use under this License of the Program or a work on which the Program is based. The work thus licensed is called the contributor's \"contributor version\". A contributor's \"essential patent claims\" are all patent claims owned or controlled by the contributor, whether already acquired or hereafter acquired, that would be infringed by some manner, permitted by this License, of making, using, or selling its contributor version, but do not include claims that would be infringed only as a consequence of further modification of the contributor version. For purposes of this definition, \"control\" includes the right to grant patent sublicenses in a manner consistent with the requirements of this License. Each contributor grants you a non-exclusive, worldwide, royalty-free patent license under the contributor's essential patent claims, to make, use, sell, offer for sale, import and otherwise run, modify and propagate the contents of its contributor version. In the following three paragraphs, a \"patent license\" is any express agreement or commitment, however denominated, not to enforce a patent (such as an express permission to practice a patent or covenant not to sue for patent infringement). To \"grant\" such a patent license to a party means to make such an agreement or commitment not to enforce a patent against the party. If you convey a covered work, knowingly relying on a patent license, and the Corresponding Source of the work is not available for anyone to copy, free of charge and under the terms of this License, through a publicly available network server or other readily accessible means, then you must either (1) cause the Corresponding Source to be so available, or (2) arrange to deprive yourself of the benefit of the patent license for this particular work, or (3) arrange, in a manner consistent with the requirements of this License, to extend the patent license to downstream recipients. \"Knowingly relying\" means you have actual knowledge that, but for the patent license, your conveying the covered work in a country, or your recipient's use of the covered work in a country, would infringe one or more identifiable patents in that country that you have reason to believe are valid. If, pursuant to or in connection with a single transaction or arrangement, you convey, or propagate by procuring conveyance of, a covered work, and grant a patent license to some of the parties receiving the covered work authorizing them to use, propagate, modify or convey a specific copy of the covered work, then the patent license you grant is automatically extended to all recipients of the covered work and works based on it. A patent license is \"discriminatory\" if it does not include within the scope of its coverage, prohibits the exercise of, or is conditioned on the non-exercise of one or more of the rights that are specifically granted under this License. You may not convey a covered work if you are a party to an arrangement with a third party that is in the business of distributing software, under which you make payment to the third party based on the extent of your activity of conveying the work, and under which the third party grants, to any of the parties who would receive the covered work from you, a discriminatory patent license (a) in connection with copies of the covered work conveyed by you (or copies made from those copies), or (b) primarily for and in connection with specific products or compilations that contain the covered work, unless you entered into that arrangement, or that patent license was granted, prior to 28 March 2007. Nothing in this License shall be construed as excluding or limiting any implied license or other defenses to infringement that may otherwise be available to you under applicable patent law. No Surrender of Others' Freedom. If conditions are imposed on you (whether by court order, agreement or otherwise) that contradict the conditions of this License, they do not excuse you from the conditions of this License. If you cannot convey a covered work so as to satisfy simultaneously your obligations under this License and any other pertinent obligations, then as a consequence you may not convey it at all. For example, if you agree to terms that obligate you to collect a royalty for further conveying from those to whom you convey the Program, the only way you could satisfy both those terms and this License would be to refrain entirely from conveying the Program. Use with the GNU Affero General Public License. Notwithstanding any other provision of this License, you have permission to link or combine any covered work with a work licensed under version 3 of the GNU Affero General Public License into a single combined work, and to convey the resulting work. The terms of this License will continue to apply to the part which is the covered work, but the special requirements of the GNU Affero General Public License, section 13, concerning interaction through a network will apply to the combination as such. Revised Versions of this License. The Free Software Foundation may publish revised and/or new versions of the GNU General Public License from time to time. Such new versions will be similar in spirit to the present version, but may differ in detail to address new problems or concerns. Each version is given a distinguishing version number. If the Program specifies that a certain numbered version of the GNU General Public License \"or any later version\" applies to it, you have the option of following the terms and conditions either of that numbered version or of any later version published by the Free Software Foundation. If the Program does not specify a version number of the GNU General Public License, you may choose any version ever published by the Free Software Foundation. If the Program specifies that a proxy can decide which future versions of the GNU General Public License can be used, that proxy's public statement of acceptance of a version permanently authorizes you to choose that version for the Program. Later license versions may give you additional or different permissions. However, no additional obligations are imposed on any author or copyright holder as a result of your choosing to follow a later version. Disclaimer of Warranty. THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY APPLICABLE LAW. EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM \"AS IS\" WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM IS WITH YOU. SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF ALL NECESSARY SERVICING, REPAIR OR CORRECTION. Limitation of Liability. IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS), EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Interpretation of Sections 15 and 16. If the disclaimer of warranty and limitation of liability provided above cannot be given local legal effect according to their terms, reviewing courts shall apply local law that most closely approximates an absolute waiver of all civil liability in connection with the Program, unless a warranty or assumption of liability accompanies a copy of the Program in return for a fee. END OF TERMS AND CONDITIONS How to Apply These Terms to Your New Programs If you develop a new program, and you want it to be of the greatest possible use to the public, the best way to achieve this is to make it free software which everyone can redistribute and change under these terms. To do so, attach the following notices to the program. It is safest to attach them to the start of each source file to most effectively state the exclusion of warranty; and each file should have at least the \"copyright\" line and a pointer to where the full notice is found. <one line to give the program's name and a brief idea of what it does.> Copyright (C) <year> <name of author> This program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details. You should have received a copy of the GNU General Public License along with this program. If not, see <http://www.gnu.org/licenses/>. Also add information on how to contact you by electronic and paper mail. If the program does terminal interaction, make it output a short notice like this when it starts in an interactive mode: {project} Copyright (C) {year} {fullname} This program comes with ABSOLUTELY NO WARRANTY; for details type `show w'. This is free software, and you are welcome to redistribute it under certain conditions; type `show c' for details. The hypothetical commands show w' and show c' should show the appropriate parts of the General Public License. Of course, your program's commands might be different; for a GUI interface, you would use an \"about box\". You should also get your employer (if you work as a programmer) or school, if any, to sign a \"copyright disclaimer\" for the program, if necessary. For more information on this, and how to apply and follow the GNU GPL, see http://www.gnu.org/licenses/ . The GNU General Public License does not permit incorporating your program into proprietary programs. If your program is a subroutine library, you may consider it more useful to permit linking proprietary applications with the library. If this is what you want to do, use the GNU Lesser General Public License instead of this License. But first, please read http://www.gnu.org/philosophy/why-not-lgpl.html .","title":"LICENSE"},{"location":"Create%20OpenSwitch%20VM/","text":"Create OpenSwitch VM Download onie-kvm.iso Create a VM - it must use BIOS boot mode WARNING If you are running on VMWare ESXi you must use an IDE hard drive! WARNING If you are running on VMWare ESXi you must use the E1000E driver for the network card! Boot from the CD, but at the grub menu, hit tab to edit the options and remove all console options See: https://askubuntu.com/questions/771871/16-04-virtualbox-vm-from-vhd-file-hangs-at-non-blocking-pool-is-initialized Once you drop to the ONIE command line run sed -i 's/vda/sda/g' /lib/onie/onie-updater Workable fix: sed 's/if \\[ \"$sha1.*/if false; then/g' /lib/onie/onie-updater Run onie-self-update -e file://lib/onie/onie-updater Reboot. For whatever reason I couldn't get the discovery process to work so I ran onie-discovery-stop Then run onie-nos-install http://<SERVER_IP>/onie-installer Faster fix that didn't work Run sha1sum /lib/onie/onie-updater and note the sha1 hash Run sed \"s/payload\\=.*/payload_sha1=<YOURHASH>/g\" /lib/onie/onie-updater I tried sed \"s/payload\\=.*/payload_sha1=$(sha1sum <THING> | cut -d \" \" -f 1)/g\" /lib/onie/onie-updater and it wouldn't let me do it","title":"Create OpenSwitch VM"},{"location":"Create%20OpenSwitch%20VM/#create-openswitch-vm","text":"Download onie-kvm.iso Create a VM - it must use BIOS boot mode WARNING If you are running on VMWare ESXi you must use an IDE hard drive! WARNING If you are running on VMWare ESXi you must use the E1000E driver for the network card! Boot from the CD, but at the grub menu, hit tab to edit the options and remove all console options See: https://askubuntu.com/questions/771871/16-04-virtualbox-vm-from-vhd-file-hangs-at-non-blocking-pool-is-initialized Once you drop to the ONIE command line run sed -i 's/vda/sda/g' /lib/onie/onie-updater Workable fix: sed 's/if \\[ \"$sha1.*/if false; then/g' /lib/onie/onie-updater Run onie-self-update -e file://lib/onie/onie-updater Reboot. For whatever reason I couldn't get the discovery process to work so I ran onie-discovery-stop Then run onie-nos-install http://<SERVER_IP>/onie-installer","title":"Create OpenSwitch VM"},{"location":"Create%20OpenSwitch%20VM/#faster-fix-that-didnt-work","text":"Run sha1sum /lib/onie/onie-updater and note the sha1 hash Run sed \"s/payload\\=.*/payload_sha1=<YOURHASH>/g\" /lib/onie/onie-updater I tried sed \"s/payload\\=.*/payload_sha1=$(sha1sum <THING> | cut -d \" \" -f 1)/g\" /lib/onie/onie-updater and it wouldn't let me do it","title":"Faster fix that didn't work"},{"location":"DHCP%20Relay%20on%20SONiC/","text":"DHCP Relay on SONiC Configuring a Cross VLAN DHCP Relay on SONiC OS My Configuration IP Addresses Test Concept 4112F-ON OS 10 Version Z9264 SONiC Version RHEL Version Configuration of Devices Configuration of RHEL DHCP Server OS10 on 4112F-ON SONiC OS on Z9264 Helpful Commands Configuration on ESXi Running the Test Lease Record on RHEL tcpdump from SONiC Address Info From OS10 My Configuration Dell 4112F-ON running OS 10 (interface eth1/1/13 to interface ethernet 0 [all VLAN 99]) | Dell Z9264 running SONiC OS (interface ethernet 257 to interface vmnic7 [all vlan 100]) | Dell R840 running ESXi (virtual switch with portgroup on vlan 100 going to VM's virtual NIC) | RHEL Virtual Machine with DHCP server IP Addresses RHEL VM: 192.168.100.5/24 Z9264 SVI interface on VLAN 99: 192.168.99.1/24 Z9264 SVI interface on VLAN 100: 192.168.100.1/24 4112F-ON SVI interface on VLAN 99 - DHCP Test Concept DHCP request will go from SVI interface on 4112F-ON residing on VLAN 99 through the Z9264, to ESXi, and onto the DHCP server running on the RHEL virtual machine on VLAN 100 running dhcpd. 4112F-ON OS 10 Version Dell EMC Networking OS10 Enterprise Copyright (c) 1999-2020 by Dell Inc. All Rights Reserved. OS Version: 10.5.1.3 Build Version: 10.5.1.3.190 Build Time: 2020-06-19T21:48:07+0000 System Type: S4112F-ON Architecture: x86_64 Up Time: 1 day 01:29:03 Z9264 SONiC Version Software Version : '3.4.0-Enterprise_Base' Product : Enterprise SONiC Distribution by Dell Technologies Distribution : '9.13' Kernel : '4.9.0-11-2-amd64' Config DB Version : version_3_3_1 Build Commit : 'e2f258af7' Build Date : Wed Jul 28 23:54:33 UTC 2021 Built By : sonicbld@sonic-lvn-csg-005 Platform : x86_64-dellemc_z9264f_c3538-r0 HwSKU : DellEMC-Z9264f-C64 ASIC : broadcom Hardware Version : A00 Serial Number : TW0XXP63DNT008970001 Uptime : 04:53:36 up 2:05, 1 user, load average: 1.04, 0.94, 0.95 Mfg : Dell EMC RHEL Version [root@freeipa ~]# cat /etc/*-release NAME=\"Red Hat Enterprise Linux\" VERSION=\"8.2 (Ootpa)\" ID=\"rhel\" ID_LIKE=\"fedora\" VERSION_ID=\"8.2\" PLATFORM_ID=\"platform:el8\" PRETTY_NAME=\"Red Hat Enterprise Linux 8.2 (Ootpa)\" ANSI_COLOR=\"0;31\" CPE_NAME=\"cpe:/o:redhat:enterprise_linux:8.2:GA\" HOME_URL=\"https://www.redhat.com/\" BUG_REPORT_URL=\"https://bugzilla.redhat.com/\" REDHAT_BUGZILLA_PRODUCT=\"Red Hat Enterprise Linux 8\" REDHAT_BUGZILLA_PRODUCT_VERSION=8.2 REDHAT_SUPPORT_PRODUCT=\"Red Hat Enterprise Linux\" REDHAT_SUPPORT_PRODUCT_VERSION=\"8.2\" Red Hat Enterprise Linux release 8.2 (Ootpa) Red Hat Enterprise Linux release 8.2 (Ootpa) Configuration of Devices Configuration of RHEL DHCP Server See: Configuring RHEL DHCP Server sudo dnf install -y dhcp-server ip route add 192.168.99.0/24 via 192.168.100.1 dev ens224 vim /etc/dhcp/dhcpd.conf I used configuration: # # DHCP Server Configuration file. # see /usr/share/doc/dhcp-server/dhcpd.conf.example # see dhcpd.conf(5) man page # option domain-name \"lan\"; default-lease-time 86400; authoritative; shared-network lan { subnet 192.168.99.0 netmask 255.255.255.0 { range 192.168.99.100 192.168.99.200; option routers 192.168.99.1; } } subnet 192.168.100.0 netmask 255.255.255.0 {} OS10 on 4112F-ON Full Configuration configure terminal interface ethernet 1/1/13 switchport mode trunk switchport trunk allowed vlan 99 exit interface vlan 99 ip address dhcp SONiC OS on Z9264 Full Configuration sonic-cli configure terminal interface ethernet 0 no shutdown fec rs speed 100000 switchport trunk allowed Vlan 99 exit interface ethernet 257 no shutdown switchport trunk allowed Vlan 100 exit interface Vlan 99 ip address 192.168.99.1/24 ip dhcp-relay 192.168.100.5 exit interface Vlan 100 ip address 192.168.100.1/24 exit NOTE : I had to manually configure FEC on the 100Gb/s interface to bring it up. Helpful Commands show interface transceiver NOTE : Unlike OS10 all interfaces start in shutdown mode so you will need to bring them up. Configuration on ESXi Running the Test Lease Record on RHEL [root@freeipa ~]# cat /var/lib/dhcpd/dhcpd.leases # The format of this file is documented in the dhcpd.leases(5) manual page. # This lease file was written by isc-dhcp-4.3.6 # authoring-byte-order entry is generated, DO NOT DELETE authoring-byte-order little-endian; server-duid \"\\000\\001\\000\\001)A@\\255\\000PV\\276\\261\\016\"; lease 192.168.99.100 { starts 1 2021/12/06 21:50:25; ends 2 2021/12/07 21:50:25; cltt 1 2021/12/06 21:50:25; binding state active; next binding state free; rewind binding state free; hardware ethernet 88:6f:d4:98:b7:b1; option agent.circuit-id \"Vlan99\"; option agent.remote-id \"20:04:0f:06:44:b4\"; client-hostname \"OS10\"; } tcpdump from SONiC Shows DHCP transiting the relay. Address Info From OS10 OS10(conf-if-vl-99)# do show ip interface brief Interface Name IP-Address OK Method Status Protocol ========================================================================================= Ethernet 1/1/1 unassigned NO unset up down Ethernet 1/1/2 unassigned NO unset up down Ethernet 1/1/3 unassigned NO unset up down Ethernet 1/1/4 unassigned NO unset up down Ethernet 1/1/5 unassigned NO unset up down Ethernet 1/1/6 unassigned NO unset up down Ethernet 1/1/7 unassigned NO unset up down Ethernet 1/1/8 unassigned NO unset up down Ethernet 1/1/9 unassigned NO unset up down Ethernet 1/1/10 unassigned NO unset up down Ethernet 1/1/11 unassigned NO unset up down Ethernet 1/1/12 unassigned NO unset up down Ethernet 1/1/13 unassigned YES unset up up Ethernet 1/1/14 unassigned NO unset up down Ethernet 1/1/15 unassigned NO unset up down Management 1/1/1 192.168.1.24/24 YES manual up up Vlan 1 unassigned NO unset up down Vlan 99 192.168.99.100/24 YES DHCP up up","title":"DHCP Relay on SONiC"},{"location":"DHCP%20Relay%20on%20SONiC/#dhcp-relay-on-sonic","text":"Configuring a Cross VLAN DHCP Relay on SONiC OS My Configuration IP Addresses Test Concept 4112F-ON OS 10 Version Z9264 SONiC Version RHEL Version Configuration of Devices Configuration of RHEL DHCP Server OS10 on 4112F-ON SONiC OS on Z9264 Helpful Commands Configuration on ESXi Running the Test Lease Record on RHEL tcpdump from SONiC Address Info From OS10","title":"DHCP Relay on SONiC"},{"location":"DHCP%20Relay%20on%20SONiC/#my-configuration","text":"Dell 4112F-ON running OS 10 (interface eth1/1/13 to interface ethernet 0 [all VLAN 99]) | Dell Z9264 running SONiC OS (interface ethernet 257 to interface vmnic7 [all vlan 100]) | Dell R840 running ESXi (virtual switch with portgroup on vlan 100 going to VM's virtual NIC) | RHEL Virtual Machine with DHCP server","title":"My Configuration"},{"location":"DHCP%20Relay%20on%20SONiC/#ip-addresses","text":"RHEL VM: 192.168.100.5/24 Z9264 SVI interface on VLAN 99: 192.168.99.1/24 Z9264 SVI interface on VLAN 100: 192.168.100.1/24 4112F-ON SVI interface on VLAN 99 - DHCP","title":"IP Addresses"},{"location":"DHCP%20Relay%20on%20SONiC/#test-concept","text":"DHCP request will go from SVI interface on 4112F-ON residing on VLAN 99 through the Z9264, to ESXi, and onto the DHCP server running on the RHEL virtual machine on VLAN 100 running dhcpd.","title":"Test Concept"},{"location":"DHCP%20Relay%20on%20SONiC/#4112f-on-os-10-version","text":"Dell EMC Networking OS10 Enterprise Copyright (c) 1999-2020 by Dell Inc. All Rights Reserved. OS Version: 10.5.1.3 Build Version: 10.5.1.3.190 Build Time: 2020-06-19T21:48:07+0000 System Type: S4112F-ON Architecture: x86_64 Up Time: 1 day 01:29:03","title":"4112F-ON OS 10 Version"},{"location":"DHCP%20Relay%20on%20SONiC/#z9264-sonic-version","text":"Software Version : '3.4.0-Enterprise_Base' Product : Enterprise SONiC Distribution by Dell Technologies Distribution : '9.13' Kernel : '4.9.0-11-2-amd64' Config DB Version : version_3_3_1 Build Commit : 'e2f258af7' Build Date : Wed Jul 28 23:54:33 UTC 2021 Built By : sonicbld@sonic-lvn-csg-005 Platform : x86_64-dellemc_z9264f_c3538-r0 HwSKU : DellEMC-Z9264f-C64 ASIC : broadcom Hardware Version : A00 Serial Number : TW0XXP63DNT008970001 Uptime : 04:53:36 up 2:05, 1 user, load average: 1.04, 0.94, 0.95 Mfg : Dell EMC","title":"Z9264 SONiC Version"},{"location":"DHCP%20Relay%20on%20SONiC/#rhel-version","text":"[root@freeipa ~]# cat /etc/*-release NAME=\"Red Hat Enterprise Linux\" VERSION=\"8.2 (Ootpa)\" ID=\"rhel\" ID_LIKE=\"fedora\" VERSION_ID=\"8.2\" PLATFORM_ID=\"platform:el8\" PRETTY_NAME=\"Red Hat Enterprise Linux 8.2 (Ootpa)\" ANSI_COLOR=\"0;31\" CPE_NAME=\"cpe:/o:redhat:enterprise_linux:8.2:GA\" HOME_URL=\"https://www.redhat.com/\" BUG_REPORT_URL=\"https://bugzilla.redhat.com/\" REDHAT_BUGZILLA_PRODUCT=\"Red Hat Enterprise Linux 8\" REDHAT_BUGZILLA_PRODUCT_VERSION=8.2 REDHAT_SUPPORT_PRODUCT=\"Red Hat Enterprise Linux\" REDHAT_SUPPORT_PRODUCT_VERSION=\"8.2\" Red Hat Enterprise Linux release 8.2 (Ootpa) Red Hat Enterprise Linux release 8.2 (Ootpa)","title":"RHEL Version"},{"location":"DHCP%20Relay%20on%20SONiC/#configuration-of-devices","text":"","title":"Configuration of Devices"},{"location":"DHCP%20Relay%20on%20SONiC/#configuration-of-rhel-dhcp-server","text":"See: Configuring RHEL DHCP Server sudo dnf install -y dhcp-server ip route add 192.168.99.0/24 via 192.168.100.1 dev ens224 vim /etc/dhcp/dhcpd.conf I used configuration: # # DHCP Server Configuration file. # see /usr/share/doc/dhcp-server/dhcpd.conf.example # see dhcpd.conf(5) man page # option domain-name \"lan\"; default-lease-time 86400; authoritative; shared-network lan { subnet 192.168.99.0 netmask 255.255.255.0 { range 192.168.99.100 192.168.99.200; option routers 192.168.99.1; } } subnet 192.168.100.0 netmask 255.255.255.0 {}","title":"Configuration of RHEL DHCP Server"},{"location":"DHCP%20Relay%20on%20SONiC/#os10-on-4112f-on","text":"Full Configuration configure terminal interface ethernet 1/1/13 switchport mode trunk switchport trunk allowed vlan 99 exit interface vlan 99 ip address dhcp","title":"OS10 on 4112F-ON"},{"location":"DHCP%20Relay%20on%20SONiC/#sonic-os-on-z9264","text":"Full Configuration sonic-cli configure terminal interface ethernet 0 no shutdown fec rs speed 100000 switchport trunk allowed Vlan 99 exit interface ethernet 257 no shutdown switchport trunk allowed Vlan 100 exit interface Vlan 99 ip address 192.168.99.1/24 ip dhcp-relay 192.168.100.5 exit interface Vlan 100 ip address 192.168.100.1/24 exit NOTE : I had to manually configure FEC on the 100Gb/s interface to bring it up.","title":"SONiC OS on Z9264"},{"location":"DHCP%20Relay%20on%20SONiC/#helpful-commands","text":"show interface transceiver NOTE : Unlike OS10 all interfaces start in shutdown mode so you will need to bring them up.","title":"Helpful Commands"},{"location":"DHCP%20Relay%20on%20SONiC/#configuration-on-esxi","text":"","title":"Configuration on ESXi"},{"location":"DHCP%20Relay%20on%20SONiC/#running-the-test","text":"","title":"Running the Test"},{"location":"DHCP%20Relay%20on%20SONiC/#lease-record-on-rhel","text":"[root@freeipa ~]# cat /var/lib/dhcpd/dhcpd.leases # The format of this file is documented in the dhcpd.leases(5) manual page. # This lease file was written by isc-dhcp-4.3.6 # authoring-byte-order entry is generated, DO NOT DELETE authoring-byte-order little-endian; server-duid \"\\000\\001\\000\\001)A@\\255\\000PV\\276\\261\\016\"; lease 192.168.99.100 { starts 1 2021/12/06 21:50:25; ends 2 2021/12/07 21:50:25; cltt 1 2021/12/06 21:50:25; binding state active; next binding state free; rewind binding state free; hardware ethernet 88:6f:d4:98:b7:b1; option agent.circuit-id \"Vlan99\"; option agent.remote-id \"20:04:0f:06:44:b4\"; client-hostname \"OS10\"; }","title":"Lease Record on RHEL"},{"location":"DHCP%20Relay%20on%20SONiC/#tcpdump-from-sonic","text":"Shows DHCP transiting the relay.","title":"tcpdump from SONiC"},{"location":"DHCP%20Relay%20on%20SONiC/#address-info-from-os10","text":"OS10(conf-if-vl-99)# do show ip interface brief Interface Name IP-Address OK Method Status Protocol ========================================================================================= Ethernet 1/1/1 unassigned NO unset up down Ethernet 1/1/2 unassigned NO unset up down Ethernet 1/1/3 unassigned NO unset up down Ethernet 1/1/4 unassigned NO unset up down Ethernet 1/1/5 unassigned NO unset up down Ethernet 1/1/6 unassigned NO unset up down Ethernet 1/1/7 unassigned NO unset up down Ethernet 1/1/8 unassigned NO unset up down Ethernet 1/1/9 unassigned NO unset up down Ethernet 1/1/10 unassigned NO unset up down Ethernet 1/1/11 unassigned NO unset up down Ethernet 1/1/12 unassigned NO unset up down Ethernet 1/1/13 unassigned YES unset up up Ethernet 1/1/14 unassigned NO unset up down Ethernet 1/1/15 unassigned NO unset up down Management 1/1/1 192.168.1.24/24 YES manual up up Vlan 1 unassigned NO unset up down Vlan 99 192.168.99.100/24 YES DHCP up up","title":"Address Info From OS10"},{"location":"Dell%20Ansible%20Testing/","text":"Dell Ansible Testing Install Ansible Modules Install Dell Ansible modules from here git clone -b devel --single-branch https://github.com/dell/dellemc-openmanage-ansible-modules.git cd dellemc-openmanage-ansible-modules python install.py Create a CIFs/NFS share from which to share files. I used Samba with this tutorial Note: If you get weird errors in the logs when starting SMB, you probably have a type-o in your smb.conf. I saw different things in different tutorials, but to get firewalld running I used firewall-cmd --permanent --add-service=samba && firewall-cmd --reload Deploy Operating System (Incomplete) Next you will need to create a \"Kickstartable\" ISO. I chose to use RHEL8. I used the free developer license of RHEL 8 I used this post to create the ISO. I used this site to generate my Kickstart config. Update I was going to use RHEL 8. However it looks like there is a bug that is a hard stop for RHEL 8 Kickstart so I'm swapping to CentOS. ![](images/error.PNG)","title":"Dell Ansible Testing"},{"location":"Dell%20Ansible%20Testing/#dell-ansible-testing","text":"","title":"Dell Ansible Testing"},{"location":"Dell%20Ansible%20Testing/#install-ansible-modules","text":"Install Dell Ansible modules from here git clone -b devel --single-branch https://github.com/dell/dellemc-openmanage-ansible-modules.git cd dellemc-openmanage-ansible-modules python install.py Create a CIFs/NFS share from which to share files. I used Samba with this tutorial Note: If you get weird errors in the logs when starting SMB, you probably have a type-o in your smb.conf. I saw different things in different tutorials, but to get firewalld running I used firewall-cmd --permanent --add-service=samba && firewall-cmd --reload","title":"Install Ansible Modules"},{"location":"Dell%20Ansible%20Testing/#deploy-operating-system-incomplete","text":"Next you will need to create a \"Kickstartable\" ISO. I chose to use RHEL8. I used the free developer license of RHEL 8 I used this post to create the ISO. I used this site to generate my Kickstart config. Update I was going to use RHEL 8. However it looks like there is a bug that is a hard stop for RHEL 8 Kickstart so I'm swapping to CentOS. ![](images/error.PNG)","title":"Deploy Operating System (Incomplete)"},{"location":"Elasticsearch%20Display%20Map%20Data/","text":"Elasticsearch Display Map Data Environment CentOS7 I tried this first with RHEL8 and the backend docker networking didn't work Installation Install docker with the following: yum install -y https://download.docker.com/linux/centos/7/x86_64/stable/Packages/containerd.io-1.2.6-3.3.el7.x86_64.rpm epel-release yum config-manager --add-repo=https://download.docker.com/linux/centos/docker-ce.repo yum install -y docker python-pip python36 curl -L \"https://github.com/docker/compose/releases/download/1.25.5/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose chmod +x /usr/local/bin/docker-compose systemctl enable docker systemctl start docker pip3.6 install geojson elasticsearch Running Elasticsearch with Docker Set VM Max Map In /etc/sysctl.conf add vm.max_map_count=262144 Turn off swap sudo swapoff -a Setup Data Directories If you are bind-mounting a local directory or file, it must be readable by the elasticsearch user. In addition, this user must have write access to the data and log dirs. A good strategy is to grant group access to gid 0 for the local directory. For example, to prepare a local directory for storing data through a bind-mount: mkdir /opt/ mkdir /opt/data01 mkdir /opt/data02 mkdir /opt/data03 mkdir /opt/data04 chmod g+rwx /opt/data* chgrp 0 /opt/data* chmod 777 -R /opt/data* ^ I got lazy. Not sure what the permissions issue is, but I couldn't get the volumes to mount so I gave up and set it to 777. My best guess is that the real problem is it isn't running as user 0 because that's root. It's probably something else. Running Elasticsearch Copy over the docker compose file Now, you must run docker-compose in the folder in which you have the directories . Otherwies you get a permissions error. cd /var/elasticsearch-data/ then run docker-compose up Importing the Data into Elasticsearch I wrote the code in csv2geojson.py to take a CSV I got from ACLED into geoJSON formatted data. The program format.py just formatted the 30 fields into the Python program for ease of use. Modify the code as necessary and then run to get geoJSON formatted data. Next you'll need to upload the mapping file. First you have to create the index with curl -X PUT \"localhost:9200/conflict-data?pretty\" -H 'Content-Type: application/json' -d' { \"settings\" : { \"index\" : { \"number_of_shards\" : 4, \"number_of_replicas\" : 3 } } } ' Then you can upload the mapping with: curl -X PUT localhost:9200/conflict-data/_mapping?pretty -H \"Content-Type: application/json\" -d @mapping.json Now you can import the data with index_data.py . NOTE Make sure you use python3.6 You may have to modify the code a bit to get it to ingest properly. Configuring Metricbeat in a container First double check the name of your elastic network with docker network ls It's probably opt_elastic. Docker compose prefixes everything with the directory from which you're running unless you specify the -p option. Pull the container and then run the setup cd /opt docker pull docker.elastic.co/beats/metricbeat:7.7.0 docker run --network opt_elastic docker.elastic.co/beats/metricbeat:7.7.0 setup -E setup.kibana.host=kib01:5601 -E output.elasticsearch.hosts=[\"es01:9200\"] Copy the metricbeat.yml to /opt Importing Map from Somewhere Else Online it will tell you that you need code to import and export objects. This is no longer the case. When I tested in 7.7.0 you could export saved objects from the saved objects menu in Kibana and then import them on the other side. I included the CPU load gauges, my custom queries, and the maps. Import the three ndjson files included in the repo. Helpful Commands Check Heap Size curl -sS -XGET \"localhost:9200/_cat/nodes?h=heap*&v\" Remove Exited Containers sudo docker rm $(docker ps -a -f status=exited -q) Running A Single Elasticsearch Instance docker run -v /opt/elasticsearch:/usr/share/elasticsearch/data --privileged -p 9200:9200 -p 9300:9300 -e \"discovery.type=single-node\" -e ES_JAVA_OPTS=\"-Xms28g -Xmx28g\" docker.elastic.co/elasticsearch/elasticsearch:7.7.0","title":"Elasticsearch Display Map Data"},{"location":"Elasticsearch%20Display%20Map%20Data/#elasticsearch-display-map-data","text":"","title":"Elasticsearch Display Map Data"},{"location":"Elasticsearch%20Display%20Map%20Data/#environment","text":"CentOS7 I tried this first with RHEL8 and the backend docker networking didn't work","title":"Environment"},{"location":"Elasticsearch%20Display%20Map%20Data/#installation","text":"Install docker with the following: yum install -y https://download.docker.com/linux/centos/7/x86_64/stable/Packages/containerd.io-1.2.6-3.3.el7.x86_64.rpm epel-release yum config-manager --add-repo=https://download.docker.com/linux/centos/docker-ce.repo yum install -y docker python-pip python36 curl -L \"https://github.com/docker/compose/releases/download/1.25.5/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose chmod +x /usr/local/bin/docker-compose systemctl enable docker systemctl start docker pip3.6 install geojson elasticsearch","title":"Installation"},{"location":"Elasticsearch%20Display%20Map%20Data/#running-elasticsearch-with-docker","text":"","title":"Running Elasticsearch with Docker"},{"location":"Elasticsearch%20Display%20Map%20Data/#set-vm-max-map","text":"In /etc/sysctl.conf add vm.max_map_count=262144","title":"Set VM Max Map"},{"location":"Elasticsearch%20Display%20Map%20Data/#turn-off-swap","text":"sudo swapoff -a","title":"Turn off swap"},{"location":"Elasticsearch%20Display%20Map%20Data/#setup-data-directories","text":"If you are bind-mounting a local directory or file, it must be readable by the elasticsearch user. In addition, this user must have write access to the data and log dirs. A good strategy is to grant group access to gid 0 for the local directory. For example, to prepare a local directory for storing data through a bind-mount: mkdir /opt/ mkdir /opt/data01 mkdir /opt/data02 mkdir /opt/data03 mkdir /opt/data04 chmod g+rwx /opt/data* chgrp 0 /opt/data* chmod 777 -R /opt/data* ^ I got lazy. Not sure what the permissions issue is, but I couldn't get the volumes to mount so I gave up and set it to 777. My best guess is that the real problem is it isn't running as user 0 because that's root. It's probably something else.","title":"Setup Data Directories"},{"location":"Elasticsearch%20Display%20Map%20Data/#running-elasticsearch","text":"Copy over the docker compose file Now, you must run docker-compose in the folder in which you have the directories . Otherwies you get a permissions error. cd /var/elasticsearch-data/ then run docker-compose up","title":"Running Elasticsearch"},{"location":"Elasticsearch%20Display%20Map%20Data/#importing-the-data-into-elasticsearch","text":"I wrote the code in csv2geojson.py to take a CSV I got from ACLED into geoJSON formatted data. The program format.py just formatted the 30 fields into the Python program for ease of use. Modify the code as necessary and then run to get geoJSON formatted data. Next you'll need to upload the mapping file. First you have to create the index with curl -X PUT \"localhost:9200/conflict-data?pretty\" -H 'Content-Type: application/json' -d' { \"settings\" : { \"index\" : { \"number_of_shards\" : 4, \"number_of_replicas\" : 3 } } } ' Then you can upload the mapping with: curl -X PUT localhost:9200/conflict-data/_mapping?pretty -H \"Content-Type: application/json\" -d @mapping.json Now you can import the data with index_data.py . NOTE Make sure you use python3.6 You may have to modify the code a bit to get it to ingest properly.","title":"Importing the Data into Elasticsearch"},{"location":"Elasticsearch%20Display%20Map%20Data/#configuring-metricbeat-in-a-container","text":"First double check the name of your elastic network with docker network ls It's probably opt_elastic. Docker compose prefixes everything with the directory from which you're running unless you specify the -p option. Pull the container and then run the setup cd /opt docker pull docker.elastic.co/beats/metricbeat:7.7.0 docker run --network opt_elastic docker.elastic.co/beats/metricbeat:7.7.0 setup -E setup.kibana.host=kib01:5601 -E output.elasticsearch.hosts=[\"es01:9200\"] Copy the metricbeat.yml to /opt","title":"Configuring Metricbeat in a container"},{"location":"Elasticsearch%20Display%20Map%20Data/#importing-map-from-somewhere-else","text":"Online it will tell you that you need code to import and export objects. This is no longer the case. When I tested in 7.7.0 you could export saved objects from the saved objects menu in Kibana and then import them on the other side. I included the CPU load gauges, my custom queries, and the maps. Import the three ndjson files included in the repo.","title":"Importing Map from Somewhere Else"},{"location":"Elasticsearch%20Display%20Map%20Data/#helpful-commands","text":"","title":"Helpful Commands"},{"location":"Elasticsearch%20Display%20Map%20Data/#check-heap-size","text":"curl -sS -XGET \"localhost:9200/_cat/nodes?h=heap*&v\"","title":"Check Heap Size"},{"location":"Elasticsearch%20Display%20Map%20Data/#remove-exited-containers","text":"sudo docker rm $(docker ps -a -f status=exited -q)","title":"Remove Exited Containers"},{"location":"Elasticsearch%20Display%20Map%20Data/#running-a-single-elasticsearch-instance","text":"docker run -v /opt/elasticsearch:/usr/share/elasticsearch/data --privileged -p 9200:9200 -p 9300:9300 -e \"discovery.type=single-node\" -e ES_JAVA_OPTS=\"-Xms28g -Xmx28g\" docker.elastic.co/elasticsearch/elasticsearch:7.7.0","title":"Running A Single Elasticsearch Instance"},{"location":"Elasticsearch%20Display%20Map%20Data/did_not_work/","text":"Stuff I tried that didn't work GDAL Attempt (Does not work) I originally tried to push the data with GDAL, but wasn't sure how to get the syntax to work. Kibana only allows you to upload 50 MB files so ingest must be done manually. I'm using GDAL. Install here wget https://github.com/OSGeo/gdal/releases/download/v3.1.0/gdal-3.1.0.tar.gz tar xf gdal* On RHEL8 run dnf install -y gcc-toolset-9 && scl enable gcc-toolset-9 bash . This will install gcc v.9 and will open a new bash shell using v9 with appropriate environment variables. On Ubuntu do the following: sudo add-apt-repository ppa:ubuntu-toolchain-r/test sudo apt update sudo apt install gcc-9 pkg-config You will need project 6 from https://download.osgeo.org/proj/proj-6.0.0.zip Run dnf install -y sqlite-devel && ./configure && make && make install You will need to install jasper and libcurl-devel with dnf install -y jasper-devel libcurl-devel Now run ./configure --with-proj=/usr/local && make -j <as many as possible> && make install The make takes an eternity if you don't give it more threads. For utility you may want to install EPEL with sudo dnf install https://dl.fedoraproject.org/pub/epel/epel-release-latest-8.noarch.rpm Creating a mapping with GDAL: ogr2ogr -overwrite -lco INDEX_NAME=gdal-data -lco NOT_ANALYZED_FIELDS={ALL} -lco WRITE_MAPPING=./mapping.json ES:http://localhost:9200 GeoObs.json Uploading with GDAL: ogr2ogr -lco INDEX_NAME=gdal-data -lco OVERWRITE_INDEX=YES -lco MAPPING=./mapping.json ES:http://localhost:9200 GeoObs.json You can run ogrinfo ES:http://localhost:9200 to check the indicies in your Elasticsearch instance and make sure that everything is connected and ready to go. Metricbeat Install Attempt (Does not work) If you want to make it easy don't set up security, but ! Run with podman run -v /opt/elasticsearch:/usr/share/elasticsearch/data --privileged -p 9200:9200 -p 9300:9300 -e \"discovery.type=single-node\" -e xpack.security.enabled=true -e xpack.monitoring.collection.enabled=true -e ES_JAVA_OPTS=\"-Xms16g [53/53]\" docker.elastic.co/elasticsearch/elasticsearch:7.7.0 to run Elasticsearch with security enabled. I ran chmod -R 774 /opt/elasticsearch to change the permissions to allow podman to run unpriveleged. If you need to, you can double check the heap size with: curl -sS -XGET \"localhost:9200/_cat/nodes?h=heap*&v\" If you want to use Metricbeat you'll also have to enable security and configure users. Do the following: Install Metricbeat from here Before starting Metricbeat, go in to the config file at /etc/metricbeat/metricbeat.yml and make sure you have the dashboard upload enabled. Next, you'll need to exec into your Elasticsearch container and setup passwords. Run podman exec -it <CONTAINER_ID> /bin/bash to get a command line on the container. Run /usr/share/elasticsearch/bin/elasticsearch-setup-passwords interactive to setup passwords. Run cd /etc/metricbeat then run metricbeat modules enable elasticsearch-xpack Then edit /etc/metricbeat/metricbeat.yml Make sure the output hosts are correct and then change username and password to what you set earlier. Also make sure that the Kibana host is set correctly. You'll also need to set the Kibana user in /etc/kibana/kibana.yml Helpful Links Examples of usage https://gist.github.com/nickpeihl/1a8f9cbecc78e9e04a73a953b30da84d Ingest geospatial data into Elasticsearch with GDAL Elasticsearch Driver for GDAL Page","title":"Stuff I tried that didn't work"},{"location":"Elasticsearch%20Display%20Map%20Data/did_not_work/#stuff-i-tried-that-didnt-work","text":"","title":"Stuff I tried that didn't work"},{"location":"Elasticsearch%20Display%20Map%20Data/did_not_work/#gdal-attempt-does-not-work","text":"I originally tried to push the data with GDAL, but wasn't sure how to get the syntax to work. Kibana only allows you to upload 50 MB files so ingest must be done manually. I'm using GDAL. Install here wget https://github.com/OSGeo/gdal/releases/download/v3.1.0/gdal-3.1.0.tar.gz tar xf gdal* On RHEL8 run dnf install -y gcc-toolset-9 && scl enable gcc-toolset-9 bash . This will install gcc v.9 and will open a new bash shell using v9 with appropriate environment variables. On Ubuntu do the following: sudo add-apt-repository ppa:ubuntu-toolchain-r/test sudo apt update sudo apt install gcc-9 pkg-config You will need project 6 from https://download.osgeo.org/proj/proj-6.0.0.zip Run dnf install -y sqlite-devel && ./configure && make && make install You will need to install jasper and libcurl-devel with dnf install -y jasper-devel libcurl-devel Now run ./configure --with-proj=/usr/local && make -j <as many as possible> && make install The make takes an eternity if you don't give it more threads. For utility you may want to install EPEL with sudo dnf install https://dl.fedoraproject.org/pub/epel/epel-release-latest-8.noarch.rpm Creating a mapping with GDAL: ogr2ogr -overwrite -lco INDEX_NAME=gdal-data -lco NOT_ANALYZED_FIELDS={ALL} -lco WRITE_MAPPING=./mapping.json ES:http://localhost:9200 GeoObs.json Uploading with GDAL: ogr2ogr -lco INDEX_NAME=gdal-data -lco OVERWRITE_INDEX=YES -lco MAPPING=./mapping.json ES:http://localhost:9200 GeoObs.json You can run ogrinfo ES:http://localhost:9200 to check the indicies in your Elasticsearch instance and make sure that everything is connected and ready to go.","title":"GDAL Attempt (Does not work)"},{"location":"Elasticsearch%20Display%20Map%20Data/did_not_work/#metricbeat-install-attempt-does-not-work","text":"If you want to make it easy don't set up security, but ! Run with podman run -v /opt/elasticsearch:/usr/share/elasticsearch/data --privileged -p 9200:9200 -p 9300:9300 -e \"discovery.type=single-node\" -e xpack.security.enabled=true -e xpack.monitoring.collection.enabled=true -e ES_JAVA_OPTS=\"-Xms16g [53/53]\" docker.elastic.co/elasticsearch/elasticsearch:7.7.0 to run Elasticsearch with security enabled. I ran chmod -R 774 /opt/elasticsearch to change the permissions to allow podman to run unpriveleged. If you need to, you can double check the heap size with: curl -sS -XGET \"localhost:9200/_cat/nodes?h=heap*&v\" If you want to use Metricbeat you'll also have to enable security and configure users. Do the following: Install Metricbeat from here Before starting Metricbeat, go in to the config file at /etc/metricbeat/metricbeat.yml and make sure you have the dashboard upload enabled. Next, you'll need to exec into your Elasticsearch container and setup passwords. Run podman exec -it <CONTAINER_ID> /bin/bash to get a command line on the container. Run /usr/share/elasticsearch/bin/elasticsearch-setup-passwords interactive to setup passwords. Run cd /etc/metricbeat then run metricbeat modules enable elasticsearch-xpack Then edit /etc/metricbeat/metricbeat.yml Make sure the output hosts are correct and then change username and password to what you set earlier. Also make sure that the Kibana host is set correctly. You'll also need to set the Kibana user in /etc/kibana/kibana.yml","title":"Metricbeat Install Attempt (Does not work)"},{"location":"Elasticsearch%20Display%20Map%20Data/did_not_work/#helpful-links","text":"Examples of usage https://gist.github.com/nickpeihl/1a8f9cbecc78e9e04a73a953b30da84d Ingest geospatial data into Elasticsearch with GDAL Elasticsearch Driver for GDAL Page","title":"Helpful Links"},{"location":"Elasticsearch%20Load%20Testing/","text":"Elasticsearch Load Testing Setup Download the chromedriver for Selenium from here . Place it in the same directory as run.py . Run yum update -y to make sure everything is up to date. Reboot if you have any kernel updates. Install python 3 with yum install -y python3 Run pip3 install selenium to install selenium Usage Run python3 run.py for usage information on CentOS. Run python run.py for usage information on Windows. Version I built and tested using Python v3.8.0 on Windows 10.","title":"Elasticsearch Load Testing"},{"location":"Elasticsearch%20Load%20Testing/#elasticsearch-load-testing","text":"","title":"Elasticsearch Load Testing"},{"location":"Elasticsearch%20Load%20Testing/#setup","text":"Download the chromedriver for Selenium from here . Place it in the same directory as run.py . Run yum update -y to make sure everything is up to date. Reboot if you have any kernel updates. Install python 3 with yum install -y python3 Run pip3 install selenium to install selenium","title":"Setup"},{"location":"Elasticsearch%20Load%20Testing/#usage","text":"Run python3 run.py for usage information on CentOS. Run python run.py for usage information on Windows.","title":"Usage"},{"location":"Elasticsearch%20Load%20Testing/#version","text":"I built and tested using Python v3.8.0 on Windows 10.","title":"Version"},{"location":"Get%20NVMe%20Drives%20from%20iDRAC%20Redfish/","text":"Get NVMe Drives from iDRAC Redfish Get NVMe Drives from iDRAC Redfish Exploring iDRAC Detected Storage Understand the Behavior of Unqualified Drives Getting a Drive's Stats Exploring iDRAC Detected Storage I used the Storage API endpoint to accomplish this. From my host I received: {\"@odata.context\":\"/redfish/v1/$metadata#StorageCollection.StorageCollection\",\"@odata.id\":\"/redfish/v1/Systems/System.Embedded.1/Storage/\",\"@odata.type\":\"#StorageCollection.StorageCollection\",\"Description\":\"Collection Of Storage entities\",\"Members\":[{\"@odata.id\":\"/redfish/v1/Systems/System.Embedded.1/Storage/RAID.Slot.4-1\"},{\"@odata.id\":\"/redfish/v1/Systems/System.Embedded.1/Storage/RAID.Embedded.1-1\"},{\"@odata.id\":\"/redfish/v1/Systems/System.Embedded.1/Storage/CPU.1\"},{\"@odata.id\":\"/redfish/v1/Systems/System.Embedded.1/Storage/AHCI.Slot.2-1\"}],\"Members@odata.count\":4,\"Name\":\"Storage Collection\"} I'm running an R840 which is Dell 14G which I know does not have NVMe RAID controllers as an option so I know my NVMe drives must be hanging off the CPU. IE: /redfish/v1/Systems/System.Embedded.1/Storage/CPU.1 . I can expect that the BOSS card is hanging off of AHCI and that any SAS/SATA drives are likely on the RAID controller. The results above also imply that the host above runs a mixed backplane given the presence of RAID and CPU.1. Checking CPU.1 gets me: { \"@odata.context\": \"/redfish/v1/$metadata#Storage.Storage\", \"@odata.id\": \"/redfish/v1/Systems/System.Embedded.1/Storage/CPU.1\", \"@odata.type\": \"#Storage.v1_8_0.Storage\", \"Description\": \"CPU.1\", \"Drives\": [ { \"@odata.id\": \"/redfish/v1/Systems/System.Embedded.1/Storage/CPU.1/Drives/Disk.Bay.21:Enclosure.Internal.0-1\" }, { \"@odata.id\": \"/redfish/v1/Systems/System.Embedded.1/Storage/CPU.1/Drives/Disk.Bay.19:Enclosure.Internal.0-1\" }, { \"@odata.id\": \"/redfish/v1/Systems/System.Embedded.1/Storage/CPU.1/Drives/Disk.Bay.20:Enclosure.Internal.0-1\" }, { \"@odata.id\": \"/redfish/v1/Systems/System.Embedded.1/Storage/CPU.1/Drives/Disk.Bay.23:Enclosure.Internal.0-1\" }, { \"@odata.id\": \"/redfish/v1/Systems/System.Embedded.1/Storage/CPU.1/Drives/Disk.Bay.18:Enclosure.Internal.0-1\" }, { \"@odata.id\": \"/redfish/v1/Systems/System.Embedded.1/Storage/CPU.1/Drives/Disk.Bay.22:Enclosure.Internal.0-1\" } ], \"Drives@odata.count\": 6, \"Id\": \"CPU.1\", \"Links\": { \"Enclosures\": [ { \"@odata.id\": \"/redfish/v1/Chassis/Enclosure.Internal.0-1\" }, { \"@odata.id\": \"/redfish/v1/Chassis/System.Embedded.1\" } ], \"Enclosures@odata.count\": 2 }, \"Name\": \"CPU.1\", \"Status\": { \"Health\": \"OK\", \"HealthRollup\": \"OK\", \"State\": \"Enabled\" }, \"Volumes\": { \"@odata.id\": \"/redfish/v1/Systems/System.Embedded.1/Storage/CPU.1/Volumes\" } } From the above I can deduce that CPU 1 has six drives attached to it. Or does it? Understand the Behavior of Unqualified Drives Here is a picture of the front of my server: Here is the front of my server. You might say, \"Wait, there are 7 drives!?\" The problem is this 7th drive isn't qualified by Dell. It will still work just fine however, iDRAC won't know how to talk to it so it won't show up: You can confirm this is the case by checking the Storage->Physical Disks tab inside the iDRAC itself: Here you can see that I only have the 6 NVMe drives plus two SATA SSDs. While the iDRAC's personality module won't be able to properly sort the drive into Storage it will detect it as a PCIe device and accurately read the vendor information: Getting a Drive's Stats We can select one of them with /redfish/v1/Systems/System.Embedded.1/Storage/CPU.1/Drives/Disk.Bay.21:Enclosure.Internal.0-1 . This achieves the desired result and gets a dump of that drive's data. The size is available under the field CapacityBytes. { \"@odata.context\": \"/redfish/v1/$metadata#Drive.Drive\", \"@odata.id\": \"/redfish/v1/Systems/System.Embedded.1/Storage/CPU.1/Drives/Disk.Bay.21:Enclosure.Internal.0-1\", \"@odata.type\": \"#Drive.v1_9_0.Drive\", \"Actions\": { \"#Drive.SecureErase\": { \"@Redfish.OperationApplyTimeSupport\": { \"@odata.type\": \"#Settings.v1_3_0.OperationApplyTimeSupport\", \"SupportedValues\": [ \"Immediate\", \"OnReset\" ] }, \"target\": \"/redfish/v1/Systems/System.Embedded.1/Storage/CPU.1/Drives/Disk.Bay.21:Enclosure.Internal.0-1/Actions/Drive.SecureErase\" } }, \"Assembly\": { \"@odata.id\": \"/redfish/v1/Chassis/System.Embedded.1/Assembly\" }, \"BlockSizeBytes\": 0, \"CapableSpeedGbs\": 7.876923076923077, \"CapacityBytes\": 3200631791616, \"Description\": \"PCIe SSD in Slot 21 in Bay 1\", \"EncryptionAbility\": \"None\", \"EncryptionStatus\": \"Unencrypted\", \"FailurePredicted\": false, \"HotspareType\": \"None\", \"Id\": \"Disk.Bay.21:Enclosure.Internal.0-1\", \"Identifiers\": [ { \"DurableName\": null, \"DurableNameFormat\": null } ], \"Identifiers@odata.count\": 1, \"Links\": { \"Chassis\": { \"@odata.id\": \"/redfish/v1/Chassis/Enclosure.Internal.0-1\" }, \"PCIeFunctions\": [], \"PCIeFunctions@odata.count\": 0, \"Volumes\": [ { \"@odata.id\": \"/redfish/v1/Systems/System.Embedded.1/Storage/CPU.1/Volumes/Disk.Bay.21:Enclosure.Internal.0-1\" } ], \"Volumes@odata.count\": 1 }, \"Location\": [], \"Manufacturer\": \"Intel Corporation \", \"MediaType\": \"SSD\", \"Model\": \"Dell Express Flash NVMe P4610 3.2TB SFF\", \"Name\": \"PCIe SSD in Slot 21 in Bay 1\", \"NegotiatedSpeedGbs\": 7.876923076923077, \"Oem\": { \"Dell\": { \"@odata.type\": \"#DellOem.v1_1_0.DellOemResources\", \"DellDriveSMARTAttributes\": null, \"DellNVMeSMARTAttributes\": { \"@odata.id\": \"/redfish/v1/Systems/System.Embedded.1/Storage/CPU.1/Drives/Disk.Bay.21:Enclosure.Internal.0-1/Oem/Dell/DellNVMeSMARTAttributes\" }, \"DellPCIeSSD\": { \"@odata.context\": \"/redfish/v1/$metadata#DellPCIeSSD.DellPCIeSSD\", \"@odata.id\": \"/redfish/v1/Systems/System.Embedded.1/Storage/CPU.1/Drives/Disk.Bay.21:Enclosure.Internal.0-1/Oem/Dell/DellPCIeSSDs/Disk.Bay.21:Enclosure.Internal.0-1\", \"@odata.type\": \"#DellPCIeSSD.v1_4_0.DellPCIeSSD\", \"Bus\": \"CA\", \"BusProtocol\": \"PCIE\", \"Description\": \"An instance of DellPCIeSSD will have PCIe Solid State Drive specific data.\", \"Device\": \"0\", \"DeviceProtocol\": \"NVMe-MI1.0\", \"DriveFormFactor\": \"2.5Inch\", \"FreeSizeInBytes\": null, \"Function\": \"0\", \"HotSpareStatus\": null, \"Id\": \"Disk.Bay.21:Enclosure.Internal.0-1\", \"MediaType\": \"SolidStateDrive\", \"Name\": \"DellPCIeSSD\", \"PCIeCapableLinkWidth\": \"x4\", \"PCIeNegotiatedLinkWidth\": \"x4\", \"ProductID\": \"a54\", \"RAIDType\": \"Unknown\", \"RaidStatus\": null, \"Slot\": 21, \"SystemEraseCapability\": \"CryptographicErasePD\", \"UsedSizeInBytes\": 0 }, \"DellPhysicalDisk\": { \"@odata.context\": \"/redfish/v1/$metadata#DellPhysicalDisk.DellPhysicalDisk\", \"@odata.id\": \"/redfish/v1/Systems/System.Embedded.1/Storage/CPU.1/Drives/Disk.Bay.21:Enclosure.Internal.0-1/Oem/Dell/DellDrives/Disk.Bay.21:Enclosure.Internal.0-1\", \"@odata.type\": \"#DellPhysicalDisk.v1_3_0.DellPhysicalDisk\", \"Certified\": null, \"Connector\": null, \"Description\": \"An instance of DellPhysicalDisk will have Physical Disk specific data.\", \"DeviceProtocol\": \"NVMe-MI1.0\", \"DriveFormFactor\": \"2.5Inch\", \"EncryptionProtocol\": null, \"ForeignKeyIdentifier\": null, \"FreeSizeInBytes\": null, \"Id\": \"Disk.Bay.21:Enclosure.Internal.0-1\", \"LastSystemInventoryTime\": null, \"LastUpdateTime\": null, \"ManufacturingDay\": null, \"ManufacturingWeek\": null, \"ManufacturingYear\": null, \"Name\": \"DellPhysicalDisk\", \"NonRAIDDiskCachePolicy\": null, \"OperationName\": null, \"OperationPercentCompletePercent\": null, \"PCIeCapableLinkWidth\": \"x4\", \"PCIeNegotiatedLinkWidth\": \"x4\", \"PPID\": null, \"PowerStatus\": null, \"PredictiveFailureState\": null, \"ProductID\": \"a54\", \"RAIDType\": \"Unknown\", \"RaidStatus\": null, \"SASAddress\": null, \"Slot\": 21, \"SystemEraseCapability\": \"CryptographicErasePD\", \"T10PICapability\": null, \"UsedSizeInBytes\": 0, \"WWN\": null } } }, \"Operations\": [], \"Operations@odata.count\": 0, \"PartNumber\": \"TW02CN1TPIHIT9A9013TA02\", \"PhysicalLocation\": { \"PartLocation\": { \"LocationOrdinalValue\": 21, \"LocationType\": \"Slot\" } }, \"PredictedMediaLifeLeftPercent\": 100, \"Protocol\": \"PCIe\", \"Revision\": \"VDV1DP23\", \"RotationSpeedRPM\": null, \"SerialNumber\": \"PHLN9396002Q3P2BGN\", \"Status\": { \"Health\": \"OK\", \"HealthRollup\": \"OK\", \"State\": \"Enabled\" }, \"WriteCacheEnabled\": false }","title":"Get NVMe Drives from iDRAC Redfish"},{"location":"Get%20NVMe%20Drives%20from%20iDRAC%20Redfish/#get-nvme-drives-from-idrac-redfish","text":"Get NVMe Drives from iDRAC Redfish Exploring iDRAC Detected Storage Understand the Behavior of Unqualified Drives Getting a Drive's Stats","title":"Get NVMe Drives from iDRAC Redfish"},{"location":"Get%20NVMe%20Drives%20from%20iDRAC%20Redfish/#exploring-idrac-detected-storage","text":"I used the Storage API endpoint to accomplish this. From my host I received: {\"@odata.context\":\"/redfish/v1/$metadata#StorageCollection.StorageCollection\",\"@odata.id\":\"/redfish/v1/Systems/System.Embedded.1/Storage/\",\"@odata.type\":\"#StorageCollection.StorageCollection\",\"Description\":\"Collection Of Storage entities\",\"Members\":[{\"@odata.id\":\"/redfish/v1/Systems/System.Embedded.1/Storage/RAID.Slot.4-1\"},{\"@odata.id\":\"/redfish/v1/Systems/System.Embedded.1/Storage/RAID.Embedded.1-1\"},{\"@odata.id\":\"/redfish/v1/Systems/System.Embedded.1/Storage/CPU.1\"},{\"@odata.id\":\"/redfish/v1/Systems/System.Embedded.1/Storage/AHCI.Slot.2-1\"}],\"Members@odata.count\":4,\"Name\":\"Storage Collection\"} I'm running an R840 which is Dell 14G which I know does not have NVMe RAID controllers as an option so I know my NVMe drives must be hanging off the CPU. IE: /redfish/v1/Systems/System.Embedded.1/Storage/CPU.1 . I can expect that the BOSS card is hanging off of AHCI and that any SAS/SATA drives are likely on the RAID controller. The results above also imply that the host above runs a mixed backplane given the presence of RAID and CPU.1. Checking CPU.1 gets me: { \"@odata.context\": \"/redfish/v1/$metadata#Storage.Storage\", \"@odata.id\": \"/redfish/v1/Systems/System.Embedded.1/Storage/CPU.1\", \"@odata.type\": \"#Storage.v1_8_0.Storage\", \"Description\": \"CPU.1\", \"Drives\": [ { \"@odata.id\": \"/redfish/v1/Systems/System.Embedded.1/Storage/CPU.1/Drives/Disk.Bay.21:Enclosure.Internal.0-1\" }, { \"@odata.id\": \"/redfish/v1/Systems/System.Embedded.1/Storage/CPU.1/Drives/Disk.Bay.19:Enclosure.Internal.0-1\" }, { \"@odata.id\": \"/redfish/v1/Systems/System.Embedded.1/Storage/CPU.1/Drives/Disk.Bay.20:Enclosure.Internal.0-1\" }, { \"@odata.id\": \"/redfish/v1/Systems/System.Embedded.1/Storage/CPU.1/Drives/Disk.Bay.23:Enclosure.Internal.0-1\" }, { \"@odata.id\": \"/redfish/v1/Systems/System.Embedded.1/Storage/CPU.1/Drives/Disk.Bay.18:Enclosure.Internal.0-1\" }, { \"@odata.id\": \"/redfish/v1/Systems/System.Embedded.1/Storage/CPU.1/Drives/Disk.Bay.22:Enclosure.Internal.0-1\" } ], \"Drives@odata.count\": 6, \"Id\": \"CPU.1\", \"Links\": { \"Enclosures\": [ { \"@odata.id\": \"/redfish/v1/Chassis/Enclosure.Internal.0-1\" }, { \"@odata.id\": \"/redfish/v1/Chassis/System.Embedded.1\" } ], \"Enclosures@odata.count\": 2 }, \"Name\": \"CPU.1\", \"Status\": { \"Health\": \"OK\", \"HealthRollup\": \"OK\", \"State\": \"Enabled\" }, \"Volumes\": { \"@odata.id\": \"/redfish/v1/Systems/System.Embedded.1/Storage/CPU.1/Volumes\" } } From the above I can deduce that CPU 1 has six drives attached to it. Or does it?","title":"Exploring iDRAC Detected Storage"},{"location":"Get%20NVMe%20Drives%20from%20iDRAC%20Redfish/#understand-the-behavior-of-unqualified-drives","text":"Here is a picture of the front of my server: Here is the front of my server. You might say, \"Wait, there are 7 drives!?\" The problem is this 7th drive isn't qualified by Dell. It will still work just fine however, iDRAC won't know how to talk to it so it won't show up: You can confirm this is the case by checking the Storage->Physical Disks tab inside the iDRAC itself: Here you can see that I only have the 6 NVMe drives plus two SATA SSDs. While the iDRAC's personality module won't be able to properly sort the drive into Storage it will detect it as a PCIe device and accurately read the vendor information:","title":"Understand the Behavior of Unqualified Drives"},{"location":"Get%20NVMe%20Drives%20from%20iDRAC%20Redfish/#getting-a-drives-stats","text":"We can select one of them with /redfish/v1/Systems/System.Embedded.1/Storage/CPU.1/Drives/Disk.Bay.21:Enclosure.Internal.0-1 . This achieves the desired result and gets a dump of that drive's data. The size is available under the field CapacityBytes. { \"@odata.context\": \"/redfish/v1/$metadata#Drive.Drive\", \"@odata.id\": \"/redfish/v1/Systems/System.Embedded.1/Storage/CPU.1/Drives/Disk.Bay.21:Enclosure.Internal.0-1\", \"@odata.type\": \"#Drive.v1_9_0.Drive\", \"Actions\": { \"#Drive.SecureErase\": { \"@Redfish.OperationApplyTimeSupport\": { \"@odata.type\": \"#Settings.v1_3_0.OperationApplyTimeSupport\", \"SupportedValues\": [ \"Immediate\", \"OnReset\" ] }, \"target\": \"/redfish/v1/Systems/System.Embedded.1/Storage/CPU.1/Drives/Disk.Bay.21:Enclosure.Internal.0-1/Actions/Drive.SecureErase\" } }, \"Assembly\": { \"@odata.id\": \"/redfish/v1/Chassis/System.Embedded.1/Assembly\" }, \"BlockSizeBytes\": 0, \"CapableSpeedGbs\": 7.876923076923077, \"CapacityBytes\": 3200631791616, \"Description\": \"PCIe SSD in Slot 21 in Bay 1\", \"EncryptionAbility\": \"None\", \"EncryptionStatus\": \"Unencrypted\", \"FailurePredicted\": false, \"HotspareType\": \"None\", \"Id\": \"Disk.Bay.21:Enclosure.Internal.0-1\", \"Identifiers\": [ { \"DurableName\": null, \"DurableNameFormat\": null } ], \"Identifiers@odata.count\": 1, \"Links\": { \"Chassis\": { \"@odata.id\": \"/redfish/v1/Chassis/Enclosure.Internal.0-1\" }, \"PCIeFunctions\": [], \"PCIeFunctions@odata.count\": 0, \"Volumes\": [ { \"@odata.id\": \"/redfish/v1/Systems/System.Embedded.1/Storage/CPU.1/Volumes/Disk.Bay.21:Enclosure.Internal.0-1\" } ], \"Volumes@odata.count\": 1 }, \"Location\": [], \"Manufacturer\": \"Intel Corporation \", \"MediaType\": \"SSD\", \"Model\": \"Dell Express Flash NVMe P4610 3.2TB SFF\", \"Name\": \"PCIe SSD in Slot 21 in Bay 1\", \"NegotiatedSpeedGbs\": 7.876923076923077, \"Oem\": { \"Dell\": { \"@odata.type\": \"#DellOem.v1_1_0.DellOemResources\", \"DellDriveSMARTAttributes\": null, \"DellNVMeSMARTAttributes\": { \"@odata.id\": \"/redfish/v1/Systems/System.Embedded.1/Storage/CPU.1/Drives/Disk.Bay.21:Enclosure.Internal.0-1/Oem/Dell/DellNVMeSMARTAttributes\" }, \"DellPCIeSSD\": { \"@odata.context\": \"/redfish/v1/$metadata#DellPCIeSSD.DellPCIeSSD\", \"@odata.id\": \"/redfish/v1/Systems/System.Embedded.1/Storage/CPU.1/Drives/Disk.Bay.21:Enclosure.Internal.0-1/Oem/Dell/DellPCIeSSDs/Disk.Bay.21:Enclosure.Internal.0-1\", \"@odata.type\": \"#DellPCIeSSD.v1_4_0.DellPCIeSSD\", \"Bus\": \"CA\", \"BusProtocol\": \"PCIE\", \"Description\": \"An instance of DellPCIeSSD will have PCIe Solid State Drive specific data.\", \"Device\": \"0\", \"DeviceProtocol\": \"NVMe-MI1.0\", \"DriveFormFactor\": \"2.5Inch\", \"FreeSizeInBytes\": null, \"Function\": \"0\", \"HotSpareStatus\": null, \"Id\": \"Disk.Bay.21:Enclosure.Internal.0-1\", \"MediaType\": \"SolidStateDrive\", \"Name\": \"DellPCIeSSD\", \"PCIeCapableLinkWidth\": \"x4\", \"PCIeNegotiatedLinkWidth\": \"x4\", \"ProductID\": \"a54\", \"RAIDType\": \"Unknown\", \"RaidStatus\": null, \"Slot\": 21, \"SystemEraseCapability\": \"CryptographicErasePD\", \"UsedSizeInBytes\": 0 }, \"DellPhysicalDisk\": { \"@odata.context\": \"/redfish/v1/$metadata#DellPhysicalDisk.DellPhysicalDisk\", \"@odata.id\": \"/redfish/v1/Systems/System.Embedded.1/Storage/CPU.1/Drives/Disk.Bay.21:Enclosure.Internal.0-1/Oem/Dell/DellDrives/Disk.Bay.21:Enclosure.Internal.0-1\", \"@odata.type\": \"#DellPhysicalDisk.v1_3_0.DellPhysicalDisk\", \"Certified\": null, \"Connector\": null, \"Description\": \"An instance of DellPhysicalDisk will have Physical Disk specific data.\", \"DeviceProtocol\": \"NVMe-MI1.0\", \"DriveFormFactor\": \"2.5Inch\", \"EncryptionProtocol\": null, \"ForeignKeyIdentifier\": null, \"FreeSizeInBytes\": null, \"Id\": \"Disk.Bay.21:Enclosure.Internal.0-1\", \"LastSystemInventoryTime\": null, \"LastUpdateTime\": null, \"ManufacturingDay\": null, \"ManufacturingWeek\": null, \"ManufacturingYear\": null, \"Name\": \"DellPhysicalDisk\", \"NonRAIDDiskCachePolicy\": null, \"OperationName\": null, \"OperationPercentCompletePercent\": null, \"PCIeCapableLinkWidth\": \"x4\", \"PCIeNegotiatedLinkWidth\": \"x4\", \"PPID\": null, \"PowerStatus\": null, \"PredictiveFailureState\": null, \"ProductID\": \"a54\", \"RAIDType\": \"Unknown\", \"RaidStatus\": null, \"SASAddress\": null, \"Slot\": 21, \"SystemEraseCapability\": \"CryptographicErasePD\", \"T10PICapability\": null, \"UsedSizeInBytes\": 0, \"WWN\": null } } }, \"Operations\": [], \"Operations@odata.count\": 0, \"PartNumber\": \"TW02CN1TPIHIT9A9013TA02\", \"PhysicalLocation\": { \"PartLocation\": { \"LocationOrdinalValue\": 21, \"LocationType\": \"Slot\" } }, \"PredictedMediaLifeLeftPercent\": 100, \"Protocol\": \"PCIe\", \"Revision\": \"VDV1DP23\", \"RotationSpeedRPM\": null, \"SerialNumber\": \"PHLN9396002Q3P2BGN\", \"Status\": { \"Health\": \"OK\", \"HealthRollup\": \"OK\", \"State\": \"Enabled\" }, \"WriteCacheEnabled\": false }","title":"Getting a Drive's Stats"},{"location":"High%20Speed%20Packet%20Capture/","text":"High Speed Packet Capture See https://github.com/grantcurell/packet_capture For testing notes see: DPDK on ESXi with CentOS 7 (Incomplete) DPDK on FC640 with RHEL 8 DPDK on R940 with Napatech ntop ntop on R940 with Napatech","title":"High Speed Packet Capture"},{"location":"High%20Speed%20Packet%20Capture/#high-speed-packet-capture","text":"See https://github.com/grantcurell/packet_capture For testing notes see: DPDK on ESXi with CentOS 7 (Incomplete) DPDK on FC640 with RHEL 8 DPDK on R940 with Napatech ntop ntop on R940 with Napatech","title":"High Speed Packet Capture"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20ESXi%20with%20CentOS%207%20%28INCOMPLETE%29/","text":"How to Get DPDK with pdump Running The purpose of this experiment is to get Intel's DPDK framework up and running on a virtual machine. Useful Materials VMWare info on Intel DPDK How to Compile DPDK Info on Linux Drivers for DPDK My Environment I am running this inital test on ESXi 6.7 in a virtual machine CentOS Release Info NAME=\"CentOS Linux\" VERSION=\"7 (Core)\" ID=\"centos\" ID_LIKE=\"rhel fedora\" VERSION_ID=\"7\" PRETTY_NAME=\"CentOS Linux 7 (Core)\" ANSI_COLOR=\"0;31\" CPE_NAME=\"cpe:/o:centos:centos:7\" HOME_URL=\"https://www.centos.org/\" BUG_REPORT_URL=\"https://bugs.centos.org/\" CENTOS_MANTISBT_PROJECT=\"CentOS-7\" CENTOS_MANTISBT_PROJECT_VERSION=\"7\" REDHAT_SUPPORT_PRODUCT=\"centos\" REDHAT_SUPPORT_PRODUCT_VERSION=\"7\" CentOS Linux release 7.7.1908 (Core) CentOS Linux release 7.7.1908 (Core) Kernel Info Linux centos.lan 3.10.0-1062.4.1.el7.x86_64 #1 SMP Fri Oct 18 17:15:30 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux Installation Configure GRUB Command Line for Virtualized DPDK In order for DPDK to work in a virtual environment you must disable memory protection. The reason for this is that with memory protection enabled CentOS will block read/write/execution to the DMA'd portion of memory. See this post. Do the following: cd /etc/default vim grub Edit GRUB-CMDLINE and Add \u201cnopku\u201d GRUB_CMDLINE_LINUX=\"crashkernel=auto rd.lvm.lv=centos/root rd.lvm.lv=centos/swap rhgb quiet nopku transparent_hugepage=never log_buf_len=8M\" Recompile grub: sudo grub2-mkconfig -o /boot/grub2/grub.cfg reboot Install DPDK Download from https://core.dpdk.org/download/ Run yum install -y gcc numactl-devel kernel-devel pciutils elfutils-libelf-devel make libpcap python3 tar vim wget tmux vim mlocate hwloc libpcap-devel Extract dpdk and cd into its directory set the environment variable RTE_SDK. It is the directory in which you extracted all the DPDK files. export RTE_SDK=<YOUR_DIR> Run make install T=x86_64-native-linux-gcc DESTDIR=<INSTALL_DIR> to build dpdk. Ensure your install directory exists. Once an DPDK target environment directory has been created (such as x86_64-native-linux-gcc), it contains all libraries and header files required to build an application. When compiling an application in the Linux* environment on the DPDK, the following variables must be exported: RTE_TARGET - Points to the DPDK target environment directory. export RTE_TARGET=/opt/dpdk-19.08/x86_64-native-linux-gcc You may want to add this variable and RTE_SDK to ~/.bash_profile Configuration Update Ulimits Edit the security limits with vim /etc/security/limits.conf Add the following lines at the end of the file: root hard memlock unlimited root soft memlock unlimited Reboot and see if the system has the newly updated value Configure vfio-pci to Load on Boot Go to /etc/modules-load.d/ cd /etc/modules-load.d Run echo vfio-pci > vfio-pci.conf If you don't reboot you will need to run modprobe vfio-pci Configure Ports Move to your INSTALL_DIR and run ./usertools/dpdk-setup.sh . This should give you a menu with all available DPDK options. The menu is setup in such a way that you must perform each step listed in the menu. If things have gone correctly to this point your Step 1 should look like the following: ---------------------------------------------------------- Step 1: Select the DPDK environment to build ---------------------------------------------------------- [1] * My menu looks like this: ---------------------------------------------------------- Step 1: Select the DPDK environment to build ---------------------------------------------------------- [1] * ---------------------------------------------------------- Step 2: Setup linux environment ---------------------------------------------------------- [2] Insert IGB UIO module [3] Insert VFIO module [4] Insert KNI module [5] Setup hugepage mappings for non-NUMA systems [6] Setup hugepage mappings for NUMA systems [7] Display current Ethernet/Baseband/Crypto device settings [8] Bind Ethernet/Baseband/Crypto device to IGB UIO module [9] Bind Ethernet/Baseband/Crypto device to VFIO module [10] Setup VFIO permissions ---------------------------------------------------------- Step 3: Run test application for linux environment ---------------------------------------------------------- [11] Run test application ($RTE_TARGET/app/test) [12] Run testpmd application in interactive mode ($RTE_TARGET/app/testpmd) ---------------------------------------------------------- Step 4: Other tools ---------------------------------------------------------- [13] List hugepage info from /proc/meminfo ---------------------------------------------------------- Step 5: Uninstall and system cleanup ---------------------------------------------------------- [14] Unbind devices from IGB UIO or VFIO driver [15] Remove IGB UIO module [16] Remove VFIO module [17] Remove KNI module [18] Remove hugepage mappings [19] Exit Script WARNING : If you run Option 3 to insert the VFIO module I found that it actually caused DPDK to stop working. Next run option 6 to instert huge pages for NUMA systems. Notice you will be prompted to select an amount of memory on a per processor basis. This is because there are pages associated with each individual processor to increase performance via locality. Run option 7 and make sure you receive output and that network devices are listed. My output looks like this: Network devices using kernel driver =================================== 0000:0b:00.0 'VMXNET3 Ethernet Controller 07b0' if=ens192 drv=vmxnet3 unused=vfio-pci *Active* 0000:13:00.0 'VMXNET3 Ethernet Controller 07b0' if=ens224 drv=vmxnet3 unused=vfio-pci *Active* No 'Baseband' devices detected ============================== No 'Crypto' devices detected ============================ No 'Eventdev' devices detected ============================== No 'Mempool' devices detected ============================= No 'Compress' devices detected ============================== No 'Misc (rawdev)' devices detected =================================== NOTE : The active keyword means that DPDK thinks the interface is under active use. This typically means it has an IP address assigned to it. Run option 9 to bind an interface to DPDK Notes You can use PCI passthrough on the x520 and x710 Run ethtool -i <your_interface> to figure out what kind of driver you have Run bash /opt/dpdk-19.08/usertools/dpdk-setup.sh Run option 47 and then enter 64 when prompted Run option 44 to insert the VRIO module Run ulimit -u unlimited to increase the memlock limit (NOTE: I don't think this did what I needed it to.) add iommu=pt intel_iommu=on grub2-mkconfig -o /boot/grub2/grub.cfg","title":"How to Get DPDK with pdump Running"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20ESXi%20with%20CentOS%207%20%28INCOMPLETE%29/#how-to-get-dpdk-with-pdump-running","text":"The purpose of this experiment is to get Intel's DPDK framework up and running on a virtual machine.","title":"How to Get DPDK with pdump Running"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20ESXi%20with%20CentOS%207%20%28INCOMPLETE%29/#useful-materials","text":"VMWare info on Intel DPDK How to Compile DPDK Info on Linux Drivers for DPDK","title":"Useful Materials"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20ESXi%20with%20CentOS%207%20%28INCOMPLETE%29/#my-environment","text":"I am running this inital test on ESXi 6.7 in a virtual machine","title":"My Environment"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20ESXi%20with%20CentOS%207%20%28INCOMPLETE%29/#centos-release-info","text":"NAME=\"CentOS Linux\" VERSION=\"7 (Core)\" ID=\"centos\" ID_LIKE=\"rhel fedora\" VERSION_ID=\"7\" PRETTY_NAME=\"CentOS Linux 7 (Core)\" ANSI_COLOR=\"0;31\" CPE_NAME=\"cpe:/o:centos:centos:7\" HOME_URL=\"https://www.centos.org/\" BUG_REPORT_URL=\"https://bugs.centos.org/\" CENTOS_MANTISBT_PROJECT=\"CentOS-7\" CENTOS_MANTISBT_PROJECT_VERSION=\"7\" REDHAT_SUPPORT_PRODUCT=\"centos\" REDHAT_SUPPORT_PRODUCT_VERSION=\"7\" CentOS Linux release 7.7.1908 (Core) CentOS Linux release 7.7.1908 (Core)","title":"CentOS Release Info"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20ESXi%20with%20CentOS%207%20%28INCOMPLETE%29/#kernel-info","text":"Linux centos.lan 3.10.0-1062.4.1.el7.x86_64 #1 SMP Fri Oct 18 17:15:30 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux","title":"Kernel Info"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20ESXi%20with%20CentOS%207%20%28INCOMPLETE%29/#installation","text":"","title":"Installation"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20ESXi%20with%20CentOS%207%20%28INCOMPLETE%29/#configure-grub-command-line-for-virtualized-dpdk","text":"In order for DPDK to work in a virtual environment you must disable memory protection. The reason for this is that with memory protection enabled CentOS will block read/write/execution to the DMA'd portion of memory. See this post. Do the following: cd /etc/default vim grub Edit GRUB-CMDLINE and Add \u201cnopku\u201d GRUB_CMDLINE_LINUX=\"crashkernel=auto rd.lvm.lv=centos/root rd.lvm.lv=centos/swap rhgb quiet nopku transparent_hugepage=never log_buf_len=8M\" Recompile grub: sudo grub2-mkconfig -o /boot/grub2/grub.cfg reboot","title":"Configure GRUB Command Line for Virtualized DPDK"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20ESXi%20with%20CentOS%207%20%28INCOMPLETE%29/#install-dpdk","text":"Download from https://core.dpdk.org/download/ Run yum install -y gcc numactl-devel kernel-devel pciutils elfutils-libelf-devel make libpcap python3 tar vim wget tmux vim mlocate hwloc libpcap-devel Extract dpdk and cd into its directory set the environment variable RTE_SDK. It is the directory in which you extracted all the DPDK files. export RTE_SDK=<YOUR_DIR> Run make install T=x86_64-native-linux-gcc DESTDIR=<INSTALL_DIR> to build dpdk. Ensure your install directory exists. Once an DPDK target environment directory has been created (such as x86_64-native-linux-gcc), it contains all libraries and header files required to build an application. When compiling an application in the Linux* environment on the DPDK, the following variables must be exported: RTE_TARGET - Points to the DPDK target environment directory. export RTE_TARGET=/opt/dpdk-19.08/x86_64-native-linux-gcc You may want to add this variable and RTE_SDK to ~/.bash_profile","title":"Install DPDK"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20ESXi%20with%20CentOS%207%20%28INCOMPLETE%29/#configuration","text":"","title":"Configuration"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20ESXi%20with%20CentOS%207%20%28INCOMPLETE%29/#update-ulimits","text":"Edit the security limits with vim /etc/security/limits.conf Add the following lines at the end of the file: root hard memlock unlimited root soft memlock unlimited Reboot and see if the system has the newly updated value","title":"Update Ulimits"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20ESXi%20with%20CentOS%207%20%28INCOMPLETE%29/#configure-vfio-pci-to-load-on-boot","text":"Go to /etc/modules-load.d/ cd /etc/modules-load.d Run echo vfio-pci > vfio-pci.conf If you don't reboot you will need to run modprobe vfio-pci","title":"Configure vfio-pci to Load on Boot"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20ESXi%20with%20CentOS%207%20%28INCOMPLETE%29/#configure-ports","text":"Move to your INSTALL_DIR and run ./usertools/dpdk-setup.sh . This should give you a menu with all available DPDK options. The menu is setup in such a way that you must perform each step listed in the menu. If things have gone correctly to this point your Step 1 should look like the following: ---------------------------------------------------------- Step 1: Select the DPDK environment to build ---------------------------------------------------------- [1] * My menu looks like this: ---------------------------------------------------------- Step 1: Select the DPDK environment to build ---------------------------------------------------------- [1] * ---------------------------------------------------------- Step 2: Setup linux environment ---------------------------------------------------------- [2] Insert IGB UIO module [3] Insert VFIO module [4] Insert KNI module [5] Setup hugepage mappings for non-NUMA systems [6] Setup hugepage mappings for NUMA systems [7] Display current Ethernet/Baseband/Crypto device settings [8] Bind Ethernet/Baseband/Crypto device to IGB UIO module [9] Bind Ethernet/Baseband/Crypto device to VFIO module [10] Setup VFIO permissions ---------------------------------------------------------- Step 3: Run test application for linux environment ---------------------------------------------------------- [11] Run test application ($RTE_TARGET/app/test) [12] Run testpmd application in interactive mode ($RTE_TARGET/app/testpmd) ---------------------------------------------------------- Step 4: Other tools ---------------------------------------------------------- [13] List hugepage info from /proc/meminfo ---------------------------------------------------------- Step 5: Uninstall and system cleanup ---------------------------------------------------------- [14] Unbind devices from IGB UIO or VFIO driver [15] Remove IGB UIO module [16] Remove VFIO module [17] Remove KNI module [18] Remove hugepage mappings [19] Exit Script WARNING : If you run Option 3 to insert the VFIO module I found that it actually caused DPDK to stop working. Next run option 6 to instert huge pages for NUMA systems. Notice you will be prompted to select an amount of memory on a per processor basis. This is because there are pages associated with each individual processor to increase performance via locality. Run option 7 and make sure you receive output and that network devices are listed. My output looks like this: Network devices using kernel driver =================================== 0000:0b:00.0 'VMXNET3 Ethernet Controller 07b0' if=ens192 drv=vmxnet3 unused=vfio-pci *Active* 0000:13:00.0 'VMXNET3 Ethernet Controller 07b0' if=ens224 drv=vmxnet3 unused=vfio-pci *Active* No 'Baseband' devices detected ============================== No 'Crypto' devices detected ============================ No 'Eventdev' devices detected ============================== No 'Mempool' devices detected ============================= No 'Compress' devices detected ============================== No 'Misc (rawdev)' devices detected =================================== NOTE : The active keyword means that DPDK thinks the interface is under active use. This typically means it has an IP address assigned to it. Run option 9 to bind an interface to DPDK","title":"Configure Ports"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20ESXi%20with%20CentOS%207%20%28INCOMPLETE%29/#notes","text":"You can use PCI passthrough on the x520 and x710 Run ethtool -i <your_interface> to figure out what kind of driver you have Run bash /opt/dpdk-19.08/usertools/dpdk-setup.sh Run option 47 and then enter 64 when prompted Run option 44 to insert the VRIO module Run ulimit -u unlimited to increase the memlock limit (NOTE: I don't think this did what I needed it to.) add iommu=pt intel_iommu=on grub2-mkconfig -o /boot/grub2/grub.cfg","title":"Notes"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20FC640%20with%20RHEL%208/","text":"How to Get DPDK with pdump Running The purpose of this experiment is to get Intel's DPDK framework up and running on a server. Useful Materials How to Compile DPDK Info on Linux Drivers for DPDK Description of the TestPMD Program DPDK Testpmd Application User Guide My Environment I am running a Dell FC640 on a TFX2HE chassis. Red Hat Release Info NAME=\"Red Hat Enterprise Linux\" VERSION=\"8.1 (Ootpa)\" ID=\"rhel\" ID_LIKE=\"fedora\" VERSION_ID=\"8.1\" PLATFORM_ID=\"platform:el8\" PRETTY_NAME=\"Red Hat Enterprise Linux 8.1 (Ootpa)\" ANSI_COLOR=\"0;31\" CPE_NAME=\"cpe:/o:redhat:enterprise_linux:8.1:GA\" HOME_URL=\"https://www.redhat.com/\" BUG_REPORT_URL=\"https://bugzilla.redhat.com/\" REDHAT_BUGZILLA_PRODUCT=\"Red Hat Enterprise Linux 8\" REDHAT_BUGZILLA_PRODUCT_VERSION=8.1 REDHAT_SUPPORT_PRODUCT=\"Red Hat Enterprise Linux\" REDHAT_SUPPORT_PRODUCT_VERSION=\"8.1\" Red Hat Enterprise Linux release 8.1 (Ootpa) Red Hat Enterprise Linux release 8.1 (Ootpa) Kernel Info Linux dpdkdemo.lan 4.18.0-147.el8.x86_64 #1 SMP Thu Sep 26 15:52:44 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux Physical Setup I ran the traffic generator to port 5 of the passthrough module which maps to port 1 my internal x710 card. Installation Enable Red Hat Repos Run subscription-manager list --available | less and find the subscription which provides CodeReady for x86. Note the pool number associated with the subscription. Run subscription-manager attach --pool=<POOL_NUMBER> to enable the subscription. Then run: subscription-manager repos --enable=codeready-builder-for-rhel-8-x86_64-rpms to enable the repo. Install DPDK Download from https://core.dpdk.org/download/ Edit the security limits with vim /etc/security/limits.conf Add the following lines at the end of the file. This assumes you are running as root: root hard memlock unlimited root soft memlock unlimited Make sure your kernel is up to date with dnf update -y && reboot . Run dnf install -y gcc numactl-devel kernel-devel pciutils elfutils-libelf-devel make libpcap python3 tar vim wget tmux vim mlocate hwloc Extract dpdk and cd into its directory set the environment variable RTE_SDK. It is the directory in which you extracted all the DPDK files. export RTE_SDK=<YOUR_DIR> Run make install T=x86_64-native-linux-gcc CONFIG_RTE_LIBRTE_PMD_PCAP=y CONFIG_RTE_LIBRTE_PDUMP=y DESTDIR=<INSTALL_DIR> to build dpdk. Ensure your install directory exists. NOTE : The option CONFIG_RTE_LIBRTE_PMD_PCAP=y enabled libpcap support in DPDK. This is required for pdump to work. Once an DPDK target environment directory has been created (such as x86_64-native-linux-gcc), it contains all libraries and header files required to build an application. When compiling an application in the Linux* environment on the DPDK, the following variables must be exported: RTE_TARGET - Points to the DPDK target environment directory. export RTE_TARGET=/opt/dpdk-19.08/x86_64-native-linux-gcc You may want to add this variable and RTE_SDK to ~/.bash_profile Install ELF Tools Run the following: dnf install -y python36-devel pip3 install numpy pip3 install elftools pip3 install pyelftools Configuration TODO: Need to update the instructions with this. Why can't the setup tool find it? I had to run modprobe uio && insmod /opt/dpdk-19.08/install/lib/modules/4.18.0-147.el8.x86_64/extra/dpdk/igb_uio.ko to get this to load. The modprobe uio is necessary because the uio module is a depency of igb_uio. Configure vfio-pci to Load on Boot (TODO REMOVE) Go to /etc/modules-load.d/ cd /etc/modules-load.d Run echo vfio-pci > vfio-pci.conf If you don't reboot you will need to run modprobe vfio-pci Configure Ports Move to your dpdk dir and run ./install/share/dpdk/usertools/dpdk-setup.sh . This should give you a menu with all available DPDK options. The menu is setup in such a way that you must perform each step listed in the menu. If things have gone correctly to this point your Step 1 should look like the following: ---------------------------------------------------------- Step 1: Select the DPDK environment to build ---------------------------------------------------------- [1] * My menu looks like this: ---------------------------------------------------------- Step 1: Select the DPDK environment to build ---------------------------------------------------------- [1] * ---------------------------------------------------------- Step 2: Setup linux environment ---------------------------------------------------------- [2] Insert IGB UIO module [3] Insert VFIO module [4] Insert KNI module [5] Setup hugepage mappings for non-NUMA systems [6] Setup hugepage mappings for NUMA systems [7] Display current Ethernet/Baseband/Crypto device settings [8] Bind Ethernet/Baseband/Crypto device to IGB UIO module [9] Bind Ethernet/Baseband/Crypto device to VFIO module [10] Setup VFIO permissions ---------------------------------------------------------- Step 3: Run test application for linux environment ---------------------------------------------------------- [11] Run test application ($RTE_TARGET/app/test) [12] Run testpmd application in interactive mode ($RTE_TARGET/app/testpmd) ---------------------------------------------------------- Step 4: Other tools ---------------------------------------------------------- [13] List hugepage info from /proc/meminfo ---------------------------------------------------------- Step 5: Uninstall and system cleanup ---------------------------------------------------------- [14] Unbind devices from IGB UIO or VFIO driver [15] Remove IGB UIO module [16] Remove VFIO module [17] Remove KNI module [18] Remove hugepage mappings [19] Exit Script Next run option 6 to instert huge pages for NUMA systems. Notice you will be prompted to select an amount of memory on a per processor basis. This is because there are pages associated with each individual processor to increase performance via locality. Run option 7 and make sure you receive output and that network devices are listed. My output looks like this: Network devices using kernel driver =================================== 0000:0b:00.0 'VMXNET3 Ethernet Controller 07b0' if=ens192 drv=vmxnet3 unused=vfio-pci *Active* 0000:13:00.0 'VMXNET3 Ethernet Controller 07b0' if=ens224 drv=vmxnet3 unused=vfio-pci *Active* No 'Baseband' devices detected ============================== No 'Crypto' devices detected ============================ No 'Eventdev' devices detected ============================== No 'Mempool' devices detected ============================= No 'Compress' devices detected ============================== No 'Misc (rawdev)' devices detected =================================== NOTE : The active keyword means that DPDK thinks the interface is under active use. This means that the interface has routes installed in the routing table. Run option 8 to bind an interface to DPDK using the IGB UIO driver. Performing Packet Capture Initial Setup Get the core layout with ./install/share/dpdk/usertools/cpu_layout.py View your port layout with ./install/share/dpdk/usertools/dpdk-devbind.py -s If you haven't already load the right kernel modules with: modprobe uio && insmod /opt/dpdk-19.08/install/lib/modules/4.18.0-147.el8.x86_64/extra/dpdk/igb_uio.ko Starting testpmd NOTE : The -- separates the argumentsn for the EAL vs TestPMD. ./install/bin/testpmd -l 4,8,10,12 -n 4 -- -i --forward-mode=rxonly After testpmd has started don't forget to run the start command on the testpmd command line. pdump ./install/bin/dpdk-pdump -- --pdump 'port=0,queue=*,rx-dev=/tmp/capture.pcap' Helpful Tips Getting CPU Info DPDK provides a tool for seeing the CPU layout with ./install/share/dpdk/usertools/cpu_layout.py You can see the logical layout of the cores with cat /proc/cpuinfo You can alse run lstopo-no-graphics Notice that the cores alternate back and forth. Process Types in DPDK DPDK runs two different types of processes. There are as follows: primary processes, which can initialize and which have full permissions on shared memory secondary processes, which cannot initialize shared memory, but can attach to pre- initialized shared memory and create objects in it. Standalone DPDK processes are primary processes, while secondary processes can only run alongside a primary process or after a primary process has already configured the hugepage shared memory for them. TestPMD The test pmd manual is available here Interactive Commands Starting transmit start Get Port Info show port info all Forwarding Modes TestPMD has different forwarding modes that can be used within the application. Input/output mode: This mode is generally referred to as IO mode. It is the most common forwarding mode and is the default mode when TestPMD is started. In IO mode a CPU core receives packets from one port (Rx) and transmits them to another port (Tx). The same port can be used for reception and transmission if required. Rx-only mode: In this mode the application polls packets from the Rx ports and frees them without transmitting them. In this way it acts as a packet sink. Tx-only mode: In this mode the application generates 64-byte IP packets and transmits them from the Tx ports. It doesn\u2019t handle the reception of packets and as such acts as a packet source. These latter two modes (Rx-only and Tx-only) are useful for checking packet reception and transmission separately. Apart from these three modes there are other forwarding modes that are explained in the TestPMD documentation . Port Topology Modes In paired mode, the forwarding is between pairs of ports, for example: (0,1), (2,3), (4,5). In chained mode, the forwarding is to the next available port in the port mask, for example: (0,1), (1,2), (2,0). The ordering of the ports can be changed using the portlist testpmd runtime function. In loop mode, ingress traffic is simply transmitted back on the same interface. Receive Side Scaling =Receive-Side Scaling (RSS), also known as multi-queue receive, distributes network receive processing across several hardware-based receive queues, allowing inbound network traffic to be processed by multiple CPUs. RSS can be used to relieve bottlenecks in receive interrupt processing caused by overloading a single CPU, and to reduce network latency. To determine whether your network interface card supports RSS, check whether multiple interrupt request queues are associated with the interface in /proc/interrupts. For example, if you are interested in the p1p1 interface: # egrep 'CPU|p1p1' /proc/interrupts CPU0 CPU1 CPU2 CPU3 CPU4 CPU5 89: 40187 0 0 0 0 0 IR-PCI-MSI-edge p1p1-0 90: 0 790 0 0 0 0 IR-PCI-MSI-edge p1p1-1 91: 0 0 959 0 0 0 IR-PCI-MSI-edge p1p1-2 92: 0 0 0 3310 0 0 IR-PCI-MSI-edge p1p1-3 93: 0 0 0 0 622 0 IR-PCI-MSI-edge p1p1-4 94: 0 0 0 0 0 2475 IR-PCI-MSI-edge p1p1-5 huge pages https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/performance_tuning_guide/sect-red_hat_enterprise_linux-performance_tuning_guide-memory-configuring-huge-pages","title":"How to Get DPDK with pdump Running"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20FC640%20with%20RHEL%208/#how-to-get-dpdk-with-pdump-running","text":"The purpose of this experiment is to get Intel's DPDK framework up and running on a server.","title":"How to Get DPDK with pdump Running"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20FC640%20with%20RHEL%208/#useful-materials","text":"How to Compile DPDK Info on Linux Drivers for DPDK Description of the TestPMD Program DPDK Testpmd Application User Guide","title":"Useful Materials"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20FC640%20with%20RHEL%208/#my-environment","text":"I am running a Dell FC640 on a TFX2HE chassis.","title":"My Environment"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20FC640%20with%20RHEL%208/#red-hat-release-info","text":"NAME=\"Red Hat Enterprise Linux\" VERSION=\"8.1 (Ootpa)\" ID=\"rhel\" ID_LIKE=\"fedora\" VERSION_ID=\"8.1\" PLATFORM_ID=\"platform:el8\" PRETTY_NAME=\"Red Hat Enterprise Linux 8.1 (Ootpa)\" ANSI_COLOR=\"0;31\" CPE_NAME=\"cpe:/o:redhat:enterprise_linux:8.1:GA\" HOME_URL=\"https://www.redhat.com/\" BUG_REPORT_URL=\"https://bugzilla.redhat.com/\" REDHAT_BUGZILLA_PRODUCT=\"Red Hat Enterprise Linux 8\" REDHAT_BUGZILLA_PRODUCT_VERSION=8.1 REDHAT_SUPPORT_PRODUCT=\"Red Hat Enterprise Linux\" REDHAT_SUPPORT_PRODUCT_VERSION=\"8.1\" Red Hat Enterprise Linux release 8.1 (Ootpa) Red Hat Enterprise Linux release 8.1 (Ootpa)","title":"Red Hat Release Info"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20FC640%20with%20RHEL%208/#kernel-info","text":"Linux dpdkdemo.lan 4.18.0-147.el8.x86_64 #1 SMP Thu Sep 26 15:52:44 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux","title":"Kernel Info"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20FC640%20with%20RHEL%208/#physical-setup","text":"I ran the traffic generator to port 5 of the passthrough module which maps to port 1 my internal x710 card.","title":"Physical Setup"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20FC640%20with%20RHEL%208/#installation","text":"","title":"Installation"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20FC640%20with%20RHEL%208/#enable-red-hat-repos","text":"Run subscription-manager list --available | less and find the subscription which provides CodeReady for x86. Note the pool number associated with the subscription. Run subscription-manager attach --pool=<POOL_NUMBER> to enable the subscription. Then run: subscription-manager repos --enable=codeready-builder-for-rhel-8-x86_64-rpms to enable the repo.","title":"Enable Red Hat Repos"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20FC640%20with%20RHEL%208/#install-dpdk","text":"Download from https://core.dpdk.org/download/ Edit the security limits with vim /etc/security/limits.conf Add the following lines at the end of the file. This assumes you are running as root: root hard memlock unlimited root soft memlock unlimited Make sure your kernel is up to date with dnf update -y && reboot . Run dnf install -y gcc numactl-devel kernel-devel pciutils elfutils-libelf-devel make libpcap python3 tar vim wget tmux vim mlocate hwloc Extract dpdk and cd into its directory set the environment variable RTE_SDK. It is the directory in which you extracted all the DPDK files. export RTE_SDK=<YOUR_DIR> Run make install T=x86_64-native-linux-gcc CONFIG_RTE_LIBRTE_PMD_PCAP=y CONFIG_RTE_LIBRTE_PDUMP=y DESTDIR=<INSTALL_DIR> to build dpdk. Ensure your install directory exists. NOTE : The option CONFIG_RTE_LIBRTE_PMD_PCAP=y enabled libpcap support in DPDK. This is required for pdump to work. Once an DPDK target environment directory has been created (such as x86_64-native-linux-gcc), it contains all libraries and header files required to build an application. When compiling an application in the Linux* environment on the DPDK, the following variables must be exported: RTE_TARGET - Points to the DPDK target environment directory. export RTE_TARGET=/opt/dpdk-19.08/x86_64-native-linux-gcc You may want to add this variable and RTE_SDK to ~/.bash_profile","title":"Install DPDK"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20FC640%20with%20RHEL%208/#install-elf-tools","text":"Run the following: dnf install -y python36-devel pip3 install numpy pip3 install elftools pip3 install pyelftools","title":"Install ELF Tools"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20FC640%20with%20RHEL%208/#configuration","text":"TODO: Need to update the instructions with this. Why can't the setup tool find it? I had to run modprobe uio && insmod /opt/dpdk-19.08/install/lib/modules/4.18.0-147.el8.x86_64/extra/dpdk/igb_uio.ko to get this to load. The modprobe uio is necessary because the uio module is a depency of igb_uio.","title":"Configuration"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20FC640%20with%20RHEL%208/#configure-vfio-pci-to-load-on-boot-todo-remove","text":"Go to /etc/modules-load.d/ cd /etc/modules-load.d Run echo vfio-pci > vfio-pci.conf If you don't reboot you will need to run modprobe vfio-pci","title":"Configure vfio-pci to Load on Boot (TODO REMOVE)"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20FC640%20with%20RHEL%208/#configure-ports","text":"Move to your dpdk dir and run ./install/share/dpdk/usertools/dpdk-setup.sh . This should give you a menu with all available DPDK options. The menu is setup in such a way that you must perform each step listed in the menu. If things have gone correctly to this point your Step 1 should look like the following: ---------------------------------------------------------- Step 1: Select the DPDK environment to build ---------------------------------------------------------- [1] * My menu looks like this: ---------------------------------------------------------- Step 1: Select the DPDK environment to build ---------------------------------------------------------- [1] * ---------------------------------------------------------- Step 2: Setup linux environment ---------------------------------------------------------- [2] Insert IGB UIO module [3] Insert VFIO module [4] Insert KNI module [5] Setup hugepage mappings for non-NUMA systems [6] Setup hugepage mappings for NUMA systems [7] Display current Ethernet/Baseband/Crypto device settings [8] Bind Ethernet/Baseband/Crypto device to IGB UIO module [9] Bind Ethernet/Baseband/Crypto device to VFIO module [10] Setup VFIO permissions ---------------------------------------------------------- Step 3: Run test application for linux environment ---------------------------------------------------------- [11] Run test application ($RTE_TARGET/app/test) [12] Run testpmd application in interactive mode ($RTE_TARGET/app/testpmd) ---------------------------------------------------------- Step 4: Other tools ---------------------------------------------------------- [13] List hugepage info from /proc/meminfo ---------------------------------------------------------- Step 5: Uninstall and system cleanup ---------------------------------------------------------- [14] Unbind devices from IGB UIO or VFIO driver [15] Remove IGB UIO module [16] Remove VFIO module [17] Remove KNI module [18] Remove hugepage mappings [19] Exit Script Next run option 6 to instert huge pages for NUMA systems. Notice you will be prompted to select an amount of memory on a per processor basis. This is because there are pages associated with each individual processor to increase performance via locality. Run option 7 and make sure you receive output and that network devices are listed. My output looks like this: Network devices using kernel driver =================================== 0000:0b:00.0 'VMXNET3 Ethernet Controller 07b0' if=ens192 drv=vmxnet3 unused=vfio-pci *Active* 0000:13:00.0 'VMXNET3 Ethernet Controller 07b0' if=ens224 drv=vmxnet3 unused=vfio-pci *Active* No 'Baseband' devices detected ============================== No 'Crypto' devices detected ============================ No 'Eventdev' devices detected ============================== No 'Mempool' devices detected ============================= No 'Compress' devices detected ============================== No 'Misc (rawdev)' devices detected =================================== NOTE : The active keyword means that DPDK thinks the interface is under active use. This means that the interface has routes installed in the routing table. Run option 8 to bind an interface to DPDK using the IGB UIO driver.","title":"Configure Ports"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20FC640%20with%20RHEL%208/#performing-packet-capture","text":"","title":"Performing Packet Capture"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20FC640%20with%20RHEL%208/#initial-setup","text":"Get the core layout with ./install/share/dpdk/usertools/cpu_layout.py View your port layout with ./install/share/dpdk/usertools/dpdk-devbind.py -s If you haven't already load the right kernel modules with: modprobe uio && insmod /opt/dpdk-19.08/install/lib/modules/4.18.0-147.el8.x86_64/extra/dpdk/igb_uio.ko","title":"Initial Setup"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20FC640%20with%20RHEL%208/#starting-testpmd","text":"NOTE : The -- separates the argumentsn for the EAL vs TestPMD. ./install/bin/testpmd -l 4,8,10,12 -n 4 -- -i --forward-mode=rxonly After testpmd has started don't forget to run the start command on the testpmd command line.","title":"Starting testpmd"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20FC640%20with%20RHEL%208/#pdump","text":"./install/bin/dpdk-pdump -- --pdump 'port=0,queue=*,rx-dev=/tmp/capture.pcap'","title":"pdump"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20FC640%20with%20RHEL%208/#helpful-tips","text":"","title":"Helpful Tips"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20FC640%20with%20RHEL%208/#getting-cpu-info","text":"DPDK provides a tool for seeing the CPU layout with ./install/share/dpdk/usertools/cpu_layout.py You can see the logical layout of the cores with cat /proc/cpuinfo You can alse run lstopo-no-graphics Notice that the cores alternate back and forth.","title":"Getting CPU Info"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20FC640%20with%20RHEL%208/#process-types-in-dpdk","text":"DPDK runs two different types of processes. There are as follows: primary processes, which can initialize and which have full permissions on shared memory secondary processes, which cannot initialize shared memory, but can attach to pre- initialized shared memory and create objects in it. Standalone DPDK processes are primary processes, while secondary processes can only run alongside a primary process or after a primary process has already configured the hugepage shared memory for them.","title":"Process Types in DPDK"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20FC640%20with%20RHEL%208/#testpmd","text":"The test pmd manual is available here","title":"TestPMD"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20FC640%20with%20RHEL%208/#interactive-commands","text":"","title":"Interactive Commands"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20FC640%20with%20RHEL%208/#starting-transmit","text":"start","title":"Starting transmit"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20FC640%20with%20RHEL%208/#get-port-info","text":"show port info all","title":"Get Port Info"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20FC640%20with%20RHEL%208/#forwarding-modes","text":"TestPMD has different forwarding modes that can be used within the application. Input/output mode: This mode is generally referred to as IO mode. It is the most common forwarding mode and is the default mode when TestPMD is started. In IO mode a CPU core receives packets from one port (Rx) and transmits them to another port (Tx). The same port can be used for reception and transmission if required. Rx-only mode: In this mode the application polls packets from the Rx ports and frees them without transmitting them. In this way it acts as a packet sink. Tx-only mode: In this mode the application generates 64-byte IP packets and transmits them from the Tx ports. It doesn\u2019t handle the reception of packets and as such acts as a packet source. These latter two modes (Rx-only and Tx-only) are useful for checking packet reception and transmission separately. Apart from these three modes there are other forwarding modes that are explained in the TestPMD documentation .","title":"Forwarding Modes"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20FC640%20with%20RHEL%208/#port-topology-modes","text":"In paired mode, the forwarding is between pairs of ports, for example: (0,1), (2,3), (4,5). In chained mode, the forwarding is to the next available port in the port mask, for example: (0,1), (1,2), (2,0). The ordering of the ports can be changed using the portlist testpmd runtime function. In loop mode, ingress traffic is simply transmitted back on the same interface.","title":"Port Topology Modes"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20FC640%20with%20RHEL%208/#receive-side-scaling","text":"=Receive-Side Scaling (RSS), also known as multi-queue receive, distributes network receive processing across several hardware-based receive queues, allowing inbound network traffic to be processed by multiple CPUs. RSS can be used to relieve bottlenecks in receive interrupt processing caused by overloading a single CPU, and to reduce network latency. To determine whether your network interface card supports RSS, check whether multiple interrupt request queues are associated with the interface in /proc/interrupts. For example, if you are interested in the p1p1 interface: # egrep 'CPU|p1p1' /proc/interrupts CPU0 CPU1 CPU2 CPU3 CPU4 CPU5 89: 40187 0 0 0 0 0 IR-PCI-MSI-edge p1p1-0 90: 0 790 0 0 0 0 IR-PCI-MSI-edge p1p1-1 91: 0 0 959 0 0 0 IR-PCI-MSI-edge p1p1-2 92: 0 0 0 3310 0 0 IR-PCI-MSI-edge p1p1-3 93: 0 0 0 0 622 0 IR-PCI-MSI-edge p1p1-4 94: 0 0 0 0 0 2475 IR-PCI-MSI-edge p1p1-5 huge pages https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/performance_tuning_guide/sect-red_hat_enterprise_linux-performance_tuning_guide-memory-configuring-huge-pages","title":"Receive Side Scaling"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20R940%20with%20Napatech/","text":"How to Get DPDK with pdump Running The purpose of this experiment is to get Intel's DPDK framework up and running on a server. Useful Materials How to Compile DPDK Info on Linux Drivers for DPDK Description of the TestPMD Program My Environment I am running a Dell R940 with Napatech Card NT200A02 CentOS Release Info NAME=\"CentOS Linux\" VERSION=\"7 (Core)\" ID=\"centos\" ID_LIKE=\"rhel fedora\" VERSION_ID=\"7\" PRETTY_NAME=\"CentOS Linux 7 (Core)\" ANSI_COLOR=\"0;31\" CPE_NAME=\"cpe:/o:centos:centos:7\" HOME_URL=\"https://www.centos.org/\" BUG_REPORT_URL=\"https://bugs.centos.org/\" CENTOS_MANTISBT_PROJECT=\"CentOS-7\" CENTOS_MANTISBT_PROJECT_VERSION=\"7\" REDHAT_SUPPORT_PRODUCT=\"centos\" REDHAT_SUPPORT_PRODUCT_VERSION=\"7\" CentOS Linux release 7.7.1908 (Core) CentOS Linux release 7.7.1908 (Core) Kernel Info Linux r940.lan 3.10.0-1062.4.3.el7.x86_64 #1 SMP Wed Nov 13 23:58:53 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux Physical Setup Installation NapaTech Driver Install Driver Download the Napatech software from here Run yum groupinstall \"Development Tools\" && yum install -y kernel-devel gettext-devel openssl-devel perl-CPAN perl-devel zlib-devel pciutils && yum install -y https://centos7.iuscommunity.org/ius-release.rpm && yum remove -y git && yum install -y git2u-all Unzip and run package_install_3gd.sh Install DPDK export NAPATECH3_PATH=/opt/napatech3 Download with git clone https://github.com/napatech/dpdk.git Edit the security limits with vim /etc/security/limits.conf . After making the below edits you will need to log out and log back in for them to take effect. Add the following lines at the end of the file. This assumes you are running as root: root hard memlock unlimited root soft memlock unlimited Run yum install -y gcc numactl-devel kernel-devel pciutils elfutils-libelf-devel make libpcap python3 tar vim wget tmux vim mlocate hwloc libpcap-devel python36-devel Extract dpdk and cd into its directory set the environment variable RTE_SDK. It is the directory in which you extracted all the DPDK files. export RTE_SDK=<YOUR_DIR> Run make config T=x86_64-native-linuxapp-gcc install CONFIG_RTE_LIBRTE_PMD_PCAP=y CONFIG_RTE_LIBRTE_PDUMP=y DESTDIR=install CONFIG_RTE_LIBRTE_PMD_NTACC=y to build dpdk. Ensure your install directory exists. make -j NOTE : The option CONFIG_RTE_LIBRTE_PMD_PCAP=y enabled libpcap support in DPDK. This is required for pdump to work. Once an DPDK target environment directory has been created (such as x86_64-native-linux-gcc), it contains all libraries and header files required to build an application. When compiling an application in the Linux* environment on the DPDK, the following variables must be exported: RTE_TARGET - Points to the DPDK target environment directory. export RTE_TARGET=/opt/dpdk-19.08/x86_64-native-linux-gcc You may want to add this variable and RTE_SDK to ~/.bash_profile Update Your Bash Profile Add: export RTE_SDK=/opt/dpdk export NAPATECH3_PATH=/opt/napatech3 export RTE_TARGET=/opt/dpdk/x86_64-native-linuxapp-gcc to ~/.bash_profile Install ELF Tools Run the following: pip3 install numpy pip3 install elftools pip3 install pyelftools Configuration Configure Ports Move to your dpdk dir and run ./install/share/dpdk/usertools/dpdk-setup.sh . This should give you a menu with all available DPDK options. The menu is setup in such a way that you must perform each step listed in the menu. If things have gone correctly to this point your Step 1 should look like the following: ---------------------------------------------------------- Step 1: Select the DPDK environment to build ---------------------------------------------------------- [1] * My menu looks like this: ---------------------------------------------------------- Step 1: Select the DPDK environment to build ---------------------------------------------------------- [1] * ---------------------------------------------------------- Step 2: Setup linux environment ---------------------------------------------------------- [2] Insert IGB UIO module [3] Insert VFIO module [4] Insert KNI module [5] Setup hugepage mappings for non-NUMA systems [6] Setup hugepage mappings for NUMA systems [7] Display current Ethernet/Baseband/Crypto device settings [8] Bind Ethernet/Baseband/Crypto device to IGB UIO module [9] Bind Ethernet/Baseband/Crypto device to VFIO module [10] Setup VFIO permissions ---------------------------------------------------------- Step 3: Run test application for linux environment ---------------------------------------------------------- [11] Run test application ($RTE_TARGET/app/test) [12] Run testpmd application in interactive mode ($RTE_TARGET/app/testpmd) ---------------------------------------------------------- Step 4: Other tools ---------------------------------------------------------- [13] List hugepage info from /proc/meminfo ---------------------------------------------------------- Step 5: Uninstall and system cleanup ---------------------------------------------------------- [14] Unbind devices from IGB UIO or VFIO driver [15] Remove IGB UIO module [16] Remove VFIO module [17] Remove KNI module [18] Remove hugepage mappings [19] Exit Script Next run option 6 to instert huge pages for NUMA systems. Notice you will be prompted to select an amount of memory on a per processor basis. This is because there are pages associated with each individual processor to increase performance via locality. I used a value of 4096 for each NUMA node. Performing Packet Capture Setup the NapaTech Card Initial Setup Get the core layout with ./install/share/dpdk/usertools/cpu_layout.py View your port layout with ./install/share/dpdk/usertools/dpdk-devbind.py -s Starting testpmd NOTE : The -- separates the argumentsn for the EAL vs TestPMD. ./install/bin/testpmd -l 0,4,8,12,16,20,24,28,32,36 -n 4 -- -i After testpmd has started don't forget to run the start command on the testpmd command line. pdump ./install/bin/dpdk-pdump -- --pdump 'device_id=0000:5b:00.0,queue=*,rx-dev=/tmp/capture.pcap' Helpful Tips NapaTech Detecting Installed Cards /opt/napatech3/bin/imgctrl -q Load the Driver /opt/napatech3/bin/ntload.sh Run the ntserver /opt/napatech3/bin/ntstart.sh Show Interface Info /opt/napatech3/bin/adapterinfo Getting CPU Info DPDK provides a tool for seeing the CPU layout with ./install/share/dpdk/usertools/cpu_layout.py You can see the logical layout of the cores with cat /proc/cpuinfo You can alse run lstopo-no-graphics Notice that the cores alternate back and forth. Process Types in DPDK DPDK runs two different types of processes. There are as follows: primary processes, which can initialize and which have full permissions on shared memory secondary processes, which cannot initialize shared memory, but can attach to pre- initialized shared memory and create objects in it. Standalone DPDK processes are primary processes, while secondary processes can only run alongside a primary process or after a primary process has already configured the hugepage shared memory for them. TestPMD The test pmd manual is available here Interactive Commands Starting transmit start Get Port Info show port info all Forwarding Modes TestPMD has different forwarding modes that can be used within the application. Input/output mode: This mode is generally referred to as IO mode. It is the most common forwarding mode and is the default mode when TestPMD is started. In IO mode a CPU core receives packets from one port (Rx) and transmits them to another port (Tx). The same port can be used for reception and transmission if required. Rx-only mode: In this mode the application polls packets from the Rx ports and frees them without transmitting them. In this way it acts as a packet sink. Tx-only mode: In this mode the application generates 64-byte IP packets and transmits them from the Tx ports. It doesn\u2019t handle the reception of packets and as such acts as a packet source. These latter two modes (Rx-only and Tx-only) are useful for checking packet reception and transmission separately. Apart from these three modes there are other forwarding modes that are explained in the TestPMD documentation . Port Topology Modes In paired mode, the forwarding is between pairs of ports, for example: (0,1), (2,3), (4,5). In chained mode, the forwarding is to the next available port in the port mask, for example: (0,1), (1,2), (2,0). The ordering of the ports can be changed using the portlist testpmd runtime function. In loop mode, ingress traffic is simply transmitted back on the same interface. Receive Side Scaling =Receive-Side Scaling (RSS), also known as multi-queue receive, distributes network receive processing across several hardware-based receive queues, allowing inbound network traffic to be processed by multiple CPUs. RSS can be used to relieve bottlenecks in receive interrupt processing caused by overloading a single CPU, and to reduce network latency. To determine whether your network interface card supports RSS, check whether multiple interrupt request queues are associated with the interface in /proc/interrupts. For example, if you are interested in the p1p1 interface: # egrep 'CPU|p1p1' /proc/interrupts CPU0 CPU1 CPU2 CPU3 CPU4 CPU5 89: 40187 0 0 0 0 0 IR-PCI-MSI-edge p1p1-0 90: 0 790 0 0 0 0 IR-PCI-MSI-edge p1p1-1 91: 0 0 959 0 0 0 IR-PCI-MSI-edge p1p1-2 92: 0 0 0 3310 0 0 IR-PCI-MSI-edge p1p1-3 93: 0 0 0 0 622 0 IR-PCI-MSI-edge p1p1-4 94: 0 0 0 0 0 2475 IR-PCI-MSI-edge p1p1-5 huge pages https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/performance_tuning_guide/sect-red_hat_enterprise_linux-performance_tuning_guide-memory-configuring-huge-pages","title":"How to Get DPDK with pdump Running"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20R940%20with%20Napatech/#how-to-get-dpdk-with-pdump-running","text":"The purpose of this experiment is to get Intel's DPDK framework up and running on a server.","title":"How to Get DPDK with pdump Running"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20R940%20with%20Napatech/#useful-materials","text":"How to Compile DPDK Info on Linux Drivers for DPDK Description of the TestPMD Program","title":"Useful Materials"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20R940%20with%20Napatech/#my-environment","text":"I am running a Dell R940 with Napatech Card NT200A02","title":"My Environment"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20R940%20with%20Napatech/#centos-release-info","text":"NAME=\"CentOS Linux\" VERSION=\"7 (Core)\" ID=\"centos\" ID_LIKE=\"rhel fedora\" VERSION_ID=\"7\" PRETTY_NAME=\"CentOS Linux 7 (Core)\" ANSI_COLOR=\"0;31\" CPE_NAME=\"cpe:/o:centos:centos:7\" HOME_URL=\"https://www.centos.org/\" BUG_REPORT_URL=\"https://bugs.centos.org/\" CENTOS_MANTISBT_PROJECT=\"CentOS-7\" CENTOS_MANTISBT_PROJECT_VERSION=\"7\" REDHAT_SUPPORT_PRODUCT=\"centos\" REDHAT_SUPPORT_PRODUCT_VERSION=\"7\" CentOS Linux release 7.7.1908 (Core) CentOS Linux release 7.7.1908 (Core)","title":"CentOS Release Info"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20R940%20with%20Napatech/#kernel-info","text":"Linux r940.lan 3.10.0-1062.4.3.el7.x86_64 #1 SMP Wed Nov 13 23:58:53 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux","title":"Kernel Info"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20R940%20with%20Napatech/#physical-setup","text":"","title":"Physical Setup"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20R940%20with%20Napatech/#installation-napatech-driver","text":"","title":"Installation NapaTech Driver"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20R940%20with%20Napatech/#install-driver","text":"Download the Napatech software from here Run yum groupinstall \"Development Tools\" && yum install -y kernel-devel gettext-devel openssl-devel perl-CPAN perl-devel zlib-devel pciutils && yum install -y https://centos7.iuscommunity.org/ius-release.rpm && yum remove -y git && yum install -y git2u-all Unzip and run package_install_3gd.sh","title":"Install Driver"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20R940%20with%20Napatech/#install-dpdk","text":"export NAPATECH3_PATH=/opt/napatech3 Download with git clone https://github.com/napatech/dpdk.git Edit the security limits with vim /etc/security/limits.conf . After making the below edits you will need to log out and log back in for them to take effect. Add the following lines at the end of the file. This assumes you are running as root: root hard memlock unlimited root soft memlock unlimited Run yum install -y gcc numactl-devel kernel-devel pciutils elfutils-libelf-devel make libpcap python3 tar vim wget tmux vim mlocate hwloc libpcap-devel python36-devel Extract dpdk and cd into its directory set the environment variable RTE_SDK. It is the directory in which you extracted all the DPDK files. export RTE_SDK=<YOUR_DIR> Run make config T=x86_64-native-linuxapp-gcc install CONFIG_RTE_LIBRTE_PMD_PCAP=y CONFIG_RTE_LIBRTE_PDUMP=y DESTDIR=install CONFIG_RTE_LIBRTE_PMD_NTACC=y to build dpdk. Ensure your install directory exists. make -j NOTE : The option CONFIG_RTE_LIBRTE_PMD_PCAP=y enabled libpcap support in DPDK. This is required for pdump to work. Once an DPDK target environment directory has been created (such as x86_64-native-linux-gcc), it contains all libraries and header files required to build an application. When compiling an application in the Linux* environment on the DPDK, the following variables must be exported: RTE_TARGET - Points to the DPDK target environment directory. export RTE_TARGET=/opt/dpdk-19.08/x86_64-native-linux-gcc You may want to add this variable and RTE_SDK to ~/.bash_profile","title":"Install DPDK"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20R940%20with%20Napatech/#update-your-bash-profile","text":"Add: export RTE_SDK=/opt/dpdk export NAPATECH3_PATH=/opt/napatech3 export RTE_TARGET=/opt/dpdk/x86_64-native-linuxapp-gcc to ~/.bash_profile","title":"Update Your Bash Profile"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20R940%20with%20Napatech/#install-elf-tools","text":"Run the following: pip3 install numpy pip3 install elftools pip3 install pyelftools","title":"Install ELF Tools"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20R940%20with%20Napatech/#configuration","text":"","title":"Configuration"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20R940%20with%20Napatech/#configure-ports","text":"Move to your dpdk dir and run ./install/share/dpdk/usertools/dpdk-setup.sh . This should give you a menu with all available DPDK options. The menu is setup in such a way that you must perform each step listed in the menu. If things have gone correctly to this point your Step 1 should look like the following: ---------------------------------------------------------- Step 1: Select the DPDK environment to build ---------------------------------------------------------- [1] * My menu looks like this: ---------------------------------------------------------- Step 1: Select the DPDK environment to build ---------------------------------------------------------- [1] * ---------------------------------------------------------- Step 2: Setup linux environment ---------------------------------------------------------- [2] Insert IGB UIO module [3] Insert VFIO module [4] Insert KNI module [5] Setup hugepage mappings for non-NUMA systems [6] Setup hugepage mappings for NUMA systems [7] Display current Ethernet/Baseband/Crypto device settings [8] Bind Ethernet/Baseband/Crypto device to IGB UIO module [9] Bind Ethernet/Baseband/Crypto device to VFIO module [10] Setup VFIO permissions ---------------------------------------------------------- Step 3: Run test application for linux environment ---------------------------------------------------------- [11] Run test application ($RTE_TARGET/app/test) [12] Run testpmd application in interactive mode ($RTE_TARGET/app/testpmd) ---------------------------------------------------------- Step 4: Other tools ---------------------------------------------------------- [13] List hugepage info from /proc/meminfo ---------------------------------------------------------- Step 5: Uninstall and system cleanup ---------------------------------------------------------- [14] Unbind devices from IGB UIO or VFIO driver [15] Remove IGB UIO module [16] Remove VFIO module [17] Remove KNI module [18] Remove hugepage mappings [19] Exit Script Next run option 6 to instert huge pages for NUMA systems. Notice you will be prompted to select an amount of memory on a per processor basis. This is because there are pages associated with each individual processor to increase performance via locality. I used a value of 4096 for each NUMA node.","title":"Configure Ports"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20R940%20with%20Napatech/#performing-packet-capture","text":"","title":"Performing Packet Capture"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20R940%20with%20Napatech/#setup-the-napatech-card","text":"","title":"Setup the NapaTech Card"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20R940%20with%20Napatech/#initial-setup","text":"Get the core layout with ./install/share/dpdk/usertools/cpu_layout.py View your port layout with ./install/share/dpdk/usertools/dpdk-devbind.py -s","title":"Initial Setup"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20R940%20with%20Napatech/#starting-testpmd","text":"NOTE : The -- separates the argumentsn for the EAL vs TestPMD. ./install/bin/testpmd -l 0,4,8,12,16,20,24,28,32,36 -n 4 -- -i After testpmd has started don't forget to run the start command on the testpmd command line.","title":"Starting testpmd"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20R940%20with%20Napatech/#pdump","text":"./install/bin/dpdk-pdump -- --pdump 'device_id=0000:5b:00.0,queue=*,rx-dev=/tmp/capture.pcap'","title":"pdump"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20R940%20with%20Napatech/#helpful-tips","text":"","title":"Helpful Tips"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20R940%20with%20Napatech/#napatech","text":"","title":"NapaTech"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20R940%20with%20Napatech/#detecting-installed-cards","text":"/opt/napatech3/bin/imgctrl -q","title":"Detecting Installed Cards"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20R940%20with%20Napatech/#load-the-driver","text":"/opt/napatech3/bin/ntload.sh","title":"Load the Driver"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20R940%20with%20Napatech/#run-the-ntserver","text":"/opt/napatech3/bin/ntstart.sh","title":"Run the ntserver"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20R940%20with%20Napatech/#show-interface-info","text":"/opt/napatech3/bin/adapterinfo","title":"Show Interface Info"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20R940%20with%20Napatech/#getting-cpu-info","text":"DPDK provides a tool for seeing the CPU layout with ./install/share/dpdk/usertools/cpu_layout.py You can see the logical layout of the cores with cat /proc/cpuinfo You can alse run lstopo-no-graphics Notice that the cores alternate back and forth.","title":"Getting CPU Info"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20R940%20with%20Napatech/#process-types-in-dpdk","text":"DPDK runs two different types of processes. There are as follows: primary processes, which can initialize and which have full permissions on shared memory secondary processes, which cannot initialize shared memory, but can attach to pre- initialized shared memory and create objects in it. Standalone DPDK processes are primary processes, while secondary processes can only run alongside a primary process or after a primary process has already configured the hugepage shared memory for them.","title":"Process Types in DPDK"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20R940%20with%20Napatech/#testpmd","text":"The test pmd manual is available here","title":"TestPMD"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20R940%20with%20Napatech/#interactive-commands","text":"","title":"Interactive Commands"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20R940%20with%20Napatech/#starting-transmit","text":"start","title":"Starting transmit"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20R940%20with%20Napatech/#get-port-info","text":"show port info all","title":"Get Port Info"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20R940%20with%20Napatech/#forwarding-modes","text":"TestPMD has different forwarding modes that can be used within the application. Input/output mode: This mode is generally referred to as IO mode. It is the most common forwarding mode and is the default mode when TestPMD is started. In IO mode a CPU core receives packets from one port (Rx) and transmits them to another port (Tx). The same port can be used for reception and transmission if required. Rx-only mode: In this mode the application polls packets from the Rx ports and frees them without transmitting them. In this way it acts as a packet sink. Tx-only mode: In this mode the application generates 64-byte IP packets and transmits them from the Tx ports. It doesn\u2019t handle the reception of packets and as such acts as a packet source. These latter two modes (Rx-only and Tx-only) are useful for checking packet reception and transmission separately. Apart from these three modes there are other forwarding modes that are explained in the TestPMD documentation .","title":"Forwarding Modes"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20R940%20with%20Napatech/#port-topology-modes","text":"In paired mode, the forwarding is between pairs of ports, for example: (0,1), (2,3), (4,5). In chained mode, the forwarding is to the next available port in the port mask, for example: (0,1), (1,2), (2,0). The ordering of the ports can be changed using the portlist testpmd runtime function. In loop mode, ingress traffic is simply transmitted back on the same interface.","title":"Port Topology Modes"},{"location":"High%20Speed%20Packet%20Capture/DPDK%20on%20R940%20with%20Napatech/#receive-side-scaling","text":"=Receive-Side Scaling (RSS), also known as multi-queue receive, distributes network receive processing across several hardware-based receive queues, allowing inbound network traffic to be processed by multiple CPUs. RSS can be used to relieve bottlenecks in receive interrupt processing caused by overloading a single CPU, and to reduce network latency. To determine whether your network interface card supports RSS, check whether multiple interrupt request queues are associated with the interface in /proc/interrupts. For example, if you are interested in the p1p1 interface: # egrep 'CPU|p1p1' /proc/interrupts CPU0 CPU1 CPU2 CPU3 CPU4 CPU5 89: 40187 0 0 0 0 0 IR-PCI-MSI-edge p1p1-0 90: 0 790 0 0 0 0 IR-PCI-MSI-edge p1p1-1 91: 0 0 959 0 0 0 IR-PCI-MSI-edge p1p1-2 92: 0 0 0 3310 0 0 IR-PCI-MSI-edge p1p1-3 93: 0 0 0 0 622 0 IR-PCI-MSI-edge p1p1-4 94: 0 0 0 0 0 2475 IR-PCI-MSI-edge p1p1-5 huge pages https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/performance_tuning_guide/sect-red_hat_enterprise_linux-performance_tuning_guide-memory-configuring-huge-pages","title":"Receive Side Scaling"},{"location":"High%20Speed%20Packet%20Capture/ntop/","text":"Helpful Materials Drill Down Deeper: Using ntopng to Zoom In, Filter Out and Go Straight to the Packets Traffic Recording Manual Configuration Hardware Tracewell TFX2HE with 1 passthrough module, 1 switch module. Operating System Version CentOS Linux release 7.7.1908 (Core) NAME=\"CentOS Linux\" VERSION=\"7 (Core)\" ID=\"centos\" ID_LIKE=\"rhel fedora\" VERSION_ID=\"7\" PRETTY_NAME=\"CentOS Linux 7 (Core)\" ANSI_COLOR=\"0;31\" CPE_NAME=\"cpe:/o:centos:centos:7\" HOME_URL=\"https://www.centos.org/\" BUG_REPORT_URL=\"https://bugs.centos.org/\" CENTOS_MANTISBT_PROJECT=\"CentOS-7\" CENTOS_MANTISBT_PROJECT_VERSION=\"7\" REDHAT_SUPPORT_PRODUCT=\"centos\" REDHAT_SUPPORT_PRODUCT_VERSION=\"7\" CentOS Linux release 7.7.1908 (Core) CentOS Linux release 7.7.1908 (Core) Kernel Version Linux ntopdemo.lan 3.10.0-1062.4.1.el7.x86_64 #1 SMP Fri Oct 18 17:15:30 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux Install n2disk and ntop Perform Installation Install epel with yum install -y epel-release Erase the zeromq3 package with yum erase zeromq && yum clean all && yum update -y && reboot Pull ntop repo with wget http://packages.ntop.org/centos-stable/ntop.repo -O /etc/yum.repos.d/ntop.repo Install required packages with yum install pfring-dkms pfring n2disk nprobe ntopng ntopng-data cento pfring-drivers-zc-dkms redis hiredis-devel Perform Configuration Zero Copy Driver List interfaces with pf_ringcfg --list-interfaces Configure the driver with pf_ringcfg --configure-driver i40e Set promiscuous mode on the interface in question with /sbin/ip link set em1 promisc on Edit the pfring configuration file with vim /etc/pf_ring/interfaces.conf and add your configuration. MANAGEMENT_INTERFACES=\"<YOUR_MANAGEMENT_INTERFACE>\" CAPTURE_INTERFACES=\"<YOUR_CAPTURE_INTERFACE>\" Open the file etc/ntopng/ntopng.conf . If you do not have a license add --community to the end Configure the firewall to accept connections to ntopng with: firewall-cmd --zone=public --permanent --add-port=3000/tcp && firewall-cmd --reload Enable and start services with: systemctl enable redis.service systemctl enable ntopng.service systemctl enable pf_ring systemctl start redis.service systemctl start ntopng.service systemctl start pf_ring Make sure the services are running correctly with: systemctl status redis.service systemctl status ntopng.service systemctl status pf_ring Performing Filtering","title":"Helpful Materials"},{"location":"High%20Speed%20Packet%20Capture/ntop/#helpful-materials","text":"Drill Down Deeper: Using ntopng to Zoom In, Filter Out and Go Straight to the Packets Traffic Recording Manual","title":"Helpful Materials"},{"location":"High%20Speed%20Packet%20Capture/ntop/#configuration","text":"","title":"Configuration"},{"location":"High%20Speed%20Packet%20Capture/ntop/#hardware","text":"Tracewell TFX2HE with 1 passthrough module, 1 switch module.","title":"Hardware"},{"location":"High%20Speed%20Packet%20Capture/ntop/#operating-system-version","text":"CentOS Linux release 7.7.1908 (Core) NAME=\"CentOS Linux\" VERSION=\"7 (Core)\" ID=\"centos\" ID_LIKE=\"rhel fedora\" VERSION_ID=\"7\" PRETTY_NAME=\"CentOS Linux 7 (Core)\" ANSI_COLOR=\"0;31\" CPE_NAME=\"cpe:/o:centos:centos:7\" HOME_URL=\"https://www.centos.org/\" BUG_REPORT_URL=\"https://bugs.centos.org/\" CENTOS_MANTISBT_PROJECT=\"CentOS-7\" CENTOS_MANTISBT_PROJECT_VERSION=\"7\" REDHAT_SUPPORT_PRODUCT=\"centos\" REDHAT_SUPPORT_PRODUCT_VERSION=\"7\" CentOS Linux release 7.7.1908 (Core) CentOS Linux release 7.7.1908 (Core)","title":"Operating System Version"},{"location":"High%20Speed%20Packet%20Capture/ntop/#kernel-version","text":"Linux ntopdemo.lan 3.10.0-1062.4.1.el7.x86_64 #1 SMP Fri Oct 18 17:15:30 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux","title":"Kernel Version"},{"location":"High%20Speed%20Packet%20Capture/ntop/#install-n2disk-and-ntop","text":"","title":"Install n2disk and ntop"},{"location":"High%20Speed%20Packet%20Capture/ntop/#perform-installation","text":"Install epel with yum install -y epel-release Erase the zeromq3 package with yum erase zeromq && yum clean all && yum update -y && reboot Pull ntop repo with wget http://packages.ntop.org/centos-stable/ntop.repo -O /etc/yum.repos.d/ntop.repo Install required packages with yum install pfring-dkms pfring n2disk nprobe ntopng ntopng-data cento pfring-drivers-zc-dkms redis hiredis-devel","title":"Perform Installation"},{"location":"High%20Speed%20Packet%20Capture/ntop/#perform-configuration-zero-copy-driver","text":"List interfaces with pf_ringcfg --list-interfaces Configure the driver with pf_ringcfg --configure-driver i40e Set promiscuous mode on the interface in question with /sbin/ip link set em1 promisc on Edit the pfring configuration file with vim /etc/pf_ring/interfaces.conf and add your configuration. MANAGEMENT_INTERFACES=\"<YOUR_MANAGEMENT_INTERFACE>\" CAPTURE_INTERFACES=\"<YOUR_CAPTURE_INTERFACE>\" Open the file etc/ntopng/ntopng.conf . If you do not have a license add --community to the end Configure the firewall to accept connections to ntopng with: firewall-cmd --zone=public --permanent --add-port=3000/tcp && firewall-cmd --reload Enable and start services with: systemctl enable redis.service systemctl enable ntopng.service systemctl enable pf_ring systemctl start redis.service systemctl start ntopng.service systemctl start pf_ring Make sure the services are running correctly with: systemctl status redis.service systemctl status ntopng.service systemctl status pf_ring","title":"Perform Configuration Zero Copy Driver"},{"location":"High%20Speed%20Packet%20Capture/ntop/#performing-filtering","text":"","title":"Performing Filtering"},{"location":"High%20Speed%20Packet%20Capture/ntop%20on%20R940%20with%20Napatech/","text":"Useful Materials NapaTech Installation Instructions (Creatinga EXT4 Filesystem)[https://thelastmaimou.wordpress.com/2013/05/04/magic-soup-ext4-with-ssd-stripes-and-strides/] My Environment I am running a Dell FC640 on a Dell R940 See for hardware details. Hard Drive Layout: I had a RAID of 12 SAS SSDs in RAID0 on the PERC740. I had 7 NVMe drives I used. I couldn't get the 8th NVMe drive working. [root@r940 /]# lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT sda 8:0 0 10.5T 0 disk /raiddata sdb 8:16 0 223.5G 0 disk \u251c\u2500sdb1 8:17 0 200M 0 part /boot/efi \u251c\u2500sdb2 8:18 0 1G 0 part /boot \u2514\u2500sdb3 8:19 0 222.3G 0 part \u251c\u2500centos-root 253:0 0 218.3G 0 lvm / \u2514\u2500centos-swap 253:1 0 4G 0 lvm [SWAP] sdc 8:32 0 59.8G 0 disk nvme0n1 259:6 0 1.5T 0 disk \u2514\u2500nvme0n1p1 259:8 0 1.5T 0 part \u2514\u2500data-data 253:2 0 10.2T 0 lvm /data nvme1n1 259:0 0 1.5T 0 disk \u2514\u2500nvme1n1p1 259:1 0 1.5T 0 part \u2514\u2500data-data 253:2 0 10.2T 0 lvm /data nvme2n1 259:2 0 1.5T 0 disk \u2514\u2500nvme2n1p1 259:3 0 1.5T 0 part \u2514\u2500data-data 253:2 0 10.2T 0 lvm /data nvme3n1 259:4 0 1.5T 0 disk \u2514\u2500nvme3n1p1 259:5 0 1.5T 0 part \u2514\u2500data-data 253:2 0 10.2T 0 lvm /data nvme4n1 259:10 0 1.5T 0 disk \u2514\u2500nvme4n1p1 259:13 0 1.5T 0 part \u2514\u2500data-data 253:2 0 10.2T 0 lvm /data nvme5n1 259:11 0 1.5T 0 disk nvme6n1 259:7 0 1.5T 0 disk \u2514\u2500nvme6n1p1 259:9 0 1.5T 0 part \u2514\u2500data-data 253:2 0 10.2T 0 lvm /data nvme7n1 259:12 0 1.5T 0 disk \u2514\u2500nvme7n1p1 259:14 0 1.5T 0 part \u2514\u2500data-data 253:2 0 10.2T 0 lvm /data [root@r940 /]# pvs PV VG Fmt Attr PSize PFree /dev/nvme0n1p1 data lvm2 a-- <1.46t 0 /dev/nvme1n1p1 data lvm2 a-- <1.46t 0 /dev/nvme2n1p1 data lvm2 a-- <1.46t 0 /dev/nvme3n1p1 data lvm2 a-- <1.46t 0 /dev/nvme4n1p1 data lvm2 a-- <1.46t 0 /dev/nvme6n1p1 data lvm2 a-- <1.46t 0 /dev/nvme7n1p1 data lvm2 a-- <1.46t 0 /dev/sdb3 centos lvm2 a-- <222.31g 0 [root@r940 /]# vgs VG #PV #LV #SN Attr VSize VFree centos 1 2 0 wz--n- <222.31g 0 data 7 1 0 wz--n- <10.19t 0 [root@r940 /]# lvs LV VG Attr LSize Pool Origin Data% Meta% Move Log Cpy%Sync Convert root centos -wi-ao---- <218.31g swap centos -wi-ao---- 4.00g data data -wi-ao---- <10.19t WARNING: fdisk GPT support is currently new, and therefore in an experimental phase. Use at your own discretion. Disk /dev/nvme1n1: 1600.3 GB, 1600321314816 bytes, 3125627568 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk label type: gpt Disk identifier: 759A1CF7-125F-469B-981E-149EBDBE3456 # Start End Size Type Name 1 2048 3125626879 1.5T Linux LVM WARNING: fdisk GPT support is currently new, and therefore in an experimental phase. Use at your own discretion. Disk /dev/nvme2n1: 1600.3 GB, 1600321314816 bytes, 3125627568 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk label type: gpt Disk identifier: 57898F4D-5B5E-4495-B695-E48EA3FCFA01 # Start End Size Type Name 1 2048 3125626879 1.5T Linux LVM WARNING: fdisk GPT support is currently new, and therefore in an experimental phase. Use at your own discretion. Disk /dev/nvme3n1: 1600.3 GB, 1600321314816 bytes, 3125627568 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk label type: gpt Disk identifier: DB46E8E3-5E96-4228-A148-E6C8F0187DF4 # Start End Size Type Name 1 2048 3125626879 1.5T Linux LVM WARNING: fdisk GPT support is currently new, and therefore in an experimental phase. Use at your own discretion. Disk /dev/nvme0n1: 1600.3 GB, 1600321314816 bytes, 3125627568 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk label type: gpt Disk identifier: 4066CDE1-E36E-41FB-8277-5E3FFDB55B4A # Start End Size Type Name 1 2048 3125626879 1.5T Linux LVM WARNING: fdisk GPT support is currently new, and therefore in an experimental phase. Use at your own discretion. Disk /dev/nvme6n1: 1600.3 GB, 1600321314816 bytes, 3125627568 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk label type: gpt Disk identifier: 0360D46A-9A41-4A8D-A449-94CCDC01FA8B # Start End Size Type Name 1 2048 3125626879 1.5T Linux LVM WARNING: fdisk GPT support is currently new, and therefore in an experimental phase. Use at your own discretion. Disk /dev/nvme4n1: 1600.3 GB, 1600321314816 bytes, 3125627568 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk label type: gpt Disk identifier: A5D3EDDF-C93D-4B33-92B0-D25B009EEB9D # Start End Size Type Name 1 2048 3125626879 1.5T Linux LVM Disk /dev/nvme5n1: 1600.3 GB, 1600321314816 bytes, 3125627568 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes WARNING: fdisk GPT support is currently new, and therefore in an experimental phase. Use at your own discretion. Disk /dev/nvme7n1: 1600.3 GB, 1600321314816 bytes, 3125627568 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk label type: gpt Disk identifier: 71A0B583-E727-45B6-A1AB-791D341709B6 # Start End Size Type Name 1 2048 3125626879 1.5T Linux LVM Disk /dev/sda: 11515.9 GB, 11515881062400 bytes, 22491955200 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 1048576 bytes / 1048576 bytes WARNING: fdisk GPT support is currently new, and therefore in an experimental phase. Use at your own discretion. Disk /dev/sdb: 240.0 GB, 239990276096 bytes, 468731008 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 4096 bytes I/O size (minimum/optimal): 4096 bytes / 4096 bytes Disk label type: gpt Disk identifier: F1E6A9A1-C85F-46C1-A27E-DBC41C9260AC # Start End Size Type Name 1 2048 411647 200M EFI System EFI System Partition 2 411648 2508799 1G Microsoft basic 3 2508800 468729855 222.3G Linux LVM Disk /dev/sdc: 64.2 GB, 64239960064 bytes, 125468672 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk /dev/mapper/centos-root: 234.4 GB, 234407067648 bytes, 457826304 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 4096 bytes I/O size (minimum/optimal): 4096 bytes / 4096 bytes Disk /dev/mapper/centos-swap: 4294 MB, 4294967296 bytes, 8388608 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 4096 bytes I/O size (minimum/optimal): 4096 bytes / 4096 bytes Disk /dev/mapper/data-data: 11202.2 GB, 11202210037760 bytes, 21879316480 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 131072 bytes / 917504 bytes CPU Layout [root@r940 data]# lscpu Architecture: x86_64 CPU op-mode(s): 32-bit, 64-bit Byte Order: Little Endian CPU(s): 176 On-line CPU(s) list: 0-175 Thread(s) per core: 2 Core(s) per socket: 22 Socket(s): 4 NUMA node(s): 4 Vendor ID: GenuineIntel CPU family: 6 Model: 85 Model name: Intel(R) Xeon(R) Gold 6152 CPU @ 2.10GHz Stepping: 4 CPU MHz: 1394.146 CPU max MHz: 3700.0000 CPU min MHz: 1000.0000 BogoMIPS: 4200.00 Virtualization: VT-x L1d cache: 32K L1i cache: 32K L2 cache: 1024K L3 cache: 30976K NUMA node0 CPU(s): 0,4,8,12,16,20,24,28,32,36,40,44,48,52,56,60,64,68,72,76,80,84,88,92,96,100,104,108,112,116,120,124,128,132,136,140,144,148,152,156,160,164,168,172 NUMA node1 CPU(s): 1,5,9,13,17,21,25,29,33,37,41,45,49,53,57,61,65,69,73,77,81,85,89,93,97,101,105,109,113,117,121,125,129,133,137,141,145,149,153,157,161,165,169,173 NUMA node2 CPU(s): 2,6,10,14,18,22,26,30,34,38,42,46,50,54,58,62,66,70,74,78,82,86,90,94,98,102,106,110,114,118,122,126,130,134,138,142,146,150,154,158,162,166,170,174 NUMA node3 CPU(s): 3,7,11,15,19,23,27,31,35,39,43,47,51,55,59,63,67,71,75,79,83,87,91,95,99,103,107,111,115,119,123,127,131,135,139,143,147,151,155,159,163,167,171,175 Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 invpcid_single intel_ppin intel_pt ssbd mba ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm mpx rdt_a avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts pku ospke md_clear spec_ctrl intel_stibp flush_l1d [root@r940 data]# cpu_layout.py ====================================================================== Core and Socket Information (as reported by '/sys/devices/system/cpu') ====================================================================== cores = [0, 5, 1, 4, 2, 3, 8, 12, 9, 11, 10, 21, 16, 20, 17, 19, 18, 28, 24, 27, 25, 26] sockets = [0, 1, 2, 3] Socket 0 Socket 1 Socket 2 Socket 3 -------- -------- -------- -------- Core 0 [0, 88] [1, 89] [2, 90] [3, 91] Core 5 [4, 92] [5, 93] [6, 94] [7, 95] Core 1 [8, 96] [9, 97] [10, 98] [11, 99] Core 4 [12, 100] [13, 101] [14, 102] [15, 103] Core 2 [16, 104] [17, 105] [18, 106] [19, 107] Core 3 [20, 108] [21, 109] [22, 110] [23, 111] Core 8 [24, 112] [25, 113] [26, 114] [27, 115] Core 12 [28, 116] [29, 117] [30, 118] [31, 119] Core 9 [32, 120] [33, 121] [34, 122] [35, 123] Core 11 [36, 124] [37, 125] [38, 126] [39, 127] Core 10 [40, 128] [41, 129] [42, 130] [43, 131] Core 21 [44, 132] [45, 133] [46, 134] [47, 135] Core 16 [48, 136] [49, 137] [50, 138] [51, 139] Core 20 [52, 140] [53, 141] [54, 142] [55, 143] Core 17 [56, 144] [57, 145] [58, 146] [59, 147] Core 19 [60, 148] [61, 149] [62, 150] [63, 151] Core 18 [64, 152] [65, 153] [66, 154] [67, 155] Core 28 [68, 156] [69, 157] [70, 158] [71, 159] Core 24 [72, 160] [73, 161] [74, 162] [75, 163] Core 27 [76, 164] [77, 165] [78, 166] [79, 167] Core 25 [80, 168] [81, 169] [82, 170] [83, 171] Core 26 [84, 172] [85, 173] [86, 174] [87, 175] CentOS 7 Release Info NAME=\"CentOS Linux\" VERSION=\"7 (Core)\" ID=\"centos\" ID_LIKE=\"rhel fedora\" VERSION_ID=\"7\" PRETTY_NAME=\"CentOS Linux 7 (Core)\" ANSI_COLOR=\"0;31\" CPE_NAME=\"cpe:/o:centos:centos:7\" HOME_URL=\"https://www.centos.org/\" BUG_REPORT_URL=\"https://bugs.centos.org/\" CENTOS_MANTISBT_PROJECT=\"CentOS-7\" CENTOS_MANTISBT_PROJECT_VERSION=\"7\" REDHAT_SUPPORT_PRODUCT=\"centos\" REDHAT_SUPPORT_PRODUCT_VERSION=\"7\" CentOS Linux release 7.7.1908 (Core) CentOS Linux release 7.7.1908 (Core) Kernel Info Linux r940.lan 3.10.0-1062.4.3.el7.x86_64 #1 SMP Wed Nov 13 23:58:53 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux Format Your Data Partitions Create physical volumes and volume groups with: pvcreate <NVMe drive> vgcreate data <List of NVMe Drives> NOTE: I only did this on the NVMe drives. To create the logical volume I used: lvcreate -l 100%FREE -i7 -I128 -n data data To format the drive I used: mkfs.ext4 -F -b 4096 -E discard,stride=16,stripe-width=256 # This was the 12 SAS SSDs I had in a RAID mkfs.ext4 -F -b 4096 -E discard,stride=16,stripe-width=256 /dev/mapper/data-data # This was the NVMes I tied together with LVM mkfs.ext4 -F -b 4096 -E discard,stride=16,stripe-width=256 /dev/mapper/data2-data2 mount -o rw,auto,discard /dev/mapper/data-data /data mount -o rw,auto,discard /dev/sda /raiddata/ mount -o rw,auto,discard /dev/mapper/data2-data2 /data2 echo noop > /sys/block/sda/queue/scheduler Install NapaTech Driver Install Driver Download the Napatech software from here Run yum groupinstall \"Development Tools\" && yum install -y kernel-devel wget gettext-devel openssl-devel perl-CPAN perl-devel zlib-devel pciutils && yum install -y https://centos7.iuscommunity.org/ius-release.rpm && yum remove -y git && yum install -y git2u-all Unzip and run package_install_3gd.sh Run the following commands: /opt/napatech3/bin/ntload.sh /opt/napatech3/bin/ntstart.sh Install n2disk and ntop Perform Installation Install epel with yum install -y epel-release Erase the zeromq3 package with yum erase zeromq && yum clean all && yum update -y && reboot Pull ntop repo with wget http://packages.ntop.org/centos-stable/ntop.repo -O /etc/yum.repos.d/ntop.repo Install required packages with yum install -y pfring-dkms pfring n2disk nprobe ntopng ntopng-data cento pfring-drivers-zc-dkms redis hiredis-devel Configure n2disk Create backup of the the NapaTech ini file cp /opt/napatech3/config/ntservice.ini /opt/napatech3/config/ntservice.ini.bak Update /opt/napatech3/config/ntservice.ini with the following values: TimestampFormat = PCAP_NS PacketDescriptor = PCAP HostBufferSegmentSizeRx = 4 TODO change these lines see notes for help HostBuffersRx = [16,16,0],[16,16,1] HostBuffersTx = [16,16,0],[16,16,1] You will need to start and stop the ntservice for these changes to take effect with: /opt/napatech3/bin/ntstop.sh /opt/napatech3/bin/ntstart.sh Perform Configuration Zero Copy Driver Edit the pfring configuration file with vim /etc/pf_ring/interfaces.conf and add your configuration. MANAGEMENT_INTERFACES=\"em1\" CAPTURE_INTERFACES=\"nt:0\" Open the file /etc/ntopng/ntopng.conf . If you do not have a license add --community to the end Configure the firewall to accept connections to ntopng with: firewall-cmd --zone=public --permanent --add-port=3000/tcp && firewall-cmd --reload Enable and start services with: systemctl enable redis.service systemctl enable ntopng.service systemctl enable pf_ring systemctl start redis.service systemctl start ntopng.service systemctl start pf_ring Make sure the services are running correctly with: systemctl status redis.service systemctl status ntopng.service systemctl status pf_ring Configure License Run zcount -i nt:0 and note the serial number Output n2disk license to /etc/n2disk.license Output ntopng license to /etc/ntopng.license Useful Tips Hardware Filtering Napatech NICs support full-blown hardware filtering out of the box. Thanks to nBPF we convert BPF expressions to hardware filters. This feature is supported transparently, and thus all PF_RING/libpcap-over-PF_RING can benefit from it. Example: pfcount -i nt:3 -f \"tcp and port 80 and src host 192.168.1.1\" Hostbuffer Notes HostBuffersRx = [16,2048,0],[16,2048,1],[16,2048,2],[16,2048,3] HostBuffersTx = [16,2048,0],[16,2048,1],[16,2048,2],[16,2048,3] First number is the number of host buffers Second number is the size of the host buffers Third number is the NUMA node You have to have one set of numbers for each NUMA node. Testing Transmit Speed ./pktgen -p 1 -r 10G Testing the PCAP transmit speed To test to see if the Napatech card is up and running run this command: ./pfcount -i nt:0 ./monitoring You can press t to switch stats between receive and transmit. Testing Receive chmod -R 777 /data rm /tmp/*-none\\.* 2>/dev/null; while true; do grep 'Dropped:\\|Slow.*:' -C50 /proc/net/pf_ring/stats/* 2>/dev/null; cp /proc/net/pf_ring/stats/*none* /tmp 2>/dev/null; sleep 1; done NUMA Lookup Run lscpu Things we tried: We used the to list the cpu layout and determine where we wanted to run what threads. Tests 1-6 were with the default settings for a RAID0 partition on Linux at setup time. Test 1 This is running at 100Gb/s generation n2disk -a -v -l -o /<Storage path> -x $(date +%s.) -i nt:0 This seemed to dump everything to one thread. We maxed out and had ~82% packet loss. Test 2 This is running at 100Gb/s generation n2disk -a -v -l -o /data/ -x $(date +%s.) -i nt:0 -n 5000 -m 10000 -p $((4*1024)) -b $((12*1024)) -C 4096 -c 32 -z 2,3,4,5,6,7 -Z -w 16,17,18,19,20,20,21 -S 22 Throughput results: [root@r940 bin]# cat /proc/net/pf_ring/stats/*none* Duration: 0:00:00:53:022 Throughput: 2.12 Mpps 17.65 Gbps Packets: 78780625 Filtered: 78780625 Dropped: 518512807 Bytes: 81931850000 DumpedBytes: 71137406724 DumpedFiles: 17 SlowSlavesLoops: 0 SlowStorageLoops: 432580 CaptureLoops: 19532 FirstDumpedEpoch: 0 LastDumpedEpoch: 1574185087 Worked better but we got a warning saying the time thread was on a different core than the reader/writer threads. Test 3 This is running at 100Gb/s generation n2disk -a -v -l -o /data/ -x $(date +%s.) -i nt:0 -n 5000 -m 10000 -p $((4*1024)) -b $((12*1024)) -C 4096 -c 32 -z 2,3,4,5,6,7 -Z -w 16,17,18,19,20,20,21 -S 56 Worked better: 19/Nov/2019 12:41:59 [n2disk.c:1109] Average Capture Throughput: 22.49 Gbit / 2.69 Mpps Test 4 This is running at 100Gb/s generation n2disk -a -v -l -o /data/ -x $(date +%s.) -i nt:0 -n 5000 -m 10000 -p $((4*1024)) -b $((12*1024)) -C 4096 -c 0 -z 8,12,16,20,24,28,32,26,40,44,48,52,56,60,64,68,72,76,80,84,88 -Z -w 92,96,100,104,108,112,116,120,124,128,132,136,140,144,148,152,156,160,164,168,172 -S 4 Test 5 This is running at 20Gb/s generation n2disk -a -v -l -o /data/ -x $(date +%s.) -i nt:0 -n 5000 -m 10000 -p $((4*1024)) -b $((12*1024)) -C 4096 -c 0 -z 8,12,16,20,24,28,32,26,40,44,48,52,56,60,64,68,72,76,80,84,88 -Z -w 92,96,100,104,108,112,116,120,124,128,132,136,140,144,148,152,156,160,164,168,172 -S 4 Failed. We got a lot of packet loss. Test 6 This is running at 10Gb/s generation n2disk -a -v -l -o /data/ -x $(date +%s.) -i nt:0 -n 5000 -m 10000 -p $((4*1024)) -b $((12*1024)) -C 4096 -c 0 -z 8,12,16,20,24,28,32,26,40,44,48,52,56,60,64,68,72,76,80,84,88 -Z -w 92,96,100,104,108,112,116,120,124,128,132,136,140,144,148,152,156,160,164,168,172 -S 4 No packet drops. Test 7 This is running at 20Gb/s generation n2disk -a -v -l -o /data/ -x $(date +%s.) -i nt:0 -n 5000 -m 10000 -p $((4*1024)) -b $((12*1024)) -C 4096 -c 0 -z 8,12,16,20,24,28,32,26,40,44,48,52,56,60,64,68,72,76,80,84,88 -Z -w 92,96,100,104,108,112,116,120,124,128,132,136,140,144,148,152,156,160,164,168,172 -S 4 This was with a data partition formatting according to the above. Test 8 /opt/napatech3/bin/ntpl -e \"Delete = All\" /opt/napatech3/bin/ntpl -e \"Setup[NumaNode=2] = StreamID == 0\" # Repeat this for each stream ID. In our case 0-4 ./profiling # use this to see traffic being received # N is the NUMA node that the profiler detects the traffic on /opt/napatech3/bin/ntpl -e \"HashMode = Hash2TupleSorted\" /opt/napatech3/bin/ntpl -e \"Assign[StreamId=(0..3)] = port == 0\" The problem with this was that we couldn't get n2disk to listen on multiple interfaces. It also didn't really give us a way to split the traffic across multiple reader threads. Test 9 I made four directories, one for each n2disk process I did the following: /opt/napatech3/bin/ntpl -e \"Delete = All\" /opt/napatech3/bin/ntpl -e \"Setup[NumaNode=2] = StreamID == 0\" # Repeat this for each stream ID. In our case 0-4 /opt/napatech3/bin/ntpl -e \"HashMode = Hash2TupleSorted\" /opt/napatech3/bin/ntpl -e \"Assign[StreamId=(0..3)] = port == 0\" ./profiling # use this to see traffic being received # N is the NUMA node that the profiler detects the traffic on Run the following commands: n2disk -a -v -l -o /data/data0 -x $(date +%s.) -i nt:stream0 -n 5000 -m 10000 -p $((20*1024)) -b $((40*1024)) -C 4096 -c 120 -z 0,4,8,12,16,20,24,28 -Z -w 88,92,96,100,104,108,112,116 -S 32 n2disk -a -v -l -o /data/data1 -x $(date +%s.) -i nt:stream1 -n 5000 -m 10000 -p $((20*1024)) -b $((40*1024)) -C 4096 -c 121 -z 1,5,9,13,17,21,25,29 -Z -w 89,93,97,101,105,109,113,117 -S 33 n2disk -a -v -l -o /data/data2 -x $(date +%s.) -i nt:stream2 -n 5000 -m 10000 -p $((20*1024)) -b $((40*1024)) -C 4096 -c 156 -z 36,40,44,48,52,56,60,64 -Z -w 124,128,132,136,140,144,148,152 -S 68 n2disk -a -v -l -o /data/data3 -x $(date +%s.) -i nt:stream3 -n 5000 -m 10000 -p $((20*1024)) -b $((40*1024)) -C 4096 -c 157 -z 37,41,45,49,53,57,61,65 -Z -w 125,129,133,137,141,145,149,153 -S 69 -a : Archive pcap file (rename to .old) instead of overwriting if already present on disk. -v : Verbose. -o : Directory where dump files will be saved (multiple -o can be specified) -x : Dump file prefix. -i : Ingress packet device. -n : Max number of nested dump sub-directories. -m : Max number of files before restarting file name. -p : Max pcap file length (MBytes). -b : Buffer length (MBytes). -C : Size (KB) of the chunk written to disk (must be multiple of 4096). Default: 64 KB. -c : Bind the reader thread to the specified core. -z : Enable multithread compression and/or indexing and bind thread(s) to the specified core ids (e.g. 0,1,2,3) (mandatory with indexing on Napatech cards) -Z : Compute index on the thread(s) used for compression (-z) instead of using the capture thread(s). -w : Bind the writer thread(s) to the specified core ids. A comma-separated list of cores (e.g. 0,1,2,3) should be specified in case of multiple dump directories (-o). -S : Enable time pulse thread (optimise sw packet timestamping) and bind it to the specified core. With this setup I was able to get 70Gb/s. I tested 100Gb/s and got drops. Did not perform further testing to narrow down exactly how much traffic I could push before moving on to the next test. Test 10 I made seven directories, one for each n2disk process. 4 assigned to the NVMe drives and 3 assigned to the SAS SSD RAID I did the following: /opt/napatech3/bin/ntpl -e \"Delete = All\" /opt/napatech3/bin/ntpl -e \"Setup[NumaNode=2] = StreamID == 0\" # Repeat this for each stream ID. In our case 0-6 /opt/napatech3/bin/ntpl -e \"HashMode = Hash2TupleSorted\" /opt/napatech3/bin/ntpl -e \"Assign[StreamId=(0..6)] = port == 0\" ./profiling # use this to see traffic being received # N is the NUMA node that the profiler detects the traffic on watch cat /proc/net/pf_ring/stats/*none* Run the following commands: n2disk -a -v -l -o /data/data0 -x $(date +%s.) -i nt:stream0 -n 5000 -m 10000 -p $((20*1024)) -b $((40*1024)) -C 4096 -c 120 -z 0,4,8,12,16,20,24,28 -Z -w 88,92,96,100,104,108,112,116 -S 32 n2disk -a -v -l -o /data/data1 -x $(date +%s.) -i nt:stream1 -n 5000 -m 10000 -p $((20*1024)) -b $((40*1024)) -C 4096 -c 121 -z 1,5,9,13,17,21,25,29 -Z -w 89,93,97,101,105,109,113,117 -S 33 n2disk -a -v -l -o /data/data2 -x $(date +%s.) -i nt:stream2 -n 5000 -m 10000 -p $((20*1024)) -b $((40*1024)) -C 4096 -c 156 -z 36,40,44,48,52,56,60,64 -Z -w 124,128,132,136,140,144,148,152 -S 68 n2disk -a -v -l -o /data/data3 -x $(date +%s.) -i nt:stream3 -n 5000 -m 10000 -p $((20*1024)) -b $((40*1024)) -C 4096 -c 157 -z 37,41,45,49,53,57,61,65 -Z -w 125,129,133,137,141,145,149,153 -S 69 n2disk -a -v -l -o /raiddata/data0 -x $(date +%s.) -i nt:stream4 -n 5000 -m 10000 -p $((20*1024)) -b $((40*1024)) -C 4096 -c 123 -z 3,7,11,15,19,23,27,31 -Z -w 91,95,99,103,107,111,115,119 -S 35 n2disk -a -v -l -o /raiddata/data1 -x $(date +%s.) -i nt:stream5 -n 5000 -m 10000 -p $((20*1024)) -b $((40*1024)) -C 4096 -c 159 -z 39,43,47,51,55,59,63,67 -Z -w 127,131,135,139,143,147,151,155 -S 71 n2disk -a -v -l -o /raiddata/data2 -x $(date +%s.) -i nt:stream6 -n 5000 -m 10000 -p $((20*1024)) -b $((40*1024)) -C 4096 -c 174 -z 54,58,62,66,70,74,78,82 -Z -w 142,146,150,154,158,162,166,170 -S 86 -a : Archive pcap file (rename to .old) instead of overwriting if already present on disk. -v : Verbose. -o : Directory where dump files will be saved (multiple -o can be specified) -x : Dump file prefix. -i : Ingress packet device. -n : Max number of nested dump sub-directories. -m : Max number of files before restarting file name. -p : Max pcap file length (MBytes). -b : Buffer length (MBytes). -C : Size (KB) of the chunk written to disk (must be multiple of 4096). Default: 64 KB. -c : Bind the reader thread to the specified core. -z : Enable multithread compression and/or indexing and bind thread(s) to the specified core ids (e.g. 0,1,2,3) (mandatory with indexing on Napatech cards) -Z : Compute index on the thread(s) used for compression (-z) instead of using the capture thread(s). -w : Bind the writer thread(s) to the specified core ids. A comma-separated list of cores (e.g. 0,1,2,3) should be specified in case of multiple dump directories (-o). -S : Enable time pulse thread (optimise sw packet timestamping) and bind it to the specified core. With this setup I was able to get 70Gb/s. I tested 100Gb/s and got drops. Did not perform further testing to narrow down exactly how much traffic I could push before moving on to the next test. Compile PF_RING Driver (PROBABLY NOT NECESSARY) NOTE There is a note in the documentation saying that installing from repository comes with NapaTech support. Move to opt run git clone https://github.com/ntop/PF_RING.git Move into the directory and run cd PF_RING/kernel && make && sudo insmod pf_ring.ko Next run cd ../userland/lib && ./configure && make Next run cd ../libpcap && ./configure && make Next run cd ../examples && make","title":"Useful Materials"},{"location":"High%20Speed%20Packet%20Capture/ntop%20on%20R940%20with%20Napatech/#useful-materials","text":"NapaTech Installation Instructions (Creatinga EXT4 Filesystem)[https://thelastmaimou.wordpress.com/2013/05/04/magic-soup-ext4-with-ssd-stripes-and-strides/]","title":"Useful Materials"},{"location":"High%20Speed%20Packet%20Capture/ntop%20on%20R940%20with%20Napatech/#my-environment","text":"I am running a Dell FC640 on a Dell R940 See for hardware details.","title":"My Environment"},{"location":"High%20Speed%20Packet%20Capture/ntop%20on%20R940%20with%20Napatech/#hard-drive-layout","text":"I had a RAID of 12 SAS SSDs in RAID0 on the PERC740. I had 7 NVMe drives I used. I couldn't get the 8th NVMe drive working. [root@r940 /]# lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT sda 8:0 0 10.5T 0 disk /raiddata sdb 8:16 0 223.5G 0 disk \u251c\u2500sdb1 8:17 0 200M 0 part /boot/efi \u251c\u2500sdb2 8:18 0 1G 0 part /boot \u2514\u2500sdb3 8:19 0 222.3G 0 part \u251c\u2500centos-root 253:0 0 218.3G 0 lvm / \u2514\u2500centos-swap 253:1 0 4G 0 lvm [SWAP] sdc 8:32 0 59.8G 0 disk nvme0n1 259:6 0 1.5T 0 disk \u2514\u2500nvme0n1p1 259:8 0 1.5T 0 part \u2514\u2500data-data 253:2 0 10.2T 0 lvm /data nvme1n1 259:0 0 1.5T 0 disk \u2514\u2500nvme1n1p1 259:1 0 1.5T 0 part \u2514\u2500data-data 253:2 0 10.2T 0 lvm /data nvme2n1 259:2 0 1.5T 0 disk \u2514\u2500nvme2n1p1 259:3 0 1.5T 0 part \u2514\u2500data-data 253:2 0 10.2T 0 lvm /data nvme3n1 259:4 0 1.5T 0 disk \u2514\u2500nvme3n1p1 259:5 0 1.5T 0 part \u2514\u2500data-data 253:2 0 10.2T 0 lvm /data nvme4n1 259:10 0 1.5T 0 disk \u2514\u2500nvme4n1p1 259:13 0 1.5T 0 part \u2514\u2500data-data 253:2 0 10.2T 0 lvm /data nvme5n1 259:11 0 1.5T 0 disk nvme6n1 259:7 0 1.5T 0 disk \u2514\u2500nvme6n1p1 259:9 0 1.5T 0 part \u2514\u2500data-data 253:2 0 10.2T 0 lvm /data nvme7n1 259:12 0 1.5T 0 disk \u2514\u2500nvme7n1p1 259:14 0 1.5T 0 part \u2514\u2500data-data 253:2 0 10.2T 0 lvm /data [root@r940 /]# pvs PV VG Fmt Attr PSize PFree /dev/nvme0n1p1 data lvm2 a-- <1.46t 0 /dev/nvme1n1p1 data lvm2 a-- <1.46t 0 /dev/nvme2n1p1 data lvm2 a-- <1.46t 0 /dev/nvme3n1p1 data lvm2 a-- <1.46t 0 /dev/nvme4n1p1 data lvm2 a-- <1.46t 0 /dev/nvme6n1p1 data lvm2 a-- <1.46t 0 /dev/nvme7n1p1 data lvm2 a-- <1.46t 0 /dev/sdb3 centos lvm2 a-- <222.31g 0 [root@r940 /]# vgs VG #PV #LV #SN Attr VSize VFree centos 1 2 0 wz--n- <222.31g 0 data 7 1 0 wz--n- <10.19t 0 [root@r940 /]# lvs LV VG Attr LSize Pool Origin Data% Meta% Move Log Cpy%Sync Convert root centos -wi-ao---- <218.31g swap centos -wi-ao---- 4.00g data data -wi-ao---- <10.19t WARNING: fdisk GPT support is currently new, and therefore in an experimental phase. Use at your own discretion. Disk /dev/nvme1n1: 1600.3 GB, 1600321314816 bytes, 3125627568 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk label type: gpt Disk identifier: 759A1CF7-125F-469B-981E-149EBDBE3456 # Start End Size Type Name 1 2048 3125626879 1.5T Linux LVM WARNING: fdisk GPT support is currently new, and therefore in an experimental phase. Use at your own discretion. Disk /dev/nvme2n1: 1600.3 GB, 1600321314816 bytes, 3125627568 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk label type: gpt Disk identifier: 57898F4D-5B5E-4495-B695-E48EA3FCFA01 # Start End Size Type Name 1 2048 3125626879 1.5T Linux LVM WARNING: fdisk GPT support is currently new, and therefore in an experimental phase. Use at your own discretion. Disk /dev/nvme3n1: 1600.3 GB, 1600321314816 bytes, 3125627568 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk label type: gpt Disk identifier: DB46E8E3-5E96-4228-A148-E6C8F0187DF4 # Start End Size Type Name 1 2048 3125626879 1.5T Linux LVM WARNING: fdisk GPT support is currently new, and therefore in an experimental phase. Use at your own discretion. Disk /dev/nvme0n1: 1600.3 GB, 1600321314816 bytes, 3125627568 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk label type: gpt Disk identifier: 4066CDE1-E36E-41FB-8277-5E3FFDB55B4A # Start End Size Type Name 1 2048 3125626879 1.5T Linux LVM WARNING: fdisk GPT support is currently new, and therefore in an experimental phase. Use at your own discretion. Disk /dev/nvme6n1: 1600.3 GB, 1600321314816 bytes, 3125627568 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk label type: gpt Disk identifier: 0360D46A-9A41-4A8D-A449-94CCDC01FA8B # Start End Size Type Name 1 2048 3125626879 1.5T Linux LVM WARNING: fdisk GPT support is currently new, and therefore in an experimental phase. Use at your own discretion. Disk /dev/nvme4n1: 1600.3 GB, 1600321314816 bytes, 3125627568 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk label type: gpt Disk identifier: A5D3EDDF-C93D-4B33-92B0-D25B009EEB9D # Start End Size Type Name 1 2048 3125626879 1.5T Linux LVM Disk /dev/nvme5n1: 1600.3 GB, 1600321314816 bytes, 3125627568 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes WARNING: fdisk GPT support is currently new, and therefore in an experimental phase. Use at your own discretion. Disk /dev/nvme7n1: 1600.3 GB, 1600321314816 bytes, 3125627568 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk label type: gpt Disk identifier: 71A0B583-E727-45B6-A1AB-791D341709B6 # Start End Size Type Name 1 2048 3125626879 1.5T Linux LVM Disk /dev/sda: 11515.9 GB, 11515881062400 bytes, 22491955200 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 1048576 bytes / 1048576 bytes WARNING: fdisk GPT support is currently new, and therefore in an experimental phase. Use at your own discretion. Disk /dev/sdb: 240.0 GB, 239990276096 bytes, 468731008 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 4096 bytes I/O size (minimum/optimal): 4096 bytes / 4096 bytes Disk label type: gpt Disk identifier: F1E6A9A1-C85F-46C1-A27E-DBC41C9260AC # Start End Size Type Name 1 2048 411647 200M EFI System EFI System Partition 2 411648 2508799 1G Microsoft basic 3 2508800 468729855 222.3G Linux LVM Disk /dev/sdc: 64.2 GB, 64239960064 bytes, 125468672 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk /dev/mapper/centos-root: 234.4 GB, 234407067648 bytes, 457826304 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 4096 bytes I/O size (minimum/optimal): 4096 bytes / 4096 bytes Disk /dev/mapper/centos-swap: 4294 MB, 4294967296 bytes, 8388608 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 4096 bytes I/O size (minimum/optimal): 4096 bytes / 4096 bytes Disk /dev/mapper/data-data: 11202.2 GB, 11202210037760 bytes, 21879316480 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 131072 bytes / 917504 bytes","title":"Hard Drive Layout:"},{"location":"High%20Speed%20Packet%20Capture/ntop%20on%20R940%20with%20Napatech/#cpu-layout","text":"[root@r940 data]# lscpu Architecture: x86_64 CPU op-mode(s): 32-bit, 64-bit Byte Order: Little Endian CPU(s): 176 On-line CPU(s) list: 0-175 Thread(s) per core: 2 Core(s) per socket: 22 Socket(s): 4 NUMA node(s): 4 Vendor ID: GenuineIntel CPU family: 6 Model: 85 Model name: Intel(R) Xeon(R) Gold 6152 CPU @ 2.10GHz Stepping: 4 CPU MHz: 1394.146 CPU max MHz: 3700.0000 CPU min MHz: 1000.0000 BogoMIPS: 4200.00 Virtualization: VT-x L1d cache: 32K L1i cache: 32K L2 cache: 1024K L3 cache: 30976K NUMA node0 CPU(s): 0,4,8,12,16,20,24,28,32,36,40,44,48,52,56,60,64,68,72,76,80,84,88,92,96,100,104,108,112,116,120,124,128,132,136,140,144,148,152,156,160,164,168,172 NUMA node1 CPU(s): 1,5,9,13,17,21,25,29,33,37,41,45,49,53,57,61,65,69,73,77,81,85,89,93,97,101,105,109,113,117,121,125,129,133,137,141,145,149,153,157,161,165,169,173 NUMA node2 CPU(s): 2,6,10,14,18,22,26,30,34,38,42,46,50,54,58,62,66,70,74,78,82,86,90,94,98,102,106,110,114,118,122,126,130,134,138,142,146,150,154,158,162,166,170,174 NUMA node3 CPU(s): 3,7,11,15,19,23,27,31,35,39,43,47,51,55,59,63,67,71,75,79,83,87,91,95,99,103,107,111,115,119,123,127,131,135,139,143,147,151,155,159,163,167,171,175 Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 invpcid_single intel_ppin intel_pt ssbd mba ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm mpx rdt_a avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts pku ospke md_clear spec_ctrl intel_stibp flush_l1d [root@r940 data]# cpu_layout.py ====================================================================== Core and Socket Information (as reported by '/sys/devices/system/cpu') ====================================================================== cores = [0, 5, 1, 4, 2, 3, 8, 12, 9, 11, 10, 21, 16, 20, 17, 19, 18, 28, 24, 27, 25, 26] sockets = [0, 1, 2, 3] Socket 0 Socket 1 Socket 2 Socket 3 -------- -------- -------- -------- Core 0 [0, 88] [1, 89] [2, 90] [3, 91] Core 5 [4, 92] [5, 93] [6, 94] [7, 95] Core 1 [8, 96] [9, 97] [10, 98] [11, 99] Core 4 [12, 100] [13, 101] [14, 102] [15, 103] Core 2 [16, 104] [17, 105] [18, 106] [19, 107] Core 3 [20, 108] [21, 109] [22, 110] [23, 111] Core 8 [24, 112] [25, 113] [26, 114] [27, 115] Core 12 [28, 116] [29, 117] [30, 118] [31, 119] Core 9 [32, 120] [33, 121] [34, 122] [35, 123] Core 11 [36, 124] [37, 125] [38, 126] [39, 127] Core 10 [40, 128] [41, 129] [42, 130] [43, 131] Core 21 [44, 132] [45, 133] [46, 134] [47, 135] Core 16 [48, 136] [49, 137] [50, 138] [51, 139] Core 20 [52, 140] [53, 141] [54, 142] [55, 143] Core 17 [56, 144] [57, 145] [58, 146] [59, 147] Core 19 [60, 148] [61, 149] [62, 150] [63, 151] Core 18 [64, 152] [65, 153] [66, 154] [67, 155] Core 28 [68, 156] [69, 157] [70, 158] [71, 159] Core 24 [72, 160] [73, 161] [74, 162] [75, 163] Core 27 [76, 164] [77, 165] [78, 166] [79, 167] Core 25 [80, 168] [81, 169] [82, 170] [83, 171] Core 26 [84, 172] [85, 173] [86, 174] [87, 175]","title":"CPU Layout"},{"location":"High%20Speed%20Packet%20Capture/ntop%20on%20R940%20with%20Napatech/#centos-7-release-info","text":"NAME=\"CentOS Linux\" VERSION=\"7 (Core)\" ID=\"centos\" ID_LIKE=\"rhel fedora\" VERSION_ID=\"7\" PRETTY_NAME=\"CentOS Linux 7 (Core)\" ANSI_COLOR=\"0;31\" CPE_NAME=\"cpe:/o:centos:centos:7\" HOME_URL=\"https://www.centos.org/\" BUG_REPORT_URL=\"https://bugs.centos.org/\" CENTOS_MANTISBT_PROJECT=\"CentOS-7\" CENTOS_MANTISBT_PROJECT_VERSION=\"7\" REDHAT_SUPPORT_PRODUCT=\"centos\" REDHAT_SUPPORT_PRODUCT_VERSION=\"7\" CentOS Linux release 7.7.1908 (Core) CentOS Linux release 7.7.1908 (Core)","title":"CentOS 7 Release Info"},{"location":"High%20Speed%20Packet%20Capture/ntop%20on%20R940%20with%20Napatech/#kernel-info","text":"Linux r940.lan 3.10.0-1062.4.3.el7.x86_64 #1 SMP Wed Nov 13 23:58:53 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux","title":"Kernel Info"},{"location":"High%20Speed%20Packet%20Capture/ntop%20on%20R940%20with%20Napatech/#format-your-data-partitions","text":"Create physical volumes and volume groups with: pvcreate <NVMe drive> vgcreate data <List of NVMe Drives> NOTE: I only did this on the NVMe drives. To create the logical volume I used: lvcreate -l 100%FREE -i7 -I128 -n data data To format the drive I used: mkfs.ext4 -F -b 4096 -E discard,stride=16,stripe-width=256 # This was the 12 SAS SSDs I had in a RAID mkfs.ext4 -F -b 4096 -E discard,stride=16,stripe-width=256 /dev/mapper/data-data # This was the NVMes I tied together with LVM mkfs.ext4 -F -b 4096 -E discard,stride=16,stripe-width=256 /dev/mapper/data2-data2 mount -o rw,auto,discard /dev/mapper/data-data /data mount -o rw,auto,discard /dev/sda /raiddata/ mount -o rw,auto,discard /dev/mapper/data2-data2 /data2 echo noop > /sys/block/sda/queue/scheduler","title":"Format Your Data Partitions"},{"location":"High%20Speed%20Packet%20Capture/ntop%20on%20R940%20with%20Napatech/#install-napatech-driver","text":"","title":"Install NapaTech Driver"},{"location":"High%20Speed%20Packet%20Capture/ntop%20on%20R940%20with%20Napatech/#install-driver","text":"Download the Napatech software from here Run yum groupinstall \"Development Tools\" && yum install -y kernel-devel wget gettext-devel openssl-devel perl-CPAN perl-devel zlib-devel pciutils && yum install -y https://centos7.iuscommunity.org/ius-release.rpm && yum remove -y git && yum install -y git2u-all Unzip and run package_install_3gd.sh Run the following commands: /opt/napatech3/bin/ntload.sh /opt/napatech3/bin/ntstart.sh","title":"Install Driver"},{"location":"High%20Speed%20Packet%20Capture/ntop%20on%20R940%20with%20Napatech/#install-n2disk-and-ntop","text":"","title":"Install n2disk and ntop"},{"location":"High%20Speed%20Packet%20Capture/ntop%20on%20R940%20with%20Napatech/#perform-installation","text":"Install epel with yum install -y epel-release Erase the zeromq3 package with yum erase zeromq && yum clean all && yum update -y && reboot Pull ntop repo with wget http://packages.ntop.org/centos-stable/ntop.repo -O /etc/yum.repos.d/ntop.repo Install required packages with yum install -y pfring-dkms pfring n2disk nprobe ntopng ntopng-data cento pfring-drivers-zc-dkms redis hiredis-devel","title":"Perform Installation"},{"location":"High%20Speed%20Packet%20Capture/ntop%20on%20R940%20with%20Napatech/#configure-n2disk","text":"Create backup of the the NapaTech ini file cp /opt/napatech3/config/ntservice.ini /opt/napatech3/config/ntservice.ini.bak Update /opt/napatech3/config/ntservice.ini with the following values: TimestampFormat = PCAP_NS PacketDescriptor = PCAP HostBufferSegmentSizeRx = 4 TODO change these lines see notes for help HostBuffersRx = [16,16,0],[16,16,1] HostBuffersTx = [16,16,0],[16,16,1] You will need to start and stop the ntservice for these changes to take effect with: /opt/napatech3/bin/ntstop.sh /opt/napatech3/bin/ntstart.sh","title":"Configure n2disk"},{"location":"High%20Speed%20Packet%20Capture/ntop%20on%20R940%20with%20Napatech/#perform-configuration-zero-copy-driver","text":"Edit the pfring configuration file with vim /etc/pf_ring/interfaces.conf and add your configuration. MANAGEMENT_INTERFACES=\"em1\" CAPTURE_INTERFACES=\"nt:0\" Open the file /etc/ntopng/ntopng.conf . If you do not have a license add --community to the end Configure the firewall to accept connections to ntopng with: firewall-cmd --zone=public --permanent --add-port=3000/tcp && firewall-cmd --reload Enable and start services with: systemctl enable redis.service systemctl enable ntopng.service systemctl enable pf_ring systemctl start redis.service systemctl start ntopng.service systemctl start pf_ring Make sure the services are running correctly with: systemctl status redis.service systemctl status ntopng.service systemctl status pf_ring","title":"Perform Configuration Zero Copy Driver"},{"location":"High%20Speed%20Packet%20Capture/ntop%20on%20R940%20with%20Napatech/#configure-license","text":"Run zcount -i nt:0 and note the serial number Output n2disk license to /etc/n2disk.license Output ntopng license to /etc/ntopng.license","title":"Configure License"},{"location":"High%20Speed%20Packet%20Capture/ntop%20on%20R940%20with%20Napatech/#useful-tips","text":"","title":"Useful Tips"},{"location":"High%20Speed%20Packet%20Capture/ntop%20on%20R940%20with%20Napatech/#hardware-filtering","text":"Napatech NICs support full-blown hardware filtering out of the box. Thanks to nBPF we convert BPF expressions to hardware filters. This feature is supported transparently, and thus all PF_RING/libpcap-over-PF_RING can benefit from it. Example: pfcount -i nt:3 -f \"tcp and port 80 and src host 192.168.1.1\"","title":"Hardware Filtering"},{"location":"High%20Speed%20Packet%20Capture/ntop%20on%20R940%20with%20Napatech/#hostbuffer-notes","text":"HostBuffersRx = [16,2048,0],[16,2048,1],[16,2048,2],[16,2048,3] HostBuffersTx = [16,2048,0],[16,2048,1],[16,2048,2],[16,2048,3] First number is the number of host buffers Second number is the size of the host buffers Third number is the NUMA node You have to have one set of numbers for each NUMA node.","title":"Hostbuffer Notes"},{"location":"High%20Speed%20Packet%20Capture/ntop%20on%20R940%20with%20Napatech/#testing-transmit-speed","text":"./pktgen -p 1 -r 10G","title":"Testing Transmit Speed"},{"location":"High%20Speed%20Packet%20Capture/ntop%20on%20R940%20with%20Napatech/#testing-the-pcap-transmit-speed","text":"To test to see if the Napatech card is up and running run this command: ./pfcount -i nt:0 ./monitoring You can press t to switch stats between receive and transmit.","title":"Testing the PCAP transmit speed"},{"location":"High%20Speed%20Packet%20Capture/ntop%20on%20R940%20with%20Napatech/#testing-receive","text":"chmod -R 777 /data rm /tmp/*-none\\.* 2>/dev/null; while true; do grep 'Dropped:\\|Slow.*:' -C50 /proc/net/pf_ring/stats/* 2>/dev/null; cp /proc/net/pf_ring/stats/*none* /tmp 2>/dev/null; sleep 1; done","title":"Testing Receive"},{"location":"High%20Speed%20Packet%20Capture/ntop%20on%20R940%20with%20Napatech/#numa-lookup","text":"Run lscpu","title":"NUMA Lookup"},{"location":"High%20Speed%20Packet%20Capture/ntop%20on%20R940%20with%20Napatech/#things-we-tried","text":"We used the to list the cpu layout and determine where we wanted to run what threads. Tests 1-6 were with the default settings for a RAID0 partition on Linux at setup time.","title":"Things we tried:"},{"location":"High%20Speed%20Packet%20Capture/ntop%20on%20R940%20with%20Napatech/#test-1","text":"This is running at 100Gb/s generation n2disk -a -v -l -o /<Storage path> -x $(date +%s.) -i nt:0 This seemed to dump everything to one thread. We maxed out and had ~82% packet loss.","title":"Test 1"},{"location":"High%20Speed%20Packet%20Capture/ntop%20on%20R940%20with%20Napatech/#test-2","text":"This is running at 100Gb/s generation n2disk -a -v -l -o /data/ -x $(date +%s.) -i nt:0 -n 5000 -m 10000 -p $((4*1024)) -b $((12*1024)) -C 4096 -c 32 -z 2,3,4,5,6,7 -Z -w 16,17,18,19,20,20,21 -S 22 Throughput results: [root@r940 bin]# cat /proc/net/pf_ring/stats/*none* Duration: 0:00:00:53:022 Throughput: 2.12 Mpps 17.65 Gbps Packets: 78780625 Filtered: 78780625 Dropped: 518512807 Bytes: 81931850000 DumpedBytes: 71137406724 DumpedFiles: 17 SlowSlavesLoops: 0 SlowStorageLoops: 432580 CaptureLoops: 19532 FirstDumpedEpoch: 0 LastDumpedEpoch: 1574185087 Worked better but we got a warning saying the time thread was on a different core than the reader/writer threads.","title":"Test 2"},{"location":"High%20Speed%20Packet%20Capture/ntop%20on%20R940%20with%20Napatech/#test-3","text":"This is running at 100Gb/s generation n2disk -a -v -l -o /data/ -x $(date +%s.) -i nt:0 -n 5000 -m 10000 -p $((4*1024)) -b $((12*1024)) -C 4096 -c 32 -z 2,3,4,5,6,7 -Z -w 16,17,18,19,20,20,21 -S 56 Worked better: 19/Nov/2019 12:41:59 [n2disk.c:1109] Average Capture Throughput: 22.49 Gbit / 2.69 Mpps","title":"Test 3"},{"location":"High%20Speed%20Packet%20Capture/ntop%20on%20R940%20with%20Napatech/#test-4","text":"This is running at 100Gb/s generation n2disk -a -v -l -o /data/ -x $(date +%s.) -i nt:0 -n 5000 -m 10000 -p $((4*1024)) -b $((12*1024)) -C 4096 -c 0 -z 8,12,16,20,24,28,32,26,40,44,48,52,56,60,64,68,72,76,80,84,88 -Z -w 92,96,100,104,108,112,116,120,124,128,132,136,140,144,148,152,156,160,164,168,172 -S 4","title":"Test 4"},{"location":"High%20Speed%20Packet%20Capture/ntop%20on%20R940%20with%20Napatech/#test-5","text":"This is running at 20Gb/s generation n2disk -a -v -l -o /data/ -x $(date +%s.) -i nt:0 -n 5000 -m 10000 -p $((4*1024)) -b $((12*1024)) -C 4096 -c 0 -z 8,12,16,20,24,28,32,26,40,44,48,52,56,60,64,68,72,76,80,84,88 -Z -w 92,96,100,104,108,112,116,120,124,128,132,136,140,144,148,152,156,160,164,168,172 -S 4 Failed. We got a lot of packet loss.","title":"Test 5"},{"location":"High%20Speed%20Packet%20Capture/ntop%20on%20R940%20with%20Napatech/#test-6","text":"This is running at 10Gb/s generation n2disk -a -v -l -o /data/ -x $(date +%s.) -i nt:0 -n 5000 -m 10000 -p $((4*1024)) -b $((12*1024)) -C 4096 -c 0 -z 8,12,16,20,24,28,32,26,40,44,48,52,56,60,64,68,72,76,80,84,88 -Z -w 92,96,100,104,108,112,116,120,124,128,132,136,140,144,148,152,156,160,164,168,172 -S 4 No packet drops.","title":"Test 6"},{"location":"High%20Speed%20Packet%20Capture/ntop%20on%20R940%20with%20Napatech/#test-7","text":"This is running at 20Gb/s generation n2disk -a -v -l -o /data/ -x $(date +%s.) -i nt:0 -n 5000 -m 10000 -p $((4*1024)) -b $((12*1024)) -C 4096 -c 0 -z 8,12,16,20,24,28,32,26,40,44,48,52,56,60,64,68,72,76,80,84,88 -Z -w 92,96,100,104,108,112,116,120,124,128,132,136,140,144,148,152,156,160,164,168,172 -S 4 This was with a data partition formatting according to the above.","title":"Test 7"},{"location":"High%20Speed%20Packet%20Capture/ntop%20on%20R940%20with%20Napatech/#test-8","text":"/opt/napatech3/bin/ntpl -e \"Delete = All\" /opt/napatech3/bin/ntpl -e \"Setup[NumaNode=2] = StreamID == 0\" # Repeat this for each stream ID. In our case 0-4 ./profiling # use this to see traffic being received # N is the NUMA node that the profiler detects the traffic on /opt/napatech3/bin/ntpl -e \"HashMode = Hash2TupleSorted\" /opt/napatech3/bin/ntpl -e \"Assign[StreamId=(0..3)] = port == 0\" The problem with this was that we couldn't get n2disk to listen on multiple interfaces. It also didn't really give us a way to split the traffic across multiple reader threads.","title":"Test 8"},{"location":"High%20Speed%20Packet%20Capture/ntop%20on%20R940%20with%20Napatech/#test-9","text":"I made four directories, one for each n2disk process I did the following: /opt/napatech3/bin/ntpl -e \"Delete = All\" /opt/napatech3/bin/ntpl -e \"Setup[NumaNode=2] = StreamID == 0\" # Repeat this for each stream ID. In our case 0-4 /opt/napatech3/bin/ntpl -e \"HashMode = Hash2TupleSorted\" /opt/napatech3/bin/ntpl -e \"Assign[StreamId=(0..3)] = port == 0\" ./profiling # use this to see traffic being received # N is the NUMA node that the profiler detects the traffic on Run the following commands: n2disk -a -v -l -o /data/data0 -x $(date +%s.) -i nt:stream0 -n 5000 -m 10000 -p $((20*1024)) -b $((40*1024)) -C 4096 -c 120 -z 0,4,8,12,16,20,24,28 -Z -w 88,92,96,100,104,108,112,116 -S 32 n2disk -a -v -l -o /data/data1 -x $(date +%s.) -i nt:stream1 -n 5000 -m 10000 -p $((20*1024)) -b $((40*1024)) -C 4096 -c 121 -z 1,5,9,13,17,21,25,29 -Z -w 89,93,97,101,105,109,113,117 -S 33 n2disk -a -v -l -o /data/data2 -x $(date +%s.) -i nt:stream2 -n 5000 -m 10000 -p $((20*1024)) -b $((40*1024)) -C 4096 -c 156 -z 36,40,44,48,52,56,60,64 -Z -w 124,128,132,136,140,144,148,152 -S 68 n2disk -a -v -l -o /data/data3 -x $(date +%s.) -i nt:stream3 -n 5000 -m 10000 -p $((20*1024)) -b $((40*1024)) -C 4096 -c 157 -z 37,41,45,49,53,57,61,65 -Z -w 125,129,133,137,141,145,149,153 -S 69 -a : Archive pcap file (rename to .old) instead of overwriting if already present on disk. -v : Verbose. -o : Directory where dump files will be saved (multiple -o can be specified) -x : Dump file prefix. -i : Ingress packet device. -n : Max number of nested dump sub-directories. -m : Max number of files before restarting file name. -p : Max pcap file length (MBytes). -b : Buffer length (MBytes). -C : Size (KB) of the chunk written to disk (must be multiple of 4096). Default: 64 KB. -c : Bind the reader thread to the specified core. -z : Enable multithread compression and/or indexing and bind thread(s) to the specified core ids (e.g. 0,1,2,3) (mandatory with indexing on Napatech cards) -Z : Compute index on the thread(s) used for compression (-z) instead of using the capture thread(s). -w : Bind the writer thread(s) to the specified core ids. A comma-separated list of cores (e.g. 0,1,2,3) should be specified in case of multiple dump directories (-o). -S : Enable time pulse thread (optimise sw packet timestamping) and bind it to the specified core. With this setup I was able to get 70Gb/s. I tested 100Gb/s and got drops. Did not perform further testing to narrow down exactly how much traffic I could push before moving on to the next test.","title":"Test 9"},{"location":"High%20Speed%20Packet%20Capture/ntop%20on%20R940%20with%20Napatech/#test-10","text":"I made seven directories, one for each n2disk process. 4 assigned to the NVMe drives and 3 assigned to the SAS SSD RAID I did the following: /opt/napatech3/bin/ntpl -e \"Delete = All\" /opt/napatech3/bin/ntpl -e \"Setup[NumaNode=2] = StreamID == 0\" # Repeat this for each stream ID. In our case 0-6 /opt/napatech3/bin/ntpl -e \"HashMode = Hash2TupleSorted\" /opt/napatech3/bin/ntpl -e \"Assign[StreamId=(0..6)] = port == 0\" ./profiling # use this to see traffic being received # N is the NUMA node that the profiler detects the traffic on watch cat /proc/net/pf_ring/stats/*none* Run the following commands: n2disk -a -v -l -o /data/data0 -x $(date +%s.) -i nt:stream0 -n 5000 -m 10000 -p $((20*1024)) -b $((40*1024)) -C 4096 -c 120 -z 0,4,8,12,16,20,24,28 -Z -w 88,92,96,100,104,108,112,116 -S 32 n2disk -a -v -l -o /data/data1 -x $(date +%s.) -i nt:stream1 -n 5000 -m 10000 -p $((20*1024)) -b $((40*1024)) -C 4096 -c 121 -z 1,5,9,13,17,21,25,29 -Z -w 89,93,97,101,105,109,113,117 -S 33 n2disk -a -v -l -o /data/data2 -x $(date +%s.) -i nt:stream2 -n 5000 -m 10000 -p $((20*1024)) -b $((40*1024)) -C 4096 -c 156 -z 36,40,44,48,52,56,60,64 -Z -w 124,128,132,136,140,144,148,152 -S 68 n2disk -a -v -l -o /data/data3 -x $(date +%s.) -i nt:stream3 -n 5000 -m 10000 -p $((20*1024)) -b $((40*1024)) -C 4096 -c 157 -z 37,41,45,49,53,57,61,65 -Z -w 125,129,133,137,141,145,149,153 -S 69 n2disk -a -v -l -o /raiddata/data0 -x $(date +%s.) -i nt:stream4 -n 5000 -m 10000 -p $((20*1024)) -b $((40*1024)) -C 4096 -c 123 -z 3,7,11,15,19,23,27,31 -Z -w 91,95,99,103,107,111,115,119 -S 35 n2disk -a -v -l -o /raiddata/data1 -x $(date +%s.) -i nt:stream5 -n 5000 -m 10000 -p $((20*1024)) -b $((40*1024)) -C 4096 -c 159 -z 39,43,47,51,55,59,63,67 -Z -w 127,131,135,139,143,147,151,155 -S 71 n2disk -a -v -l -o /raiddata/data2 -x $(date +%s.) -i nt:stream6 -n 5000 -m 10000 -p $((20*1024)) -b $((40*1024)) -C 4096 -c 174 -z 54,58,62,66,70,74,78,82 -Z -w 142,146,150,154,158,162,166,170 -S 86 -a : Archive pcap file (rename to .old) instead of overwriting if already present on disk. -v : Verbose. -o : Directory where dump files will be saved (multiple -o can be specified) -x : Dump file prefix. -i : Ingress packet device. -n : Max number of nested dump sub-directories. -m : Max number of files before restarting file name. -p : Max pcap file length (MBytes). -b : Buffer length (MBytes). -C : Size (KB) of the chunk written to disk (must be multiple of 4096). Default: 64 KB. -c : Bind the reader thread to the specified core. -z : Enable multithread compression and/or indexing and bind thread(s) to the specified core ids (e.g. 0,1,2,3) (mandatory with indexing on Napatech cards) -Z : Compute index on the thread(s) used for compression (-z) instead of using the capture thread(s). -w : Bind the writer thread(s) to the specified core ids. A comma-separated list of cores (e.g. 0,1,2,3) should be specified in case of multiple dump directories (-o). -S : Enable time pulse thread (optimise sw packet timestamping) and bind it to the specified core. With this setup I was able to get 70Gb/s. I tested 100Gb/s and got drops. Did not perform further testing to narrow down exactly how much traffic I could push before moving on to the next test.","title":"Test 10"},{"location":"High%20Speed%20Packet%20Capture/ntop%20on%20R940%20with%20Napatech/#compile-pf_ring-driver-probably-not-necessary","text":"NOTE There is a note in the documentation saying that installing from repository comes with NapaTech support. Move to opt run git clone https://github.com/ntop/PF_RING.git Move into the directory and run cd PF_RING/kernel && make && sudo insmod pf_ring.ko Next run cd ../userland/lib && ./configure && make Next run cd ../libpcap && ./configure && make Next run cd ../examples && make","title":"Compile PF_RING Driver (PROBABLY NOT NECESSARY)"},{"location":"How%20OS10%20Installer%20Works/","text":"How OS10 Installer Works The OS10 installer is a binary file with a bash stub: #!/bin/sh ####################################################################### # Dell OS10 Installer ####################################################################### ####################################################################### # OS10 Data export OS_NAME=\"Dell EMC Networking OS10 Enterprise\" export OS_VERSION=\"10.5.2.7\" export PLATFORM=\"generic-x86_64\" export ARCHITECTURE=\"x86_64\" export INTERNAL_BUILD_ID=\"Dell EMC OS10 Enterprise Edition Blueprint 1.0.0\" export BUILD_VERSION=\"10.5.2.7.374\" export BUILD_DATE=\"2021-07-28T04:48:06+0000\" ####################################################################### # Magic cookies for OS10 feature detection. DO NOT CHANGE! # !OS10!1PART! # Enable error handling set -e INSTALLER=$(realpath \"$0\") TMP_DIR=$(mktemp -d) cd $TMP_DIR # Extract installer scripts echo -n \"Initializing installer ... \" sed -e '1,/^__INSTALLER__$/d;/^__IMAGE__$/,$d' \"$INSTALLER\" | base64 -d | tar xzf - echo \"OK\" # Load the installer library files cd installer . install_support.sh install_main \"$@\" rc=\"$?\" exit $rc __INSTALLER__ <BASE_64_ENCODED_INSTALLER> __IMAGE__ <BINARY_IMAGE_HERE> What this does is grab the installer's name with INSTALLER=$(realpath \"$0\") and then extracts itself with sed -e '1,/^__INSTALLER__$/d;/^__IMAGE__$/,$d' \"$INSTALLER\" | base64 -d | tar xzf - . This grabs everything between the INSTALLER and IMAGE tags, base64 decodes it, and then extracts it with tar.","title":"How OS10 Installer Works"},{"location":"How%20OS10%20Installer%20Works/#how-os10-installer-works","text":"The OS10 installer is a binary file with a bash stub: #!/bin/sh ####################################################################### # Dell OS10 Installer ####################################################################### ####################################################################### # OS10 Data export OS_NAME=\"Dell EMC Networking OS10 Enterprise\" export OS_VERSION=\"10.5.2.7\" export PLATFORM=\"generic-x86_64\" export ARCHITECTURE=\"x86_64\" export INTERNAL_BUILD_ID=\"Dell EMC OS10 Enterprise Edition Blueprint 1.0.0\" export BUILD_VERSION=\"10.5.2.7.374\" export BUILD_DATE=\"2021-07-28T04:48:06+0000\" ####################################################################### # Magic cookies for OS10 feature detection. DO NOT CHANGE! # !OS10!1PART! # Enable error handling set -e INSTALLER=$(realpath \"$0\") TMP_DIR=$(mktemp -d) cd $TMP_DIR # Extract installer scripts echo -n \"Initializing installer ... \" sed -e '1,/^__INSTALLER__$/d;/^__IMAGE__$/,$d' \"$INSTALLER\" | base64 -d | tar xzf - echo \"OK\" # Load the installer library files cd installer . install_support.sh install_main \"$@\" rc=\"$?\" exit $rc __INSTALLER__ <BASE_64_ENCODED_INSTALLER> __IMAGE__ <BINARY_IMAGE_HERE> What this does is grab the installer's name with INSTALLER=$(realpath \"$0\") and then extracts itself with sed -e '1,/^__INSTALLER__$/d;/^__IMAGE__$/,$d' \"$INSTALLER\" | base64 -d | tar xzf - . This grabs everything between the INSTALLER and IMAGE tags, base64 decodes it, and then extracts it with tar.","title":"How OS10 Installer Works"},{"location":"How%20to%20Read%20lstopo%20and%20a%20PCIe%20Overview/","text":"How to Read lstopo and a PCIe Overview The first time I looked at lstopo , I found the output rather overwhelming so I wrote this guide to break down what I was looking at. Subject Platform Dell R840 Package The word package is synonymous with the word socket. In the R840's case there are four separate, physical, sockets. Caches (LXi, LXd, and L3) This in particular confused me at first as the nomenclature is specific to your architecture (ex: Intel or AMD). Here you see three caches LXi, LXd, and L3. L i refers to an instruction cache , L d refers to a data cache , and L3 is a mixed cache including data and instructions. In the Intel architecture the first couple of levels may be dedicated to data or instruction caches but higher levels are mixed. Cores It helps here to look at the output of lscpu [root@r8402 ~]# lscpu Architecture: x86_64 CPU op-mode(s): 32-bit, 64-bit Byte Order: Little Endian CPU(s): 80 On-line CPU(s) list: 0-79 Thread(s) per core: 2 Core(s) per socket: 10 Socket(s): 4 NUMA node(s): 4 Vendor ID: GenuineIntel BIOS Vendor ID: Intel CPU family: 6 Model: 85 Model name: Intel(R) Xeon(R) Gold 5215 CPU @ 2.50GHz BIOS Model name: Intel(R) Xeon(R) Gold 5215 CPU @ 2.50GHz Stepping: 7 CPU MHz: 2643.471 CPU max MHz: 3400.0000 CPU min MHz: 1000.0000 BogoMIPS: 5000.00 Virtualization: VT-x L1d cache: 32K L1i cache: 32K L2 cache: 1024K L3 cache: 14080K NUMA node0 CPU(s): 0,4,8,12,16,20,24,28,32,36,40,44,48,52,56,60,64,68,72,76 NUMA node1 CPU(s): 1,5,9,13,17,21,25,29,33,37,41,45,49,53,57,61,65,69,73,77 NUMA node2 CPU(s): 2,6,10,14,18,22,26,30,34,38,42,46,50,54,58,62,66,70,74,78 NUMA node3 CPU(s): 3,7,11,15,19,23,27,31,35,39,43,47,51,55,59,63,67,71,75,79 First we need to understand a bit about computer architecture. In modern servers, you have the physical processors, cores within those processors, and finally logical cores (due to HyperThreading in Intel architectures ar Simultaneous Multi-Threading [SMT] in AMD architectures). AS you can see from the output of lscpu , in my server, each processor has 10 cores and each of those cores, due to hyperthreading, has 20 logical cores (as indicated by the NUMA node field). What you are seeing in this part: are the 10 cores in each proc. Notice that the numbers are in ascending order starting with package L#0 which has cores 0-9, then L#1 has 10-19, on up to 39 in package L#3. Also notice that the output omits some of the procs for space reasons and simply says \"10x total\" to indicate that there are actually 10 procs total. You can see that each individual core: Further has PU L#0 and PU L#1. These refer to the two aforementioned logacl cores created from hyperthreading. NUMA Nodes Perhaps more confusing are the smaller numbers. In package L#0 we see P#0, P#4, P#36, P#40, P#44, and P#76 actually drawn. As other posts have mentioned, these correspond to the various NUMA Nodes . As mentioned in Wikipedia, modern architectures use some sort of cache coherent NUMA (ccNUMA). The subject can be quite complicated but a simple explanation is this: we want memory access to be faster. The best way to do this is to literally put the memory closer to the processor. NUMA nodes do this by directly attaching processors to some part of memory. The smaller numbers indicate the logical cores created by HyperThreading. From the output of lscpu you can see that all the logical processes shown in package L#0: are included in NUMA node 0. You may also notice at the top it says NUMANode L#0 P#0 (45GB). This refers to the fact that NUMA node 0 has direct access to 1/4 of the total memory or 45GB. We can confirm this by looking at cat /proc/meminfo : [root@r8402 ~]# cat /proc/meminfo MemTotal: 196207488 kB MemFree: 191452708 kB MemAvailable: 191700740 kB ...SNIP... In our example, if a process is running on Core L#0, logical processor P#36, then it can directly access any of the 45GBs directly attached to package L#0. An important note: modern operating systems are NUMA aware and will do their absolute best when you spawn a process to make sure that the memory allocated for that process will be on the same NUMA node. However, let's say that there is some larger process and that multiple threads are sharing access to memory. In this case you may have a scenario where a process is running on logical processor P#5 on package L#1 in which case that process will have to reach out from physical package of L#1 through what's called the QPI bus, through package L#0, and then gain access to the memory local to package L#0. The QPI bus is part of the Intel architecture and it provides interconnects between all the physical packages for exactly this purpose. AMD uses something called XGMI. This gets us pretty in the weeds on computer architecture because this actually changes over time. For example, on Intel's newer Skylake processors it is no longer the QPI bus but ULtra Path Interconnect that serves this function so when you're working at this level you have to pay attention to exactly what processor you are using. Fun fact: When looking at things like the Technical Guide for servers you have to account for this when looking at total PCIe lane availability. The AMD EPYC gen 2/3 processors both support up to 128 PCIe lanes per processor BUT this is a bit misleading. When you have servers like the Dell R7525 (or any other vendor's two socket AMD server like HP's DL385) the two procs are connected via XGMI however XGMI consumes 48 PCIe lanes per processor so when doing your calculus you actually only end up seeing 80 per processor for a total of 160. PCIe Background The last part of the diagram we haven't touched on is PCIe. Not to be confused with NUMA, PCIe devices also have locality to processors. Each processor has a certain number of PCIe lanes attached directly to it. Just like memory, if you have a process running on, let's say, package L#3 and it is writing to NVMe drive nvme3n1 that process will write more quickly than a process running on package L#0 which must write across the QPI bus, through package L#3, and then to the drive. To fully understand how to interpret the PCIe results, it helps to understand a few PCIe basics. This is lengthy but all the things mentioned will help you interpret a potential configuration you might see in lstopo. PCIe Root Complex All PCIe Express (PCIe) devices connect to the processor and memory through what is called the PCIe root complex It is important to understand that PCIe devices can write directly to memory in modern architectures without ever touching the CPU. This is generically called Direct Memory Access (DMA). Each PCIe device has some region of memory assigned to it. This gets pretty deep - see here for an overview. In modern computer architectures, there is more than one root complex in a system ( NOTE that link does a good job of explaining things but has references to older architectures with pieces that are no longer in existence [Ex: the platform controller hub (PCH) is no longer a thing]). There is a root complex for every processor on the system. Now, where this gets confusing is a mixing of terminology. In an attempt to abstract the very architecture-specific nature of PCIe people use different terms like host bridge, root complex, and system on chip, etc. In modern architectures, from a physical perspective, there is no longer a physically separate thing for the root complex. Modern architectures have what is called system on chip (SoC) where all that stuff is built into the processor die (including the PCIe root complex). PCIe Switches Another concept that can be confusing is that you generally have two different devices which attach to the root complex. Either a PCIe switch or a PCIe endpoint. PCIe works more or less just like a network does and like a network it has switches. Say for example that a manufacturer wants to cram more PCIe devices onto a server than the server actually has PCIe lanes. A real world example: when the new AMD Rome processors dropped, manufacturers rushed to get servers to market. There wasn't time to actually make new motherboards fully supporting all the features the Rome processor offered so they instead modified existing motherboards which lead to some sub-optimal designs which included oversubscribing PCIe (it was not a single vendor that did this - I've seen this across the board). Let's say you have a server that supports 24 NVMe drives on a single backplane but each side of the backplane only has two x8 cables. NVMe drives run at x4 speeds so if you have 12 drives per side of the backplane, you're looking at a total of 48 PCIe lanes required to not have oversubscription. However, you only have 16 lanes available to play with. What do you do? Put a PCIe switch in front of the drives. It works just like an oversubscribed network. So in this case the NVMe drives are oversubscribed at a rate of 48:16 or simplified, 3:1. Another common use is bifurcation. Let's say you have one x16 lane but you want to use two x8 devices. You can bifurcate the lane to do just that - break a single x16 lane into two x8 lanes. This is where you might also hear the terminology \"electrically x16\". For example, a vendor might make a riser that allows two x8 devices but in reality, the electrical traces will each independently support x16. The BIOS will let you reconfigure the PCIe switch to instead operate at x16 speeds and you can ignore one of the riser slots. There are a lot of other fancy uses for PCIe switches but those two are the most common. For example NVLink will let graphics cards talk directly to each other. PCIe Bridge This term is also overused and confusing. In older architectures there's a lot about connecting to legacy PCI which isn't really a concern in 2022. In 2022 all a bridge does is connect a PCIe slot to the microprocessor responsible for controlling it. Generically, it is just a hardware connection between two different buses. What is the PCIe bus : This term is also super confusing in 2022. Way back in the original PCI spec before PCIe was a thing there was a literal parallel bus. As in, you had a whole bunch of devices sharing a physical bus along with all the problems that brings (of which there are many). In PCIe devices aren't attached to this kind of bus. In fact, I find it a bit obnoxious that we even use the word bus (even though it is technically correct). When I see people use the word \"bus\" in terms of PCIe what they usually mean is the serial connection consisting of multiple bidirectional PCIe lanes connecting the PCIe device (endpoint) to the root complex or PCIe switch. Interpreting PCIe As you might be able to guess, this is extremely device specific. The numbers shown next to the wires (3.9, 1.2, etc) are the unrounded transfer speeds in gigabytes per second . The various NVMe devices are fairly self explanatory. However, when it comes to networking, this is where it gets interesting. The network device shown is actually a Dell network daughter card (NDC) and all four interfaces shown in Package L#0 are actually the same network card. I confirmed this by cross referencing the xml output of lstopo with ip a s . See below picture It would seem that under the hood, for the NDC, Dell actually ran a x4 lane to the two SFP interfaces and a x2 lane for the copper interfaces. This makes sense because those ethernet interfaces are only 1Gb/s. The rest of the devices are as follows: Bus 25:00:0 is a BOSS card Bus 00:11.5 is the platform controller hub (PCH) If you're curious why you don't see the iDRAC's ethernet interface, this is because all communication with the iDRAC, assuming you aren't using RMI, goes through the PCH. Bus 03:00.0 is the VGA port For the eagle eyed you may notice the PERC is absent - I have it disconnected right now. It would normally have shown up on package L#0. Understanding PCIe Switches vs Functions The last thing I thought was a bit hard to descipher without some cross reference Research Translation Lookaside Buffer See: https://en.wikipedia.org/wiki/Translation_lookaside_buffer A translation lookaside buffer (TLB) is a memory cache that is used to reduce the time taken to access a user memory location.[1] It is a part of the chip's memory-management unit (MMU). The TLB stores the recent translations of virtual memory to physical memory and can be called an address-translation cache. A TLB may reside between the CPU and the CPU cache, between CPU cache and the main memory or between the different levels of the multi-level cache. The majority of desktop, laptop, and server processors include one or more TLBs in the memory-management hardware, and it is nearly always present in any processor that utilizes paged or segmented virtual memory. TLB Misses See 18-notes.pdf Data and Instruction Caches See 18-notes.pdf StackExchange Explanaton of lstopo https://unix.stackexchange.com/a/113549/240147","title":"How to Read lstopo and a PCIe Overview"},{"location":"How%20to%20Read%20lstopo%20and%20a%20PCIe%20Overview/#how-to-read-lstopo-and-a-pcie-overview","text":"The first time I looked at lstopo , I found the output rather overwhelming so I wrote this guide to break down what I was looking at.","title":"How to Read lstopo and a PCIe Overview"},{"location":"How%20to%20Read%20lstopo%20and%20a%20PCIe%20Overview/#subject-platform","text":"Dell R840","title":"Subject Platform"},{"location":"How%20to%20Read%20lstopo%20and%20a%20PCIe%20Overview/#package","text":"The word package is synonymous with the word socket. In the R840's case there are four separate, physical, sockets.","title":"Package"},{"location":"How%20to%20Read%20lstopo%20and%20a%20PCIe%20Overview/#caches-lxi-lxd-and-l3","text":"This in particular confused me at first as the nomenclature is specific to your architecture (ex: Intel or AMD). Here you see three caches LXi, LXd, and L3. L i refers to an instruction cache , L d refers to a data cache , and L3 is a mixed cache including data and instructions. In the Intel architecture the first couple of levels may be dedicated to data or instruction caches but higher levels are mixed.","title":"Caches (LXi, LXd, and L3)"},{"location":"How%20to%20Read%20lstopo%20and%20a%20PCIe%20Overview/#cores","text":"It helps here to look at the output of lscpu [root@r8402 ~]# lscpu Architecture: x86_64 CPU op-mode(s): 32-bit, 64-bit Byte Order: Little Endian CPU(s): 80 On-line CPU(s) list: 0-79 Thread(s) per core: 2 Core(s) per socket: 10 Socket(s): 4 NUMA node(s): 4 Vendor ID: GenuineIntel BIOS Vendor ID: Intel CPU family: 6 Model: 85 Model name: Intel(R) Xeon(R) Gold 5215 CPU @ 2.50GHz BIOS Model name: Intel(R) Xeon(R) Gold 5215 CPU @ 2.50GHz Stepping: 7 CPU MHz: 2643.471 CPU max MHz: 3400.0000 CPU min MHz: 1000.0000 BogoMIPS: 5000.00 Virtualization: VT-x L1d cache: 32K L1i cache: 32K L2 cache: 1024K L3 cache: 14080K NUMA node0 CPU(s): 0,4,8,12,16,20,24,28,32,36,40,44,48,52,56,60,64,68,72,76 NUMA node1 CPU(s): 1,5,9,13,17,21,25,29,33,37,41,45,49,53,57,61,65,69,73,77 NUMA node2 CPU(s): 2,6,10,14,18,22,26,30,34,38,42,46,50,54,58,62,66,70,74,78 NUMA node3 CPU(s): 3,7,11,15,19,23,27,31,35,39,43,47,51,55,59,63,67,71,75,79 First we need to understand a bit about computer architecture. In modern servers, you have the physical processors, cores within those processors, and finally logical cores (due to HyperThreading in Intel architectures ar Simultaneous Multi-Threading [SMT] in AMD architectures). AS you can see from the output of lscpu , in my server, each processor has 10 cores and each of those cores, due to hyperthreading, has 20 logical cores (as indicated by the NUMA node field). What you are seeing in this part: are the 10 cores in each proc. Notice that the numbers are in ascending order starting with package L#0 which has cores 0-9, then L#1 has 10-19, on up to 39 in package L#3. Also notice that the output omits some of the procs for space reasons and simply says \"10x total\" to indicate that there are actually 10 procs total. You can see that each individual core: Further has PU L#0 and PU L#1. These refer to the two aforementioned logacl cores created from hyperthreading.","title":"Cores"},{"location":"How%20to%20Read%20lstopo%20and%20a%20PCIe%20Overview/#numa-nodes","text":"Perhaps more confusing are the smaller numbers. In package L#0 we see P#0, P#4, P#36, P#40, P#44, and P#76 actually drawn. As other posts have mentioned, these correspond to the various NUMA Nodes . As mentioned in Wikipedia, modern architectures use some sort of cache coherent NUMA (ccNUMA). The subject can be quite complicated but a simple explanation is this: we want memory access to be faster. The best way to do this is to literally put the memory closer to the processor. NUMA nodes do this by directly attaching processors to some part of memory. The smaller numbers indicate the logical cores created by HyperThreading. From the output of lscpu you can see that all the logical processes shown in package L#0: are included in NUMA node 0. You may also notice at the top it says NUMANode L#0 P#0 (45GB). This refers to the fact that NUMA node 0 has direct access to 1/4 of the total memory or 45GB. We can confirm this by looking at cat /proc/meminfo : [root@r8402 ~]# cat /proc/meminfo MemTotal: 196207488 kB MemFree: 191452708 kB MemAvailable: 191700740 kB ...SNIP... In our example, if a process is running on Core L#0, logical processor P#36, then it can directly access any of the 45GBs directly attached to package L#0. An important note: modern operating systems are NUMA aware and will do their absolute best when you spawn a process to make sure that the memory allocated for that process will be on the same NUMA node. However, let's say that there is some larger process and that multiple threads are sharing access to memory. In this case you may have a scenario where a process is running on logical processor P#5 on package L#1 in which case that process will have to reach out from physical package of L#1 through what's called the QPI bus, through package L#0, and then gain access to the memory local to package L#0. The QPI bus is part of the Intel architecture and it provides interconnects between all the physical packages for exactly this purpose. AMD uses something called XGMI. This gets us pretty in the weeds on computer architecture because this actually changes over time. For example, on Intel's newer Skylake processors it is no longer the QPI bus but ULtra Path Interconnect that serves this function so when you're working at this level you have to pay attention to exactly what processor you are using. Fun fact: When looking at things like the Technical Guide for servers you have to account for this when looking at total PCIe lane availability. The AMD EPYC gen 2/3 processors both support up to 128 PCIe lanes per processor BUT this is a bit misleading. When you have servers like the Dell R7525 (or any other vendor's two socket AMD server like HP's DL385) the two procs are connected via XGMI however XGMI consumes 48 PCIe lanes per processor so when doing your calculus you actually only end up seeing 80 per processor for a total of 160.","title":"NUMA Nodes"},{"location":"How%20to%20Read%20lstopo%20and%20a%20PCIe%20Overview/#pcie-background","text":"The last part of the diagram we haven't touched on is PCIe. Not to be confused with NUMA, PCIe devices also have locality to processors. Each processor has a certain number of PCIe lanes attached directly to it. Just like memory, if you have a process running on, let's say, package L#3 and it is writing to NVMe drive nvme3n1 that process will write more quickly than a process running on package L#0 which must write across the QPI bus, through package L#3, and then to the drive. To fully understand how to interpret the PCIe results, it helps to understand a few PCIe basics. This is lengthy but all the things mentioned will help you interpret a potential configuration you might see in lstopo.","title":"PCIe Background"},{"location":"How%20to%20Read%20lstopo%20and%20a%20PCIe%20Overview/#pcie-root-complex","text":"All PCIe Express (PCIe) devices connect to the processor and memory through what is called the PCIe root complex It is important to understand that PCIe devices can write directly to memory in modern architectures without ever touching the CPU. This is generically called Direct Memory Access (DMA). Each PCIe device has some region of memory assigned to it. This gets pretty deep - see here for an overview. In modern computer architectures, there is more than one root complex in a system ( NOTE that link does a good job of explaining things but has references to older architectures with pieces that are no longer in existence [Ex: the platform controller hub (PCH) is no longer a thing]). There is a root complex for every processor on the system. Now, where this gets confusing is a mixing of terminology. In an attempt to abstract the very architecture-specific nature of PCIe people use different terms like host bridge, root complex, and system on chip, etc. In modern architectures, from a physical perspective, there is no longer a physically separate thing for the root complex. Modern architectures have what is called system on chip (SoC) where all that stuff is built into the processor die (including the PCIe root complex).","title":"PCIe Root Complex"},{"location":"How%20to%20Read%20lstopo%20and%20a%20PCIe%20Overview/#pcie-switches","text":"Another concept that can be confusing is that you generally have two different devices which attach to the root complex. Either a PCIe switch or a PCIe endpoint. PCIe works more or less just like a network does and like a network it has switches. Say for example that a manufacturer wants to cram more PCIe devices onto a server than the server actually has PCIe lanes. A real world example: when the new AMD Rome processors dropped, manufacturers rushed to get servers to market. There wasn't time to actually make new motherboards fully supporting all the features the Rome processor offered so they instead modified existing motherboards which lead to some sub-optimal designs which included oversubscribing PCIe (it was not a single vendor that did this - I've seen this across the board). Let's say you have a server that supports 24 NVMe drives on a single backplane but each side of the backplane only has two x8 cables. NVMe drives run at x4 speeds so if you have 12 drives per side of the backplane, you're looking at a total of 48 PCIe lanes required to not have oversubscription. However, you only have 16 lanes available to play with. What do you do? Put a PCIe switch in front of the drives. It works just like an oversubscribed network. So in this case the NVMe drives are oversubscribed at a rate of 48:16 or simplified, 3:1. Another common use is bifurcation. Let's say you have one x16 lane but you want to use two x8 devices. You can bifurcate the lane to do just that - break a single x16 lane into two x8 lanes. This is where you might also hear the terminology \"electrically x16\". For example, a vendor might make a riser that allows two x8 devices but in reality, the electrical traces will each independently support x16. The BIOS will let you reconfigure the PCIe switch to instead operate at x16 speeds and you can ignore one of the riser slots. There are a lot of other fancy uses for PCIe switches but those two are the most common. For example NVLink will let graphics cards talk directly to each other.","title":"PCIe Switches"},{"location":"How%20to%20Read%20lstopo%20and%20a%20PCIe%20Overview/#pcie-bridge","text":"This term is also overused and confusing. In older architectures there's a lot about connecting to legacy PCI which isn't really a concern in 2022. In 2022 all a bridge does is connect a PCIe slot to the microprocessor responsible for controlling it. Generically, it is just a hardware connection between two different buses. What is the PCIe bus : This term is also super confusing in 2022. Way back in the original PCI spec before PCIe was a thing there was a literal parallel bus. As in, you had a whole bunch of devices sharing a physical bus along with all the problems that brings (of which there are many). In PCIe devices aren't attached to this kind of bus. In fact, I find it a bit obnoxious that we even use the word bus (even though it is technically correct). When I see people use the word \"bus\" in terms of PCIe what they usually mean is the serial connection consisting of multiple bidirectional PCIe lanes connecting the PCIe device (endpoint) to the root complex or PCIe switch.","title":"PCIe Bridge"},{"location":"How%20to%20Read%20lstopo%20and%20a%20PCIe%20Overview/#interpreting-pcie","text":"As you might be able to guess, this is extremely device specific. The numbers shown next to the wires (3.9, 1.2, etc) are the unrounded transfer speeds in gigabytes per second . The various NVMe devices are fairly self explanatory. However, when it comes to networking, this is where it gets interesting. The network device shown is actually a Dell network daughter card (NDC) and all four interfaces shown in Package L#0 are actually the same network card. I confirmed this by cross referencing the xml output of lstopo with ip a s . See below picture It would seem that under the hood, for the NDC, Dell actually ran a x4 lane to the two SFP interfaces and a x2 lane for the copper interfaces. This makes sense because those ethernet interfaces are only 1Gb/s. The rest of the devices are as follows: Bus 25:00:0 is a BOSS card Bus 00:11.5 is the platform controller hub (PCH) If you're curious why you don't see the iDRAC's ethernet interface, this is because all communication with the iDRAC, assuming you aren't using RMI, goes through the PCH. Bus 03:00.0 is the VGA port For the eagle eyed you may notice the PERC is absent - I have it disconnected right now. It would normally have shown up on package L#0.","title":"Interpreting PCIe"},{"location":"How%20to%20Read%20lstopo%20and%20a%20PCIe%20Overview/#understanding-pcie-switches-vs-functions","text":"The last thing I thought was a bit hard to descipher without some cross reference","title":"Understanding PCIe Switches vs Functions"},{"location":"How%20to%20Read%20lstopo%20and%20a%20PCIe%20Overview/#research","text":"","title":"Research"},{"location":"How%20to%20Read%20lstopo%20and%20a%20PCIe%20Overview/#translation-lookaside-buffer","text":"See: https://en.wikipedia.org/wiki/Translation_lookaside_buffer A translation lookaside buffer (TLB) is a memory cache that is used to reduce the time taken to access a user memory location.[1] It is a part of the chip's memory-management unit (MMU). The TLB stores the recent translations of virtual memory to physical memory and can be called an address-translation cache. A TLB may reside between the CPU and the CPU cache, between CPU cache and the main memory or between the different levels of the multi-level cache. The majority of desktop, laptop, and server processors include one or more TLBs in the memory-management hardware, and it is nearly always present in any processor that utilizes paged or segmented virtual memory.","title":"Translation Lookaside Buffer"},{"location":"How%20to%20Read%20lstopo%20and%20a%20PCIe%20Overview/#tlb-misses","text":"See 18-notes.pdf","title":"TLB Misses"},{"location":"How%20to%20Read%20lstopo%20and%20a%20PCIe%20Overview/#data-and-instruction-caches","text":"See 18-notes.pdf","title":"Data and Instruction Caches"},{"location":"How%20to%20Read%20lstopo%20and%20a%20PCIe%20Overview/#stackexchange-explanaton-of-lstopo","text":"https://unix.stackexchange.com/a/113549/240147","title":"StackExchange Explanaton of lstopo"},{"location":"IO%20Identities%20with%20LifeCycle%20Controller/","text":"IO Identities with LifeCycle Controller Sources Explanation On a per card basis there is a way to set virtual attributes for all of the following:\u200b Virtual MAC Address Virtual iSCSI MAC Address Virtual FIP MAC Address Virtual WWN Virtual WWPN When you create an identity pool in OME, it leverages this capability under the hood to make it happen. The way it works is Dell seems to provide an upper API capability described here: https://downloads.dell.com/manuals/common/dell-simple_nic_profile.pdf\u200b via the idrac with a thing called CIM (Common Information Model - see https://downloads.dell.com/solutions/general-solution-resources/White%20Papers/Dell_SystemInfo_Profile.pdf). The common information model is basically our standardized way of representing resources on the idrac. One of those resources is \"Simple NIC profile\": \u200b You can see from the spec sheet that the following functions have to be implemented for any given NIC: The DCIM_NICService class is the one that implements the SetAttributes method mentioned in the white paper. The white paper explores how this is done for each card, but the TLDR is that we really do have a card-specific API implementation for all our major vendors and that is how we are changing this stuff under the hood.","title":"IO Identities with LifeCycle Controller"},{"location":"IO%20Identities%20with%20LifeCycle%20Controller/#io-identities-with-lifecycle-controller","text":"","title":"IO Identities with LifeCycle Controller"},{"location":"IO%20Identities%20with%20LifeCycle%20Controller/#sources","text":"","title":"Sources"},{"location":"IO%20Identities%20with%20LifeCycle%20Controller/#explanation","text":"On a per card basis there is a way to set virtual attributes for all of the following:\u200b Virtual MAC Address Virtual iSCSI MAC Address Virtual FIP MAC Address Virtual WWN Virtual WWPN When you create an identity pool in OME, it leverages this capability under the hood to make it happen. The way it works is Dell seems to provide an upper API capability described here: https://downloads.dell.com/manuals/common/dell-simple_nic_profile.pdf\u200b via the idrac with a thing called CIM (Common Information Model - see https://downloads.dell.com/solutions/general-solution-resources/White%20Papers/Dell_SystemInfo_Profile.pdf). The common information model is basically our standardized way of representing resources on the idrac. One of those resources is \"Simple NIC profile\": \u200b You can see from the spec sheet that the following functions have to be implemented for any given NIC: The DCIM_NICService class is the one that implements the SetAttributes method mentioned in the white paper. The white paper explores how this is done for each card, but the TLDR is that we really do have a card-specific API implementation for all our major vendors and that is how we are changing this stuff under the hood.","title":"Explanation"},{"location":"Importing%20Elasticsearch%20Data/","text":"Importing Elasticsearch Data WARNING These instructions are rough. Move your Elasticsearch snapshot folder to a known folder. For our example, I used /opt/snapshots . Next you will need to tell Elasticsearch about this path by adding the path.repo directive in the configuration file. Run: vim /etc/elasticsearch/elasticsearch.yml # Add `path.repo: [\"/opt/snapshots\"]` Now you need to import the data itself into Elasticsearch. Import with: curl -X PUT \"localhost:9200/_snapshot/esdata?pretty\" -H 'Content-Type: application/json' -d' { \"type\": \"fs\", \"settings\": { \"location\": \"/opt/snapshots\", \"compress\": true } } ' Note: It took about 5 minutes for the snapshot to load Next you will need to restore the snapshot by doing the following: Go to management, saved objects, import, select file on your local computer Import dashboards into Elasticsearch curl -O http://192.168.122.1:8000/kibana-export.json Helpful Commands View a list of the snapshots: curl localhost:9200/_cat/snapshots/esdata curl -X GET \"localhost:9200/_snapshot/_status?pretty\" Restore the snapshot with: curl -X POST \"localhost:9200/_snapshot/esdata/snapshot_1/_restore?pretty\" Install Moloch Run yum install -y https://files.molo.ch/builds/centos-7/moloch-1.1.1-1.x86_64.rpm Configure Moloch with /data/moloch/bin/Configure Run /data/moloch/db/db.pl http://ESHOST:9200 init to init the cluster Add user /data/moloch/bin/moloch_add_user.sh admin \"Admin User\" password --admin Import your data with ./bin/moloch-capture -c etc/config.ini -R <YOUR_DIRECTORY> . You can import individual files with lowercase -r.","title":"Importing Elasticsearch Data"},{"location":"Importing%20Elasticsearch%20Data/#importing-elasticsearch-data","text":"WARNING These instructions are rough. Move your Elasticsearch snapshot folder to a known folder. For our example, I used /opt/snapshots . Next you will need to tell Elasticsearch about this path by adding the path.repo directive in the configuration file. Run: vim /etc/elasticsearch/elasticsearch.yml # Add `path.repo: [\"/opt/snapshots\"]` Now you need to import the data itself into Elasticsearch. Import with: curl -X PUT \"localhost:9200/_snapshot/esdata?pretty\" -H 'Content-Type: application/json' -d' { \"type\": \"fs\", \"settings\": { \"location\": \"/opt/snapshots\", \"compress\": true } } ' Note: It took about 5 minutes for the snapshot to load Next you will need to restore the snapshot by doing the following: Go to management, saved objects, import, select file on your local computer Import dashboards into Elasticsearch curl -O http://192.168.122.1:8000/kibana-export.json","title":"Importing Elasticsearch Data"},{"location":"Importing%20Elasticsearch%20Data/#helpful-commands","text":"View a list of the snapshots: curl localhost:9200/_cat/snapshots/esdata curl -X GET \"localhost:9200/_snapshot/_status?pretty\" Restore the snapshot with: curl -X POST \"localhost:9200/_snapshot/esdata/snapshot_1/_restore?pretty\"","title":"Helpful Commands"},{"location":"Importing%20Elasticsearch%20Data/#install-moloch","text":"Run yum install -y https://files.molo.ch/builds/centos-7/moloch-1.1.1-1.x86_64.rpm Configure Moloch with /data/moloch/bin/Configure Run /data/moloch/db/db.pl http://ESHOST:9200 init to init the cluster Add user /data/moloch/bin/moloch_add_user.sh admin \"Admin User\" password --admin Import your data with ./bin/moloch-capture -c etc/config.ini -R <YOUR_DIRECTORY> . You can import individual files with lowercase -r.","title":"Install Moloch"},{"location":"Installing%20DPDK%20with%20NapaTech%20Card/","text":"Installing DPDK with NapaTech Card System Info I wrote this on CentOS 7 Release CentOS Linux release 7.7.1908 (Core) NAME=\"CentOS Linux\" VERSION=\"7 (Core)\" ID=\"centos\" ID_LIKE=\"rhel fedora\" VERSION_ID=\"7\" PRETTY_NAME=\"CentOS Linux 7 (Core)\" ANSI_COLOR=\"0;31\" CPE_NAME=\"cpe:/o:centos:centos:7\" HOME_URL=\"https://www.centos.org/\" BUG_REPORT_URL=\"https://bugs.centos.org/\" CENTOS_MANTISBT_PROJECT=\"CentOS-7\" CENTOS_MANTISBT_PROJECT_VERSION=\"7\" REDHAT_SUPPORT_PRODUCT=\"centos\" REDHAT_SUPPORT_PRODUCT_VERSION=\"7\" CentOS Linux release 7.7.1908 (Core) CentOS Linux release 7.7.1908 (Core) Kernel Linux r840-1.lan 3.10.0-1062.9.1.el7.x86_64 #1 SMP Fri Dec 6 15:49:49 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux Helpful Documents NUMA Nodes and NapaTech Optimizing the NapaTech Card Settings Install NapaTech Driver Download the Napatech software from here Run yum groupinstall \"Development Tools\" && yum install -y kernel-devel gettext-devel openssl-devel perl-CPAN perl-devel zlib-devel pciutils && yum install -y https://centos7.iuscommunity.org/ius-release.rpm && yum remove -y git && yum install -y git2u-all Unzip and run package_install_3gd.sh Run /opt/napatech3/bin/ntstop.sh && /opt/napatech3/bin/ntstart.sh to restart the NapaTech driver which will create a default configuration file. Update Host Buffers The /opt/napatech3/config/ntservice.ini contains the following settings that control the number and size of hostbuffers. To increase the number of CPUs/Cores which we can leverage with the SmartNIC, we need to increase the number of hostbuffers. Host buffers in Napatech Software Suite are the buffers used for moving (DMA) data packets between the accelerator and the applications. In the config file it should look like this: HostBuffersRx = [64,16,-1] # [x1, x2, x3], ... HostBuffersTx = [64,16,-1] # [x1, x2, x3], ... First number is the number of host buffers Second number is the size of the host buffers in MegaBytes Third number is the NUMA node You have to have one set of numbers for each NUMA node. -1 means read the NUMA node setting in the NumaNode directive in the configuration file. HostBuffersRx = [16,2048,0],[16,2048,1],[16,2048,2],[16,2048,3] HostBuffersTx = [16,2048,0],[16,2048,1],[16,2048,2],[16,2048,3] You can check what NUMA node your card is on with: cat \"/sys/bus/pci/devices/0000:5b:00.0/numa_node\" . You'll just have to replace the bus ID with your card's bus ID. You can retrieve that with the /opt/napatech3/bin/adapterinfo command. Install DPDK export NAPATECH3_PATH=/opt/napatech3 Download with git clone https://github.com/napatech/dpdk.git I put this in /opt and will assume you have done the same in this guide. Edit the security limits with vim /etc/security/limits.conf . After making the below edits you will need to log out and log back in for them to take effect. Add the following lines at the end of the file. This assumes you are running as root: root hard memlock unlimited root soft memlock unlimited Run yum install -y gcc numactl-devel kernel-devel pciutils elfutils-libelf-devel make libpcap python3 tar vim wget tmux vim mlocate hwloc libpcap-devel python36-devel set the environment variable RTE_SDK. It is the directory in which you extracted all the DPDK files. export RTE_SDK=/opt/dpdk (or your directory) Run make config T=x86_64-native-linuxapp-gcc install CONFIG_RTE_LIBRTE_PMD_PCAP=y CONFIG_RTE_LIBRTE_PDUMP=y DESTDIR=install CONFIG_RTE_LIBRTE_PMD_NTACC=y to build dpdk. Ensure your install directory exists. make -j NOTE : The option CONFIG_RTE_LIBRTE_PMD_PCAP=y enabled libpcap support in DPDK. This is required for pdump to work. Once an DPDK target environment directory has been created (such as x86_64-native-linux-gcc), it contains all libraries and header files required to build an application. When compiling an application in the Linux* environment on the DPDK, the following variables must be exported: RTE_TARGET - Points to the DPDK target environment directory. export RTE_TARGET=/opt/dpdk-19.08/x86_64-native-linux-gcc You may want to add this variable and RTE_SDK to ~/.bash_profile Update Your Bash Profile Add: export RTE_SDK=/opt/dpdk export NAPATECH3_PATH=/opt/napatech3 export RTE_TARGET=/opt/dpdk/x86_64-native-linuxapp-gcc to ~/.bash_profile Install ELF Tools Run the following: pip3 install numpy pip3 install elftools pip3 install pyelftools Configuration Configure Ports Move to your dpdk dir and run ./install/share/dpdk/usertools/dpdk-setup.sh . This should give you a menu with all available DPDK options. The menu is setup in such a way that you must perform each step listed in the menu. If things have gone correctly to this point your Step 1 should look like the following: ---------------------------------------------------------- Step 1: Select the DPDK environment to build ---------------------------------------------------------- [1] * My menu looks like this: ---------------------------------------------------------- Step 1: Select the DPDK environment to build ---------------------------------------------------------- [1] * ---------------------------------------------------------- Step 2: Setup linux environment ---------------------------------------------------------- [2] Insert IGB UIO module [3] Insert VFIO module [4] Insert KNI module [5] Setup hugepage mappings for non-NUMA systems [6] Setup hugepage mappings for NUMA systems [7] Display current Ethernet/Baseband/Crypto device settings [8] Bind Ethernet/Baseband/Crypto device to IGB UIO module [9] Bind Ethernet/Baseband/Crypto device to VFIO module [10] Setup VFIO permissions ---------------------------------------------------------- Step 3: Run test application for linux environment ---------------------------------------------------------- [11] Run test application ($RTE_TARGET/app/test) [12] Run testpmd application in interactive mode ($RTE_TARGET/app/testpmd) ---------------------------------------------------------- Step 4: Other tools ---------------------------------------------------------- [13] List hugepage info from /proc/meminfo ---------------------------------------------------------- Step 5: Uninstall and system cleanup ---------------------------------------------------------- [14] Unbind devices from IGB UIO or VFIO driver [15] Remove IGB UIO module [16] Remove VFIO module [17] Remove KNI module [18] Remove hugepage mappings [19] Exit Script Next run option 6 to instert huge pages for NUMA systems. Notice you will be prompted to select an amount of memory on a per processor basis. This is because there are pages associated with each individual processor to increase performance via locality. I used a value of 4096 for each NUMA node. For 1GB per NUMA node insert 512.","title":"Installing DPDK with NapaTech Card"},{"location":"Installing%20DPDK%20with%20NapaTech%20Card/#installing-dpdk-with-napatech-card","text":"","title":"Installing DPDK with NapaTech Card"},{"location":"Installing%20DPDK%20with%20NapaTech%20Card/#system-info","text":"I wrote this on CentOS 7","title":"System Info"},{"location":"Installing%20DPDK%20with%20NapaTech%20Card/#release","text":"CentOS Linux release 7.7.1908 (Core) NAME=\"CentOS Linux\" VERSION=\"7 (Core)\" ID=\"centos\" ID_LIKE=\"rhel fedora\" VERSION_ID=\"7\" PRETTY_NAME=\"CentOS Linux 7 (Core)\" ANSI_COLOR=\"0;31\" CPE_NAME=\"cpe:/o:centos:centos:7\" HOME_URL=\"https://www.centos.org/\" BUG_REPORT_URL=\"https://bugs.centos.org/\" CENTOS_MANTISBT_PROJECT=\"CentOS-7\" CENTOS_MANTISBT_PROJECT_VERSION=\"7\" REDHAT_SUPPORT_PRODUCT=\"centos\" REDHAT_SUPPORT_PRODUCT_VERSION=\"7\" CentOS Linux release 7.7.1908 (Core) CentOS Linux release 7.7.1908 (Core)","title":"Release"},{"location":"Installing%20DPDK%20with%20NapaTech%20Card/#kernel","text":"Linux r840-1.lan 3.10.0-1062.9.1.el7.x86_64 #1 SMP Fri Dec 6 15:49:49 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux","title":"Kernel"},{"location":"Installing%20DPDK%20with%20NapaTech%20Card/#helpful-documents","text":"NUMA Nodes and NapaTech Optimizing the NapaTech Card Settings","title":"Helpful Documents"},{"location":"Installing%20DPDK%20with%20NapaTech%20Card/#install-napatech-driver","text":"Download the Napatech software from here Run yum groupinstall \"Development Tools\" && yum install -y kernel-devel gettext-devel openssl-devel perl-CPAN perl-devel zlib-devel pciutils && yum install -y https://centos7.iuscommunity.org/ius-release.rpm && yum remove -y git && yum install -y git2u-all Unzip and run package_install_3gd.sh Run /opt/napatech3/bin/ntstop.sh && /opt/napatech3/bin/ntstart.sh to restart the NapaTech driver which will create a default configuration file.","title":"Install NapaTech Driver"},{"location":"Installing%20DPDK%20with%20NapaTech%20Card/#update-host-buffers","text":"The /opt/napatech3/config/ntservice.ini contains the following settings that control the number and size of hostbuffers. To increase the number of CPUs/Cores which we can leverage with the SmartNIC, we need to increase the number of hostbuffers. Host buffers in Napatech Software Suite are the buffers used for moving (DMA) data packets between the accelerator and the applications. In the config file it should look like this: HostBuffersRx = [64,16,-1] # [x1, x2, x3], ... HostBuffersTx = [64,16,-1] # [x1, x2, x3], ... First number is the number of host buffers Second number is the size of the host buffers in MegaBytes Third number is the NUMA node You have to have one set of numbers for each NUMA node. -1 means read the NUMA node setting in the NumaNode directive in the configuration file. HostBuffersRx = [16,2048,0],[16,2048,1],[16,2048,2],[16,2048,3] HostBuffersTx = [16,2048,0],[16,2048,1],[16,2048,2],[16,2048,3] You can check what NUMA node your card is on with: cat \"/sys/bus/pci/devices/0000:5b:00.0/numa_node\" . You'll just have to replace the bus ID with your card's bus ID. You can retrieve that with the /opt/napatech3/bin/adapterinfo command.","title":"Update Host Buffers"},{"location":"Installing%20DPDK%20with%20NapaTech%20Card/#install-dpdk","text":"export NAPATECH3_PATH=/opt/napatech3 Download with git clone https://github.com/napatech/dpdk.git I put this in /opt and will assume you have done the same in this guide. Edit the security limits with vim /etc/security/limits.conf . After making the below edits you will need to log out and log back in for them to take effect. Add the following lines at the end of the file. This assumes you are running as root: root hard memlock unlimited root soft memlock unlimited Run yum install -y gcc numactl-devel kernel-devel pciutils elfutils-libelf-devel make libpcap python3 tar vim wget tmux vim mlocate hwloc libpcap-devel python36-devel set the environment variable RTE_SDK. It is the directory in which you extracted all the DPDK files. export RTE_SDK=/opt/dpdk (or your directory) Run make config T=x86_64-native-linuxapp-gcc install CONFIG_RTE_LIBRTE_PMD_PCAP=y CONFIG_RTE_LIBRTE_PDUMP=y DESTDIR=install CONFIG_RTE_LIBRTE_PMD_NTACC=y to build dpdk. Ensure your install directory exists. make -j NOTE : The option CONFIG_RTE_LIBRTE_PMD_PCAP=y enabled libpcap support in DPDK. This is required for pdump to work. Once an DPDK target environment directory has been created (such as x86_64-native-linux-gcc), it contains all libraries and header files required to build an application. When compiling an application in the Linux* environment on the DPDK, the following variables must be exported: RTE_TARGET - Points to the DPDK target environment directory. export RTE_TARGET=/opt/dpdk-19.08/x86_64-native-linux-gcc You may want to add this variable and RTE_SDK to ~/.bash_profile","title":"Install DPDK"},{"location":"Installing%20DPDK%20with%20NapaTech%20Card/#update-your-bash-profile","text":"Add: export RTE_SDK=/opt/dpdk export NAPATECH3_PATH=/opt/napatech3 export RTE_TARGET=/opt/dpdk/x86_64-native-linuxapp-gcc to ~/.bash_profile","title":"Update Your Bash Profile"},{"location":"Installing%20DPDK%20with%20NapaTech%20Card/#install-elf-tools","text":"Run the following: pip3 install numpy pip3 install elftools pip3 install pyelftools","title":"Install ELF Tools"},{"location":"Installing%20DPDK%20with%20NapaTech%20Card/#configuration","text":"","title":"Configuration"},{"location":"Installing%20DPDK%20with%20NapaTech%20Card/#configure-ports","text":"Move to your dpdk dir and run ./install/share/dpdk/usertools/dpdk-setup.sh . This should give you a menu with all available DPDK options. The menu is setup in such a way that you must perform each step listed in the menu. If things have gone correctly to this point your Step 1 should look like the following: ---------------------------------------------------------- Step 1: Select the DPDK environment to build ---------------------------------------------------------- [1] * My menu looks like this: ---------------------------------------------------------- Step 1: Select the DPDK environment to build ---------------------------------------------------------- [1] * ---------------------------------------------------------- Step 2: Setup linux environment ---------------------------------------------------------- [2] Insert IGB UIO module [3] Insert VFIO module [4] Insert KNI module [5] Setup hugepage mappings for non-NUMA systems [6] Setup hugepage mappings for NUMA systems [7] Display current Ethernet/Baseband/Crypto device settings [8] Bind Ethernet/Baseband/Crypto device to IGB UIO module [9] Bind Ethernet/Baseband/Crypto device to VFIO module [10] Setup VFIO permissions ---------------------------------------------------------- Step 3: Run test application for linux environment ---------------------------------------------------------- [11] Run test application ($RTE_TARGET/app/test) [12] Run testpmd application in interactive mode ($RTE_TARGET/app/testpmd) ---------------------------------------------------------- Step 4: Other tools ---------------------------------------------------------- [13] List hugepage info from /proc/meminfo ---------------------------------------------------------- Step 5: Uninstall and system cleanup ---------------------------------------------------------- [14] Unbind devices from IGB UIO or VFIO driver [15] Remove IGB UIO module [16] Remove VFIO module [17] Remove KNI module [18] Remove hugepage mappings [19] Exit Script Next run option 6 to instert huge pages for NUMA systems. Notice you will be prompted to select an amount of memory on a per processor basis. This is because there are pages associated with each individual processor to increase performance via locality. I used a value of 4096 for each NUMA node. For 1GB per NUMA node insert 512.","title":"Configure Ports"},{"location":"LDAP%20with%20OpenManage/","text":"LDAP with OpenManage My Environment RHEL Version NAME=\"Red Hat Enterprise Linux\" VERSION=\"8.2 (Ootpa)\" ID=\"rhel\" ID_LIKE=\"fedora\" VERSION_ID=\"8.2\" PLATFORM_ID=\"platform:el8\" PRETTY_NAME=\"Red Hat Enterprise Linux 8.2 (Ootpa)\" ANSI_COLOR=\"0;31\" CPE_NAME=\"cpe:/o:redhat:enterprise_linux:8.2:GA\" HOME_URL=\"https://www.redhat.com/\" BUG_REPORT_URL=\"https://bugzilla.redhat.com/\" REDHAT_BUGZILLA_PRODUCT=\"Red Hat Enterprise Linux 8\" REDHAT_BUGZILLA_PRODUCT_VERSION=8.2 REDHAT_SUPPORT_PRODUCT=\"Red Hat Enterprise Linux\" REDHAT_SUPPORT_PRODUCT_VERSION=\"8.2\" Red Hat Enterprise Linux release 8.2 (Ootpa) Red Hat Enterprise Linux release 8.2 (Ootpa) FreeIPA Version [root@centos ~]# ipa --version VERSION: 4.8.4, API_VERSION: 2.235 OpenManage Version Version 3.4.1 (Build 24) Helpful Resources Dell Tutorial Logs Explained LDAP Result Codes Helpful Post on Bind DN Install Instructions Install RHEL Change hostname hostname freeipa.grant.lan && hostnamectl set-hostname freeipa.grant.lan Change in /etc/hostname Configure DNS to return for this hostname. Double check with dig +short freeipa.grant.lan A && dig +short -x 192.168.1.95 Follow RHEL's instructions I used Chapter 5 for primary installation Make sure you add the requested DNS entries at the end Run firewall-cmd --permanent --add-port={80/tcp,443/tcp,389/tcp,636/tcp,88/tcp,464/tcp,88/udp,464/udp,123/udp} && firewall-cmd --reload to allow the appropriate ports Run kinit admin - this allows you to use the command line tools otherwise they'll complain about kerberos. Log into FreeIPA server at https://<your_hostname> . In my case, Windows popped up a username and password prompt. That prompt didn't work - I had to exit it and then log into the webGUI. Add a user other than administrator. Go to Users and then directory services in OpenManage. I used the following: Note: You can get the Bind DN by running ldapsearch from the command line. Create a new user and new group in the UI and assign the new user to the new group. Install OpenManage Go to Application Settings -> Directory Services Substitute with your values and then click test. I wasn't able to get this to work with the generic admin user. In the test screen I used that new user to connect to directory services Helpful Commands To start the IPA service use ipactl start|stop|restart . You can check the status with ipactl status . Test LDAP credentials: ldapwhoami -vvv -h 192.168.1.95 -p 389 -D 'uid=grant,cn=users,cn=accounts,dc=grant,dc=lan' -x -w <PASSWORD> Dump the structure of FreeIPA: ldapsearch -x -H ldap://localhost -b \"cn=accounts,dc=grant,dc=lan\" -D \"uid=grant,cn=users,cn=accounts,dc=grant,dc=lan\" -w <YOUR_USERS_PASSWORD> | less You can also use this to check to see if a user is a member of a group by changing the base path to search (-b). Change it to what you think the FQDN of the group is. The user in question should show up. Update FreeIPA Schema Request received: [23/Oct/2020:07:12:13.558111497 -0400] conn=22 fd=101 slot=101 SSL connection from 192.168.1.93 to 192.168.1.95 [23/Oct/2020:07:12:13.605452505 -0400] conn=22 TLS1.2 128-bit AES-GCM [23/Oct/2020:07:12:13.607423551 -0400] conn=22 op=0 BIND dn=\"uid=grant,cn=users,cn=accounts,dc=grant,dc=lan\" method=128 version=3 [23/Oct/2020:07:12:13.633734712 -0400] conn=22 op=0 RESULT err=0 tag=97 nentries=0 etime=0.075252172 dn=\"uid=grant,cn=users,cn=accounts,dc=grant,dc=lan\" [23/Oct/2020:07:12:13.634444660 -0400] conn=22 op=1 SRCH base=\"cn=accounts,dc=grant,dc=lan\" scope=2 filter=\"(uid=grant)\" attrs=ALL [23/Oct/2020:07:12:13.636377426 -0400] conn=22 op=1 RESULT err=0 tag=101 nentries=1 etime=0.001994528 [23/Oct/2020:07:12:13.637728783 -0400] conn=22 op=2 SRCH base=\"cn=accounts,dc=grant,dc=lan\" scope=2 filter=\"(|(member=uid=grant,cn=users,cn=accounts,dc=grant,dc=lan)(member=grant))\" attrs=ALL [23/Oct/2020:07:12:13.638251981 -0400] conn=22 op=2 RESULT err=0 tag=101 nentries=2 etime=0.000588138 [23/Oct/2020:07:12:13.640124457 -0400] conn=23 fd=103 slot=103 SSL connection from 192.168.1.93 to 192.168.1.95 [23/Oct/2020:07:12:13.684231843 -0400] conn=23 TLS1.2 128-bit AES-GCM [23/Oct/2020:07:12:13.685087831 -0400] conn=23 op=0 BIND dn=\"uid=grant,cn=users,cn=accounts,dc=grant,dc=lan\" method=128 version=3 [23/Oct/2020:07:12:13.685478906 -0400] conn=23 op=0 RESULT err=0 tag=97 nentries=0 etime=0.044995726 dn=\"uid=grant,cn=users,cn=accounts,dc=grant,dc=lan\" [23/Oct/2020:07:12:13.686411703 -0400] conn=23 op=1 SRCH base=\"cn=accounts,dc=grant,dc=lan\" scope=2 filter=\"(|(member=uid=grant,cn=users,cn=accounts,dc=grant,dc=lan)(member=grant))\" attrs=\"entryuuid cn\" [23/Oct/2020:07:12:13.686675968 -0400] conn=23 op=1 RESULT err=0 tag=101 nentries=2 etime=0.000324070 [23/Oct/2020:07:12:13.687691917 -0400] conn=23 op=2 UNBIND [23/Oct/2020:07:12:13.687704782 -0400] conn=23 op=2 fd=103 closed - U1 sed -i -e \"s/NAME 'ipaUniqueID'/NAME ('ipaUniqueID' 'entryUUID')/\" /etc/dirsrv/slapd-*/schema/60basev2.ldif ipa-ldap-updater -u --schema-file=$(ls /etc/dirsrv/slapd-*/schema/60basev2.ldif) After making the above modification I now get: [23/Oct/2020:08:16:51.891366602 -0400] conn=30 fd=101 slot=101 SSL connection from 192.168.1.93 to 192.168.1.95 [23/Oct/2020:08:16:51.938289338 -0400] conn=30 TLS1.2 128-bit AES-GCM [23/Oct/2020:08:16:51.939209205 -0400] conn=30 op=0 BIND dn=\"uid=grant,cn=users,cn=accounts,dc=grant,dc=lan\" method=128 version=3 [23/Oct/2020:08:16:51.965507560 -0400] conn=30 op=0 RESULT err=49 tag=97 nentries=0 etime=0.073775627 - Invalid credentials [23/Oct/2020:08:16:51.966077916 -0400] conn=30 op=-1 fd=101 closed - B1 [23/Oct/2020:08:16:51.970300516 -0400] conn=31 fd=101 slot=101 SSL connection from 192.168.1.93 to 192.168.1.95 [23/Oct/2020:08:16:52.015244484 -0400] conn=31 TLS1.2 128-bit AES-GCM [23/Oct/2020:08:16:52.015949382 -0400] conn=31 op=0 BIND dn=\"\" method=128 version=3 [23/Oct/2020:08:16:52.016032928 -0400] conn=31 op=0 RESULT err=0 tag=97 nentries=0 etime=0.045312604 dn=\"\" [23/Oct/2020:08:16:52.017337373 -0400] conn=31 op=1 SRCH base=\"cn=accounts,dc=grant,dc=lan\" scope=2 filter=\"(|(member=null)(member=grant))\" attrs=\"ipaUniqueID cn\" [23/Oct/2020:08:16:52.017436879 -0400] conn=31 op=1 RESULT err=0 tag=101 nentries=0 etime=0.000150394 [23/Oct/2020:08:16:52.018338463 -0400] conn=31 op=2 UNBIND [23/Oct/2020:08:16:52.018350155 -0400] conn=31 op=2 fd=101 closed - U1 [23/Oct/2020:08:20:26.725516663 -0400] conn=21 op=2 SRCH base=\"ou=sessions,ou=Security Domain,o=ipaca\" scope=2 filter=\"(objectClass=securityDomainSessionEntry)\" attrs=\"cn\" [23/Oct/2020:08:20:26.725732986 -0400] conn=21 op=2 RESULT err=32 tag=101 nentries=0 etime=0.000316769","title":"LDAP with OpenManage"},{"location":"LDAP%20with%20OpenManage/#ldap-with-openmanage","text":"","title":"LDAP with OpenManage"},{"location":"LDAP%20with%20OpenManage/#my-environment","text":"","title":"My Environment"},{"location":"LDAP%20with%20OpenManage/#rhel-version","text":"NAME=\"Red Hat Enterprise Linux\" VERSION=\"8.2 (Ootpa)\" ID=\"rhel\" ID_LIKE=\"fedora\" VERSION_ID=\"8.2\" PLATFORM_ID=\"platform:el8\" PRETTY_NAME=\"Red Hat Enterprise Linux 8.2 (Ootpa)\" ANSI_COLOR=\"0;31\" CPE_NAME=\"cpe:/o:redhat:enterprise_linux:8.2:GA\" HOME_URL=\"https://www.redhat.com/\" BUG_REPORT_URL=\"https://bugzilla.redhat.com/\" REDHAT_BUGZILLA_PRODUCT=\"Red Hat Enterprise Linux 8\" REDHAT_BUGZILLA_PRODUCT_VERSION=8.2 REDHAT_SUPPORT_PRODUCT=\"Red Hat Enterprise Linux\" REDHAT_SUPPORT_PRODUCT_VERSION=\"8.2\" Red Hat Enterprise Linux release 8.2 (Ootpa) Red Hat Enterprise Linux release 8.2 (Ootpa)","title":"RHEL Version"},{"location":"LDAP%20with%20OpenManage/#freeipa-version","text":"[root@centos ~]# ipa --version VERSION: 4.8.4, API_VERSION: 2.235","title":"FreeIPA Version"},{"location":"LDAP%20with%20OpenManage/#openmanage-version","text":"Version 3.4.1 (Build 24)","title":"OpenManage Version"},{"location":"LDAP%20with%20OpenManage/#helpful-resources","text":"Dell Tutorial Logs Explained LDAP Result Codes Helpful Post on Bind DN","title":"Helpful Resources"},{"location":"LDAP%20with%20OpenManage/#install-instructions","text":"Install RHEL Change hostname hostname freeipa.grant.lan && hostnamectl set-hostname freeipa.grant.lan Change in /etc/hostname Configure DNS to return for this hostname. Double check with dig +short freeipa.grant.lan A && dig +short -x 192.168.1.95 Follow RHEL's instructions I used Chapter 5 for primary installation Make sure you add the requested DNS entries at the end Run firewall-cmd --permanent --add-port={80/tcp,443/tcp,389/tcp,636/tcp,88/tcp,464/tcp,88/udp,464/udp,123/udp} && firewall-cmd --reload to allow the appropriate ports Run kinit admin - this allows you to use the command line tools otherwise they'll complain about kerberos. Log into FreeIPA server at https://<your_hostname> . In my case, Windows popped up a username and password prompt. That prompt didn't work - I had to exit it and then log into the webGUI. Add a user other than administrator. Go to Users and then directory services in OpenManage. I used the following: Note: You can get the Bind DN by running ldapsearch from the command line. Create a new user and new group in the UI and assign the new user to the new group. Install OpenManage Go to Application Settings -> Directory Services Substitute with your values and then click test. I wasn't able to get this to work with the generic admin user. In the test screen I used that new user to connect to directory services","title":"Install Instructions"},{"location":"LDAP%20with%20OpenManage/#helpful-commands","text":"To start the IPA service use ipactl start|stop|restart . You can check the status with ipactl status . Test LDAP credentials: ldapwhoami -vvv -h 192.168.1.95 -p 389 -D 'uid=grant,cn=users,cn=accounts,dc=grant,dc=lan' -x -w <PASSWORD> Dump the structure of FreeIPA: ldapsearch -x -H ldap://localhost -b \"cn=accounts,dc=grant,dc=lan\" -D \"uid=grant,cn=users,cn=accounts,dc=grant,dc=lan\" -w <YOUR_USERS_PASSWORD> | less You can also use this to check to see if a user is a member of a group by changing the base path to search (-b). Change it to what you think the FQDN of the group is. The user in question should show up.","title":"Helpful Commands"},{"location":"LDAP%20with%20OpenManage/#update-freeipa-schema","text":"Request received: [23/Oct/2020:07:12:13.558111497 -0400] conn=22 fd=101 slot=101 SSL connection from 192.168.1.93 to 192.168.1.95 [23/Oct/2020:07:12:13.605452505 -0400] conn=22 TLS1.2 128-bit AES-GCM [23/Oct/2020:07:12:13.607423551 -0400] conn=22 op=0 BIND dn=\"uid=grant,cn=users,cn=accounts,dc=grant,dc=lan\" method=128 version=3 [23/Oct/2020:07:12:13.633734712 -0400] conn=22 op=0 RESULT err=0 tag=97 nentries=0 etime=0.075252172 dn=\"uid=grant,cn=users,cn=accounts,dc=grant,dc=lan\" [23/Oct/2020:07:12:13.634444660 -0400] conn=22 op=1 SRCH base=\"cn=accounts,dc=grant,dc=lan\" scope=2 filter=\"(uid=grant)\" attrs=ALL [23/Oct/2020:07:12:13.636377426 -0400] conn=22 op=1 RESULT err=0 tag=101 nentries=1 etime=0.001994528 [23/Oct/2020:07:12:13.637728783 -0400] conn=22 op=2 SRCH base=\"cn=accounts,dc=grant,dc=lan\" scope=2 filter=\"(|(member=uid=grant,cn=users,cn=accounts,dc=grant,dc=lan)(member=grant))\" attrs=ALL [23/Oct/2020:07:12:13.638251981 -0400] conn=22 op=2 RESULT err=0 tag=101 nentries=2 etime=0.000588138 [23/Oct/2020:07:12:13.640124457 -0400] conn=23 fd=103 slot=103 SSL connection from 192.168.1.93 to 192.168.1.95 [23/Oct/2020:07:12:13.684231843 -0400] conn=23 TLS1.2 128-bit AES-GCM [23/Oct/2020:07:12:13.685087831 -0400] conn=23 op=0 BIND dn=\"uid=grant,cn=users,cn=accounts,dc=grant,dc=lan\" method=128 version=3 [23/Oct/2020:07:12:13.685478906 -0400] conn=23 op=0 RESULT err=0 tag=97 nentries=0 etime=0.044995726 dn=\"uid=grant,cn=users,cn=accounts,dc=grant,dc=lan\" [23/Oct/2020:07:12:13.686411703 -0400] conn=23 op=1 SRCH base=\"cn=accounts,dc=grant,dc=lan\" scope=2 filter=\"(|(member=uid=grant,cn=users,cn=accounts,dc=grant,dc=lan)(member=grant))\" attrs=\"entryuuid cn\" [23/Oct/2020:07:12:13.686675968 -0400] conn=23 op=1 RESULT err=0 tag=101 nentries=2 etime=0.000324070 [23/Oct/2020:07:12:13.687691917 -0400] conn=23 op=2 UNBIND [23/Oct/2020:07:12:13.687704782 -0400] conn=23 op=2 fd=103 closed - U1 sed -i -e \"s/NAME 'ipaUniqueID'/NAME ('ipaUniqueID' 'entryUUID')/\" /etc/dirsrv/slapd-*/schema/60basev2.ldif ipa-ldap-updater -u --schema-file=$(ls /etc/dirsrv/slapd-*/schema/60basev2.ldif) After making the above modification I now get: [23/Oct/2020:08:16:51.891366602 -0400] conn=30 fd=101 slot=101 SSL connection from 192.168.1.93 to 192.168.1.95 [23/Oct/2020:08:16:51.938289338 -0400] conn=30 TLS1.2 128-bit AES-GCM [23/Oct/2020:08:16:51.939209205 -0400] conn=30 op=0 BIND dn=\"uid=grant,cn=users,cn=accounts,dc=grant,dc=lan\" method=128 version=3 [23/Oct/2020:08:16:51.965507560 -0400] conn=30 op=0 RESULT err=49 tag=97 nentries=0 etime=0.073775627 - Invalid credentials [23/Oct/2020:08:16:51.966077916 -0400] conn=30 op=-1 fd=101 closed - B1 [23/Oct/2020:08:16:51.970300516 -0400] conn=31 fd=101 slot=101 SSL connection from 192.168.1.93 to 192.168.1.95 [23/Oct/2020:08:16:52.015244484 -0400] conn=31 TLS1.2 128-bit AES-GCM [23/Oct/2020:08:16:52.015949382 -0400] conn=31 op=0 BIND dn=\"\" method=128 version=3 [23/Oct/2020:08:16:52.016032928 -0400] conn=31 op=0 RESULT err=0 tag=97 nentries=0 etime=0.045312604 dn=\"\" [23/Oct/2020:08:16:52.017337373 -0400] conn=31 op=1 SRCH base=\"cn=accounts,dc=grant,dc=lan\" scope=2 filter=\"(|(member=null)(member=grant))\" attrs=\"ipaUniqueID cn\" [23/Oct/2020:08:16:52.017436879 -0400] conn=31 op=1 RESULT err=0 tag=101 nentries=0 etime=0.000150394 [23/Oct/2020:08:16:52.018338463 -0400] conn=31 op=2 UNBIND [23/Oct/2020:08:16:52.018350155 -0400] conn=31 op=2 fd=101 closed - U1 [23/Oct/2020:08:20:26.725516663 -0400] conn=21 op=2 SRCH base=\"ou=sessions,ou=Security Domain,o=ipaca\" scope=2 filter=\"(objectClass=securityDomainSessionEntry)\" attrs=\"cn\" [23/Oct/2020:08:20:26.725732986 -0400] conn=21 op=2 RESULT err=32 tag=101 nentries=0 etime=0.000316769","title":"Update FreeIPA Schema"},{"location":"LDAP%20with%20OpenManage/README-CentOS-OpenLDAP/","text":"Setting Up OpenLDAP with OpenManage My Environment CentOS Version CentOS Linux release 8.2.2004 (Core) NAME=\"CentOS Linux\" VERSION=\"8 (Core)\" ID=\"centos\" ID_LIKE=\"rhel fedora\" VERSION_ID=\"8\" PLATFORM_ID=\"platform:el8\" PRETTY_NAME=\"CentOS Linux 8 (Core)\" ANSI_COLOR=\"0;31\" CPE_NAME=\"cpe:/o:centos:centos:8\" HOME_URL=\"https://www.centos.org/\" BUG_REPORT_URL=\"https://bugs.centos.org/\" CENTOS_MANTISBT_PROJECT=\"CentOS-8\" CENTOS_MANTISBT_PROJECT_VERSION=\"8\" REDHAT_SUPPORT_PRODUCT=\"centos\" REDHAT_SUPPORT_PRODUCT_VERSION=\"8\" CentOS Linux release 8.2.2004 (Core) CentOS Linux release 8.2.2004 (Core) IPA Version [root@centos ~]# ipa --version VERSION: 4.8.4, API_VERSION: 2.235 OpenManage Version Version 3.5.0 (Build 60) Helpful Resources Dell Tutorial Logs Explained LDAP Result Codes Helpful Post on Bind DN OpenManage User's Guide Install Instructions Install CentOS8 I installed CentOS minimal Make sure NTP is working correctly Install OpenManage For the OpenLDAP setup I followed Install and Setup OpenLDAP on CentOS 8 by koromicha I want the current version of OpenLDAP so I'll be building it from source. Install dependencies with dnf install -y cyrus-sasl-devel make libtool autoconf libtool-ltdl-devel openssl-devel libdb-devel tar gcc perl perl-devel wget vim Create non privileged system user: useradd -r -M -d /var/lib/openldap -u 55 -s /usr/sbin/nologin ldap Pull tarball wget https://www.openldap.org/software/download/OpenLDAP/openldap-release/openldap-2.4.54.tgz && tar xzf openldap-2.4.54.tgz && cd openldap-2.4.54 Build and install OpenLDAP ./configure --prefix=/usr --sysconfdir=/etc --disable-static --enable-debug --with-tls=openssl --with-cyrus-sasl --enable-dynamic --enable-crypt --enable-spasswd --enable-slapd --enable-modules --enable-rlookups --enable-backends=mod --disable-ndb --disable-sql --disable-shell --disable-bdb --disable-hdb --enable-overlays=mod && make depend && make -j2 && make install Configure OpenLDAP mkdir /var/lib/openldap /etc/openldap/slapd.d && chown -R ldap:ldap /var/lib/openldap && chown root:ldap /etc/openldap/slapd.conf Add an OpenLDAP systemd service vim /etc/systemd/system/slapd.service [Unit] Description=OpenLDAP Server Daemon After=syslog.target network-online.target Documentation=man:slapd Documentation=man:slapd-mdb [Service] Type=forking PIDFile=/var/lib/openldap/slapd.pid Environment=\"SLAPD_URLS=ldap:/// ldapi:/// ldaps:///\" Environment=\"SLAPD_OPTIONS=-F /etc/openldap/slapd.d\" ExecStart=/usr/libexec/slapd -u ldap -g ldap -h ${SLAPD_URLS} $SLAPD_OPTIONS [Install] WantedBy=multi-user.target Check if your version of sudo supports lday with sudo -V | grep -i \"ldap\" and confirm the below lines are present: ldap.conf path: /etc/sudo-ldap.conf ldap.secret path: /etc/ldap.secret Make sure LDAP sudo schema is available with rpm -ql sudo | grep -i schema.openldap . You should see /usr/share/doc/sudo/schema.OpenLDAP Run: cp /usr/share/doc/sudo/schema.OpenLDAP /etc/openldap/schema/sudo.schema cat << 'EOL' > /etc/openldap/schema/sudo.ldif dn: cn=sudo,cn=schema,cn=config objectClass: olcSchemaConfig cn: sudo olcAttributeTypes: ( 1.3.6.1.4.1.15953.9.1.1 NAME 'sudoUser' DESC 'User(s) who may run sudo' EQUALITY caseExactIA5Match SUBSTR caseExactIA5SubstringsMatch SYNTAX 1.3.6.1.4.1.1466.115.121.1.26 ) olcAttributeTypes: ( 1.3.6.1.4.1.15953.9.1.2 NAME 'sudoHost' DESC 'Host(s) who may run sudo' EQUALITY caseExactIA5Match SUBSTR caseExactIA5SubstringsMatch SYNTAX 1.3.6.1.4.1.1466.115.121.1.26 ) olcAttributeTypes: ( 1.3.6.1.4.1.15953.9.1.3 NAME 'sudoCommand' DESC 'Command(s) to be executed by sudo' EQUALITY caseExactIA5Match SYNTAX 1.3.6.1.4.1.1466.115.121.1.26 ) olcAttributeTypes: ( 1.3.6.1.4.1.15953.9.1.4 NAME 'sudoRunAs' DESC 'User(s) impersonated by sudo (deprecated)' EQUALITY caseExactIA5Match SYNTAX 1.3.6.1.4.1.1466.115.121.1.26 ) olcAttributeTypes: ( 1.3.6.1.4.1.15953.9.1.5 NAME 'sudoOption' DESC 'Options(s) followed by sudo' EQUALITY caseExactIA5Match SYNTAX 1.3.6.1.4.1.1466.115.121.1.26 ) olcAttributeTypes: ( 1.3.6.1.4.1.15953.9.1.6 NAME 'sudoRunAsUser' DESC 'User(s) impersonated by sudo' EQUALITY caseExactIA5Match SYNTAX 1.3.6.1.4.1.1466.115.121.1.26 ) olcAttributeTypes: ( 1.3.6.1.4.1.15953.9.1.7 NAME 'sudoRunAsGroup' DESC 'Group(s) impersonated by sudo' EQUALITY caseExactIA5Match SYNTAX 1.3.6.1.4.1.1466.115.121.1.26 ) olcObjectClasses: ( 1.3.6.1.4.1.15953.9.2.1 NAME 'sudoRole' SUP top STRUCTURAL DESC 'Sudoer Entries' MUST ( cn ) MAY ( sudoUser $ sudoHost $ sudoCommand $ sudoRunAs $ sudoRunAsUser $ sudoRunAsGroup $ sudoOption $ description ) ) EOL mv /etc/openldap/slapd.ldif /etc/openldap/slapd.ldif.bak vim /etc/openldap/slapd.ldif dn: cn=config objectClass: olcGlobal cn: config olcArgsFile: /var/lib/openldap/slapd.args olcPidFile: /var/lib/openldap/slapd.pid dn: cn=schema,cn=config objectClass: olcSchemaConfig cn: schema dn: cn=module,cn=config objectClass: olcModuleList cn: module olcModulepath: /usr/libexec/openldap olcModuleload: back_mdb.la include: file:///etc/openldap/schema/core.ldif include: file:///etc/openldap/schema/cosine.ldif include: file:///etc/openldap/schema/nis.ldif include: file:///etc/openldap/schema/inetorgperson.ldif include: file:///etc/openldap/schema/ppolicy.ldif include: file:///etc/openldap/schema/sudo.ldif dn: olcDatabase=frontend,cn=config objectClass: olcDatabaseConfig objectClass: olcFrontendConfig olcDatabase: frontend olcAccess: to dn.base=\"cn=Subschema\" by * read olcAccess: to * by dn.base=\"gidNumber=0+uidNumber=0,cn=peercred,cn=external,cn=auth\" manage by * none dn: olcDatabase=config,cn=config objectClass: olcDatabaseConfig olcDatabase: config olcRootDN: cn=config olcAccess: to * by dn.base=\"gidNumber=0+uidNumber=0,cn=peercred,cn=external,cn=auth\" manage by * none Make sure slapadd -n 0 -F /etc/openldap/slapd.d -l /etc/openldap/slapd.ldif -u runs without error Run slapadd -n 0 -F /etc/openldap/slapd.d -l /etc/openldap/slapd.ldif && chown -R ldap:ldap /etc/openldap/slapd.d/* && systemctl daemon-reload && systemctl enable --now slapd && systemctl status slapd SLAPD LOGGING NOT WORKING Run slappasswd and note the hash output. Helpful Commands To start the IPA service use ipactl start|stop|restart . You can check the status with ipactl status .","title":"Setting Up OpenLDAP with OpenManage"},{"location":"LDAP%20with%20OpenManage/README-CentOS-OpenLDAP/#setting-up-openldap-with-openmanage","text":"","title":"Setting Up OpenLDAP with OpenManage"},{"location":"LDAP%20with%20OpenManage/README-CentOS-OpenLDAP/#my-environment","text":"","title":"My Environment"},{"location":"LDAP%20with%20OpenManage/README-CentOS-OpenLDAP/#centos-version","text":"CentOS Linux release 8.2.2004 (Core) NAME=\"CentOS Linux\" VERSION=\"8 (Core)\" ID=\"centos\" ID_LIKE=\"rhel fedora\" VERSION_ID=\"8\" PLATFORM_ID=\"platform:el8\" PRETTY_NAME=\"CentOS Linux 8 (Core)\" ANSI_COLOR=\"0;31\" CPE_NAME=\"cpe:/o:centos:centos:8\" HOME_URL=\"https://www.centos.org/\" BUG_REPORT_URL=\"https://bugs.centos.org/\" CENTOS_MANTISBT_PROJECT=\"CentOS-8\" CENTOS_MANTISBT_PROJECT_VERSION=\"8\" REDHAT_SUPPORT_PRODUCT=\"centos\" REDHAT_SUPPORT_PRODUCT_VERSION=\"8\" CentOS Linux release 8.2.2004 (Core) CentOS Linux release 8.2.2004 (Core)","title":"CentOS Version"},{"location":"LDAP%20with%20OpenManage/README-CentOS-OpenLDAP/#ipa-version","text":"[root@centos ~]# ipa --version VERSION: 4.8.4, API_VERSION: 2.235","title":"IPA Version"},{"location":"LDAP%20with%20OpenManage/README-CentOS-OpenLDAP/#openmanage-version","text":"Version 3.5.0 (Build 60)","title":"OpenManage Version"},{"location":"LDAP%20with%20OpenManage/README-CentOS-OpenLDAP/#helpful-resources","text":"Dell Tutorial Logs Explained LDAP Result Codes Helpful Post on Bind DN OpenManage User's Guide","title":"Helpful Resources"},{"location":"LDAP%20with%20OpenManage/README-CentOS-OpenLDAP/#install-instructions","text":"Install CentOS8 I installed CentOS minimal Make sure NTP is working correctly Install OpenManage For the OpenLDAP setup I followed Install and Setup OpenLDAP on CentOS 8 by koromicha I want the current version of OpenLDAP so I'll be building it from source. Install dependencies with dnf install -y cyrus-sasl-devel make libtool autoconf libtool-ltdl-devel openssl-devel libdb-devel tar gcc perl perl-devel wget vim Create non privileged system user: useradd -r -M -d /var/lib/openldap -u 55 -s /usr/sbin/nologin ldap Pull tarball wget https://www.openldap.org/software/download/OpenLDAP/openldap-release/openldap-2.4.54.tgz && tar xzf openldap-2.4.54.tgz && cd openldap-2.4.54 Build and install OpenLDAP ./configure --prefix=/usr --sysconfdir=/etc --disable-static --enable-debug --with-tls=openssl --with-cyrus-sasl --enable-dynamic --enable-crypt --enable-spasswd --enable-slapd --enable-modules --enable-rlookups --enable-backends=mod --disable-ndb --disable-sql --disable-shell --disable-bdb --disable-hdb --enable-overlays=mod && make depend && make -j2 && make install Configure OpenLDAP mkdir /var/lib/openldap /etc/openldap/slapd.d && chown -R ldap:ldap /var/lib/openldap && chown root:ldap /etc/openldap/slapd.conf Add an OpenLDAP systemd service vim /etc/systemd/system/slapd.service [Unit] Description=OpenLDAP Server Daemon After=syslog.target network-online.target Documentation=man:slapd Documentation=man:slapd-mdb [Service] Type=forking PIDFile=/var/lib/openldap/slapd.pid Environment=\"SLAPD_URLS=ldap:/// ldapi:/// ldaps:///\" Environment=\"SLAPD_OPTIONS=-F /etc/openldap/slapd.d\" ExecStart=/usr/libexec/slapd -u ldap -g ldap -h ${SLAPD_URLS} $SLAPD_OPTIONS [Install] WantedBy=multi-user.target Check if your version of sudo supports lday with sudo -V | grep -i \"ldap\" and confirm the below lines are present: ldap.conf path: /etc/sudo-ldap.conf ldap.secret path: /etc/ldap.secret Make sure LDAP sudo schema is available with rpm -ql sudo | grep -i schema.openldap . You should see /usr/share/doc/sudo/schema.OpenLDAP Run: cp /usr/share/doc/sudo/schema.OpenLDAP /etc/openldap/schema/sudo.schema cat << 'EOL' > /etc/openldap/schema/sudo.ldif dn: cn=sudo,cn=schema,cn=config objectClass: olcSchemaConfig cn: sudo olcAttributeTypes: ( 1.3.6.1.4.1.15953.9.1.1 NAME 'sudoUser' DESC 'User(s) who may run sudo' EQUALITY caseExactIA5Match SUBSTR caseExactIA5SubstringsMatch SYNTAX 1.3.6.1.4.1.1466.115.121.1.26 ) olcAttributeTypes: ( 1.3.6.1.4.1.15953.9.1.2 NAME 'sudoHost' DESC 'Host(s) who may run sudo' EQUALITY caseExactIA5Match SUBSTR caseExactIA5SubstringsMatch SYNTAX 1.3.6.1.4.1.1466.115.121.1.26 ) olcAttributeTypes: ( 1.3.6.1.4.1.15953.9.1.3 NAME 'sudoCommand' DESC 'Command(s) to be executed by sudo' EQUALITY caseExactIA5Match SYNTAX 1.3.6.1.4.1.1466.115.121.1.26 ) olcAttributeTypes: ( 1.3.6.1.4.1.15953.9.1.4 NAME 'sudoRunAs' DESC 'User(s) impersonated by sudo (deprecated)' EQUALITY caseExactIA5Match SYNTAX 1.3.6.1.4.1.1466.115.121.1.26 ) olcAttributeTypes: ( 1.3.6.1.4.1.15953.9.1.5 NAME 'sudoOption' DESC 'Options(s) followed by sudo' EQUALITY caseExactIA5Match SYNTAX 1.3.6.1.4.1.1466.115.121.1.26 ) olcAttributeTypes: ( 1.3.6.1.4.1.15953.9.1.6 NAME 'sudoRunAsUser' DESC 'User(s) impersonated by sudo' EQUALITY caseExactIA5Match SYNTAX 1.3.6.1.4.1.1466.115.121.1.26 ) olcAttributeTypes: ( 1.3.6.1.4.1.15953.9.1.7 NAME 'sudoRunAsGroup' DESC 'Group(s) impersonated by sudo' EQUALITY caseExactIA5Match SYNTAX 1.3.6.1.4.1.1466.115.121.1.26 ) olcObjectClasses: ( 1.3.6.1.4.1.15953.9.2.1 NAME 'sudoRole' SUP top STRUCTURAL DESC 'Sudoer Entries' MUST ( cn ) MAY ( sudoUser $ sudoHost $ sudoCommand $ sudoRunAs $ sudoRunAsUser $ sudoRunAsGroup $ sudoOption $ description ) ) EOL mv /etc/openldap/slapd.ldif /etc/openldap/slapd.ldif.bak vim /etc/openldap/slapd.ldif dn: cn=config objectClass: olcGlobal cn: config olcArgsFile: /var/lib/openldap/slapd.args olcPidFile: /var/lib/openldap/slapd.pid dn: cn=schema,cn=config objectClass: olcSchemaConfig cn: schema dn: cn=module,cn=config objectClass: olcModuleList cn: module olcModulepath: /usr/libexec/openldap olcModuleload: back_mdb.la include: file:///etc/openldap/schema/core.ldif include: file:///etc/openldap/schema/cosine.ldif include: file:///etc/openldap/schema/nis.ldif include: file:///etc/openldap/schema/inetorgperson.ldif include: file:///etc/openldap/schema/ppolicy.ldif include: file:///etc/openldap/schema/sudo.ldif dn: olcDatabase=frontend,cn=config objectClass: olcDatabaseConfig objectClass: olcFrontendConfig olcDatabase: frontend olcAccess: to dn.base=\"cn=Subschema\" by * read olcAccess: to * by dn.base=\"gidNumber=0+uidNumber=0,cn=peercred,cn=external,cn=auth\" manage by * none dn: olcDatabase=config,cn=config objectClass: olcDatabaseConfig olcDatabase: config olcRootDN: cn=config olcAccess: to * by dn.base=\"gidNumber=0+uidNumber=0,cn=peercred,cn=external,cn=auth\" manage by * none Make sure slapadd -n 0 -F /etc/openldap/slapd.d -l /etc/openldap/slapd.ldif -u runs without error Run slapadd -n 0 -F /etc/openldap/slapd.d -l /etc/openldap/slapd.ldif && chown -R ldap:ldap /etc/openldap/slapd.d/* && systemctl daemon-reload && systemctl enable --now slapd && systemctl status slapd SLAPD LOGGING NOT WORKING Run slappasswd and note the hash output.","title":"Install Instructions"},{"location":"LDAP%20with%20OpenManage/README-CentOS-OpenLDAP/#helpful-commands","text":"To start the IPA service use ipactl start|stop|restart . You can check the status with ipactl status .","title":"Helpful Commands"},{"location":"LDAP%20with%20OpenManage/README-FreeIPA-CENTOS/","text":"Setting Up FreeIPA with OpenManage Conclusion: Currently FreeIPA isn't supported or tested against OpenManage. See the User's Guide page 137. I'm going to try it with OpenLDAP My Environment CentOS Version CentOS Linux release 8.2.2004 (Core) NAME=\"CentOS Linux\" VERSION=\"8 (Core)\" ID=\"centos\" ID_LIKE=\"rhel fedora\" VERSION_ID=\"8\" PLATFORM_ID=\"platform:el8\" PRETTY_NAME=\"CentOS Linux 8 (Core)\" ANSI_COLOR=\"0;31\" CPE_NAME=\"cpe:/o:centos:centos:8\" HOME_URL=\"https://www.centos.org/\" BUG_REPORT_URL=\"https://bugs.centos.org/\" CENTOS_MANTISBT_PROJECT=\"CentOS-8\" CENTOS_MANTISBT_PROJECT_VERSION=\"8\" REDHAT_SUPPORT_PRODUCT=\"centos\" REDHAT_SUPPORT_PRODUCT_VERSION=\"8\" CentOS Linux release 8.2.2004 (Core) CentOS Linux release 8.2.2004 (Core) FreeIPA Version [root@centos ~]# ipa --version VERSION: 4.8.4, API_VERSION: 2.235 OpenManage Version Version 3.4.1 (Build 24) Helpful Resources Dell Tutorial Logs Explained LDAP Result Codes Helpful Post on Bind DN Install Instructions Install CentOS8 I installed CentOS minimal Make sure NTP is working correctly Install OpenManage Install the idm system module with dnf install -y @idm:DL1 freeipa-server Configure your DNS server ( /etc/hosts did not work for me) with a record for the hostname of your FreeIPA server. I added a record for centos.grant.lan . Run ipa-server-install If you have any DNS failures edit the file /tmp/ipa.system.records.tu5qyl09.db (you may have to change the name) and add the record centos.grant.lan 86400 IN A 192.168.1.92 (adjust accordingly). Afterwards run ipa dns-update-system-records Run kinit admin Open firewall ports firewall-cmd --add-port=80/tcp --permanent --zone=public firewall-cmd --add-port=443/tcp --permanent --zone=public firewall-cmd --add-port=389/tcp --permanent --zone=public firewall-cmd --add-port=636/tcp --permanent --zone=public firewall-cmd --add-port=88/tcp --permanent --zone=public firewall-cmd --add-port=464/tcp --permanent --zone=public firewall-cmd --add-port=88/udp --permanent --zone=public firewall-cmd --add-port=464/udp --permanent --zone=public firewall-cmd --add-port=123/udp --permanent --zone=public firewall-cmd --reload Log into FreeIPA server at https://centos.grant.lan . In my case, Windows popped up a username and password prompt. That prompt didn't work - I had to exit it and then log into the webGUI. Under Active users I added an admin user. Go to Users and then directory services in OpenManage. I used the following: Note: You can get the Bind DN by running ldapsearch from the command line. Helpful Commands To start the IPA service use ipactl start|stop|restart . You can check the status with ipactl status . Testing Scenario 1 This got me a working test connection: Output [15/Oct/2020:14:09:18.440103345 -0400] conn=107 fd=74 slot=74 SSL connection from 192.168.1.93 to 192.168.1.92 [15/Oct/2020:14:09:18.484254084 -0400] conn=107 TLS1.2 128-bit AES-GCM [15/Oct/2020:14:09:18.484862509 -0400] conn=107 op=0 BIND dn=\"\" method=128 version=3 [15/Oct/2020:14:09:18.485048511 -0400] conn=107 op=0 RESULT err=0 tag=97 nentries=0 etime=0.044677059 dn=\"\" [15/Oct/2020:14:09:18.485743204 -0400] conn=107 op=1 SRCH base=\"dc=grant,dc=lan\" scope=2 filter=\"(uid=grant)\" attrs=ALL [15/Oct/2020:14:09:18.487440884 -0400] conn=107 op=1 RESULT err=0 tag=101 nentries=1 etime=0.001795760 [15/Oct/2020:14:09:18.488143502 -0400] conn=107 op=2 UNBIND [15/Oct/2020:14:09:18.488159848 -0400] conn=107 op=2 fd=74 closed - U1 [15/Oct/2020:14:09:18.491313979 -0400] conn=108 fd=74 slot=74 SSL connection from 192.168.1.93 to 192.168.1.92 [15/Oct/2020:14:09:18.536590087 -0400] conn=108 TLS1.2 128-bit AES-GCM [15/Oct/2020:14:09:18.537372005 -0400] conn=108 op=0 BIND dn=\"uid=grant,cn=users,cn=compat,dc=grant,dc=lan\" method=128 version=3 [15/Oct/2020:14:09:18.538144502 -0400] conn=108 op=0 RESULT err=0 tag=97 nentries=0 etime=0.046223517 dn=\"uid=grant,cn=users,cn=accounts,dc=grant,dc=lan\" [15/Oct/2020:14:09:18.538536207 -0400] conn=108 op=1 UNBIND [15/Oct/2020:14:09:18.538566004 -0400] conn=108 op=1 fd=74 closed - U1 [15/Oct/2020:14:09:28.961238173 -0400] conn=109 fd=74 slot=74 SSL connection from 192.168.1.93 to 192.168.1.92 [15/Oct/2020:14:09:29.005228025 -0400] conn=109 TLS1.2 128-bit AES-GCM [15/Oct/2020:14:09:29.005755286 -0400] conn=109 op=0 BIND dn=\"\" method=128 version=3 [15/Oct/2020:14:09:29.005931161 -0400] conn=109 op=0 RESULT err=0 tag=97 nentries=0 etime=0.044397507 dn=\"\" [15/Oct/2020:14:09:29.006898618 -0400] conn=109 op=1 SRCH base=\"dc=grant,dc=lan\" scope=2 filter=\"(uid=grant)\" attrs=ALL [15/Oct/2020:14:09:29.008536186 -0400] conn=109 op=1 RESULT err=0 tag=101 nentries=1 etime=0.001740822 [15/Oct/2020:14:09:29.009182689 -0400] conn=109 op=2 UNBIND [15/Oct/2020:14:09:29.009196697 -0400] conn=109 op=2 fd=74 closed - U1 [15/Oct/2020:14:09:29.012428320 -0400] conn=110 fd=74 slot=74 SSL connection from 192.168.1.93 to 192.168.1.92 [15/Oct/2020:14:09:29.057469132 -0400] conn=110 TLS1.2 128-bit AES-GCM [15/Oct/2020:14:09:29.058084024 -0400] conn=110 op=0 BIND dn=\"uid=grant,cn=users,cn=compat,dc=grant,dc=lan\" method=128 version=3 [15/Oct/2020:14:09:29.058825635 -0400] conn=110 op=0 RESULT err=0 tag=97 nentries=0 etime=0.046016675 dn=\"uid=grant,cn=users,cn=accounts,dc=grant,dc=lan\" [15/Oct/2020:14:09:29.059166870 -0400] conn=110 op=1 UNBIND [15/Oct/2020:14:09:29.059195118 -0400] conn=110 op=1 fd=74 closed - U1 Scenario 2 When I added a Bind DN as shown in this video I get a failure. Error Message Log Output [15/Oct/2020:14:12:07.504942397 -0400] conn=112 fd=74 slot=74 SSL connection from 192.168.1.93 to 192.168.1.92 [15/Oct/2020:14:12:07.550456498 -0400] conn=112 TLS1.2 128-bit AES-GCM [15/Oct/2020:14:12:07.551077266 -0400] conn=112 op=0 BIND dn=\"uid=grant,cn=users,cn=accounts,dc=grant,dc=lan\" method=128 version=3 [15/Oct/2020:14:12:07.551767359 -0400] conn=112 op=0 RESULT err=0 tag=97 nentries=0 etime=0.046428458 dn=\"uid=grant,cn=users,cn=accounts,dc=grant,dc=lan\" [15/Oct/2020:14:12:07.552283396 -0400] conn=112 op=1 SRCH base=\"dc=grant,dc=lan\" scope=2 filter=\"(uid=grant)\" attrs=ALL [15/Oct/2020:14:12:07.553922212 -0400] conn=112 op=1 RESULT err=0 tag=101 nentries=1 etime=0.001752227 [15/Oct/2020:14:12:07.555518270 -0400] conn=112 op=2 UNBIND [15/Oct/2020:14:12:07.555534158 -0400] conn=112 op=2 fd=74 closed - U1 Scenario 3 - Current Sticking Point Using the settings from scenario 1, I continued. When trying to add a group though, no groups are displayed in available groups. Output [15/Oct/2020:15:19:49.806462707 -0400] conn=169 fd=103 slot=103 SSL connection from 192.168.1.18 to 192.168.1.92 [15/Oct/2020:15:19:49.856773316 -0400] conn=169 TLS1.2 128-bit AES-GCM [15/Oct/2020:15:19:49.858681446 -0400] conn=169 op=0 BIND dn=\"\" method=128 version=3 [15/Oct/2020:15:19:49.858906917 -0400] conn=169 op=0 RESULT err=0 tag=97 nentries=0 etime=0.051584941 dn=\"\" [15/Oct/2020:15:19:49.864335125 -0400] conn=169 op=1 SRCH base=\"dc=grant,dc=lan\" scope=2 filter=\"(uid=grant)\" attrs=ALL [15/Oct/2020:15:19:49.866001831 -0400] conn=169 op=1 RESULT err=0 tag=101 nentries=1 etime=0.001790286 [15/Oct/2020:15:19:49.867368889 -0400] conn=169 op=2 UNBIND [15/Oct/2020:15:19:49.867386461 -0400] conn=169 op=2 fd=103 closed - U1 [15/Oct/2020:15:19:49.873375865 -0400] conn=170 fd=103 slot=103 SSL connection from 192.168.1.18 to 192.168.1.92 [15/Oct/2020:15:19:49.925956643 -0400] conn=170 TLS1.2 128-bit AES-GCM [15/Oct/2020:15:19:49.928180447 -0400] conn=170 op=0 BIND dn=\"uid=grant,cn=users,cn=compat,dc=grant,dc=lan\" method=128 version=3 [15/Oct/2020:15:19:49.928993026 -0400] conn=170 op=0 RESULT err=0 tag=97 nentries=0 etime=0.053916831 dn=\"uid=grant,cn=users,cn=accounts,dc=grant,dc=lan\" [15/Oct/2020:15:19:49.934942058 -0400] conn=170 op=1 SRCH base=\"dc=grant,dc=lan\" scope=2 filter=\"(&(cn=grantgroup*)(uniqueMember=*))\" attrs=\"cn entryuuid\" [15/Oct/2020:15:19:49.935438340 -0400] conn=170 op=1 RESULT err=0 tag=101 nentries=0 etime=0.000639539 [15/Oct/2020:15:19:49.936729725 -0400] conn=170 op=2 UNBIND [15/Oct/2020:15:19:49.936744908 -0400] conn=170 op=2 fd=103 closed - U1","title":"Setting Up FreeIPA with OpenManage"},{"location":"LDAP%20with%20OpenManage/README-FreeIPA-CENTOS/#setting-up-freeipa-with-openmanage","text":"Conclusion: Currently FreeIPA isn't supported or tested against OpenManage. See the User's Guide page 137. I'm going to try it with OpenLDAP","title":"Setting Up FreeIPA with OpenManage"},{"location":"LDAP%20with%20OpenManage/README-FreeIPA-CENTOS/#my-environment","text":"","title":"My Environment"},{"location":"LDAP%20with%20OpenManage/README-FreeIPA-CENTOS/#centos-version","text":"CentOS Linux release 8.2.2004 (Core) NAME=\"CentOS Linux\" VERSION=\"8 (Core)\" ID=\"centos\" ID_LIKE=\"rhel fedora\" VERSION_ID=\"8\" PLATFORM_ID=\"platform:el8\" PRETTY_NAME=\"CentOS Linux 8 (Core)\" ANSI_COLOR=\"0;31\" CPE_NAME=\"cpe:/o:centos:centos:8\" HOME_URL=\"https://www.centos.org/\" BUG_REPORT_URL=\"https://bugs.centos.org/\" CENTOS_MANTISBT_PROJECT=\"CentOS-8\" CENTOS_MANTISBT_PROJECT_VERSION=\"8\" REDHAT_SUPPORT_PRODUCT=\"centos\" REDHAT_SUPPORT_PRODUCT_VERSION=\"8\" CentOS Linux release 8.2.2004 (Core) CentOS Linux release 8.2.2004 (Core)","title":"CentOS Version"},{"location":"LDAP%20with%20OpenManage/README-FreeIPA-CENTOS/#freeipa-version","text":"[root@centos ~]# ipa --version VERSION: 4.8.4, API_VERSION: 2.235","title":"FreeIPA Version"},{"location":"LDAP%20with%20OpenManage/README-FreeIPA-CENTOS/#openmanage-version","text":"Version 3.4.1 (Build 24)","title":"OpenManage Version"},{"location":"LDAP%20with%20OpenManage/README-FreeIPA-CENTOS/#helpful-resources","text":"Dell Tutorial Logs Explained LDAP Result Codes Helpful Post on Bind DN","title":"Helpful Resources"},{"location":"LDAP%20with%20OpenManage/README-FreeIPA-CENTOS/#install-instructions","text":"Install CentOS8 I installed CentOS minimal Make sure NTP is working correctly Install OpenManage Install the idm system module with dnf install -y @idm:DL1 freeipa-server Configure your DNS server ( /etc/hosts did not work for me) with a record for the hostname of your FreeIPA server. I added a record for centos.grant.lan . Run ipa-server-install If you have any DNS failures edit the file /tmp/ipa.system.records.tu5qyl09.db (you may have to change the name) and add the record centos.grant.lan 86400 IN A 192.168.1.92 (adjust accordingly). Afterwards run ipa dns-update-system-records Run kinit admin Open firewall ports firewall-cmd --add-port=80/tcp --permanent --zone=public firewall-cmd --add-port=443/tcp --permanent --zone=public firewall-cmd --add-port=389/tcp --permanent --zone=public firewall-cmd --add-port=636/tcp --permanent --zone=public firewall-cmd --add-port=88/tcp --permanent --zone=public firewall-cmd --add-port=464/tcp --permanent --zone=public firewall-cmd --add-port=88/udp --permanent --zone=public firewall-cmd --add-port=464/udp --permanent --zone=public firewall-cmd --add-port=123/udp --permanent --zone=public firewall-cmd --reload Log into FreeIPA server at https://centos.grant.lan . In my case, Windows popped up a username and password prompt. That prompt didn't work - I had to exit it and then log into the webGUI. Under Active users I added an admin user. Go to Users and then directory services in OpenManage. I used the following: Note: You can get the Bind DN by running ldapsearch from the command line.","title":"Install Instructions"},{"location":"LDAP%20with%20OpenManage/README-FreeIPA-CENTOS/#helpful-commands","text":"To start the IPA service use ipactl start|stop|restart . You can check the status with ipactl status .","title":"Helpful Commands"},{"location":"LDAP%20with%20OpenManage/README-FreeIPA-CENTOS/#testing","text":"","title":"Testing"},{"location":"LDAP%20with%20OpenManage/README-FreeIPA-CENTOS/#scenario-1","text":"This got me a working test connection:","title":"Scenario 1"},{"location":"LDAP%20with%20OpenManage/README-FreeIPA-CENTOS/#output","text":"[15/Oct/2020:14:09:18.440103345 -0400] conn=107 fd=74 slot=74 SSL connection from 192.168.1.93 to 192.168.1.92 [15/Oct/2020:14:09:18.484254084 -0400] conn=107 TLS1.2 128-bit AES-GCM [15/Oct/2020:14:09:18.484862509 -0400] conn=107 op=0 BIND dn=\"\" method=128 version=3 [15/Oct/2020:14:09:18.485048511 -0400] conn=107 op=0 RESULT err=0 tag=97 nentries=0 etime=0.044677059 dn=\"\" [15/Oct/2020:14:09:18.485743204 -0400] conn=107 op=1 SRCH base=\"dc=grant,dc=lan\" scope=2 filter=\"(uid=grant)\" attrs=ALL [15/Oct/2020:14:09:18.487440884 -0400] conn=107 op=1 RESULT err=0 tag=101 nentries=1 etime=0.001795760 [15/Oct/2020:14:09:18.488143502 -0400] conn=107 op=2 UNBIND [15/Oct/2020:14:09:18.488159848 -0400] conn=107 op=2 fd=74 closed - U1 [15/Oct/2020:14:09:18.491313979 -0400] conn=108 fd=74 slot=74 SSL connection from 192.168.1.93 to 192.168.1.92 [15/Oct/2020:14:09:18.536590087 -0400] conn=108 TLS1.2 128-bit AES-GCM [15/Oct/2020:14:09:18.537372005 -0400] conn=108 op=0 BIND dn=\"uid=grant,cn=users,cn=compat,dc=grant,dc=lan\" method=128 version=3 [15/Oct/2020:14:09:18.538144502 -0400] conn=108 op=0 RESULT err=0 tag=97 nentries=0 etime=0.046223517 dn=\"uid=grant,cn=users,cn=accounts,dc=grant,dc=lan\" [15/Oct/2020:14:09:18.538536207 -0400] conn=108 op=1 UNBIND [15/Oct/2020:14:09:18.538566004 -0400] conn=108 op=1 fd=74 closed - U1 [15/Oct/2020:14:09:28.961238173 -0400] conn=109 fd=74 slot=74 SSL connection from 192.168.1.93 to 192.168.1.92 [15/Oct/2020:14:09:29.005228025 -0400] conn=109 TLS1.2 128-bit AES-GCM [15/Oct/2020:14:09:29.005755286 -0400] conn=109 op=0 BIND dn=\"\" method=128 version=3 [15/Oct/2020:14:09:29.005931161 -0400] conn=109 op=0 RESULT err=0 tag=97 nentries=0 etime=0.044397507 dn=\"\" [15/Oct/2020:14:09:29.006898618 -0400] conn=109 op=1 SRCH base=\"dc=grant,dc=lan\" scope=2 filter=\"(uid=grant)\" attrs=ALL [15/Oct/2020:14:09:29.008536186 -0400] conn=109 op=1 RESULT err=0 tag=101 nentries=1 etime=0.001740822 [15/Oct/2020:14:09:29.009182689 -0400] conn=109 op=2 UNBIND [15/Oct/2020:14:09:29.009196697 -0400] conn=109 op=2 fd=74 closed - U1 [15/Oct/2020:14:09:29.012428320 -0400] conn=110 fd=74 slot=74 SSL connection from 192.168.1.93 to 192.168.1.92 [15/Oct/2020:14:09:29.057469132 -0400] conn=110 TLS1.2 128-bit AES-GCM [15/Oct/2020:14:09:29.058084024 -0400] conn=110 op=0 BIND dn=\"uid=grant,cn=users,cn=compat,dc=grant,dc=lan\" method=128 version=3 [15/Oct/2020:14:09:29.058825635 -0400] conn=110 op=0 RESULT err=0 tag=97 nentries=0 etime=0.046016675 dn=\"uid=grant,cn=users,cn=accounts,dc=grant,dc=lan\" [15/Oct/2020:14:09:29.059166870 -0400] conn=110 op=1 UNBIND [15/Oct/2020:14:09:29.059195118 -0400] conn=110 op=1 fd=74 closed - U1","title":"Output"},{"location":"LDAP%20with%20OpenManage/README-FreeIPA-CENTOS/#scenario-2","text":"When I added a Bind DN as shown in this video I get a failure.","title":"Scenario 2"},{"location":"LDAP%20with%20OpenManage/README-FreeIPA-CENTOS/#error-message","text":"","title":"Error Message"},{"location":"LDAP%20with%20OpenManage/README-FreeIPA-CENTOS/#log-output","text":"[15/Oct/2020:14:12:07.504942397 -0400] conn=112 fd=74 slot=74 SSL connection from 192.168.1.93 to 192.168.1.92 [15/Oct/2020:14:12:07.550456498 -0400] conn=112 TLS1.2 128-bit AES-GCM [15/Oct/2020:14:12:07.551077266 -0400] conn=112 op=0 BIND dn=\"uid=grant,cn=users,cn=accounts,dc=grant,dc=lan\" method=128 version=3 [15/Oct/2020:14:12:07.551767359 -0400] conn=112 op=0 RESULT err=0 tag=97 nentries=0 etime=0.046428458 dn=\"uid=grant,cn=users,cn=accounts,dc=grant,dc=lan\" [15/Oct/2020:14:12:07.552283396 -0400] conn=112 op=1 SRCH base=\"dc=grant,dc=lan\" scope=2 filter=\"(uid=grant)\" attrs=ALL [15/Oct/2020:14:12:07.553922212 -0400] conn=112 op=1 RESULT err=0 tag=101 nentries=1 etime=0.001752227 [15/Oct/2020:14:12:07.555518270 -0400] conn=112 op=2 UNBIND [15/Oct/2020:14:12:07.555534158 -0400] conn=112 op=2 fd=74 closed - U1","title":"Log Output"},{"location":"LDAP%20with%20OpenManage/README-FreeIPA-CENTOS/#scenario-3-current-sticking-point","text":"Using the settings from scenario 1, I continued. When trying to add a group though, no groups are displayed in available groups.","title":"Scenario 3 - Current Sticking Point"},{"location":"LDAP%20with%20OpenManage/README-FreeIPA-CENTOS/#output_1","text":"[15/Oct/2020:15:19:49.806462707 -0400] conn=169 fd=103 slot=103 SSL connection from 192.168.1.18 to 192.168.1.92 [15/Oct/2020:15:19:49.856773316 -0400] conn=169 TLS1.2 128-bit AES-GCM [15/Oct/2020:15:19:49.858681446 -0400] conn=169 op=0 BIND dn=\"\" method=128 version=3 [15/Oct/2020:15:19:49.858906917 -0400] conn=169 op=0 RESULT err=0 tag=97 nentries=0 etime=0.051584941 dn=\"\" [15/Oct/2020:15:19:49.864335125 -0400] conn=169 op=1 SRCH base=\"dc=grant,dc=lan\" scope=2 filter=\"(uid=grant)\" attrs=ALL [15/Oct/2020:15:19:49.866001831 -0400] conn=169 op=1 RESULT err=0 tag=101 nentries=1 etime=0.001790286 [15/Oct/2020:15:19:49.867368889 -0400] conn=169 op=2 UNBIND [15/Oct/2020:15:19:49.867386461 -0400] conn=169 op=2 fd=103 closed - U1 [15/Oct/2020:15:19:49.873375865 -0400] conn=170 fd=103 slot=103 SSL connection from 192.168.1.18 to 192.168.1.92 [15/Oct/2020:15:19:49.925956643 -0400] conn=170 TLS1.2 128-bit AES-GCM [15/Oct/2020:15:19:49.928180447 -0400] conn=170 op=0 BIND dn=\"uid=grant,cn=users,cn=compat,dc=grant,dc=lan\" method=128 version=3 [15/Oct/2020:15:19:49.928993026 -0400] conn=170 op=0 RESULT err=0 tag=97 nentries=0 etime=0.053916831 dn=\"uid=grant,cn=users,cn=accounts,dc=grant,dc=lan\" [15/Oct/2020:15:19:49.934942058 -0400] conn=170 op=1 SRCH base=\"dc=grant,dc=lan\" scope=2 filter=\"(&(cn=grantgroup*)(uniqueMember=*))\" attrs=\"cn entryuuid\" [15/Oct/2020:15:19:49.935438340 -0400] conn=170 op=1 RESULT err=0 tag=101 nentries=0 etime=0.000639539 [15/Oct/2020:15:19:49.936729725 -0400] conn=170 op=2 UNBIND [15/Oct/2020:15:19:49.936744908 -0400] conn=170 op=2 fd=103 closed - U1","title":"Output"},{"location":"LDAP%20with%20OpenManage/README-Turnkey-Linux/","text":"Setting Up OpenLDAP with OpenManage My Environment OpenManage Version Version 3.5.0 (Build 60) Helpful Resources Dell Tutorial LDAP Result Codes Helpful Post on Bind DN OpenManage User's Guide Fix This base cannot be created with PLA in phpldapadmin Turnkey OpenLDAP Instructions Download OpenLDAP appliance from here Alternatively, you can build it yourself. [This tutorial](https://medium.com/@benjamin.dronen/installing-openldap-and-phpldapadmin-on-ubuntu-20-04-lts-7ef3ca40dc00 is helpful however you will have to add LDAPS for it to work with OpenManage Enterprise. Helpful Commands/Things Run slapd in Foreground sudo slapd -d 256 -d 128 View Database Configuration The database configuration for OpenLDAP is stored at /etc/ldap/slapd.d You can find a config your interested in with grep -R <THING> * . For example my user config was at cn\\=config/olcDatabase\\=\\{1\\}mdb.ldif . phpldapadmin Config Location /etc/phpldapadmin/config.php Line 300 has login stuff Use a SRV record for Discovery If you use DNS for Domain Controller Lookup when setting up LDAP what it will do is use a SRV record lookup to find your LDAP server. If you want discovery to happen this way, you just need to add the appropriate SRV record to your DNS server. I was using PFSense so I added the following in Custom options: server: local-data: \"_ldap._tcp.ubuntuldap.grant.lan 3600 IN SRV 0 100 389 ubuntuldap.grant.lan\" Potential Bug? When testing LDAP on OpenManage I noticed it would issue the message \"Unable to connect to the LDAP or AD server because the entered credentials are invalid.\" However, while watching Wireshark I noted this coincided with a failed DNS query. This error message appears to be a erroneous.","title":"Setting Up OpenLDAP with OpenManage"},{"location":"LDAP%20with%20OpenManage/README-Turnkey-Linux/#setting-up-openldap-with-openmanage","text":"","title":"Setting Up OpenLDAP with OpenManage"},{"location":"LDAP%20with%20OpenManage/README-Turnkey-Linux/#my-environment","text":"","title":"My Environment"},{"location":"LDAP%20with%20OpenManage/README-Turnkey-Linux/#openmanage-version","text":"Version 3.5.0 (Build 60)","title":"OpenManage Version"},{"location":"LDAP%20with%20OpenManage/README-Turnkey-Linux/#helpful-resources","text":"Dell Tutorial LDAP Result Codes Helpful Post on Bind DN OpenManage User's Guide Fix This base cannot be created with PLA in phpldapadmin Turnkey OpenLDAP","title":"Helpful Resources"},{"location":"LDAP%20with%20OpenManage/README-Turnkey-Linux/#instructions","text":"Download OpenLDAP appliance from here Alternatively, you can build it yourself. [This tutorial](https://medium.com/@benjamin.dronen/installing-openldap-and-phpldapadmin-on-ubuntu-20-04-lts-7ef3ca40dc00 is helpful however you will have to add LDAPS for it to work with OpenManage Enterprise.","title":"Instructions"},{"location":"LDAP%20with%20OpenManage/README-Turnkey-Linux/#helpful-commandsthings","text":"","title":"Helpful Commands/Things"},{"location":"LDAP%20with%20OpenManage/README-Turnkey-Linux/#run-slapd-in-foreground","text":"sudo slapd -d 256 -d 128","title":"Run slapd in Foreground"},{"location":"LDAP%20with%20OpenManage/README-Turnkey-Linux/#view-database-configuration","text":"The database configuration for OpenLDAP is stored at /etc/ldap/slapd.d You can find a config your interested in with grep -R <THING> * . For example my user config was at cn\\=config/olcDatabase\\=\\{1\\}mdb.ldif .","title":"View Database Configuration"},{"location":"LDAP%20with%20OpenManage/README-Turnkey-Linux/#phpldapadmin-config-location","text":"/etc/phpldapadmin/config.php Line 300 has login stuff","title":"phpldapadmin Config Location"},{"location":"LDAP%20with%20OpenManage/README-Turnkey-Linux/#use-a-srv-record-for-discovery","text":"If you use DNS for Domain Controller Lookup when setting up LDAP what it will do is use a SRV record lookup to find your LDAP server. If you want discovery to happen this way, you just need to add the appropriate SRV record to your DNS server. I was using PFSense so I added the following in Custom options: server: local-data: \"_ldap._tcp.ubuntuldap.grant.lan 3600 IN SRV 0 100 389 ubuntuldap.grant.lan\"","title":"Use a SRV record for Discovery"},{"location":"LDAP%20with%20OpenManage/README-Turnkey-Linux/#potential-bug","text":"When testing LDAP on OpenManage I noticed it would issue the message \"Unable to connect to the LDAP or AD server because the entered credentials are invalid.\" However, while watching Wireshark I noted this coincided with a failed DNS query. This error message appears to be a erroneous.","title":"Potential Bug?"},{"location":"LDAP%20with%20OpenManage/keycloak/","text":"Some people like this: https://www.keycloak.org/","title":"Keycloak"},{"location":"LDAP%20with%20OpenManage/LDAP%20Group%20Import%20Bug/","text":"Setting Up FreeIPA with OpenManage My Environment RHEL Version NAME=\"Red Hat Enterprise Linux\" VERSION=\"8.2 (Ootpa)\" ID=\"rhel\" ID_LIKE=\"fedora\" VERSION_ID=\"8.2\" PLATFORM_ID=\"platform:el8\" PRETTY_NAME=\"Red Hat Enterprise Linux 8.2 (Ootpa)\" ANSI_COLOR=\"0;31\" CPE_NAME=\"cpe:/o:redhat:enterprise_linux:8.2:GA\" HOME_URL=\"https://www.redhat.com/\" BUG_REPORT_URL=\"https://bugzilla.redhat.com/\" REDHAT_BUGZILLA_PRODUCT=\"Red Hat Enterprise Linux 8\" REDHAT_BUGZILLA_PRODUCT_VERSION=8.2 REDHAT_SUPPORT_PRODUCT=\"Red Hat Enterprise Linux\" REDHAT_SUPPORT_PRODUCT_VERSION=\"8.2\" Red Hat Enterprise Linux release 8.2 (Ootpa) Red Hat Enterprise Linux release 8.2 (Ootpa) FreeIPA Version [root@centos ~]# ipa --version VERSION: 4.8.4, API_VERSION: 2.235 OpenManage Version Version 3.4.1 (Build 24) Helpful Resources Dell Tutorial Logs Explained LDAP Result Codes Helpful Post on Bind DN Install Instructions Install RHEL Change hostname hostname freeipa.grant.lan && hostnamectl set-hostname freeipa.grant.lan Change in /etc/hostname Configure DNS to return for this hostname. Double check with dig +short freeipa.grant.lan A && dig +short -x 192.168.1.95 Follow RHEL's instructions I used Chapter 5 for primary installation Run kinit admin - this allows you to use the command line tools otherwise they'll complain about kerberos. Log into FreeIPA server at https://<your_hostname> . In my case, Windows popped up a username and password prompt. That prompt didn't work - I had to exit it and then log into the webGUI. Go to Users and then directory services in OpenManage. I used the following: Note: You can get the Bind DN by running ldapsearch from the command line. Create a new user and new group in the UI and assign the new user to the new group. Install OpenManage Go to Application Settings -> Directory Services Substitute with your values and then click test. I wasn't able to get this to work with the generic admin user. In the test screen I used that new user to connect to directory services Helpful Commands To start the IPA service use ipactl start|stop|restart . You can check the status with ipactl status . Bug I used the settings defined here: When I went to import the users from a group I received the following: The code in question: Below was the value of u at runtime: [ { \"userTypeId\":2, \"objectGuid\":null, \"objectSid\":null, \"directoryServiceId\":13483, \"name\":\"grantgroup\", \"password\":\"\", \"userName\":\"grantgroup\", \"roleId\":\"10\", \"locked\":false, \"isBuiltin\":false, \"enabled\":true } ] Error in logs [ERROR] 2020-10-22 07:33:08.392 [ajp-bio-8009-exec-2] BaseController - com.dell.enterprise.exception.ui.ConsoleException: error.general_known_error_occurred com.dell.enterprise.exception.ui.ConsoleException: error.general_known_error_occurred at com.dell.enterprise.controller.console.ADController.importGroups(ADController.java:400) ~[UI.ADPlugin-0.0.1-SNAPSHOT.jar:?] at sun.reflect.GeneratedMethodAccessor824.invoke(Unknown Source) ~[?:?] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_262] at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_262] at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:205) ~[spring-web-4.3.28.RELEASE.jar:4.3.28.RELEASE] at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:133) ~[spring-web-4.3.28.RELEASE.jar:4.3.28.RELEASE] at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:97) ~[spring-webmvc-4.3.28.RELEASE.jar:4.3.28.RELEASE] at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:854) ~[spring-webmvc-4.3.28.RELEASE.jar:4.3.28.RELEASE] at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:765) ~[spring-webmvc-4.3.28.RELEASE.jar:4.3.28.RELEASE] at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:85) ~[spring-webmvc-4.3.28.RELEASE.jar:4.3.28.RELEASE] at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:967) [spring-webmvc-4.3.28.RELEASE.jar:4.3.28.RELEASE] at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:901) [spring-webmvc-4.3.28.RELEASE.jar:4.3.28.RELEASE] at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:970) [spring-webmvc-4.3.28.RELEASE.jar:4.3.28.RELEASE] at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:872) [spring-webmvc-4.3.28.RELEASE.jar:4.3.28.RELEASE] at javax.servlet.http.HttpServlet.service(HttpServlet.java:650) [tomcat-servlet-3.0-api.jar:?] at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:846) [spring-webmvc-4.3.28.RELEASE.jar:4.3.28.RELEASE] at javax.servlet.http.HttpServlet.service(HttpServlet.java:731) [tomcat-servlet-3.0-api.jar:?] at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:303) [catalina.jar:7.0.76] at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208) [catalina.jar:7.0.76] at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:52) [tomcat7-websocket.jar:7.0.76] at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241) [catalina.jar:7.0.76] at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208) [catalina.jar:7.0.76] at com.dell.enterprise.filter.ui.CacheControlFilter.doFilterInternal(CacheControlFilter.java:23) [classes/:?] at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-4.3.28.RELEASE.jar:4.3.28.RELEASE] at org.springframework.web.filter.DelegatingFilterProxy.invokeDelegate(DelegatingFilterProxy.java:347) [spring-web-4.3.28.RELEASE.jar:4.3.28.RELEASE] at org.springframework.web.filter.DelegatingFilterProxy.doFilter(DelegatingFilterProxy.java:263) [spring-web-4.3.28.RELEASE.jar:4.3.28.RELEASE] at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241) [catalina.jar:7.0.76] at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208) [catalina.jar:7.0.76] at org.apache.catalina.filters.SetCharacterEncodingFilter.doFilter(SetCharacterEncodingFilter.java:108) [catalina.jar:7.0.76] at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241) [catalina.jar:7.0.76] at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208) [catalina.jar:7.0.76] at org.apache.shiro.web.servlet.ProxiedFilterChain.doFilter(ProxiedFilterChain.java:61) [shiro-web-1.6.0.jar:1.6.0] at org.apache.shiro.web.servlet.AdviceFilter.executeChain(AdviceFilter.java:108) [shiro-web-1.6.0.jar:1.6.0] at org.apache.shiro.web.servlet.AdviceFilter.doFilterInternal(AdviceFilter.java:137) [shiro-web-1.6.0.jar:1.6.0] at org.apache.shiro.web.servlet.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:125) [shiro-web-1.6.0.jar:1.6.0] at org.apache.shiro.web.servlet.ProxiedFilterChain.doFilter(ProxiedFilterChain.java:66) [shiro-web-1.6.0.jar:1.6.0] at org.apache.shiro.web.servlet.AdviceFilter.executeChain(AdviceFilter.java:108) [shiro-web-1.6.0.jar:1.6.0] at org.apache.shiro.web.servlet.AdviceFilter.doFilterInternal(AdviceFilter.java:137) [shiro-web-1.6.0.jar:1.6.0] at org.apache.shiro.web.servlet.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:125) [shiro-web-1.6.0.jar:1.6.0] at org.apache.shiro.web.servlet.ProxiedFilterChain.doFilter(ProxiedFilterChain.java:66) [shiro-web-1.6.0.jar:1.6.0] at org.apache.shiro.web.servlet.AbstractShiroFilter.executeChain(AbstractShiroFilter.java:450) [shiro-web-1.6.0.jar:1.6.0] at org.apache.shiro.web.servlet.AbstractShiroFilter$1.call(AbstractShiroFilter.java:365) [shiro-web-1.6.0.jar:1.6.0] at org.apache.shiro.subject.support.SubjectCallable.doCall(SubjectCallable.java:90) [shiro-core-1.6.0.jar:1.6.0] at org.apache.shiro.subject.support.SubjectCallable.call(SubjectCallable.java:83) [shiro-core-1.6.0.jar:1.6.0] at org.apache.shiro.subject.support.DelegatingSubject.execute(DelegatingSubject.java:387) [shiro-core-1.6.0.jar:1.6.0] at org.apache.shiro.web.servlet.AbstractShiroFilter.doFilterInternal(AbstractShiroFilter.java:362) [shiro-web-1.6.0.jar:1.6.0] at org.apache.shiro.web.servlet.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:125) [shiro-web-1.6.0.jar:1.6.0] at org.springframework.web.filter.DelegatingFilterProxy.invokeDelegate(DelegatingFilterProxy.java:347) [spring-web-4.3.28.RELEASE.jar:4.3.28.RELEASE] at org.springframework.web.filter.DelegatingFilterProxy.doFilter(DelegatingFilterProxy.java:263) [spring-web-4.3.28.RELEASE.jar:4.3.28.RELEASE] at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241) [catalina.jar:7.0.76] at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208) [catalina.jar:7.0.76] at com.dell.enterprise.core.integration.lib.common.filter.RequestFilter.doFilter(RequestFilter.java:103) [common-0.0.1-SNAPSHOT.jar:?] at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241) [catalina.jar:7.0.76] at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208) [catalina.jar:7.0.76] at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:218) [catalina.jar:7.0.76] at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:110) [catalina.jar:7.0.76] at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:498) [catalina.jar:7.0.76] at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:169) [catalina.jar:7.0.76] at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:103) [catalina.jar:7.0.76] at org.apache.catalina.valves.AccessLogValve.invoke(AccessLogValve.java:962) [catalina.jar:7.0.76] at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:116) [catalina.jar:7.0.76] at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:445) [catalina.jar:7.0.76] at org.apache.coyote.ajp.AjpProcessor.process(AjpProcessor.java:190) [tomcat-coyote.jar:7.0.76] at org.apache.coyote.AbstractProtocol$AbstractConnectionHandler.process(AbstractProtocol.java:637) [tomcat-coyote.jar:7.0.76] at org.apache.tomcat.util.net.JIoEndpoint$SocketProcessor.run(JIoEndpoint.java:316) [tomcat-coyote.jar:7.0.76] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_262] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_262] at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61) [tomcat-coyote.jar:7.0.76] at java.lang.Thread.run(Thread.java:748) [?:1.8.0_262] Further Explanation The URI endpoint is: https://192.168.1.93/core/api/Console/import-groups This is the JSON returned from the call. {severity: \"IGNORE\", message: \"error.general_known_error_occurred\",\u2026} debug: false details: {error: {code: \"Base.1.0.GeneralError\",\u2026}} error: {code: \"Base.1.0.GeneralError\",\u2026} @Message.ExtendedInfo: [{MessageId: \"CGEN1004\", RelatedProperties: [],\u2026}] code: \"Base.1.0.GeneralError\" message: \"A general error has occurred. See ExtendedInfo for more information.\" message: \"error.general_known_error_occurred\" severity: \"IGNORE\" timestamp: \"2020-10-21T14:43:44.385-0500\" The errors occurs at: M.send(y(u) ? null : u) See here for a description of Javascript's ternary operator. In this case it is saying if y(u) is true then the value is set to null, otherwise it is set to u . The y function is: function y(e) { return \"undefined\" == typeof e } It is just a basic check to see if u is defined or not. M is an instance of XMLHttpRequest . We can see M being called with open M.open(i, s, !0) where i is \"POST\" and s is \"/core/api/Console/import-groups\". The problem occurs because objectGuid and objectSid are set to null. Resolution See duplicate_bug.py for a replication of the problem. Replace the payload: { \"userTypeId\":2, \"objectGuid\":null, \"objectSid\":null, \"directoryServiceId\":13483, \"name\":\"grantgroup\", \"password\":\"\", \"userName\":\"grantgroup\", \"roleId\":\"10\", \"locked\":false, \"isBuiltin\":false, \"enabled\":true } with the data from your instance. I grabbed this out of the javascript debugger. To fix the problem, you have to lookup the uid/gid (which correspond to objectSid and objectGuid respectively) on your LDAP server and replace the null values. I used ldapsearch to find mine: # grant, users, compat, grant.lan dn: uid=grant,cn=users,cn=compat,dc=grant,dc=lan objectClass: posixAccount objectClass: ipaOverrideTarget objectClass: top gecos: Grant Curell cn: Grant Curell uidNumber: 1314600001 gidNumber: 1314600001 loginShell: /bin/sh homeDirectory: /home/grant ipaAnchorUUID:: OklQQTpncmFudC5sYW46OWIzOTYwNDQtMTNhZS0xMWViLTllNzctMDA1MDU2Ym U4NGIw uid: grant You can see the uidNumber and gidNumber fields. Change the payload out in duplicate_bug.py and it will correctly import the group. test_payload = [ { \"userTypeId\": 2, \"objectGuid\": 1314600001, \"objectSid\": 1314600001, \"directoryServiceId\": 13483, \"name\": \"grantgroup\", \"password\": \"\", \"userName\": \"grant\", \"roleId\": \"10\", \"locked\": False, \"isBuiltin\": False, \"enabled\": True } ]","title":"Setting Up FreeIPA with OpenManage"},{"location":"LDAP%20with%20OpenManage/LDAP%20Group%20Import%20Bug/#setting-up-freeipa-with-openmanage","text":"","title":"Setting Up FreeIPA with OpenManage"},{"location":"LDAP%20with%20OpenManage/LDAP%20Group%20Import%20Bug/#my-environment","text":"","title":"My Environment"},{"location":"LDAP%20with%20OpenManage/LDAP%20Group%20Import%20Bug/#rhel-version","text":"NAME=\"Red Hat Enterprise Linux\" VERSION=\"8.2 (Ootpa)\" ID=\"rhel\" ID_LIKE=\"fedora\" VERSION_ID=\"8.2\" PLATFORM_ID=\"platform:el8\" PRETTY_NAME=\"Red Hat Enterprise Linux 8.2 (Ootpa)\" ANSI_COLOR=\"0;31\" CPE_NAME=\"cpe:/o:redhat:enterprise_linux:8.2:GA\" HOME_URL=\"https://www.redhat.com/\" BUG_REPORT_URL=\"https://bugzilla.redhat.com/\" REDHAT_BUGZILLA_PRODUCT=\"Red Hat Enterprise Linux 8\" REDHAT_BUGZILLA_PRODUCT_VERSION=8.2 REDHAT_SUPPORT_PRODUCT=\"Red Hat Enterprise Linux\" REDHAT_SUPPORT_PRODUCT_VERSION=\"8.2\" Red Hat Enterprise Linux release 8.2 (Ootpa) Red Hat Enterprise Linux release 8.2 (Ootpa)","title":"RHEL Version"},{"location":"LDAP%20with%20OpenManage/LDAP%20Group%20Import%20Bug/#freeipa-version","text":"[root@centos ~]# ipa --version VERSION: 4.8.4, API_VERSION: 2.235","title":"FreeIPA Version"},{"location":"LDAP%20with%20OpenManage/LDAP%20Group%20Import%20Bug/#openmanage-version","text":"Version 3.4.1 (Build 24)","title":"OpenManage Version"},{"location":"LDAP%20with%20OpenManage/LDAP%20Group%20Import%20Bug/#helpful-resources","text":"Dell Tutorial Logs Explained LDAP Result Codes Helpful Post on Bind DN","title":"Helpful Resources"},{"location":"LDAP%20with%20OpenManage/LDAP%20Group%20Import%20Bug/#install-instructions","text":"Install RHEL Change hostname hostname freeipa.grant.lan && hostnamectl set-hostname freeipa.grant.lan Change in /etc/hostname Configure DNS to return for this hostname. Double check with dig +short freeipa.grant.lan A && dig +short -x 192.168.1.95 Follow RHEL's instructions I used Chapter 5 for primary installation Run kinit admin - this allows you to use the command line tools otherwise they'll complain about kerberos. Log into FreeIPA server at https://<your_hostname> . In my case, Windows popped up a username and password prompt. That prompt didn't work - I had to exit it and then log into the webGUI. Go to Users and then directory services in OpenManage. I used the following: Note: You can get the Bind DN by running ldapsearch from the command line. Create a new user and new group in the UI and assign the new user to the new group. Install OpenManage Go to Application Settings -> Directory Services Substitute with your values and then click test. I wasn't able to get this to work with the generic admin user. In the test screen I used that new user to connect to directory services","title":"Install Instructions"},{"location":"LDAP%20with%20OpenManage/LDAP%20Group%20Import%20Bug/#helpful-commands","text":"To start the IPA service use ipactl start|stop|restart . You can check the status with ipactl status .","title":"Helpful Commands"},{"location":"LDAP%20with%20OpenManage/LDAP%20Group%20Import%20Bug/#bug","text":"I used the settings defined here: When I went to import the users from a group I received the following: The code in question: Below was the value of u at runtime: [ { \"userTypeId\":2, \"objectGuid\":null, \"objectSid\":null, \"directoryServiceId\":13483, \"name\":\"grantgroup\", \"password\":\"\", \"userName\":\"grantgroup\", \"roleId\":\"10\", \"locked\":false, \"isBuiltin\":false, \"enabled\":true } ] Error in logs [ERROR] 2020-10-22 07:33:08.392 [ajp-bio-8009-exec-2] BaseController - com.dell.enterprise.exception.ui.ConsoleException: error.general_known_error_occurred com.dell.enterprise.exception.ui.ConsoleException: error.general_known_error_occurred at com.dell.enterprise.controller.console.ADController.importGroups(ADController.java:400) ~[UI.ADPlugin-0.0.1-SNAPSHOT.jar:?] at sun.reflect.GeneratedMethodAccessor824.invoke(Unknown Source) ~[?:?] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_262] at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_262] at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:205) ~[spring-web-4.3.28.RELEASE.jar:4.3.28.RELEASE] at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:133) ~[spring-web-4.3.28.RELEASE.jar:4.3.28.RELEASE] at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:97) ~[spring-webmvc-4.3.28.RELEASE.jar:4.3.28.RELEASE] at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:854) ~[spring-webmvc-4.3.28.RELEASE.jar:4.3.28.RELEASE] at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:765) ~[spring-webmvc-4.3.28.RELEASE.jar:4.3.28.RELEASE] at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:85) ~[spring-webmvc-4.3.28.RELEASE.jar:4.3.28.RELEASE] at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:967) [spring-webmvc-4.3.28.RELEASE.jar:4.3.28.RELEASE] at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:901) [spring-webmvc-4.3.28.RELEASE.jar:4.3.28.RELEASE] at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:970) [spring-webmvc-4.3.28.RELEASE.jar:4.3.28.RELEASE] at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:872) [spring-webmvc-4.3.28.RELEASE.jar:4.3.28.RELEASE] at javax.servlet.http.HttpServlet.service(HttpServlet.java:650) [tomcat-servlet-3.0-api.jar:?] at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:846) [spring-webmvc-4.3.28.RELEASE.jar:4.3.28.RELEASE] at javax.servlet.http.HttpServlet.service(HttpServlet.java:731) [tomcat-servlet-3.0-api.jar:?] at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:303) [catalina.jar:7.0.76] at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208) [catalina.jar:7.0.76] at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:52) [tomcat7-websocket.jar:7.0.76] at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241) [catalina.jar:7.0.76] at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208) [catalina.jar:7.0.76] at com.dell.enterprise.filter.ui.CacheControlFilter.doFilterInternal(CacheControlFilter.java:23) [classes/:?] at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-4.3.28.RELEASE.jar:4.3.28.RELEASE] at org.springframework.web.filter.DelegatingFilterProxy.invokeDelegate(DelegatingFilterProxy.java:347) [spring-web-4.3.28.RELEASE.jar:4.3.28.RELEASE] at org.springframework.web.filter.DelegatingFilterProxy.doFilter(DelegatingFilterProxy.java:263) [spring-web-4.3.28.RELEASE.jar:4.3.28.RELEASE] at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241) [catalina.jar:7.0.76] at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208) [catalina.jar:7.0.76] at org.apache.catalina.filters.SetCharacterEncodingFilter.doFilter(SetCharacterEncodingFilter.java:108) [catalina.jar:7.0.76] at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241) [catalina.jar:7.0.76] at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208) [catalina.jar:7.0.76] at org.apache.shiro.web.servlet.ProxiedFilterChain.doFilter(ProxiedFilterChain.java:61) [shiro-web-1.6.0.jar:1.6.0] at org.apache.shiro.web.servlet.AdviceFilter.executeChain(AdviceFilter.java:108) [shiro-web-1.6.0.jar:1.6.0] at org.apache.shiro.web.servlet.AdviceFilter.doFilterInternal(AdviceFilter.java:137) [shiro-web-1.6.0.jar:1.6.0] at org.apache.shiro.web.servlet.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:125) [shiro-web-1.6.0.jar:1.6.0] at org.apache.shiro.web.servlet.ProxiedFilterChain.doFilter(ProxiedFilterChain.java:66) [shiro-web-1.6.0.jar:1.6.0] at org.apache.shiro.web.servlet.AdviceFilter.executeChain(AdviceFilter.java:108) [shiro-web-1.6.0.jar:1.6.0] at org.apache.shiro.web.servlet.AdviceFilter.doFilterInternal(AdviceFilter.java:137) [shiro-web-1.6.0.jar:1.6.0] at org.apache.shiro.web.servlet.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:125) [shiro-web-1.6.0.jar:1.6.0] at org.apache.shiro.web.servlet.ProxiedFilterChain.doFilter(ProxiedFilterChain.java:66) [shiro-web-1.6.0.jar:1.6.0] at org.apache.shiro.web.servlet.AbstractShiroFilter.executeChain(AbstractShiroFilter.java:450) [shiro-web-1.6.0.jar:1.6.0] at org.apache.shiro.web.servlet.AbstractShiroFilter$1.call(AbstractShiroFilter.java:365) [shiro-web-1.6.0.jar:1.6.0] at org.apache.shiro.subject.support.SubjectCallable.doCall(SubjectCallable.java:90) [shiro-core-1.6.0.jar:1.6.0] at org.apache.shiro.subject.support.SubjectCallable.call(SubjectCallable.java:83) [shiro-core-1.6.0.jar:1.6.0] at org.apache.shiro.subject.support.DelegatingSubject.execute(DelegatingSubject.java:387) [shiro-core-1.6.0.jar:1.6.0] at org.apache.shiro.web.servlet.AbstractShiroFilter.doFilterInternal(AbstractShiroFilter.java:362) [shiro-web-1.6.0.jar:1.6.0] at org.apache.shiro.web.servlet.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:125) [shiro-web-1.6.0.jar:1.6.0] at org.springframework.web.filter.DelegatingFilterProxy.invokeDelegate(DelegatingFilterProxy.java:347) [spring-web-4.3.28.RELEASE.jar:4.3.28.RELEASE] at org.springframework.web.filter.DelegatingFilterProxy.doFilter(DelegatingFilterProxy.java:263) [spring-web-4.3.28.RELEASE.jar:4.3.28.RELEASE] at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241) [catalina.jar:7.0.76] at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208) [catalina.jar:7.0.76] at com.dell.enterprise.core.integration.lib.common.filter.RequestFilter.doFilter(RequestFilter.java:103) [common-0.0.1-SNAPSHOT.jar:?] at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241) [catalina.jar:7.0.76] at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208) [catalina.jar:7.0.76] at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:218) [catalina.jar:7.0.76] at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:110) [catalina.jar:7.0.76] at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:498) [catalina.jar:7.0.76] at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:169) [catalina.jar:7.0.76] at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:103) [catalina.jar:7.0.76] at org.apache.catalina.valves.AccessLogValve.invoke(AccessLogValve.java:962) [catalina.jar:7.0.76] at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:116) [catalina.jar:7.0.76] at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:445) [catalina.jar:7.0.76] at org.apache.coyote.ajp.AjpProcessor.process(AjpProcessor.java:190) [tomcat-coyote.jar:7.0.76] at org.apache.coyote.AbstractProtocol$AbstractConnectionHandler.process(AbstractProtocol.java:637) [tomcat-coyote.jar:7.0.76] at org.apache.tomcat.util.net.JIoEndpoint$SocketProcessor.run(JIoEndpoint.java:316) [tomcat-coyote.jar:7.0.76] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_262] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_262] at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61) [tomcat-coyote.jar:7.0.76] at java.lang.Thread.run(Thread.java:748) [?:1.8.0_262]","title":"Bug"},{"location":"LDAP%20with%20OpenManage/LDAP%20Group%20Import%20Bug/#further-explanation","text":"The URI endpoint is: https://192.168.1.93/core/api/Console/import-groups This is the JSON returned from the call. {severity: \"IGNORE\", message: \"error.general_known_error_occurred\",\u2026} debug: false details: {error: {code: \"Base.1.0.GeneralError\",\u2026}} error: {code: \"Base.1.0.GeneralError\",\u2026} @Message.ExtendedInfo: [{MessageId: \"CGEN1004\", RelatedProperties: [],\u2026}] code: \"Base.1.0.GeneralError\" message: \"A general error has occurred. See ExtendedInfo for more information.\" message: \"error.general_known_error_occurred\" severity: \"IGNORE\" timestamp: \"2020-10-21T14:43:44.385-0500\" The errors occurs at: M.send(y(u) ? null : u) See here for a description of Javascript's ternary operator. In this case it is saying if y(u) is true then the value is set to null, otherwise it is set to u . The y function is: function y(e) { return \"undefined\" == typeof e } It is just a basic check to see if u is defined or not. M is an instance of XMLHttpRequest . We can see M being called with open M.open(i, s, !0) where i is \"POST\" and s is \"/core/api/Console/import-groups\". The problem occurs because objectGuid and objectSid are set to null.","title":"Further Explanation"},{"location":"LDAP%20with%20OpenManage/LDAP%20Group%20Import%20Bug/#resolution","text":"See duplicate_bug.py for a replication of the problem. Replace the payload: { \"userTypeId\":2, \"objectGuid\":null, \"objectSid\":null, \"directoryServiceId\":13483, \"name\":\"grantgroup\", \"password\":\"\", \"userName\":\"grantgroup\", \"roleId\":\"10\", \"locked\":false, \"isBuiltin\":false, \"enabled\":true } with the data from your instance. I grabbed this out of the javascript debugger. To fix the problem, you have to lookup the uid/gid (which correspond to objectSid and objectGuid respectively) on your LDAP server and replace the null values. I used ldapsearch to find mine: # grant, users, compat, grant.lan dn: uid=grant,cn=users,cn=compat,dc=grant,dc=lan objectClass: posixAccount objectClass: ipaOverrideTarget objectClass: top gecos: Grant Curell cn: Grant Curell uidNumber: 1314600001 gidNumber: 1314600001 loginShell: /bin/sh homeDirectory: /home/grant ipaAnchorUUID:: OklQQTpncmFudC5sYW46OWIzOTYwNDQtMTNhZS0xMWViLTllNzctMDA1MDU2Ym U4NGIw uid: grant You can see the uidNumber and gidNumber fields. Change the payload out in duplicate_bug.py and it will correctly import the group. test_payload = [ { \"userTypeId\": 2, \"objectGuid\": 1314600001, \"objectSid\": 1314600001, \"directoryServiceId\": 13483, \"name\": \"grantgroup\", \"password\": \"\", \"userName\": \"grant\", \"roleId\": \"10\", \"locked\": False, \"isBuiltin\": False, \"enabled\": True } ]","title":"Resolution"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/","text":"Load Balance Testing on 4112F-ON w/OS10 See: Test Case 1 Test Case 2 Test Case 3 Test Case 4","title":"Load Balance Testing on 4112F-ON w/OS10"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/#load-balance-testing-on-4112f-on-wos10","text":"See: Test Case 1 Test Case 2 Test Case 3 Test Case 4","title":"Load Balance Testing on 4112F-ON w/OS10"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%201/","text":"Dell OS10 Load Balancing with LAG Config In this test case the goal is to create a simple load balancer using a reverse LAG port. The idea is to have one input port which is then mirrored to a logical LAG port and at the other end of the LAG port is a number of security sensors. Helpful Links ONIE Network Install Process Overview Dell OS10 Manual My Configuration General Configuration ONIE host is running RHEL 8 I am using a Dell S4112F-ON for testing OS10 10.5.0.2 PFSense running DNS and DHCP as services RHEL Release Info NAME=\"Red Hat Enterprise Linux\" VERSION=\"8.0 (Ootpa)\" ID=\"rhel\" ID_LIKE=\"fedora\" VERSION_ID=\"8.0\" PLATFORM_ID=\"platform:el8\" PRETTY_NAME=\"Red Hat Enterprise Linux 8.0 (Ootpa)\" ANSI_COLOR=\"0;31\" CPE_NAME=\"cpe:/o:redhat:enterprise_linux:8.0:GA\" HOME_URL=\"https://www.redhat.com/\" BUG_REPORT_URL=\"https://bugzilla.redhat.com/\" REDHAT_BUGZILLA_PRODUCT=\"Red Hat Enterprise Linux 8\" REDHAT_BUGZILLA_PRODUCT_VERSION=8.0 REDHAT_SUPPORT_PRODUCT=\"Red Hat Enterprise Linux\" REDHAT_SUPPORT_PRODUCT_VERSION=\"8.0\" Red Hat Enterprise Linux release 8.0 (Ootpa) Red Hat Enterprise Linux release 8.0 (Ootpa) OS 10 Version OS10# show version Dell EMC Networking OS10 Enterprise Copyright (c) 1999-2019 by Dell Inc. All Rights Reserved. OS Version: 10.5.0.2 Build Version: 10.5.0.2.468 Build Time: 2019-10-19T00:29:00+0000 System Type: S4112F-ON Architecture: x86_64 Up Time: 00:03:39 Setup ONIE Prerequisites See ONIE Install Setup for instructions. Configure Management Interface See Configure Management Interface on Dell OS10 Configure Device for Reverse LAG Physical Configuration I used the following SFPs 1, 1Gb/s copper SFP (Ethernet 1/1/1) for input 2, 1Gb/s copper SFPs (Ethernet 1/1/5/Ethernet 1/1/9) and 1, 1Gb/s, fiber SFP (Ethernet 1/1/12) for output I used the following optics: Input Port Output Ports LAG Configuration Enable LAG Ports and Input Port Verify All Interfaces are Running at the Same Speed All interfaces must be the same speed in a LAG. In my case, the fiber interface was running at 10Gb/s so I brought that down to 1Gb/s by doing the following: OS10(config)# interface ethernet 1/1/12 OS10(conf-if-eth1/1/12)# speed 1000 OS10(conf-if-eth1/1/12)# <165>1 2019-10-28T19:10:22.616888+00:00 OS10 dn_alm 669 - - Node.1-Unit.1:PRI [event], Dell EMC (OS10) %IFM_OSTATE_DN: Interface operational state is down :ethernet1/1/12 OS10(conf-if-eth1/1/12)# OS10(conf-if-eth1/1/12)# <165>1 2019-10-28T19:10:29.591467+00:00 OS10 dn_alm 669 - - Node.1-Unit.1:PRI [event], Dell EMC (OS10) %IFM_OSTATE_UP: Interface operational state is up :ethernet1/1/12 Add Interfaces to the Port Channel Group OS10(config)# interface port-channel 1 OS10(conf-if-po-1)# exit OS10(config)# interface ethernet 1/1/5 OS10(conf-if-eth1/1/5)# channel-group 1 mode on OS10(conf-if-eth1/1/5)# <165>1 2019-10-28T19:17:33.746593+00:00 OS10 dn_alm 669 - - Node.1-Unit.1:PRI [event], Dell EMC (OS10) %IFM_OSTATE_UP: Interface operational state is up :port-channel1 OS10(conf-if-eth1/1/5)# exit OS10(config)# interface ethernet 1/1/9 OS10(conf-if-eth1/1/9)# channel-group 1 mode on OS10(conf-if-eth1/1/9)# exit OS10(config)# interface ethernet 1/1/12 OS10(conf-if-eth1/1/12)# channel-group 1 mode on Configure the Port Channel Hash Algorithm We want to load balance on the standard network 5 tuple. You can configure this with OS10(config)# load-balancing ip-selection destination-ip source-ip protocol l4-destination-port l4-source-port Configure Mirror Port Session from Source to LAG Interface Next we need to send all the traffic from our \"TAP\" input interface to our port channel to be load balanced out to all of our listening devices. OS10(config)# monitor session 1 OS10(conf-mon-local-1)# source interface ethernet 1/1/1 OS10(conf-mon-local-1)# destination interface port-channel 1 OS10(conf-mon-local-1)# no shut Final Configuration OS10# show running-configuration ! Version 10.5.0.2 ! Last configuration change at Oct 29 14:53:37 2019 ! ip vrf default ! interface breakout 1/1/13 map 100g-1x interface breakout 1/1/14 map 100g-1x interface breakout 1/1/15 map 100g-1x iscsi enable iscsi target port 860 iscsi target port 3260 system-user linuxadmin password XXXXX username admin password XXXXX role sysadmin priv-lvl 15 aaa authentication login default local aaa authentication login console local ! class-map type application class-iscsi ! policy-map type application policy-iscsi ! interface vlan1 no shutdown ! interface port-channel1 no shutdown switchport access vlan 1 ! interface mgmt1/1/1 no shutdown no ip address dhcp ip address 192.168.1.20/24 ipv6 address autoconfig ! interface ethernet1/1/1 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/2 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/3 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/4 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/5 no shutdown channel-group 1 no switchport flowcontrol receive on ! interface ethernet1/1/6 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/7 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/8 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/9 no shutdown channel-group 1 no switchport flowcontrol receive on ! interface ethernet1/1/10 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/11 no shutdown no switchport flowcontrol receive on ! interface ethernet1/1/12 no shutdown channel-group 1 no switchport speed 1000 flowcontrol receive on ! interface ethernet1/1/13 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/14 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/15 no shutdown switchport access vlan 1 flowcontrol receive on ! monitor session 1 destination interface port-channel1 source interface ethernet1/1/1 no shut ! snmp-server contact \"Contact Support\" ! telemetry Findings The reverse LAG strategy will load balance traffic, but there is a critical problem. The hash algorithm is sensitive to the order of the fields. This means that in a standard TCP conversation as the IP/TCP/UDP source and destinations reverse for inbound and outbound traffic they will always go to different hosts on a five tuple hash. For example, see the below: Host 1 Host 2 Host 3 If you look at host 1 and host 3 you can see that both sides of the traffic consistently landed on different sessions. Without modifying the guts of how the algorithm itself is implemented, there isn't a way to fix this. IE: The idea isn't going to work. The reason for this is that security sensors like Bro and Suricata require the complete conversation be sent to a single instance. That is to say, a single instance of Bro or Suricata must see the entire conversation. The configuration above will cause an instance to see only one side of any given conversation. Other Notes The default VLAN on our OS10 switch is VLAN 1 and is untagged. The default configuration of a port is Switchport access vlan 1 on all ports (factory default) All ports will show in vlan 1, and vlan 1 will be labeled as the default vlan using command \u201csho vlan\u201d If you change the default vlan using the command \u201cdefault vlan-id\u201d it will change the switchport access vlan on all interfaces that were in the default vlan to the new specified default vlan. default vlan-id 3 all vlan 1 ports get changed to vlan 3 ports automatically (vlan 3 is the new default vlan), and the interfaces will sho Switchport access vlan 3 If you want any port to be in a different untagged vlan other the default vlan, you must change it via the command \u201cswitchport access vlan \u201d On a trunk port, the default vlan is the native vlan. If you want to change the native vlan on trunk port, then you use the command \u201cswitchport access vlan \u201d So in my example I sent earlier The default vlan is vlan 1 on all ports except the trunk port. sho run will sho Switchport access vlan 1 on all interfaces except the trunk port because I changed it. I specified vlan 2 as the native vlan for the trunk port only. Untagged VLAN ==> switchport access vlan 2 Tagged VLAN ==> switchport trunk allowed vlan 1612-1615,3939 Example: interface ethernet1/1/17 description Node1_Port1 switchport mode trunk switchport access vlan 2 switchport trunk allowed vlan 1612-1615,3939 spanning-tree port type edge no shutdown","title":"Dell OS10 Load Balancing with LAG Config"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%201/#dell-os10-load-balancing-with-lag-config","text":"In this test case the goal is to create a simple load balancer using a reverse LAG port. The idea is to have one input port which is then mirrored to a logical LAG port and at the other end of the LAG port is a number of security sensors.","title":"Dell OS10 Load Balancing with LAG Config"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%201/#helpful-links","text":"ONIE Network Install Process Overview Dell OS10 Manual","title":"Helpful Links"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%201/#my-configuration","text":"","title":"My Configuration"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%201/#general-configuration","text":"ONIE host is running RHEL 8 I am using a Dell S4112F-ON for testing OS10 10.5.0.2 PFSense running DNS and DHCP as services","title":"General Configuration"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%201/#rhel-release-info","text":"NAME=\"Red Hat Enterprise Linux\" VERSION=\"8.0 (Ootpa)\" ID=\"rhel\" ID_LIKE=\"fedora\" VERSION_ID=\"8.0\" PLATFORM_ID=\"platform:el8\" PRETTY_NAME=\"Red Hat Enterprise Linux 8.0 (Ootpa)\" ANSI_COLOR=\"0;31\" CPE_NAME=\"cpe:/o:redhat:enterprise_linux:8.0:GA\" HOME_URL=\"https://www.redhat.com/\" BUG_REPORT_URL=\"https://bugzilla.redhat.com/\" REDHAT_BUGZILLA_PRODUCT=\"Red Hat Enterprise Linux 8\" REDHAT_BUGZILLA_PRODUCT_VERSION=8.0 REDHAT_SUPPORT_PRODUCT=\"Red Hat Enterprise Linux\" REDHAT_SUPPORT_PRODUCT_VERSION=\"8.0\" Red Hat Enterprise Linux release 8.0 (Ootpa) Red Hat Enterprise Linux release 8.0 (Ootpa)","title":"RHEL Release Info"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%201/#os-10-version","text":"OS10# show version Dell EMC Networking OS10 Enterprise Copyright (c) 1999-2019 by Dell Inc. All Rights Reserved. OS Version: 10.5.0.2 Build Version: 10.5.0.2.468 Build Time: 2019-10-19T00:29:00+0000 System Type: S4112F-ON Architecture: x86_64 Up Time: 00:03:39","title":"OS 10 Version"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%201/#setup-onie-prerequisites","text":"See ONIE Install Setup for instructions.","title":"Setup ONIE Prerequisites"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%201/#configure-management-interface","text":"See Configure Management Interface on Dell OS10","title":"Configure Management Interface"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%201/#configure-device-for-reverse-lag","text":"","title":"Configure Device for Reverse LAG"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%201/#physical-configuration","text":"I used the following SFPs 1, 1Gb/s copper SFP (Ethernet 1/1/1) for input 2, 1Gb/s copper SFPs (Ethernet 1/1/5/Ethernet 1/1/9) and 1, 1Gb/s, fiber SFP (Ethernet 1/1/12) for output I used the following optics:","title":"Physical Configuration"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%201/#input-port","text":"","title":"Input Port"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%201/#output-ports","text":"","title":"Output Ports"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%201/#lag-configuration","text":"","title":"LAG Configuration"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%201/#enable-lag-ports-and-input-port","text":"","title":"Enable LAG Ports and Input Port"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%201/#verify-all-interfaces-are-running-at-the-same-speed","text":"All interfaces must be the same speed in a LAG. In my case, the fiber interface was running at 10Gb/s so I brought that down to 1Gb/s by doing the following: OS10(config)# interface ethernet 1/1/12 OS10(conf-if-eth1/1/12)# speed 1000 OS10(conf-if-eth1/1/12)# <165>1 2019-10-28T19:10:22.616888+00:00 OS10 dn_alm 669 - - Node.1-Unit.1:PRI [event], Dell EMC (OS10) %IFM_OSTATE_DN: Interface operational state is down :ethernet1/1/12 OS10(conf-if-eth1/1/12)# OS10(conf-if-eth1/1/12)# <165>1 2019-10-28T19:10:29.591467+00:00 OS10 dn_alm 669 - - Node.1-Unit.1:PRI [event], Dell EMC (OS10) %IFM_OSTATE_UP: Interface operational state is up :ethernet1/1/12","title":"Verify All Interfaces are Running at the Same Speed"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%201/#add-interfaces-to-the-port-channel-group","text":"OS10(config)# interface port-channel 1 OS10(conf-if-po-1)# exit OS10(config)# interface ethernet 1/1/5 OS10(conf-if-eth1/1/5)# channel-group 1 mode on OS10(conf-if-eth1/1/5)# <165>1 2019-10-28T19:17:33.746593+00:00 OS10 dn_alm 669 - - Node.1-Unit.1:PRI [event], Dell EMC (OS10) %IFM_OSTATE_UP: Interface operational state is up :port-channel1 OS10(conf-if-eth1/1/5)# exit OS10(config)# interface ethernet 1/1/9 OS10(conf-if-eth1/1/9)# channel-group 1 mode on OS10(conf-if-eth1/1/9)# exit OS10(config)# interface ethernet 1/1/12 OS10(conf-if-eth1/1/12)# channel-group 1 mode on","title":"Add Interfaces to the Port Channel Group"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%201/#configure-the-port-channel-hash-algorithm","text":"We want to load balance on the standard network 5 tuple. You can configure this with OS10(config)# load-balancing ip-selection destination-ip source-ip protocol l4-destination-port l4-source-port","title":"Configure the Port Channel Hash Algorithm"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%201/#configure-mirror-port-session-from-source-to-lag-interface","text":"Next we need to send all the traffic from our \"TAP\" input interface to our port channel to be load balanced out to all of our listening devices. OS10(config)# monitor session 1 OS10(conf-mon-local-1)# source interface ethernet 1/1/1 OS10(conf-mon-local-1)# destination interface port-channel 1 OS10(conf-mon-local-1)# no shut","title":"Configure Mirror Port Session from Source to LAG Interface"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%201/#final-configuration","text":"OS10# show running-configuration ! Version 10.5.0.2 ! Last configuration change at Oct 29 14:53:37 2019 ! ip vrf default ! interface breakout 1/1/13 map 100g-1x interface breakout 1/1/14 map 100g-1x interface breakout 1/1/15 map 100g-1x iscsi enable iscsi target port 860 iscsi target port 3260 system-user linuxadmin password XXXXX username admin password XXXXX role sysadmin priv-lvl 15 aaa authentication login default local aaa authentication login console local ! class-map type application class-iscsi ! policy-map type application policy-iscsi ! interface vlan1 no shutdown ! interface port-channel1 no shutdown switchport access vlan 1 ! interface mgmt1/1/1 no shutdown no ip address dhcp ip address 192.168.1.20/24 ipv6 address autoconfig ! interface ethernet1/1/1 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/2 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/3 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/4 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/5 no shutdown channel-group 1 no switchport flowcontrol receive on ! interface ethernet1/1/6 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/7 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/8 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/9 no shutdown channel-group 1 no switchport flowcontrol receive on ! interface ethernet1/1/10 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/11 no shutdown no switchport flowcontrol receive on ! interface ethernet1/1/12 no shutdown channel-group 1 no switchport speed 1000 flowcontrol receive on ! interface ethernet1/1/13 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/14 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/15 no shutdown switchport access vlan 1 flowcontrol receive on ! monitor session 1 destination interface port-channel1 source interface ethernet1/1/1 no shut ! snmp-server contact \"Contact Support\" ! telemetry","title":"Final Configuration"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%201/#findings","text":"The reverse LAG strategy will load balance traffic, but there is a critical problem. The hash algorithm is sensitive to the order of the fields. This means that in a standard TCP conversation as the IP/TCP/UDP source and destinations reverse for inbound and outbound traffic they will always go to different hosts on a five tuple hash. For example, see the below:","title":"Findings"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%201/#host-1","text":"","title":"Host 1"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%201/#host-2","text":"","title":"Host 2"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%201/#host-3","text":"If you look at host 1 and host 3 you can see that both sides of the traffic consistently landed on different sessions. Without modifying the guts of how the algorithm itself is implemented, there isn't a way to fix this. IE: The idea isn't going to work. The reason for this is that security sensors like Bro and Suricata require the complete conversation be sent to a single instance. That is to say, a single instance of Bro or Suricata must see the entire conversation. The configuration above will cause an instance to see only one side of any given conversation.","title":"Host 3"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%201/#other-notes","text":"The default VLAN on our OS10 switch is VLAN 1 and is untagged. The default configuration of a port is Switchport access vlan 1 on all ports (factory default) All ports will show in vlan 1, and vlan 1 will be labeled as the default vlan using command \u201csho vlan\u201d If you change the default vlan using the command \u201cdefault vlan-id\u201d it will change the switchport access vlan on all interfaces that were in the default vlan to the new specified default vlan. default vlan-id 3 all vlan 1 ports get changed to vlan 3 ports automatically (vlan 3 is the new default vlan), and the interfaces will sho Switchport access vlan 3 If you want any port to be in a different untagged vlan other the default vlan, you must change it via the command \u201cswitchport access vlan \u201d On a trunk port, the default vlan is the native vlan. If you want to change the native vlan on trunk port, then you use the command \u201cswitchport access vlan \u201d So in my example I sent earlier The default vlan is vlan 1 on all ports except the trunk port. sho run will sho Switchport access vlan 1 on all interfaces except the trunk port because I changed it. I specified vlan 2 as the native vlan for the trunk port only. Untagged VLAN ==> switchport access vlan 2 Tagged VLAN ==> switchport trunk allowed vlan 1612-1615,3939 Example: interface ethernet1/1/17 description Node1_Port1 switchport mode trunk switchport access vlan 2 switchport trunk allowed vlan 1612-1615,3939 spanning-tree port type edge no shutdown","title":"Other Notes"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%202/","text":"Dell OS10 Load Balancing with LAG Config In this test case the goal is to create a simple load balancer using a reverse LAG port. The idea is to have one input port which is then mirrored to a logical LAG port and at the other end of the LAG port is a number of security sensors. This test case was a duplicate of test 1 except with 1 port unplugged to see how it affected the algorithm. Helpful Links ONIE Network Install Process Overview Dell OS10 Manual My Configuration General Configuration ONIE host is running RHEL 8 I am using a Dell S4112F-ON for testing OS10 10.5.0.2 PFSense running DNS and DHCP as services RHEL Release Info NAME=\"Red Hat Enterprise Linux\" VERSION=\"8.0 (Ootpa)\" ID=\"rhel\" ID_LIKE=\"fedora\" VERSION_ID=\"8.0\" PLATFORM_ID=\"platform:el8\" PRETTY_NAME=\"Red Hat Enterprise Linux 8.0 (Ootpa)\" ANSI_COLOR=\"0;31\" CPE_NAME=\"cpe:/o:redhat:enterprise_linux:8.0:GA\" HOME_URL=\"https://www.redhat.com/\" BUG_REPORT_URL=\"https://bugzilla.redhat.com/\" REDHAT_BUGZILLA_PRODUCT=\"Red Hat Enterprise Linux 8\" REDHAT_BUGZILLA_PRODUCT_VERSION=8.0 REDHAT_SUPPORT_PRODUCT=\"Red Hat Enterprise Linux\" REDHAT_SUPPORT_PRODUCT_VERSION=\"8.0\" Red Hat Enterprise Linux release 8.0 (Ootpa) Red Hat Enterprise Linux release 8.0 (Ootpa) OS 10 Version OS10# show version Dell EMC Networking OS10 Enterprise Copyright (c) 1999-2019 by Dell Inc. All Rights Reserved. OS Version: 10.5.0.2 Build Version: 10.5.0.2.468 Build Time: 2019-10-19T00:29:00+0000 System Type: S4112F-ON Architecture: x86_64 Up Time: 00:03:39 Setup ONIE Prerequisites See ONIE Install Setup for instructions. Configure Management Interface See Configure Management Interface on Dell OS10 Configure Device for LAG Physical Configuration 1, 1Gb/s copper SFP (Ethernet 1/1/1) for input 1, 1Gb/s copper SFPs (Ethernet 1/1/5) and 1, 1Gb/s, fiber SFP (Ethernet 1/1/12) for output MAJOR DIFFERENCE WITH TEST 1 : In this test Ethernet 1/1/9 was disconnected. I actually did this by accident originally. I discovered VMWare autonegotiates to 10Gb/s and if you leave the interface at 1Gb/s the interface will not come up. Configuration - Same as Test 1 Except with Ethernet 1/1/9 Unplugged OS10(config)# do show ip interface brief Interface Name IP-Address OK Method Status Protocol ========================================================================================= Ethernet 1/1/1 unassigned YES unset up up Ethernet 1/1/2 unassigned NO unset up down Ethernet 1/1/3 unassigned NO unset up down Ethernet 1/1/4 unassigned NO unset up down Ethernet 1/1/5 unassigned YES unset up up Ethernet 1/1/6 unassigned NO unset up down Ethernet 1/1/7 unassigned NO unset up down Ethernet 1/1/8 unassigned NO unset up down Ethernet 1/1/9 unassigned NO unset up down Ethernet 1/1/10 unassigned NO unset up down Ethernet 1/1/11 unassigned NO unset up down Ethernet 1/1/12 unassigned YES unset up up Ethernet 1/1/13 unassigned NO unset up down Ethernet 1/1/14 unassigned NO unset up down Ethernet 1/1/15 unassigned NO unset up down Management 1/1/1 192.168.1.20/24 YES manual up up Vlan 1 unassigned YES unset up up Port-channel 1 unassigned YES unset up up OS10(config)# do show running-configuration ! Version 10.5.0.2 ! Last configuration change at Nov 01 01:09:30 2019 ! ip vrf default ! interface breakout 1/1/13 map 100g-1x interface breakout 1/1/14 map 100g-1x interface breakout 1/1/15 map 100g-1x iscsi enable iscsi target port 860 iscsi target port 3260 system-user linuxadmin password $6$5DdOHYg5$JCE1vMSmkQOrbh31U74PIPv7lyOgRmba1IxhkYibppMXs1KM4Y.gbTPcxyMP/PHUkMc5rdk/ZLv9Sfv3ALtB61 username admin password $6$q9QBeYjZ$jfxzVqGhkxX3smxJSH9DDz7/3OJc6m5wjF8nnLD7/VKx8SloIhp4NoGZs0I/UNwh8WVuxwfd9q4pWIgNs5BKH. role sysadmin priv-lvl 15 aaa authentication login default local aaa authentication login console local ! class-map type application class-iscsi ! policy-map type application policy-iscsi ! interface vlan1 no shutdown ! interface port-channel1 no shutdown switchport access vlan 1 ! interface mgmt1/1/1 no shutdown no ip address dhcp ip address 192.168.1.20/24 ipv6 address autoconfig ! interface ethernet1/1/1 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/2 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/3 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/4 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/5 no shutdown channel-group 1 no switchport speed 1000 flowcontrol receive on ! interface ethernet1/1/6 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/7 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/8 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/9 no shutdown channel-group 1 no switchport speed 1000 flowcontrol receive on ! interface ethernet1/1/10 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/11 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/12 no shutdown channel-group 1 no switchport speed 1000 flowcontrol receive on ! interface ethernet1/1/13 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/14 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/15 no shutdown switchport access vlan 1 flowcontrol receive on ! monitor session 1 destination interface port-channel1 source interface ethernet1/1/1 no shut ! snmp-server contact \"Contact Support\" ! telemetry Findings ~~I noticed in this configuration traffic appears to balance correctly. Will need to sit down and think on the math.~~ I am no longer convinced these results are valid. See test case 3. After further examination it looks like on the surface it is working when in reality it may not be. Interface 1/1/9 was down because ESXi was set to 10Gb/s and I had set the speed on 9 manually to 1Gb/s causing it to go down. Host 1 Host 2","title":"Dell OS10 Load Balancing with LAG Config"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%202/#dell-os10-load-balancing-with-lag-config","text":"In this test case the goal is to create a simple load balancer using a reverse LAG port. The idea is to have one input port which is then mirrored to a logical LAG port and at the other end of the LAG port is a number of security sensors. This test case was a duplicate of test 1 except with 1 port unplugged to see how it affected the algorithm.","title":"Dell OS10 Load Balancing with LAG Config"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%202/#helpful-links","text":"ONIE Network Install Process Overview Dell OS10 Manual","title":"Helpful Links"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%202/#my-configuration","text":"","title":"My Configuration"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%202/#general-configuration","text":"ONIE host is running RHEL 8 I am using a Dell S4112F-ON for testing OS10 10.5.0.2 PFSense running DNS and DHCP as services","title":"General Configuration"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%202/#rhel-release-info","text":"NAME=\"Red Hat Enterprise Linux\" VERSION=\"8.0 (Ootpa)\" ID=\"rhel\" ID_LIKE=\"fedora\" VERSION_ID=\"8.0\" PLATFORM_ID=\"platform:el8\" PRETTY_NAME=\"Red Hat Enterprise Linux 8.0 (Ootpa)\" ANSI_COLOR=\"0;31\" CPE_NAME=\"cpe:/o:redhat:enterprise_linux:8.0:GA\" HOME_URL=\"https://www.redhat.com/\" BUG_REPORT_URL=\"https://bugzilla.redhat.com/\" REDHAT_BUGZILLA_PRODUCT=\"Red Hat Enterprise Linux 8\" REDHAT_BUGZILLA_PRODUCT_VERSION=8.0 REDHAT_SUPPORT_PRODUCT=\"Red Hat Enterprise Linux\" REDHAT_SUPPORT_PRODUCT_VERSION=\"8.0\" Red Hat Enterprise Linux release 8.0 (Ootpa) Red Hat Enterprise Linux release 8.0 (Ootpa)","title":"RHEL Release Info"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%202/#os-10-version","text":"OS10# show version Dell EMC Networking OS10 Enterprise Copyright (c) 1999-2019 by Dell Inc. All Rights Reserved. OS Version: 10.5.0.2 Build Version: 10.5.0.2.468 Build Time: 2019-10-19T00:29:00+0000 System Type: S4112F-ON Architecture: x86_64 Up Time: 00:03:39","title":"OS 10 Version"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%202/#setup-onie-prerequisites","text":"See ONIE Install Setup for instructions.","title":"Setup ONIE Prerequisites"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%202/#configure-management-interface","text":"See Configure Management Interface on Dell OS10","title":"Configure Management Interface"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%202/#configure-device-for-lag","text":"","title":"Configure Device for LAG"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%202/#physical-configuration","text":"1, 1Gb/s copper SFP (Ethernet 1/1/1) for input 1, 1Gb/s copper SFPs (Ethernet 1/1/5) and 1, 1Gb/s, fiber SFP (Ethernet 1/1/12) for output MAJOR DIFFERENCE WITH TEST 1 : In this test Ethernet 1/1/9 was disconnected. I actually did this by accident originally. I discovered VMWare autonegotiates to 10Gb/s and if you leave the interface at 1Gb/s the interface will not come up.","title":"Physical Configuration"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%202/#configuration-same-as-test-1-except-with-ethernet-119-unplugged","text":"OS10(config)# do show ip interface brief Interface Name IP-Address OK Method Status Protocol ========================================================================================= Ethernet 1/1/1 unassigned YES unset up up Ethernet 1/1/2 unassigned NO unset up down Ethernet 1/1/3 unassigned NO unset up down Ethernet 1/1/4 unassigned NO unset up down Ethernet 1/1/5 unassigned YES unset up up Ethernet 1/1/6 unassigned NO unset up down Ethernet 1/1/7 unassigned NO unset up down Ethernet 1/1/8 unassigned NO unset up down Ethernet 1/1/9 unassigned NO unset up down Ethernet 1/1/10 unassigned NO unset up down Ethernet 1/1/11 unassigned NO unset up down Ethernet 1/1/12 unassigned YES unset up up Ethernet 1/1/13 unassigned NO unset up down Ethernet 1/1/14 unassigned NO unset up down Ethernet 1/1/15 unassigned NO unset up down Management 1/1/1 192.168.1.20/24 YES manual up up Vlan 1 unassigned YES unset up up Port-channel 1 unassigned YES unset up up OS10(config)# do show running-configuration ! Version 10.5.0.2 ! Last configuration change at Nov 01 01:09:30 2019 ! ip vrf default ! interface breakout 1/1/13 map 100g-1x interface breakout 1/1/14 map 100g-1x interface breakout 1/1/15 map 100g-1x iscsi enable iscsi target port 860 iscsi target port 3260 system-user linuxadmin password $6$5DdOHYg5$JCE1vMSmkQOrbh31U74PIPv7lyOgRmba1IxhkYibppMXs1KM4Y.gbTPcxyMP/PHUkMc5rdk/ZLv9Sfv3ALtB61 username admin password $6$q9QBeYjZ$jfxzVqGhkxX3smxJSH9DDz7/3OJc6m5wjF8nnLD7/VKx8SloIhp4NoGZs0I/UNwh8WVuxwfd9q4pWIgNs5BKH. role sysadmin priv-lvl 15 aaa authentication login default local aaa authentication login console local ! class-map type application class-iscsi ! policy-map type application policy-iscsi ! interface vlan1 no shutdown ! interface port-channel1 no shutdown switchport access vlan 1 ! interface mgmt1/1/1 no shutdown no ip address dhcp ip address 192.168.1.20/24 ipv6 address autoconfig ! interface ethernet1/1/1 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/2 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/3 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/4 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/5 no shutdown channel-group 1 no switchport speed 1000 flowcontrol receive on ! interface ethernet1/1/6 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/7 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/8 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/9 no shutdown channel-group 1 no switchport speed 1000 flowcontrol receive on ! interface ethernet1/1/10 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/11 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/12 no shutdown channel-group 1 no switchport speed 1000 flowcontrol receive on ! interface ethernet1/1/13 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/14 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/15 no shutdown switchport access vlan 1 flowcontrol receive on ! monitor session 1 destination interface port-channel1 source interface ethernet1/1/1 no shut ! snmp-server contact \"Contact Support\" ! telemetry","title":"Configuration - Same as Test 1 Except with Ethernet 1/1/9 Unplugged"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%202/#findings","text":"~~I noticed in this configuration traffic appears to balance correctly. Will need to sit down and think on the math.~~ I am no longer convinced these results are valid. See test case 3. After further examination it looks like on the surface it is working when in reality it may not be. Interface 1/1/9 was down because ESXi was set to 10Gb/s and I had set the speed on 9 manually to 1Gb/s causing it to go down.","title":"Findings"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%202/#host-1","text":"","title":"Host 1"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%202/#host-2","text":"","title":"Host 2"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%203/","text":"Dell OS10 Load Balancing with LAG Config In this test case the goal is to create a simple load balancer using a reverse LAG port. The idea is to have one input port which is then mirrored to a logical LAG port and at the other end of the LAG port is a number of security sensors. This is a duplicate of test 1 to verify my results. I began questioning myself after looking more closely at the data. Helpful Links ONIE Network Install Process Overview Dell OS10 Manual My Configuration General Configuration ONIE host is running RHEL 8 I am using a Dell S4112F-ON for testing OS10 10.5.0.2 PFSense running DNS and DHCP as services RHEL Release Info NAME=\"Red Hat Enterprise Linux\" VERSION=\"8.0 (Ootpa)\" ID=\"rhel\" ID_LIKE=\"fedora\" VERSION_ID=\"8.0\" PLATFORM_ID=\"platform:el8\" PRETTY_NAME=\"Red Hat Enterprise Linux 8.0 (Ootpa)\" ANSI_COLOR=\"0;31\" CPE_NAME=\"cpe:/o:redhat:enterprise_linux:8.0:GA\" HOME_URL=\"https://www.redhat.com/\" BUG_REPORT_URL=\"https://bugzilla.redhat.com/\" REDHAT_BUGZILLA_PRODUCT=\"Red Hat Enterprise Linux 8\" REDHAT_BUGZILLA_PRODUCT_VERSION=8.0 REDHAT_SUPPORT_PRODUCT=\"Red Hat Enterprise Linux\" REDHAT_SUPPORT_PRODUCT_VERSION=\"8.0\" Red Hat Enterprise Linux release 8.0 (Ootpa) Red Hat Enterprise Linux release 8.0 (Ootpa) OS 10 Version OS10# show version Dell EMC Networking OS10 Enterprise Copyright (c) 1999-2019 by Dell Inc. All Rights Reserved. OS Version: 10.5.0.2 Build Version: 10.5.0.2.468 Build Time: 2019-10-19T00:29:00+0000 System Type: S4112F-ON Architecture: x86_64 Up Time: 00:03:39 Setup ONIE Prerequisites See ONIE Install Setup for instructions. Configure Management Interface See Configure Management Interface on Dell OS10 Configure Device for LAG Physical Configuration 1, 1Gb/s copper SFP (Ethernet 1/1/1) for input 2, 1Gb/s copper SFPs (Ethernet 1/1/5/Ethernet 1/1/10) and 1, 1Gb/s, fiber SFP (Ethernet 1/1/12) for output Configuration - Same as Test 1 The only change was I moved from port 9 to port 10. ! Version 10.5.0.2 ! Last configuration change at Nov 01 01:52:29 2019 ! ip vrf default ! interface breakout 1/1/13 map 100g-1x interface breakout 1/1/14 map 100g-1x interface breakout 1/1/15 map 100g-1x iscsi enable iscsi target port 860 iscsi target port 3260 system-user linuxadmin password $6$5DdOHYg5$JCE1vMSmkQOrbh31U74PIPv7lyOgRmba1IxhkYibppMXs1KM4Y.gbTPcxyMP/PHUkMc5rdk/ZLv9Sfv3ALtB61 username admin password $6$q9QBeYjZ$jfxzVqGhkxX3smxJSH9DDz7/3OJc6m5wjF8nnLD7/VKx8SloIhp4NoGZs0I/UNwh8WVuxwfd9q4pWIgNs5BKH. role sysadmin priv-lvl 15 aaa authentication login default local aaa authentication login console local ! class-map type application class-iscsi ! policy-map type application policy-iscsi ! interface vlan1 no shutdown ! interface port-channel1 no shutdown switchport access vlan 1 ! interface mgmt1/1/1 no shutdown no ip address dhcp ip address 192.168.1.20/24 ipv6 address autoconfig ! interface ethernet1/1/1 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/2 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/3 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/4 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/5 no shutdown channel-group 1 no switchport speed 1000 flowcontrol receive on ! interface ethernet1/1/6 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/7 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/8 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/9 no shutdown switchport access vlan 1 speed 1000 flowcontrol receive on ! interface ethernet1/1/10 no shutdown channel-group 1 no switchport speed 1000 flowcontrol receive on ! interface ethernet1/1/11 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/12 no shutdown channel-group 1 no switchport speed 1000 flowcontrol receive on ! interface ethernet1/1/13 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/14 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/15 no shutdown switchport access vlan 1 flowcontrol receive on ! monitor session 1 destination interface port-channel1 source interface ethernet1/1/1 no shut ! snmp-server contact \"Contact Support\" ! telemetry Findings I confirmed that the traffic did indeed go to different simulated sensors. This time I stopped the traffic generator, reset all the Wireshark sessions and then restarted the traffic generator. I set the filter on Wireshark to 13.107.42.12 ahead of time on all three before examining a specific stream more closely as seen on host 3. On host 3 you can see the synchronize packet go out with sequence number 3195700332 and you notice that the SYN, ACK response is missing. Look at host 2 and you can see the SYN, ACK with the expected response of 3195700333. Host 1 Host 2 Host 3","title":"Dell OS10 Load Balancing with LAG Config"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%203/#dell-os10-load-balancing-with-lag-config","text":"In this test case the goal is to create a simple load balancer using a reverse LAG port. The idea is to have one input port which is then mirrored to a logical LAG port and at the other end of the LAG port is a number of security sensors. This is a duplicate of test 1 to verify my results. I began questioning myself after looking more closely at the data.","title":"Dell OS10 Load Balancing with LAG Config"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%203/#helpful-links","text":"ONIE Network Install Process Overview Dell OS10 Manual","title":"Helpful Links"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%203/#my-configuration","text":"","title":"My Configuration"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%203/#general-configuration","text":"ONIE host is running RHEL 8 I am using a Dell S4112F-ON for testing OS10 10.5.0.2 PFSense running DNS and DHCP as services","title":"General Configuration"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%203/#rhel-release-info","text":"NAME=\"Red Hat Enterprise Linux\" VERSION=\"8.0 (Ootpa)\" ID=\"rhel\" ID_LIKE=\"fedora\" VERSION_ID=\"8.0\" PLATFORM_ID=\"platform:el8\" PRETTY_NAME=\"Red Hat Enterprise Linux 8.0 (Ootpa)\" ANSI_COLOR=\"0;31\" CPE_NAME=\"cpe:/o:redhat:enterprise_linux:8.0:GA\" HOME_URL=\"https://www.redhat.com/\" BUG_REPORT_URL=\"https://bugzilla.redhat.com/\" REDHAT_BUGZILLA_PRODUCT=\"Red Hat Enterprise Linux 8\" REDHAT_BUGZILLA_PRODUCT_VERSION=8.0 REDHAT_SUPPORT_PRODUCT=\"Red Hat Enterprise Linux\" REDHAT_SUPPORT_PRODUCT_VERSION=\"8.0\" Red Hat Enterprise Linux release 8.0 (Ootpa) Red Hat Enterprise Linux release 8.0 (Ootpa)","title":"RHEL Release Info"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%203/#os-10-version","text":"OS10# show version Dell EMC Networking OS10 Enterprise Copyright (c) 1999-2019 by Dell Inc. All Rights Reserved. OS Version: 10.5.0.2 Build Version: 10.5.0.2.468 Build Time: 2019-10-19T00:29:00+0000 System Type: S4112F-ON Architecture: x86_64 Up Time: 00:03:39","title":"OS 10 Version"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%203/#setup-onie-prerequisites","text":"See ONIE Install Setup for instructions.","title":"Setup ONIE Prerequisites"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%203/#configure-management-interface","text":"See Configure Management Interface on Dell OS10","title":"Configure Management Interface"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%203/#configure-device-for-lag","text":"","title":"Configure Device for LAG"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%203/#physical-configuration","text":"1, 1Gb/s copper SFP (Ethernet 1/1/1) for input 2, 1Gb/s copper SFPs (Ethernet 1/1/5/Ethernet 1/1/10) and 1, 1Gb/s, fiber SFP (Ethernet 1/1/12) for output","title":"Physical Configuration"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%203/#configuration-same-as-test-1","text":"The only change was I moved from port 9 to port 10. ! Version 10.5.0.2 ! Last configuration change at Nov 01 01:52:29 2019 ! ip vrf default ! interface breakout 1/1/13 map 100g-1x interface breakout 1/1/14 map 100g-1x interface breakout 1/1/15 map 100g-1x iscsi enable iscsi target port 860 iscsi target port 3260 system-user linuxadmin password $6$5DdOHYg5$JCE1vMSmkQOrbh31U74PIPv7lyOgRmba1IxhkYibppMXs1KM4Y.gbTPcxyMP/PHUkMc5rdk/ZLv9Sfv3ALtB61 username admin password $6$q9QBeYjZ$jfxzVqGhkxX3smxJSH9DDz7/3OJc6m5wjF8nnLD7/VKx8SloIhp4NoGZs0I/UNwh8WVuxwfd9q4pWIgNs5BKH. role sysadmin priv-lvl 15 aaa authentication login default local aaa authentication login console local ! class-map type application class-iscsi ! policy-map type application policy-iscsi ! interface vlan1 no shutdown ! interface port-channel1 no shutdown switchport access vlan 1 ! interface mgmt1/1/1 no shutdown no ip address dhcp ip address 192.168.1.20/24 ipv6 address autoconfig ! interface ethernet1/1/1 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/2 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/3 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/4 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/5 no shutdown channel-group 1 no switchport speed 1000 flowcontrol receive on ! interface ethernet1/1/6 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/7 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/8 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/9 no shutdown switchport access vlan 1 speed 1000 flowcontrol receive on ! interface ethernet1/1/10 no shutdown channel-group 1 no switchport speed 1000 flowcontrol receive on ! interface ethernet1/1/11 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/12 no shutdown channel-group 1 no switchport speed 1000 flowcontrol receive on ! interface ethernet1/1/13 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/14 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/15 no shutdown switchport access vlan 1 flowcontrol receive on ! monitor session 1 destination interface port-channel1 source interface ethernet1/1/1 no shut ! snmp-server contact \"Contact Support\" ! telemetry","title":"Configuration - Same as Test 1"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%203/#findings","text":"I confirmed that the traffic did indeed go to different simulated sensors. This time I stopped the traffic generator, reset all the Wireshark sessions and then restarted the traffic generator. I set the filter on Wireshark to 13.107.42.12 ahead of time on all three before examining a specific stream more closely as seen on host 3. On host 3 you can see the synchronize packet go out with sequence number 3195700332 and you notice that the SYN, ACK response is missing. Look at host 2 and you can see the SYN, ACK with the expected response of 3195700333.","title":"Findings"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%203/#host-1","text":"","title":"Host 1"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%203/#host-2","text":"","title":"Host 2"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%203/#host-3","text":"","title":"Host 3"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%204/","text":"Dell OS10 Load Balancing with LAG Config In this test case the goal is to create a simple load balancer using a reverse LAG port. The idea is to have one input port which is then mirrored to a logical LAG port and at the other end of the LAG port is a number of security sensors. After test 3 I added the command: OS10(config)# enhanced-hashing resilient-hashing lag Helpful Links ONIE Network Install Process Overview Dell OS10 Manual My Configuration General Configuration ONIE host is running RHEL 8 I am using a Dell S4112F-ON for testing OS10 10.5.0.2 PFSense running DNS and DHCP as services RHEL Release Info NAME=\"Red Hat Enterprise Linux\" VERSION=\"8.0 (Ootpa)\" ID=\"rhel\" ID_LIKE=\"fedora\" VERSION_ID=\"8.0\" PLATFORM_ID=\"platform:el8\" PRETTY_NAME=\"Red Hat Enterprise Linux 8.0 (Ootpa)\" ANSI_COLOR=\"0;31\" CPE_NAME=\"cpe:/o:redhat:enterprise_linux:8.0:GA\" HOME_URL=\"https://www.redhat.com/\" BUG_REPORT_URL=\"https://bugzilla.redhat.com/\" REDHAT_BUGZILLA_PRODUCT=\"Red Hat Enterprise Linux 8\" REDHAT_BUGZILLA_PRODUCT_VERSION=8.0 REDHAT_SUPPORT_PRODUCT=\"Red Hat Enterprise Linux\" REDHAT_SUPPORT_PRODUCT_VERSION=\"8.0\" Red Hat Enterprise Linux release 8.0 (Ootpa) Red Hat Enterprise Linux release 8.0 (Ootpa) OS 10 Version OS10# show version Dell EMC Networking OS10 Enterprise Copyright (c) 1999-2019 by Dell Inc. All Rights Reserved. OS Version: 10.5.0.2 Build Version: 10.5.0.2.468 Build Time: 2019-10-19T00:29:00+0000 System Type: S4112F-ON Architecture: x86_64 Up Time: 00:03:39 Setup ONIE Prerequisites See ONIE Install Setup for instructions. Configure Management Interface See Configure Management Interface on Dell OS10 Configure Device for LAG Physical Configuration 1, 1Gb/s copper SFP (Ethernet 1/1/1) for input 2, 1Gb/s copper SFPs (Ethernet 1/1/5/Ethernet 1/1/10) and 1, 1Gb/s, fiber SFP (Ethernet 1/1/12) for output Configuration ! Version 10.5.0.2 ! Last configuration change at Nov 01 02:25:00 2019 ! ip vrf default ! interface breakout 1/1/13 map 100g-1x interface breakout 1/1/14 map 100g-1x interface breakout 1/1/15 map 100g-1x iscsi enable iscsi target port 860 iscsi target port 3260 system-user linuxadmin password $6$5DdOHYg5$JCE1vMSmkQOrbh31U74PIPv7lyOgRmba1IxhkYibppMXs1KM4Y.gbTPcxyMP/PHUkMc5rdk/ZLv9Sfv3ALtB61 enhanced-hashing resilient-hashing lag username admin password $6$q9QBeYjZ$jfxzVqGhkxX3smxJSH9DDz7/3OJc6m5wjF8nnLD7/VKx8SloIhp4NoGZs0I/UNwh8WVuxwfd9q4pWIgNs5BKH. role sysadmin priv-lvl 15 aaa authentication login default local aaa authentication login console local ! class-map type application class-iscsi ! policy-map type application policy-iscsi ! interface vlan1 no shutdown ! interface port-channel1 no shutdown switchport access vlan 1 ! interface mgmt1/1/1 no shutdown no ip address dhcp ip address 192.168.1.20/24 ipv6 address autoconfig ! interface ethernet1/1/1 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/2 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/3 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/4 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/5 no shutdown channel-group 1 no switchport speed 1000 flowcontrol receive on ! interface ethernet1/1/6 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/7 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/8 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/9 no shutdown switchport access vlan 1 speed 1000 flowcontrol receive on ! interface ethernet1/1/10 no shutdown channel-group 1 no switchport speed 1000 flowcontrol receive on ! interface ethernet1/1/11 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/12 no shutdown channel-group 1 no switchport speed 1000 flowcontrol receive on ! interface ethernet1/1/13 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/14 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/15 no shutdown switchport access vlan 1 flowcontrol receive on ! monitor session 1 destination interface port-channel1 source interface ethernet1/1/1 no shut ! snmp-server contact \"Contact Support\" ! telemetry Findings This time traffic still went to different Wireshark sessions as you can see in the below. On host 3 you can see the synchronize packet go out with sequence number 3195700332 and you notice that the SYN, ACK response is missing. Look at host 2 and you can see the SYN, ACK with the expected response of 3195700333. Traffic to Different Wireshark Sessions However a session on host 1 seems to work correctly. Host 1 Session with Correct Output A More Definitive Test I wanted to be sure of my findings so I crafted a new PCAP. This time, I started a capture on my desktop and opened a new connection to vCenter knowing this should generate several new streams. I then closed the browser entirely to ensure those same sessions would close. I saved the capture off and sent it to my traffic replay system. I then played it back with tcpreplay . I then grabbed a random stream from the sequence to confirm whether I could see the entire three way hand shake on one host or not. As suspected the initial syn hit one Wireshark session and the response went to a separate Wireshark session. Host 1 Host 3","title":"Dell OS10 Load Balancing with LAG Config"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%204/#dell-os10-load-balancing-with-lag-config","text":"In this test case the goal is to create a simple load balancer using a reverse LAG port. The idea is to have one input port which is then mirrored to a logical LAG port and at the other end of the LAG port is a number of security sensors. After test 3 I added the command: OS10(config)# enhanced-hashing resilient-hashing lag","title":"Dell OS10 Load Balancing with LAG Config"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%204/#helpful-links","text":"ONIE Network Install Process Overview Dell OS10 Manual","title":"Helpful Links"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%204/#my-configuration","text":"","title":"My Configuration"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%204/#general-configuration","text":"ONIE host is running RHEL 8 I am using a Dell S4112F-ON for testing OS10 10.5.0.2 PFSense running DNS and DHCP as services","title":"General Configuration"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%204/#rhel-release-info","text":"NAME=\"Red Hat Enterprise Linux\" VERSION=\"8.0 (Ootpa)\" ID=\"rhel\" ID_LIKE=\"fedora\" VERSION_ID=\"8.0\" PLATFORM_ID=\"platform:el8\" PRETTY_NAME=\"Red Hat Enterprise Linux 8.0 (Ootpa)\" ANSI_COLOR=\"0;31\" CPE_NAME=\"cpe:/o:redhat:enterprise_linux:8.0:GA\" HOME_URL=\"https://www.redhat.com/\" BUG_REPORT_URL=\"https://bugzilla.redhat.com/\" REDHAT_BUGZILLA_PRODUCT=\"Red Hat Enterprise Linux 8\" REDHAT_BUGZILLA_PRODUCT_VERSION=8.0 REDHAT_SUPPORT_PRODUCT=\"Red Hat Enterprise Linux\" REDHAT_SUPPORT_PRODUCT_VERSION=\"8.0\" Red Hat Enterprise Linux release 8.0 (Ootpa) Red Hat Enterprise Linux release 8.0 (Ootpa)","title":"RHEL Release Info"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%204/#os-10-version","text":"OS10# show version Dell EMC Networking OS10 Enterprise Copyright (c) 1999-2019 by Dell Inc. All Rights Reserved. OS Version: 10.5.0.2 Build Version: 10.5.0.2.468 Build Time: 2019-10-19T00:29:00+0000 System Type: S4112F-ON Architecture: x86_64 Up Time: 00:03:39","title":"OS 10 Version"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%204/#setup-onie-prerequisites","text":"See ONIE Install Setup for instructions.","title":"Setup ONIE Prerequisites"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%204/#configure-management-interface","text":"See Configure Management Interface on Dell OS10","title":"Configure Management Interface"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%204/#configure-device-for-lag","text":"","title":"Configure Device for LAG"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%204/#physical-configuration","text":"1, 1Gb/s copper SFP (Ethernet 1/1/1) for input 2, 1Gb/s copper SFPs (Ethernet 1/1/5/Ethernet 1/1/10) and 1, 1Gb/s, fiber SFP (Ethernet 1/1/12) for output","title":"Physical Configuration"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%204/#configuration","text":"! Version 10.5.0.2 ! Last configuration change at Nov 01 02:25:00 2019 ! ip vrf default ! interface breakout 1/1/13 map 100g-1x interface breakout 1/1/14 map 100g-1x interface breakout 1/1/15 map 100g-1x iscsi enable iscsi target port 860 iscsi target port 3260 system-user linuxadmin password $6$5DdOHYg5$JCE1vMSmkQOrbh31U74PIPv7lyOgRmba1IxhkYibppMXs1KM4Y.gbTPcxyMP/PHUkMc5rdk/ZLv9Sfv3ALtB61 enhanced-hashing resilient-hashing lag username admin password $6$q9QBeYjZ$jfxzVqGhkxX3smxJSH9DDz7/3OJc6m5wjF8nnLD7/VKx8SloIhp4NoGZs0I/UNwh8WVuxwfd9q4pWIgNs5BKH. role sysadmin priv-lvl 15 aaa authentication login default local aaa authentication login console local ! class-map type application class-iscsi ! policy-map type application policy-iscsi ! interface vlan1 no shutdown ! interface port-channel1 no shutdown switchport access vlan 1 ! interface mgmt1/1/1 no shutdown no ip address dhcp ip address 192.168.1.20/24 ipv6 address autoconfig ! interface ethernet1/1/1 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/2 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/3 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/4 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/5 no shutdown channel-group 1 no switchport speed 1000 flowcontrol receive on ! interface ethernet1/1/6 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/7 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/8 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/9 no shutdown switchport access vlan 1 speed 1000 flowcontrol receive on ! interface ethernet1/1/10 no shutdown channel-group 1 no switchport speed 1000 flowcontrol receive on ! interface ethernet1/1/11 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/12 no shutdown channel-group 1 no switchport speed 1000 flowcontrol receive on ! interface ethernet1/1/13 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/14 no shutdown switchport access vlan 1 flowcontrol receive on ! interface ethernet1/1/15 no shutdown switchport access vlan 1 flowcontrol receive on ! monitor session 1 destination interface port-channel1 source interface ethernet1/1/1 no shut ! snmp-server contact \"Contact Support\" ! telemetry","title":"Configuration"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%204/#findings","text":"This time traffic still went to different Wireshark sessions as you can see in the below. On host 3 you can see the synchronize packet go out with sequence number 3195700332 and you notice that the SYN, ACK response is missing. Look at host 2 and you can see the SYN, ACK with the expected response of 3195700333.","title":"Findings"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%204/#traffic-to-different-wireshark-sessions","text":"However a session on host 1 seems to work correctly.","title":"Traffic to Different Wireshark Sessions"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%204/#host-1-session-with-correct-output","text":"","title":"Host 1 Session with Correct Output"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%204/#a-more-definitive-test","text":"I wanted to be sure of my findings so I crafted a new PCAP. This time, I started a capture on my desktop and opened a new connection to vCenter knowing this should generate several new streams. I then closed the browser entirely to ensure those same sessions would close. I saved the capture off and sent it to my traffic replay system. I then played it back with tcpreplay . I then grabbed a random stream from the sequence to confirm whether I could see the entire three way hand shake on one host or not. As suspected the initial syn hit one Wireshark session and the response went to a separate Wireshark session.","title":"A More Definitive Test"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%204/#host-1","text":"","title":"Host 1"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OS10/Test%20Case%204/#host-3","text":"","title":"Host 3"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OpenSwitch%20%28OPX%29/","text":"Load Balancing with LAG OPX In this test case the goal is to create a simple packet broker using a reverse LAG port. Helpful Links ONIE Network Install Process Overview OPX Install Instructions for Dell EMC Equipment OPX Tools Source Code OPX Command Reference OPX LAG Command Documenattion OPX Docs Home List of Supported Hardware Helpful Debug Commands cps_get_oid.py -qua target base-switch/switching-entities/switching-entity cps_model_info base-switch/switching-entities/switching-entity cps_set_oid.py -qua target base-switch/switching-entities/switching-entity name=lag-hash-fields attr=src-ip,dest-ip,l4-dest-port,l4-src-port,ip-protocol My Configuration General Configuration ONIE host is running RHEL 8 I am using a Dell S4112F-ON for testing OpenSwitch version PKGS_OPX-3.2.0-installer-x86_64 PFSense running DNS and DHCP as services RHEL Release Info NAME=\"Red Hat Enterprise Linux\" VERSION=\"8.0 (Ootpa)\" ID=\"rhel\" ID_LIKE=\"fedora\" VERSION_ID=\"8.0\" PLATFORM_ID=\"platform:el8\" PRETTY_NAME=\"Red Hat Enterprise Linux 8.0 (Ootpa)\" ANSI_COLOR=\"0;31\" CPE_NAME=\"cpe:/o:redhat:enterprise_linux:8.0:GA\" HOME_URL=\"https://www.redhat.com/\" BUG_REPORT_URL=\"https://bugzilla.redhat.com/\" REDHAT_BUGZILLA_PRODUCT=\"Red Hat Enterprise Linux 8\" REDHAT_BUGZILLA_PRODUCT_VERSION=8.0 REDHAT_SUPPORT_PRODUCT=\"Red Hat Enterprise Linux\" REDHAT_SUPPORT_PRODUCT_VERSION=\"8.0\" Red Hat Enterprise Linux release 8.0 (Ootpa) Red Hat Enterprise Linux release 8.0 (Ootpa) OPX Version OS_NAME=\"OPX\" OS_VERSION=\"unstable\" PLATFORM=\"S4112F-ON\" ARCHITECTURE=\"x86_64\" INTERNAL_BUILD_ID=\"OpenSwitch blueprint for Dell 1.0.0\" BUILD_VERSION=\"unstable.0-stretch\" BUILD_DATE=\"2019-06-21T19:04:22+0000\" INSTALL_DATE=\"2019-10-23T23:16:10+00:00\" SYSTEM_UPTIME= 1 day, 5 minutes SYSTEM_STATE= running UPGRADED_PACKAGES=no ALTERED_PACKAGES=yes Wasn't sure why I got the unstable version after installation. It didn't cause any problems for testing so I just left it as is. Setup ONIE Prerequisites See ONIE Install Setup for instructions. Configure Device for Reverse LAG Physical Configuration I used the following SFPs 1, 1Gb/s copper SFP (e101-001-0) for input 2, 1Gb/s copper SFPs (e101-005-0/e101-009-0) and 1, 10Gb/s, fiber SFP (e101-012-0) for output Input Port Output Ports LAG Configuration Enable LAG Ports and Input Port root@OPX:~# ip link set e101-001-0 up root@OPX:~# ip link set e101-005-0 up root@OPX:~# ip link set e101-009-0 up root@OPX:~# ip link set e101-012-0 up root@OPX:~# opx-show-interface --summary Port | Enabled | Operational status | Supported speed ----------------------------------------------------------- e101-001-0 | yes | up | 1G 10G e101-002-0 | no | down | 1G 10G e101-003-0 | no | down | 1G 10G e101-004-0 | no | down | 1G 10G e101-005-0 | yes | up | 1G 10G e101-006-0 | no | down | 1G 10G e101-007-0 | no | down | 1G 10G e101-008-0 | no | down | 1G 10G e101-009-0 | yes | up | 1G 10G e101-010-0 | no | down | 1G 10G e101-011-0 | no | down | 1G 10G e101-012-0 | yes | up | 1G 10G e101-013-0 | no | down | 100G e101-014-0 | no | down | 100G e101-015-0 | no | down | 100G eth0 | yes | UNKNOWN | UNKNOWN Configure LAG Configure LAG Algorithm You can see the switch's global paramters with the opx-show-global-switch command: root@OPX:~# opx-show-global-switch Switch id 0 ACL entry max priority: 2147483647 ACL entry min priority: 0 ACL table max priority: 11 ACL table min priority: 0 Bridge table size: 147456 BST enable: off BST tracking mode: current Counter refresh interval: 5 s Default mac address: 88:6f:d4:98:b7:80 ECMP group size: 256 ECMP hash algorithm: crc ECMP hash seed value: 0 Egress buffer pool num: 4 Ingress buffer pool num: 4 IPv6 extended prefix routes: 0 IPv6 extended prefix routes lpm block size: 1024 L3 nexthop table size: 32768 LAG hash algorithm: crc LAG hash seed value: 0 MAC address aging timer: 1800 s Max ECMP entries per group: 0 Max IPv6 extended prefix routes: 3072 Max MTU: 9216 Max VXLAN overlay nexthops: 4096 Max VXLAN overlay rifs: 2048 Number of multicast queues per port: 10 Number of queues cpu port: 43 Number of queues per port: 20 Number of unicast queues per port: 10 QoS rate adjust: 0 RIF table size: 12288 Switch mode: store and forward Temperature: 49 deg. C Total buffer size: 12188 UFT mode: default UFT host table size: 135168 UFT L2 mac table size: 147456 UFT L3 route table size: 16384 VXLAN riot enable: on If you want to change the hash algorithm you can do so with opx-config-global-switch --lag-hash-alg <crc | random | xor> I went ahead and left mine are CRC. This article from Dell can be helpful in deciding. Configure LAG Fields More imporantly you will probably want to configure what fields are used to determine the hash. The options are: src-mac: The source MAC address of the frame dest-mac: The destination MAC of the frame vlan-id: The VLAN ID listed in the frame ethertype: The ethertype of the frame ip-protocol: The IP protocol field in the IPv4 header src-ip: The packet source IP dest-ip: The destination IP of the packet l4-dest-port: The destination port of the segment l4-src-port: The source port of the segment in-port: The port from which the packet entered. It is unlikely you would want to use this for a reverse LAG I will use a standard 5-tuple configuration (src/dst IP, src/dest port, protocol #) Create LAG opx-config-lag create --name reverse_lag --unblockedports e101-005-0,e101-009-0,e101-012-0 --enable Results I wasn't able to complete the config. There is a bug in OPX preventing you from being able to set the fields on which the LAG will hash. I spent about a day working my way through the problem. See current status on this bug ticket","title":"Load Balancing with LAG OPX"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OpenSwitch%20%28OPX%29/#load-balancing-with-lag-opx","text":"In this test case the goal is to create a simple packet broker using a reverse LAG port.","title":"Load Balancing with LAG OPX"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OpenSwitch%20%28OPX%29/#helpful-links","text":"ONIE Network Install Process Overview OPX Install Instructions for Dell EMC Equipment OPX Tools Source Code OPX Command Reference OPX LAG Command Documenattion OPX Docs Home List of Supported Hardware","title":"Helpful Links"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OpenSwitch%20%28OPX%29/#helpful-debug-commands","text":"cps_get_oid.py -qua target base-switch/switching-entities/switching-entity cps_model_info base-switch/switching-entities/switching-entity cps_set_oid.py -qua target base-switch/switching-entities/switching-entity name=lag-hash-fields attr=src-ip,dest-ip,l4-dest-port,l4-src-port,ip-protocol","title":"Helpful Debug Commands"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OpenSwitch%20%28OPX%29/#my-configuration","text":"","title":"My Configuration"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OpenSwitch%20%28OPX%29/#general-configuration","text":"ONIE host is running RHEL 8 I am using a Dell S4112F-ON for testing OpenSwitch version PKGS_OPX-3.2.0-installer-x86_64 PFSense running DNS and DHCP as services","title":"General Configuration"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OpenSwitch%20%28OPX%29/#rhel-release-info","text":"NAME=\"Red Hat Enterprise Linux\" VERSION=\"8.0 (Ootpa)\" ID=\"rhel\" ID_LIKE=\"fedora\" VERSION_ID=\"8.0\" PLATFORM_ID=\"platform:el8\" PRETTY_NAME=\"Red Hat Enterprise Linux 8.0 (Ootpa)\" ANSI_COLOR=\"0;31\" CPE_NAME=\"cpe:/o:redhat:enterprise_linux:8.0:GA\" HOME_URL=\"https://www.redhat.com/\" BUG_REPORT_URL=\"https://bugzilla.redhat.com/\" REDHAT_BUGZILLA_PRODUCT=\"Red Hat Enterprise Linux 8\" REDHAT_BUGZILLA_PRODUCT_VERSION=8.0 REDHAT_SUPPORT_PRODUCT=\"Red Hat Enterprise Linux\" REDHAT_SUPPORT_PRODUCT_VERSION=\"8.0\" Red Hat Enterprise Linux release 8.0 (Ootpa) Red Hat Enterprise Linux release 8.0 (Ootpa)","title":"RHEL Release Info"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OpenSwitch%20%28OPX%29/#opx-version","text":"OS_NAME=\"OPX\" OS_VERSION=\"unstable\" PLATFORM=\"S4112F-ON\" ARCHITECTURE=\"x86_64\" INTERNAL_BUILD_ID=\"OpenSwitch blueprint for Dell 1.0.0\" BUILD_VERSION=\"unstable.0-stretch\" BUILD_DATE=\"2019-06-21T19:04:22+0000\" INSTALL_DATE=\"2019-10-23T23:16:10+00:00\" SYSTEM_UPTIME= 1 day, 5 minutes SYSTEM_STATE= running UPGRADED_PACKAGES=no ALTERED_PACKAGES=yes Wasn't sure why I got the unstable version after installation. It didn't cause any problems for testing so I just left it as is.","title":"OPX Version"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OpenSwitch%20%28OPX%29/#setup-onie-prerequisites","text":"See ONIE Install Setup for instructions.","title":"Setup ONIE Prerequisites"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OpenSwitch%20%28OPX%29/#configure-device-for-reverse-lag","text":"","title":"Configure Device for Reverse LAG"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OpenSwitch%20%28OPX%29/#physical-configuration","text":"I used the following SFPs 1, 1Gb/s copper SFP (e101-001-0) for input 2, 1Gb/s copper SFPs (e101-005-0/e101-009-0) and 1, 10Gb/s, fiber SFP (e101-012-0) for output","title":"Physical Configuration"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OpenSwitch%20%28OPX%29/#input-port","text":"","title":"Input Port"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OpenSwitch%20%28OPX%29/#output-ports","text":"","title":"Output Ports"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OpenSwitch%20%28OPX%29/#lag-configuration","text":"","title":"LAG Configuration"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OpenSwitch%20%28OPX%29/#enable-lag-ports-and-input-port","text":"root@OPX:~# ip link set e101-001-0 up root@OPX:~# ip link set e101-005-0 up root@OPX:~# ip link set e101-009-0 up root@OPX:~# ip link set e101-012-0 up root@OPX:~# opx-show-interface --summary Port | Enabled | Operational status | Supported speed ----------------------------------------------------------- e101-001-0 | yes | up | 1G 10G e101-002-0 | no | down | 1G 10G e101-003-0 | no | down | 1G 10G e101-004-0 | no | down | 1G 10G e101-005-0 | yes | up | 1G 10G e101-006-0 | no | down | 1G 10G e101-007-0 | no | down | 1G 10G e101-008-0 | no | down | 1G 10G e101-009-0 | yes | up | 1G 10G e101-010-0 | no | down | 1G 10G e101-011-0 | no | down | 1G 10G e101-012-0 | yes | up | 1G 10G e101-013-0 | no | down | 100G e101-014-0 | no | down | 100G e101-015-0 | no | down | 100G eth0 | yes | UNKNOWN | UNKNOWN","title":"Enable LAG Ports and Input Port"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OpenSwitch%20%28OPX%29/#configure-lag","text":"","title":"Configure LAG"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OpenSwitch%20%28OPX%29/#configure-lag-algorithm","text":"You can see the switch's global paramters with the opx-show-global-switch command: root@OPX:~# opx-show-global-switch Switch id 0 ACL entry max priority: 2147483647 ACL entry min priority: 0 ACL table max priority: 11 ACL table min priority: 0 Bridge table size: 147456 BST enable: off BST tracking mode: current Counter refresh interval: 5 s Default mac address: 88:6f:d4:98:b7:80 ECMP group size: 256 ECMP hash algorithm: crc ECMP hash seed value: 0 Egress buffer pool num: 4 Ingress buffer pool num: 4 IPv6 extended prefix routes: 0 IPv6 extended prefix routes lpm block size: 1024 L3 nexthop table size: 32768 LAG hash algorithm: crc LAG hash seed value: 0 MAC address aging timer: 1800 s Max ECMP entries per group: 0 Max IPv6 extended prefix routes: 3072 Max MTU: 9216 Max VXLAN overlay nexthops: 4096 Max VXLAN overlay rifs: 2048 Number of multicast queues per port: 10 Number of queues cpu port: 43 Number of queues per port: 20 Number of unicast queues per port: 10 QoS rate adjust: 0 RIF table size: 12288 Switch mode: store and forward Temperature: 49 deg. C Total buffer size: 12188 UFT mode: default UFT host table size: 135168 UFT L2 mac table size: 147456 UFT L3 route table size: 16384 VXLAN riot enable: on If you want to change the hash algorithm you can do so with opx-config-global-switch --lag-hash-alg <crc | random | xor> I went ahead and left mine are CRC. This article from Dell can be helpful in deciding.","title":"Configure LAG Algorithm"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OpenSwitch%20%28OPX%29/#configure-lag-fields","text":"More imporantly you will probably want to configure what fields are used to determine the hash. The options are: src-mac: The source MAC address of the frame dest-mac: The destination MAC of the frame vlan-id: The VLAN ID listed in the frame ethertype: The ethertype of the frame ip-protocol: The IP protocol field in the IPv4 header src-ip: The packet source IP dest-ip: The destination IP of the packet l4-dest-port: The destination port of the segment l4-src-port: The source port of the segment in-port: The port from which the packet entered. It is unlikely you would want to use this for a reverse LAG I will use a standard 5-tuple configuration (src/dst IP, src/dest port, protocol #)","title":"Configure LAG Fields"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OpenSwitch%20%28OPX%29/#create-lag","text":"opx-config-lag create --name reverse_lag --unblockedports e101-005-0,e101-009-0,e101-012-0 --enable","title":"Create LAG"},{"location":"Load%20Balance%20Testing%20on%204112F-ON/OpenSwitch%20%28OPX%29/#results","text":"I wasn't able to complete the config. There is a bug in OPX preventing you from being able to set the fields on which the LAG will hash. I spent about a day working my way through the problem. See current status on this bug ticket","title":"Results"},{"location":"Load%20Balance%20Testing%20with%20OpenVSwitch/","text":"Load Balance Testing with OpenVSwitch From tutorial ovs-conntrack Testing performed on OS10 v Setup ip netns add left ip netns add right ip link add veth_l0 type veth peer name veth_l1 ip link set veth_l1 netns left ip link add veth_r0 type veth peer name veth_r1 ip link set veth_r1 netns right ovs-vsctl add-br br0 ip a s | less ovs-vsctl add-port br0 veth_l0 ovs-vsctl add-port br0 veth_r0 ip netns exec left sudo ip link set lo up ip netns exec right sudo ip link set lo up Generate TCP segments ip netns exec left sudo `which scapy` ip netns exec right sudo `which scapy` Matching TCP packets Simple flows for port to port ovs-ofctl add-flow br0 \"table=0, priority=10, in_port=veth_l0, actions=veth_r0\" ovs-ofctl add-flow br0 \"table=0, priority=10, in_port=veth_r0, actions=veth_l0\" Flow matching ovs-ofctl add-flow br0 \"table=0, priority=50, ct_state=-trk, tcp, in_port=veth_l0, actions=ct(table=0)\" ovs-ofctl add-flow br0 \"table=0, priority=50, ct_state=+trk+new, tcp, in_port=veth_l0, actions=ct(commit),veth_r0\" ovs-ofctl add-flow br0 \"table=0, priority=50, ct_state=-trk, tcp, in_port=veth_r0, actions=ct(table=0)\" ovs-ofctl add-flow br0 \"table=0, priority=50, ct_state=+trk+est, tcp, in_port=veth_r0, actions=veth_l0\" ovs-ofctl add-flow br0 \"table=0, priority=50, ct_state=+trk+est, tcp, in_port=veth_l0, actions=veth_r0\" End result You can do cool stuff, but it won't work/wouldn't be a great way to do this.","title":"Load Balance Testing with OpenVSwitch"},{"location":"Load%20Balance%20Testing%20with%20OpenVSwitch/#load-balance-testing-with-openvswitch","text":"From tutorial ovs-conntrack Testing performed on OS10 v","title":"Load Balance Testing with OpenVSwitch"},{"location":"Load%20Balance%20Testing%20with%20OpenVSwitch/#setup","text":"ip netns add left ip netns add right ip link add veth_l0 type veth peer name veth_l1 ip link set veth_l1 netns left ip link add veth_r0 type veth peer name veth_r1 ip link set veth_r1 netns right ovs-vsctl add-br br0 ip a s | less ovs-vsctl add-port br0 veth_l0 ovs-vsctl add-port br0 veth_r0 ip netns exec left sudo ip link set lo up ip netns exec right sudo ip link set lo up","title":"Setup"},{"location":"Load%20Balance%20Testing%20with%20OpenVSwitch/#generate-tcp-segments","text":"ip netns exec left sudo `which scapy` ip netns exec right sudo `which scapy`","title":"Generate TCP segments"},{"location":"Load%20Balance%20Testing%20with%20OpenVSwitch/#matching-tcp-packets","text":"","title":"Matching TCP packets"},{"location":"Load%20Balance%20Testing%20with%20OpenVSwitch/#simple-flows-for-port-to-port","text":"ovs-ofctl add-flow br0 \"table=0, priority=10, in_port=veth_l0, actions=veth_r0\" ovs-ofctl add-flow br0 \"table=0, priority=10, in_port=veth_r0, actions=veth_l0\"","title":"Simple flows for port to port"},{"location":"Load%20Balance%20Testing%20with%20OpenVSwitch/#flow-matching","text":"ovs-ofctl add-flow br0 \"table=0, priority=50, ct_state=-trk, tcp, in_port=veth_l0, actions=ct(table=0)\" ovs-ofctl add-flow br0 \"table=0, priority=50, ct_state=+trk+new, tcp, in_port=veth_l0, actions=ct(commit),veth_r0\" ovs-ofctl add-flow br0 \"table=0, priority=50, ct_state=-trk, tcp, in_port=veth_r0, actions=ct(table=0)\" ovs-ofctl add-flow br0 \"table=0, priority=50, ct_state=+trk+est, tcp, in_port=veth_r0, actions=veth_l0\" ovs-ofctl add-flow br0 \"table=0, priority=50, ct_state=+trk+est, tcp, in_port=veth_l0, actions=veth_r0\"","title":"Flow matching"},{"location":"Load%20Balance%20Testing%20with%20OpenVSwitch/#end-result","text":"You can do cool stuff, but it won't work/wouldn't be a great way to do this.","title":"End result"},{"location":"Load%20Balancing%20on%20Mellanox%20Switches/","text":"Load Balancing on Mellanox Switches In this test case the goal is to create a simple load balancer using a reverse LAG port. The idea is to have one input port which is then mirrored to a logical LAG port and at the other end of the LAG port is a number of security sensors. Version Info mellanox.lan [standalone: master] # show version Product name: Onyx Product release: 3.8.2004 Build ID: #1-dev Build date: 2019-09-23 14:19:47 Target arch: x86_64 Target hw: x86_64 Built by: jenkins@7ae5fd122b61 Version summary: X86_64 3.8.2004 2019-09-23 14:19:47 x86_64 Product model: x86onie Host ID: B8599FD560BE System serial num: MT1940T00588 System UUID: 5f4c5ed2-e60b-11e9-8000-b8599f7f6f40 Uptime: 17h 27m 36.480s CPU load averages: 3.17 / 3.17 / 3.11 Number of CPUs: 4 System memory: 2738 MB used / 5065 MB free / 7803 MB total Swap: 0 MB used / 0 MB free / 0 MB total Connect to the Console Port and Management Ethernet Port Plug in both the management Ethernet cable and the serial cable. The console port is the bottom port and the ethernet management port is the top port. I had to plug the console cable into a specific USB slot on the server. It didn't work in the first one I tried. See picture below. This likely has nothing to do with the Mellanox switch itself, but as a note for those that come after you may want to try different USB ports if you find you aren't getting output on the first one you try and are confident you have the correct settings. I used the following console configuration: Baud Rate: 115200 Data Bits: 8 Stop Bits: 1 Parity: None Flow Control: None Update to Latest Version of MLNX-OS I pulled updates here The system uses a web server target for updates. I had Apache running on a RHEL 8 box. Download the update file and then upload it to your web server's root directory. On the switch itself (over a console port) do the following: switch > enable switch # configure terminal switch (config) # show images # Delete the old image if it exists. It will be under # \"Images available to be installed\" switch (config) # image delete <old_image> Download the new image from your web server with mellanox.lan [standalone: master] # image fetch http://rhel8.lan/onyx-X86_64-3.8.2004.img 100.0% [################################################################################################################################################################################################################################################################] Next install the updated OS with: mellanox.lan [standalone: master] # image install onyx-X86_64-3.8.2004.img location 2 progress track verify check-sig Step 1 of 4: Verify Image 100.0% [#################################################################] Step 2 of 4: Uncompress Image 100.0% [#################################################################] Step 3 of 4: Create Filesystems 100.0% [#################################################################] Step 4 of 4: Extract Image 98.6% [################################################################ ] 100.0% [#################################################################] Now set the switch to load from the new operating system and reload: mellanox.lan [standalone: master] # image boot next mellanox.lan [standalone: master] # reload Physical Configuration I used the following port configuration: 1, 1Gb/s copper SFP (Eth1/1) for input 2, 1Gb/s copper SFPs (Ethernet 1/1/5/Ethernet 1/1/9) and 1, 1Gb/s, fiber SFP (Ethernet 1/1/12) for output I used the following optics: Connect the input port to port 1. I connected my output ports in the following way: Bringing the Interfaces Up Mellanox does not perform testing with 3rd party NICs. During testing we found that autonegotiation of speed will not work on standard Dell SFPs. Use the below command on each interface to set the speed manually: mellanox.lan [standalone: master] (config interface ethernet 1/1) # speed 1G Configure the LAG Initial Configuration mellanox.lan [standalone: master] (config) # port-channel load-balance ethernet source-destination-mac source-destination-ip source-destination-port symmetric mellanox.lan [standalone: master] (config) # interface port-channel 1 mellanox.lan [standalone: master] (config interface port-channel 1) # switchport mode hybrid mellanox.lan [standalone: master] (config interface port-channel 1) # description load balance group mellanox.lan [standalone: master] (config interface port-channel 1) # no shut mellanox.lan [standalone: master] (config interface port-channel 1) # mtu 9000 force mellanox.lan [standalone: master] (config interface port-channel 1) # exit mellanox.lan [standalone: master] (config) # interface ethernet 1/5 switchport mode hybrid mellanox.lan [standalone: master] (config) # interface ethernet 1/9 switchport mode hybrid mellanox.lan [standalone: master] (config) # interface ethernet 1/12 switchport mode hybrid mellanox.lan [standalone: master] (config) # interface ethernet 1/1 switchport mode hybrid mellanox.lan [standalone: master] (config) # interface ethernet 1/1 mtu 9000 force mellanox.lan [standalone: master] (config) # interface ethernet 1/5 mtu 9000 force mellanox.lan [standalone: master] (config) # interface ethernet 1/9 mtu 9000 force mellanox.lan [standalone: master] (config) # interface ethernet 1/12 mtu 9000 force mellanox.lan [standalone: master] (config) # interface ethernet 1/5 channel-group 1 mode on mellanox.lan [standalone: master] (config) # interface ethernet 1/9 channel-group 1 mode on mellanox.lan [standalone: master] (config) # interface ethernet 1/12 channel-group 1 mode on Problem Using Port Mirroring Originally my plan was to use a mirror port to send all the traffic from port 1 to our LAG interface. In contrast to OS10, MLNX-OS will not allow you to do this. The problem is that MLNX-OS will not allow you to create a mirror session from interface 1 to the LAG port which prevents this configuration from working. Moreover you cannot access the Linux command line to use a utility like tc to perform the config either. Configure OpenFlow Instead of using port mirroring to send the traffic from interface one we can instead use a static OpenFlow configuration to redirect the traffic. mellanox.lan [standalone: master] (config) # interface ethernet 1/1 openflow mode hybrid mellanox.lan [standalone: master] (config) # interface port-channel 1 openflow mode hybrid mellanox.lan [standalone: master] (config) # openflow add-flows 1000 priority=50,in_port=Eth1/1,actions=output:Po1 Findings The Mellanox SN2010 works correctly and will appropriately load balance full sessions across each member of the LAG. See below for screenshots. Host 1 Host 2 Host 3","title":"Load Balancing on Mellanox Switches"},{"location":"Load%20Balancing%20on%20Mellanox%20Switches/#load-balancing-on-mellanox-switches","text":"In this test case the goal is to create a simple load balancer using a reverse LAG port. The idea is to have one input port which is then mirrored to a logical LAG port and at the other end of the LAG port is a number of security sensors.","title":"Load Balancing on Mellanox Switches"},{"location":"Load%20Balancing%20on%20Mellanox%20Switches/#version-info","text":"mellanox.lan [standalone: master] # show version Product name: Onyx Product release: 3.8.2004 Build ID: #1-dev Build date: 2019-09-23 14:19:47 Target arch: x86_64 Target hw: x86_64 Built by: jenkins@7ae5fd122b61 Version summary: X86_64 3.8.2004 2019-09-23 14:19:47 x86_64 Product model: x86onie Host ID: B8599FD560BE System serial num: MT1940T00588 System UUID: 5f4c5ed2-e60b-11e9-8000-b8599f7f6f40 Uptime: 17h 27m 36.480s CPU load averages: 3.17 / 3.17 / 3.11 Number of CPUs: 4 System memory: 2738 MB used / 5065 MB free / 7803 MB total Swap: 0 MB used / 0 MB free / 0 MB total","title":"Version Info"},{"location":"Load%20Balancing%20on%20Mellanox%20Switches/#connect-to-the-console-port-and-management-ethernet-port","text":"Plug in both the management Ethernet cable and the serial cable. The console port is the bottom port and the ethernet management port is the top port. I had to plug the console cable into a specific USB slot on the server. It didn't work in the first one I tried. See picture below. This likely has nothing to do with the Mellanox switch itself, but as a note for those that come after you may want to try different USB ports if you find you aren't getting output on the first one you try and are confident you have the correct settings. I used the following console configuration: Baud Rate: 115200 Data Bits: 8 Stop Bits: 1 Parity: None Flow Control: None","title":"Connect to the Console Port and Management Ethernet Port"},{"location":"Load%20Balancing%20on%20Mellanox%20Switches/#update-to-latest-version-of-mlnx-os","text":"I pulled updates here The system uses a web server target for updates. I had Apache running on a RHEL 8 box. Download the update file and then upload it to your web server's root directory. On the switch itself (over a console port) do the following: switch > enable switch # configure terminal switch (config) # show images # Delete the old image if it exists. It will be under # \"Images available to be installed\" switch (config) # image delete <old_image> Download the new image from your web server with mellanox.lan [standalone: master] # image fetch http://rhel8.lan/onyx-X86_64-3.8.2004.img 100.0% [################################################################################################################################################################################################################################################################] Next install the updated OS with: mellanox.lan [standalone: master] # image install onyx-X86_64-3.8.2004.img location 2 progress track verify check-sig Step 1 of 4: Verify Image 100.0% [#################################################################] Step 2 of 4: Uncompress Image 100.0% [#################################################################] Step 3 of 4: Create Filesystems 100.0% [#################################################################] Step 4 of 4: Extract Image 98.6% [################################################################ ] 100.0% [#################################################################] Now set the switch to load from the new operating system and reload: mellanox.lan [standalone: master] # image boot next mellanox.lan [standalone: master] # reload","title":"Update to Latest Version of MLNX-OS"},{"location":"Load%20Balancing%20on%20Mellanox%20Switches/#physical-configuration","text":"I used the following port configuration: 1, 1Gb/s copper SFP (Eth1/1) for input 2, 1Gb/s copper SFPs (Ethernet 1/1/5/Ethernet 1/1/9) and 1, 1Gb/s, fiber SFP (Ethernet 1/1/12) for output I used the following optics: Connect the input port to port 1. I connected my output ports in the following way:","title":"Physical Configuration"},{"location":"Load%20Balancing%20on%20Mellanox%20Switches/#bringing-the-interfaces-up","text":"Mellanox does not perform testing with 3rd party NICs. During testing we found that autonegotiation of speed will not work on standard Dell SFPs. Use the below command on each interface to set the speed manually: mellanox.lan [standalone: master] (config interface ethernet 1/1) # speed 1G","title":"Bringing the Interfaces Up"},{"location":"Load%20Balancing%20on%20Mellanox%20Switches/#configure-the-lag","text":"","title":"Configure the LAG"},{"location":"Load%20Balancing%20on%20Mellanox%20Switches/#initial-configuration","text":"mellanox.lan [standalone: master] (config) # port-channel load-balance ethernet source-destination-mac source-destination-ip source-destination-port symmetric mellanox.lan [standalone: master] (config) # interface port-channel 1 mellanox.lan [standalone: master] (config interface port-channel 1) # switchport mode hybrid mellanox.lan [standalone: master] (config interface port-channel 1) # description load balance group mellanox.lan [standalone: master] (config interface port-channel 1) # no shut mellanox.lan [standalone: master] (config interface port-channel 1) # mtu 9000 force mellanox.lan [standalone: master] (config interface port-channel 1) # exit mellanox.lan [standalone: master] (config) # interface ethernet 1/5 switchport mode hybrid mellanox.lan [standalone: master] (config) # interface ethernet 1/9 switchport mode hybrid mellanox.lan [standalone: master] (config) # interface ethernet 1/12 switchport mode hybrid mellanox.lan [standalone: master] (config) # interface ethernet 1/1 switchport mode hybrid mellanox.lan [standalone: master] (config) # interface ethernet 1/1 mtu 9000 force mellanox.lan [standalone: master] (config) # interface ethernet 1/5 mtu 9000 force mellanox.lan [standalone: master] (config) # interface ethernet 1/9 mtu 9000 force mellanox.lan [standalone: master] (config) # interface ethernet 1/12 mtu 9000 force mellanox.lan [standalone: master] (config) # interface ethernet 1/5 channel-group 1 mode on mellanox.lan [standalone: master] (config) # interface ethernet 1/9 channel-group 1 mode on mellanox.lan [standalone: master] (config) # interface ethernet 1/12 channel-group 1 mode on","title":"Initial Configuration"},{"location":"Load%20Balancing%20on%20Mellanox%20Switches/#problem-using-port-mirroring","text":"Originally my plan was to use a mirror port to send all the traffic from port 1 to our LAG interface. In contrast to OS10, MLNX-OS will not allow you to do this. The problem is that MLNX-OS will not allow you to create a mirror session from interface 1 to the LAG port which prevents this configuration from working. Moreover you cannot access the Linux command line to use a utility like tc to perform the config either.","title":"Problem Using Port Mirroring"},{"location":"Load%20Balancing%20on%20Mellanox%20Switches/#configure-openflow","text":"Instead of using port mirroring to send the traffic from interface one we can instead use a static OpenFlow configuration to redirect the traffic. mellanox.lan [standalone: master] (config) # interface ethernet 1/1 openflow mode hybrid mellanox.lan [standalone: master] (config) # interface port-channel 1 openflow mode hybrid mellanox.lan [standalone: master] (config) # openflow add-flows 1000 priority=50,in_port=Eth1/1,actions=output:Po1","title":"Configure OpenFlow"},{"location":"Load%20Balancing%20on%20Mellanox%20Switches/#findings","text":"The Mellanox SN2010 works correctly and will appropriately load balance full sessions across each member of the LAG. See below for screenshots.","title":"Findings"},{"location":"Load%20Balancing%20on%20Mellanox%20Switches/#host-1","text":"","title":"Host 1"},{"location":"Load%20Balancing%20on%20Mellanox%20Switches/#host-2","text":"","title":"Host 2"},{"location":"Load%20Balancing%20on%20Mellanox%20Switches/#host-3","text":"","title":"Host 3"},{"location":"Load%20Balancing%20with%20LAG%20on%205112F-ON/","text":"Load Balancing with LAG on 5112F-ON Load Balancing with LAG on 5112F-ON Version/Hardware Information The Test Environment Configure Devices Configure 5212F-ON Configure RHEL 8 Test Results Version/Hardware Information Note This requires at least 10.5.3.0 and is only supported on switches with the Trident proc OS10(config)# do show version Dell EMC Networking OS10 Enterprise Copyright (c) 1999-2021 by Dell Inc. All Rights Reserved. OS Version: 10.5.3.0 Build Version: 10.5.3.0.44 Build Time: 2021-10-06T23:03:55+0000 System Type: S5212F-ON Architecture: x86_64 Up Time: 00:03:27 The Test Environment I used a RHEL box to generate traffic inbound to the switch on port ethernet 1/1/5:1 which I then expected to load balance across ports ethernet 1/1/1:1 and 1/1/4:1. I used source and dest IP and source and dest port as the criteria for load balancing. Traffic was captured with a single laptop which was directly connected to the two independent receiving interfaces. A single Wireshark monitor session was established for each of the individual interfaces to ensure that they received independent, mutually-exclusive, session load balancing. The traffic was a session I generated by using a laptop that was serving as a virtual web gateway along with being used as an end user device. Configure Devices Configure 5212F-ON configure terminal load-balancing ip-selection source-ip destination-ip l4-destination-port l4-source-port port-group 1/1/1 mode eth 10g-4x interface port-channel 1 no shutdown no switchport exit interface ethernet 1/1/1:1 no shut channel-group 1 speed 1000 exit interface ethernet 1/1/4:1 no shut channel-group 1 speed 1000 exit monitor session 1 destination interface port-channel 1 source interface ethernet 1/1/5:1 no shut exit Your listening source captures traffic be that a tap, span, Linux host, etc Traffic is pushed into a physical interface (ethernet 1/1/5:1) on the 5212 Traffic is mirrored from the physical interface (Ethernet 1/1/5:1) to the virtual port group interface The port group interface is tied to two physical interfaces and set to perform load balancing The virtual port group interface will load balance based on a hash of the aforementioned attributes (ip-selection source-ip destination-ip l4-destination-port l4-source-port) Configure RHEL 8 sudo dnf install -y https://dl.fedoraproject.org/pub/epel/epel-release-latest-8.noarch.rpm sudo subscription-manager repos --enable codeready-builder-for-rhel-8-x86_64-rpms dnf group install \"Development Tools\" dnf install -y libpcap-devel make make install /usr/local/bin/tcpreplay -i ens32 test.pcap Test Results The test succeeded. This was verified by checking that the sessions seen by two separate streams were mutually exclusive. At no point did we see effects as described in 4112F-ON Test Case 4 where a single session was sent down two separate lanes. Moreover, by checking the individual sessions you can see the bidirectional flow:","title":"Load Balancing with LAG on 5112F-ON"},{"location":"Load%20Balancing%20with%20LAG%20on%205112F-ON/#load-balancing-with-lag-on-5112f-on","text":"Load Balancing with LAG on 5112F-ON Version/Hardware Information The Test Environment Configure Devices Configure 5212F-ON Configure RHEL 8 Test Results","title":"Load Balancing with LAG on 5112F-ON"},{"location":"Load%20Balancing%20with%20LAG%20on%205112F-ON/#versionhardware-information","text":"Note This requires at least 10.5.3.0 and is only supported on switches with the Trident proc OS10(config)# do show version Dell EMC Networking OS10 Enterprise Copyright (c) 1999-2021 by Dell Inc. All Rights Reserved. OS Version: 10.5.3.0 Build Version: 10.5.3.0.44 Build Time: 2021-10-06T23:03:55+0000 System Type: S5212F-ON Architecture: x86_64 Up Time: 00:03:27","title":"Version/Hardware Information"},{"location":"Load%20Balancing%20with%20LAG%20on%205112F-ON/#the-test-environment","text":"I used a RHEL box to generate traffic inbound to the switch on port ethernet 1/1/5:1 which I then expected to load balance across ports ethernet 1/1/1:1 and 1/1/4:1. I used source and dest IP and source and dest port as the criteria for load balancing. Traffic was captured with a single laptop which was directly connected to the two independent receiving interfaces. A single Wireshark monitor session was established for each of the individual interfaces to ensure that they received independent, mutually-exclusive, session load balancing. The traffic was a session I generated by using a laptop that was serving as a virtual web gateway along with being used as an end user device.","title":"The Test Environment"},{"location":"Load%20Balancing%20with%20LAG%20on%205112F-ON/#configure-devices","text":"","title":"Configure Devices"},{"location":"Load%20Balancing%20with%20LAG%20on%205112F-ON/#configure-5212f-on","text":"configure terminal load-balancing ip-selection source-ip destination-ip l4-destination-port l4-source-port port-group 1/1/1 mode eth 10g-4x interface port-channel 1 no shutdown no switchport exit interface ethernet 1/1/1:1 no shut channel-group 1 speed 1000 exit interface ethernet 1/1/4:1 no shut channel-group 1 speed 1000 exit monitor session 1 destination interface port-channel 1 source interface ethernet 1/1/5:1 no shut exit Your listening source captures traffic be that a tap, span, Linux host, etc Traffic is pushed into a physical interface (ethernet 1/1/5:1) on the 5212 Traffic is mirrored from the physical interface (Ethernet 1/1/5:1) to the virtual port group interface The port group interface is tied to two physical interfaces and set to perform load balancing The virtual port group interface will load balance based on a hash of the aforementioned attributes (ip-selection source-ip destination-ip l4-destination-port l4-source-port)","title":"Configure 5212F-ON"},{"location":"Load%20Balancing%20with%20LAG%20on%205112F-ON/#configure-rhel-8","text":"sudo dnf install -y https://dl.fedoraproject.org/pub/epel/epel-release-latest-8.noarch.rpm sudo subscription-manager repos --enable codeready-builder-for-rhel-8-x86_64-rpms dnf group install \"Development Tools\" dnf install -y libpcap-devel make make install /usr/local/bin/tcpreplay -i ens32 test.pcap","title":"Configure RHEL 8"},{"location":"Load%20Balancing%20with%20LAG%20on%205112F-ON/#test-results","text":"The test succeeded. This was verified by checking that the sessions seen by two separate streams were mutually exclusive. At no point did we see effects as described in 4112F-ON Test Case 4 where a single session was sent down two separate lanes. Moreover, by checking the individual sessions you can see the bidirectional flow:","title":"Test Results"},{"location":"Make%20USB%20Read%20Only/","text":"Make USB Read Only Note : These instructions are for a UEFI bootable device with an ext4 filesystem Boot up a separate Linux machine and plug in the USB device you would like to make readonly Run sudo fdisk -l to list the available partitions Confirm you have not built the device with a swap partition grant@telemetrytest:/media$ sudo fdisk -l ...SNIP.. Disk /dev/sdb: 57.29 GiB, 61505273856 bytes, 120127488 sectors Disk model: Ultra Units: sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disklabel type: gpt Disk identifier: C02A7A30-1742-47CE-9DFB-5B3AB96C958C Device Start End Sectors Size Type /dev/sdb1 2048 1050623 1048576 512M EFI System /dev/sdb2 1050624 118126591 117075968 55.8G Linux filesystem The first thing we need to do is get the uuid for the EFI partition. Run sudo blkid /dev/<YOUR_EFI_PARTITION> , in my case /dev/sdb1 grant@telemetrytest:/media$ sudo blkid /dev/sdb1 /dev/sdb1: UUID=\"CD68-8FEA\" TYPE=\"vfat\" PARTUUID=\"d27eda17-c6df-4115-80f3-bd86b56882ac\" Next, we need to edit the base filesystem's fstab to make sure that when this filesystem loads, it will load as readonly. Mount your ext4 filesystem with sudo mount /dev/<YOUR_PARTITION> <YOUR_MOUNT_POINT> . Ex: sudo mount /dev/sdb2 /media . Next, we need to edit fstab. sudo vim /media/etc/fstab # /etc/fstab: static file system information. # # Use 'blkid' to print the universally unique identifier for a # device; this may be used with UUID= as a more robust way to name devices # that works even if disks are added and removed. See fstab(5). # # systemd generates mount units based on this file, see systemd.mount(5). # Please run 'systemctl daemon-reload' after making changes here. # # <file system> <mount point> <type> <options> <dump> <pass> # / was on /dev/sdb2 during installation UUID=6634741f-250a-47b7-96b9-379e517d4591 / ext4 errors=remount-ro 0 1 # /boot/efi was on /dev/sdb1 during installation UUID=CD68-8FEA /boot/efi vfat umask=0077 0 1 # swap was on /dev/sdb3 during installation UUID=4391d5d0-08cb-4fbe-81a8-0a67fe00758c none swap sw 0 0 /dev/sr0 /media/cdrom0 udf,iso9660 user,noauto 0 0 Look for the mount point /boot/efi and confirm that its UUID matches what you saw earlier. Change the options from UUID=CD68-8FEA /boot/efi vfat umask=0077 0 1 to UUID=CD68-8FEA /boot/efi vfat umask=0077,ro 0 1 Next we need to set the primary ext4 partition as read-only. Run sudo umount <YOUR_MOUNT> . Ex: sudo umount /media Set the ext4 filesystem as read only with sudo tune2fs -O read-only /dev/<YOUR_PARTITION> . Ex: sudo tune2fs -O read-only /dev/sdb2 Disconnect the USB drive, boot from it, and confirm read-only behavior.","title":"Make USB Read Only"},{"location":"Make%20USB%20Read%20Only/#make-usb-read-only","text":"Note : These instructions are for a UEFI bootable device with an ext4 filesystem Boot up a separate Linux machine and plug in the USB device you would like to make readonly Run sudo fdisk -l to list the available partitions Confirm you have not built the device with a swap partition grant@telemetrytest:/media$ sudo fdisk -l ...SNIP.. Disk /dev/sdb: 57.29 GiB, 61505273856 bytes, 120127488 sectors Disk model: Ultra Units: sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disklabel type: gpt Disk identifier: C02A7A30-1742-47CE-9DFB-5B3AB96C958C Device Start End Sectors Size Type /dev/sdb1 2048 1050623 1048576 512M EFI System /dev/sdb2 1050624 118126591 117075968 55.8G Linux filesystem The first thing we need to do is get the uuid for the EFI partition. Run sudo blkid /dev/<YOUR_EFI_PARTITION> , in my case /dev/sdb1 grant@telemetrytest:/media$ sudo blkid /dev/sdb1 /dev/sdb1: UUID=\"CD68-8FEA\" TYPE=\"vfat\" PARTUUID=\"d27eda17-c6df-4115-80f3-bd86b56882ac\" Next, we need to edit the base filesystem's fstab to make sure that when this filesystem loads, it will load as readonly. Mount your ext4 filesystem with sudo mount /dev/<YOUR_PARTITION> <YOUR_MOUNT_POINT> . Ex: sudo mount /dev/sdb2 /media . Next, we need to edit fstab. sudo vim /media/etc/fstab # /etc/fstab: static file system information. # # Use 'blkid' to print the universally unique identifier for a # device; this may be used with UUID= as a more robust way to name devices # that works even if disks are added and removed. See fstab(5). # # systemd generates mount units based on this file, see systemd.mount(5). # Please run 'systemctl daemon-reload' after making changes here. # # <file system> <mount point> <type> <options> <dump> <pass> # / was on /dev/sdb2 during installation UUID=6634741f-250a-47b7-96b9-379e517d4591 / ext4 errors=remount-ro 0 1 # /boot/efi was on /dev/sdb1 during installation UUID=CD68-8FEA /boot/efi vfat umask=0077 0 1 # swap was on /dev/sdb3 during installation UUID=4391d5d0-08cb-4fbe-81a8-0a67fe00758c none swap sw 0 0 /dev/sr0 /media/cdrom0 udf,iso9660 user,noauto 0 0 Look for the mount point /boot/efi and confirm that its UUID matches what you saw earlier. Change the options from UUID=CD68-8FEA /boot/efi vfat umask=0077 0 1 to UUID=CD68-8FEA /boot/efi vfat umask=0077,ro 0 1 Next we need to set the primary ext4 partition as read-only. Run sudo umount <YOUR_MOUNT> . Ex: sudo umount /media Set the ext4 filesystem as read only with sudo tune2fs -O read-only /dev/<YOUR_PARTITION> . Ex: sudo tune2fs -O read-only /dev/sdb2 Disconnect the USB drive, boot from it, and confirm read-only behavior.","title":"Make USB Read Only"},{"location":"Migrating%20Storage%20Volumes%20to%20PowerStore/","text":"Migrating Storage Volumes to PowerStore In my scenario I wanted to migrate storage from a Compellent attached with FC to a PowerStore attached to an MX7000 with an M9116n. Note: This is only relevant for older devices in a FC or iSCSI configuration. For NAS you would use any NAS migration technique (rsync, DobiMigrate, etc) Migrating Storage Volumes to PowerStore MX7000 FC Topology Migrating from an Old Device Other Useful Resources PowerStore Educational Videos Operating Systems Compatible with Multipath Drivers Requirements for Non-Disruptive Migration M9116n Compatibility Matrix MX7000 FC Topology Migrating from an Old Device This video describes how the migration from an old device (like a Compellent) to a PowerStore works. In general, on all effected devices, you must install a host plugin which comes with a multipath driver. Before the migration is complete, the host driver will direct all reads/writes to the old device and post migration you will use a cutover option which causes the reads/writes to be redirected to the PowerStore. There is an iSCSI connection between the PowerStore and the compellent which has a synchronization feature that will keep any updates made against the Compellent (or other older device) synced to the in progress copy to the PowerStore Other Useful Resources PowerStore Educational Videos https://www.dell.com/support/kbdoc/en-us/000130110/powerstore-info-hub-product-documentation-videos Operating Systems Compatible with Multipath Drivers https://www.dell.com/support/kbdoc/en-us/000105896/powerstore-supported-host-os-for-non-disruptive-migration-of-storage-resources?lang=en Requirements for Non-Disruptive Migration https://www.dell.com/support/manuals/en-us/powerstore-1000t/pwrstr-import/additional-resources?guid=guid-f5b0a9d3-2eae-447c-b4c3-40e0927ac5f4&lang=en-us M9116n Compatibility Matrix","title":"Migrating Storage Volumes to PowerStore"},{"location":"Migrating%20Storage%20Volumes%20to%20PowerStore/#migrating-storage-volumes-to-powerstore","text":"In my scenario I wanted to migrate storage from a Compellent attached with FC to a PowerStore attached to an MX7000 with an M9116n. Note: This is only relevant for older devices in a FC or iSCSI configuration. For NAS you would use any NAS migration technique (rsync, DobiMigrate, etc) Migrating Storage Volumes to PowerStore MX7000 FC Topology Migrating from an Old Device Other Useful Resources PowerStore Educational Videos Operating Systems Compatible with Multipath Drivers Requirements for Non-Disruptive Migration M9116n Compatibility Matrix","title":"Migrating Storage Volumes to PowerStore"},{"location":"Migrating%20Storage%20Volumes%20to%20PowerStore/#mx7000-fc-topology","text":"","title":"MX7000 FC Topology"},{"location":"Migrating%20Storage%20Volumes%20to%20PowerStore/#migrating-from-an-old-device","text":"This video describes how the migration from an old device (like a Compellent) to a PowerStore works. In general, on all effected devices, you must install a host plugin which comes with a multipath driver. Before the migration is complete, the host driver will direct all reads/writes to the old device and post migration you will use a cutover option which causes the reads/writes to be redirected to the PowerStore. There is an iSCSI connection between the PowerStore and the compellent which has a synchronization feature that will keep any updates made against the Compellent (or other older device) synced to the in progress copy to the PowerStore","title":"Migrating from an Old Device"},{"location":"Migrating%20Storage%20Volumes%20to%20PowerStore/#other-useful-resources","text":"","title":"Other Useful Resources"},{"location":"Migrating%20Storage%20Volumes%20to%20PowerStore/#powerstore-educational-videos","text":"https://www.dell.com/support/kbdoc/en-us/000130110/powerstore-info-hub-product-documentation-videos","title":"PowerStore Educational Videos"},{"location":"Migrating%20Storage%20Volumes%20to%20PowerStore/#operating-systems-compatible-with-multipath-drivers","text":"https://www.dell.com/support/kbdoc/en-us/000105896/powerstore-supported-host-os-for-non-disruptive-migration-of-storage-resources?lang=en","title":"Operating Systems Compatible with Multipath Drivers"},{"location":"Migrating%20Storage%20Volumes%20to%20PowerStore/#requirements-for-non-disruptive-migration","text":"https://www.dell.com/support/manuals/en-us/powerstore-1000t/pwrstr-import/additional-resources?guid=guid-f5b0a9d3-2eae-447c-b4c3-40e0927ac5f4&lang=en-us","title":"Requirements for Non-Disruptive Migration"},{"location":"Migrating%20Storage%20Volumes%20to%20PowerStore/#m9116n-compatibility-matrix","text":"","title":"M9116n Compatibility Matrix"},{"location":"Mulitple%20Span%20on%204112F-ON%20with%20OpenSwitch/","text":"Mulitple Span on 4112F-ON with OpenSwitch In this test case I am testing to see if we can configure a Dell 4112F-ON with OpenSwitch to create a one to many port configuration using SPAN. Helpful Links ONIE Network Install Process Overview OPX Install Instructions for Dell EMC Equipment OPX Tools Source Code OPX Command Reference OPX Docs Home List of Supported Hardware My Configuration General Configuration ONIE host is running RHEL 8 I am using a Dell S4112F-ON for testing OpenSwitch version PKGS_OPX-3.2.0-installer-x86_64 PFSense running DNS and DHCP as services RHEL Release Info NAME=\"Red Hat Enterprise Linux\" VERSION=\"8.0 (Ootpa)\" ID=\"rhel\" ID_LIKE=\"fedora\" VERSION_ID=\"8.0\" PLATFORM_ID=\"platform:el8\" PRETTY_NAME=\"Red Hat Enterprise Linux 8.0 (Ootpa)\" ANSI_COLOR=\"0;31\" CPE_NAME=\"cpe:/o:redhat:enterprise_linux:8.0:GA\" HOME_URL=\"https://www.redhat.com/\" BUG_REPORT_URL=\"https://bugzilla.redhat.com/\" REDHAT_BUGZILLA_PRODUCT=\"Red Hat Enterprise Linux 8\" REDHAT_BUGZILLA_PRODUCT_VERSION=8.0 REDHAT_SUPPORT_PRODUCT=\"Red Hat Enterprise Linux\" REDHAT_SUPPORT_PRODUCT_VERSION=\"8.0\" Red Hat Enterprise Linux release 8.0 (Ootpa) Red Hat Enterprise Linux release 8.0 (Ootpa) OPX Version OS_NAME=\"OPX\" OS_VERSION=\"unstable\" PLATFORM=\"S4112F-ON\" ARCHITECTURE=\"x86_64\" INTERNAL_BUILD_ID=\"OpenSwitch blueprint for Dell 1.0.0\" BUILD_VERSION=\"unstable.0-stretch\" BUILD_DATE=\"2019-06-21T19:04:22+0000\" INSTALL_DATE=\"2019-10-23T23:16:10+00:00\" SYSTEM_UPTIME= 1 day, 5 minutes SYSTEM_STATE= running UPGRADED_PACKAGES=no ALTERED_PACKAGES=yes Wasn't sure why I got the unstable version after installation. It didn't cause any problems for testing so I just left it as is. Setup ONIE Prerequisites See ONIE Install Setup for instructions. Configure Device as TAP Physical Configuration For this configuration to work, we will use the management interface as the input interface for the tap. See image below. You will need to move your network cable over from your usual network to your traffic generator. Configure Management Interface (Optional) Update: After I got it working I ended up using this as an ingress interface so this step is more or less optional. You won't be able to SSH into this interface in the end config (at least not easily). Start by running sudo -i to move to privileged mode. Warning: I noticed the OPX command line tools won't behave correctly unless you are privileged. Ex: opx-show-interface won't list any interfaces. I added vim to my box before continuing with sudo apt-get install -y vim The management interface is configured like a typicaly Debian interface with vim /etc/network/interface.d/eth0 Use the following configuration modified to your needs: auto eth0 allow-hotplug eth0 iface eth0 inet static address 192.168.1.20 netmask 255.255.255.0 gateway 192.168.1.1 When you are finished with your configuration run systemctl restart networking to apply the changes. Confirm the changes were applied with ip address show dev eth0 . If you see two IP addresses because you picked one up from DHCP you can delete the other with ip address del [IP ADDRESS] dev eth0 and then run systemctl restart networking At this juncture your management interface should be up and running and you should be able to SSH to it. I went ahead and swapped to SSH so as not to deal with the oddities that come with running in the console port. Bridge/tc Configuration After attempt 3 I started thinking about other ways to connect things. Realized I could just pump everything to a bridge and let that do the replication. That worked! Do the following to get it up and running: tc is a feature of modern Linux kernels designed for mirroring traffic. I found this article helpful. This Mellanox article was also useful. WARNING: You must use your management interface for the ingress port or this solution will not work! I noticed the other ports do not behave like normal Linux ports. More investigation required to figure out the difference. Create a bridge interface with brctl addbr br0 Attach all interfaces you want as part of the port mirroring to the bridge with brctl addif br0 < INTERFACE > Make sure all interfaces in use are enabled with ip link set < INTERFACE > up Disable MAC address learning on the bridge with brctl setageing br0 0 Set the device's management interface to promiscuous mode with ip link set < MGMT_INTERFACE > promisc on The first thing I did was create an ingress queue on my input interface with tc qdisc add dev < MGMT_INTERFACE > handle ffff: ingress If you need to delete a qdisc you can do it with tc qdisc del dev < MGMT_INTERFACE > [ root | ingress ] Double check your queue with handle ffff was created with tc -s qdisc ls dev < MGMT_INTERFACE > Next we want to mirror all traffic from the ingress port to an output port with tc filter add dev < MGMT_INTERFACE > parent ffff: protocol all u32 match u32 0 0 action mirred egress mirror dev br0 Check that your port mirror appeared in the config with tc -s -p filter ls dev < MGMT_INTERFACE > parent ffff: If you need to delete the filters you can do so with tc filter del dev < MGMT_INTERFACE > parent ffff: Set queue to not shape traffic with tc qdisc add dev < MGMT_INTERFACE > handle 1: root prio Things I Tried I added 7 interfaces to my bridge to make sure there weren't any strange limitations I moved the SFPs around to multiple different ports to make sure the traffic was mirroing on all of them I double checked the traffic I was capturing belonged to the PCAP in question. Easy enough to see because it has IP addresses the hosts in question wouldn't ever otherwise see. Screenshots for confirmation below. Host 1 Host 2 - I checked that pure L3 traffic was passed correctly using ICMP. Noted Problem The only major issue I noticed is that pure layer 2 traffic didn't get passed. Haven't figured out how to fix that yet. Failed Ideas Attempt 1 - Mirror Ports My first go is to try using OpenSwitch's built in mirroring capability. Physical Configuration I didn't have enough target hosts to try outputting from one port to all ports so I simulated it. The purple cable in the image is the input port from the traffic generator (tcpreplay) and the white and yellow cables go out to the hosts listed as host 1 and host 2 in the test results section. The ports with the white and yellow cables were configured as the mirror's target ports. Mirror Configuration For each port you want to mirror to run opx-config-mirror create --src_intf e101-001-0 --dest_intf e101-005-0 --direction ingress --type span . Substitute your source and destination ports appropriately. Results Mirror Port Failure After 4 I was only able to get this to work on up to 4 ports. After that I received errors. See output below: root@OPX:~# opx-config-mirror create --src_intf e101-001-0 --dest_intf e101-005-0 --direction ingress --type span root@OPX:~# ip link set e101-009-0 up root@OPX:~# opx-config-mirror create --src_intf e101-001-0 --dest_intf e101-009-0 --direction ingress --type span root@OPX:~# opx-config-mirror create --src_intf e101-001-0 --dest_intf e101-002-0 --direction ingress --type span root@OPX:~# opx-config-mirror create --src_intf e101-001-0 --dest_intf e101-003-0 --direction ingress --type span root@OPX:~# opx-config-mirror create --src_intf e101-001-0 --dest_intf e101-004-0 --direction ingress --type span {'data': {'base-mirror/entry/dst-intf': bytearray(b'\\x0f\\x00\\x00\\x00'), 'base-mirror/entry/type': bytearray(b'\\x01\\x00\\x00\\x00'), 'base-mirror/entry/intf': {'0': {'base-mirror/entry/intf/src': bytearray(b'\\x0c\\x00\\x00\\x00'), 'base-mirror/entry/intf/direction': bytearray(b'\\x01\\x00\\x00\\x00')}}}, 'key': '1.27.1769488.1769473.'} opx-config-mirror: Commit failed root@OPX:~# opx-config-mirror create --src_intf e101-001-0 --dest_intf e101-006-0 --direction ingress --type span {'data': {'base-mirror/entry/dst-intf': bytearray(b'\\x11\\x00\\x00\\x00'), 'base-mirror/entry/type': bytearray(b'\\x01\\x00\\x00\\x00'), 'base-mirror/entry/intf': {'0': {'base-mirror/entry/intf/src': bytearray(b'\\x0c\\x00\\x00\\x00'), 'base-mirror/entry/intf/direction': bytearray(b'\\x01\\x00\\x00\\x00')}}}, 'key': '1.27.1769488.1769473.'} opx-config-mirror: Commit failed root@OPX:~# opx-config-mirror create --src_intf e101-001-0 --dest_intf e101-007-0 --direction ingress --type span {'data': {'base-mirror/entry/dst-intf': bytearray(b'\\x12\\x00\\x00\\x00'), 'base-mirror/entry/type': bytearray(b'\\x01\\x00\\x00\\x00'), 'base-mirror/entry/intf': {'0': {'base-mirror/entry/intf/src': bytearray(b'\\x0c\\x00\\x00\\x00'), 'base-mirror/entry/intf/direction': bytearray(b'\\x01\\x00\\x00\\x00')}}}, 'key': '1.27.1769488.1769473.'} opx-config-mirror: Commit failed Pretty printed version for ease of reading: { 'data': { 'base-mirror/entry/dst-intf': bytearray(b '\\x12\\x00\\x00\\x00'), 'base-mirror/entry/type': bytearray(b '\\x01\\x00\\x00\\x00'), 'base-mirror/entry/intf': { '0': { 'base-mirror/entry/intf/src': bytearray(b '\\x0c\\x00\\x00\\x00'), 'base-mirror/entry/intf/direction': bytearray(b '\\x01\\x00\\x00\\x00') } } }, 'key': '1.27.1769488.1769473.' } Functioning Mirror Ports Before I caught the error, I did test the first two mirror ports I made and they worked as expected. See the below. I used tcpreplay with some traffic I captured on my desktop to test the idea. I just uploaded the PCAP and replayed it with tcpreplay -i ens224 ./test_pcap.pcap --loop 500 I then confirmed that all target ports received traffic. See screenshots below: Host 1 Host 2 The host I collected the traffic on was 192.168.1.6 and as you can see from the images both hosts were able to see traffic from the tcpreplay session. Attempt 2 - tc Configuration tc is a feature of modern Linux kernels designed for mirroring traffic. I found this article helpful. This Mellanox article was also useful. The first thing I did was create an ingress queue on my input interface with tc qdisc add dev e101-001-0 handle ffff: ingress If you need to delete a qdisc you can do it with tc qdisc del dev e101-001-0 [ root | ingress ] Double check your queue with handle ffff was created with tc -s qdisc ls dev e101-001-0 Next we want to mirror all traffic from the ingress port to an output port with tc filter add dev e101-001-0 parent ffff: protocol all u32 match u32 0 0 action mirred egress mirror dev e101-005-0 Check that your port mirror appeared in the config with tc -s -p filter ls dev e101-001-0 parent ffff: If you need to delete the filters you can do so with tc filter del dev e101-001-0 parent ffff: Set queue to not shape traffic with tc qdisc add dev e101-001-0 handle 1: root prio Alternate Configuration I tried instead running: tc qdisc add dev e101-001-0 clsact tc filter add dev e101-001-0 ingress matchall skip_sw action mirred egress mirror dev e101-005-0 Other Things Tried Haven't been able to figure out why just yet, but only Layer 2 traffic is making it through the port mirror. Everything above gets dropped. I thought maybe it was MAC address learning, but the problem persisted when I ran opx-config-global-switch --mac-age-time 0 I also thought that it was the port not being set to promiscuous mode so I gave it ifconfig e101-001-0 promisc . That didn't work either. Conclusions I'm pretty confident that because this is a network OS for switching something funky is going on. Ex: When you run a port mirror, all the traffic passes correctly, but you won't see any of that traffic on a tcpdump session. Need to study up on the architecture. I'm pretty sure there's a way to make this particular tactic work, but for time's sake I'm going to try something else. Attempt 3 - tc on Management Interface I realized something is going on with the forwarding tables on the switch at a low level that was intercepting our traffic in attempt 2. That said, I noticed that the management interface for the switch effectively works like a standard Linux interface. I did the same thing I did in attempt 2 except I used the managament interface instead of one of the other interfaces. Results The switch accepts the config. However, the traffic only goes out to one port at a time.","title":"Mulitple Span on 4112F-ON with OpenSwitch"},{"location":"Mulitple%20Span%20on%204112F-ON%20with%20OpenSwitch/#mulitple-span-on-4112f-on-with-openswitch","text":"In this test case I am testing to see if we can configure a Dell 4112F-ON with OpenSwitch to create a one to many port configuration using SPAN.","title":"Mulitple Span on 4112F-ON with OpenSwitch"},{"location":"Mulitple%20Span%20on%204112F-ON%20with%20OpenSwitch/#helpful-links","text":"ONIE Network Install Process Overview OPX Install Instructions for Dell EMC Equipment OPX Tools Source Code OPX Command Reference OPX Docs Home List of Supported Hardware","title":"Helpful Links"},{"location":"Mulitple%20Span%20on%204112F-ON%20with%20OpenSwitch/#my-configuration","text":"","title":"My Configuration"},{"location":"Mulitple%20Span%20on%204112F-ON%20with%20OpenSwitch/#general-configuration","text":"ONIE host is running RHEL 8 I am using a Dell S4112F-ON for testing OpenSwitch version PKGS_OPX-3.2.0-installer-x86_64 PFSense running DNS and DHCP as services","title":"General Configuration"},{"location":"Mulitple%20Span%20on%204112F-ON%20with%20OpenSwitch/#rhel-release-info","text":"NAME=\"Red Hat Enterprise Linux\" VERSION=\"8.0 (Ootpa)\" ID=\"rhel\" ID_LIKE=\"fedora\" VERSION_ID=\"8.0\" PLATFORM_ID=\"platform:el8\" PRETTY_NAME=\"Red Hat Enterprise Linux 8.0 (Ootpa)\" ANSI_COLOR=\"0;31\" CPE_NAME=\"cpe:/o:redhat:enterprise_linux:8.0:GA\" HOME_URL=\"https://www.redhat.com/\" BUG_REPORT_URL=\"https://bugzilla.redhat.com/\" REDHAT_BUGZILLA_PRODUCT=\"Red Hat Enterprise Linux 8\" REDHAT_BUGZILLA_PRODUCT_VERSION=8.0 REDHAT_SUPPORT_PRODUCT=\"Red Hat Enterprise Linux\" REDHAT_SUPPORT_PRODUCT_VERSION=\"8.0\" Red Hat Enterprise Linux release 8.0 (Ootpa) Red Hat Enterprise Linux release 8.0 (Ootpa)","title":"RHEL Release Info"},{"location":"Mulitple%20Span%20on%204112F-ON%20with%20OpenSwitch/#opx-version","text":"OS_NAME=\"OPX\" OS_VERSION=\"unstable\" PLATFORM=\"S4112F-ON\" ARCHITECTURE=\"x86_64\" INTERNAL_BUILD_ID=\"OpenSwitch blueprint for Dell 1.0.0\" BUILD_VERSION=\"unstable.0-stretch\" BUILD_DATE=\"2019-06-21T19:04:22+0000\" INSTALL_DATE=\"2019-10-23T23:16:10+00:00\" SYSTEM_UPTIME= 1 day, 5 minutes SYSTEM_STATE= running UPGRADED_PACKAGES=no ALTERED_PACKAGES=yes Wasn't sure why I got the unstable version after installation. It didn't cause any problems for testing so I just left it as is.","title":"OPX Version"},{"location":"Mulitple%20Span%20on%204112F-ON%20with%20OpenSwitch/#setup-onie-prerequisites","text":"See ONIE Install Setup for instructions.","title":"Setup ONIE Prerequisites"},{"location":"Mulitple%20Span%20on%204112F-ON%20with%20OpenSwitch/#configure-device-as-tap","text":"","title":"Configure Device as TAP"},{"location":"Mulitple%20Span%20on%204112F-ON%20with%20OpenSwitch/#physical-configuration","text":"For this configuration to work, we will use the management interface as the input interface for the tap. See image below. You will need to move your network cable over from your usual network to your traffic generator.","title":"Physical Configuration"},{"location":"Mulitple%20Span%20on%204112F-ON%20with%20OpenSwitch/#configure-management-interface-optional","text":"Update: After I got it working I ended up using this as an ingress interface so this step is more or less optional. You won't be able to SSH into this interface in the end config (at least not easily). Start by running sudo -i to move to privileged mode. Warning: I noticed the OPX command line tools won't behave correctly unless you are privileged. Ex: opx-show-interface won't list any interfaces. I added vim to my box before continuing with sudo apt-get install -y vim The management interface is configured like a typicaly Debian interface with vim /etc/network/interface.d/eth0 Use the following configuration modified to your needs: auto eth0 allow-hotplug eth0 iface eth0 inet static address 192.168.1.20 netmask 255.255.255.0 gateway 192.168.1.1 When you are finished with your configuration run systemctl restart networking to apply the changes. Confirm the changes were applied with ip address show dev eth0 . If you see two IP addresses because you picked one up from DHCP you can delete the other with ip address del [IP ADDRESS] dev eth0 and then run systemctl restart networking At this juncture your management interface should be up and running and you should be able to SSH to it. I went ahead and swapped to SSH so as not to deal with the oddities that come with running in the console port.","title":"Configure Management Interface (Optional)"},{"location":"Mulitple%20Span%20on%204112F-ON%20with%20OpenSwitch/#bridgetc-configuration","text":"After attempt 3 I started thinking about other ways to connect things. Realized I could just pump everything to a bridge and let that do the replication. That worked! Do the following to get it up and running: tc is a feature of modern Linux kernels designed for mirroring traffic. I found this article helpful. This Mellanox article was also useful. WARNING: You must use your management interface for the ingress port or this solution will not work! I noticed the other ports do not behave like normal Linux ports. More investigation required to figure out the difference. Create a bridge interface with brctl addbr br0 Attach all interfaces you want as part of the port mirroring to the bridge with brctl addif br0 < INTERFACE > Make sure all interfaces in use are enabled with ip link set < INTERFACE > up Disable MAC address learning on the bridge with brctl setageing br0 0 Set the device's management interface to promiscuous mode with ip link set < MGMT_INTERFACE > promisc on The first thing I did was create an ingress queue on my input interface with tc qdisc add dev < MGMT_INTERFACE > handle ffff: ingress If you need to delete a qdisc you can do it with tc qdisc del dev < MGMT_INTERFACE > [ root | ingress ] Double check your queue with handle ffff was created with tc -s qdisc ls dev < MGMT_INTERFACE > Next we want to mirror all traffic from the ingress port to an output port with tc filter add dev < MGMT_INTERFACE > parent ffff: protocol all u32 match u32 0 0 action mirred egress mirror dev br0 Check that your port mirror appeared in the config with tc -s -p filter ls dev < MGMT_INTERFACE > parent ffff: If you need to delete the filters you can do so with tc filter del dev < MGMT_INTERFACE > parent ffff: Set queue to not shape traffic with tc qdisc add dev < MGMT_INTERFACE > handle 1: root prio","title":"Bridge/tc Configuration"},{"location":"Mulitple%20Span%20on%204112F-ON%20with%20OpenSwitch/#things-i-tried","text":"I added 7 interfaces to my bridge to make sure there weren't any strange limitations I moved the SFPs around to multiple different ports to make sure the traffic was mirroing on all of them I double checked the traffic I was capturing belonged to the PCAP in question. Easy enough to see because it has IP addresses the hosts in question wouldn't ever otherwise see. Screenshots for confirmation below.","title":"Things I Tried"},{"location":"Mulitple%20Span%20on%204112F-ON%20with%20OpenSwitch/#host-1","text":"","title":"Host 1"},{"location":"Mulitple%20Span%20on%204112F-ON%20with%20OpenSwitch/#host-2","text":"- I checked that pure L3 traffic was passed correctly using ICMP.","title":"Host 2"},{"location":"Mulitple%20Span%20on%204112F-ON%20with%20OpenSwitch/#noted-problem","text":"The only major issue I noticed is that pure layer 2 traffic didn't get passed. Haven't figured out how to fix that yet.","title":"Noted Problem"},{"location":"Mulitple%20Span%20on%204112F-ON%20with%20OpenSwitch/#failed-ideas","text":"","title":"Failed Ideas"},{"location":"Mulitple%20Span%20on%204112F-ON%20with%20OpenSwitch/#attempt-1-mirror-ports","text":"My first go is to try using OpenSwitch's built in mirroring capability.","title":"Attempt 1 - Mirror Ports"},{"location":"Mulitple%20Span%20on%204112F-ON%20with%20OpenSwitch/#physical-configuration_1","text":"I didn't have enough target hosts to try outputting from one port to all ports so I simulated it. The purple cable in the image is the input port from the traffic generator (tcpreplay) and the white and yellow cables go out to the hosts listed as host 1 and host 2 in the test results section. The ports with the white and yellow cables were configured as the mirror's target ports.","title":"Physical Configuration"},{"location":"Mulitple%20Span%20on%204112F-ON%20with%20OpenSwitch/#mirror-configuration","text":"For each port you want to mirror to run opx-config-mirror create --src_intf e101-001-0 --dest_intf e101-005-0 --direction ingress --type span . Substitute your source and destination ports appropriately.","title":"Mirror Configuration"},{"location":"Mulitple%20Span%20on%204112F-ON%20with%20OpenSwitch/#results","text":"","title":"Results"},{"location":"Mulitple%20Span%20on%204112F-ON%20with%20OpenSwitch/#mirror-port-failure-after-4","text":"I was only able to get this to work on up to 4 ports. After that I received errors. See output below: root@OPX:~# opx-config-mirror create --src_intf e101-001-0 --dest_intf e101-005-0 --direction ingress --type span root@OPX:~# ip link set e101-009-0 up root@OPX:~# opx-config-mirror create --src_intf e101-001-0 --dest_intf e101-009-0 --direction ingress --type span root@OPX:~# opx-config-mirror create --src_intf e101-001-0 --dest_intf e101-002-0 --direction ingress --type span root@OPX:~# opx-config-mirror create --src_intf e101-001-0 --dest_intf e101-003-0 --direction ingress --type span root@OPX:~# opx-config-mirror create --src_intf e101-001-0 --dest_intf e101-004-0 --direction ingress --type span {'data': {'base-mirror/entry/dst-intf': bytearray(b'\\x0f\\x00\\x00\\x00'), 'base-mirror/entry/type': bytearray(b'\\x01\\x00\\x00\\x00'), 'base-mirror/entry/intf': {'0': {'base-mirror/entry/intf/src': bytearray(b'\\x0c\\x00\\x00\\x00'), 'base-mirror/entry/intf/direction': bytearray(b'\\x01\\x00\\x00\\x00')}}}, 'key': '1.27.1769488.1769473.'} opx-config-mirror: Commit failed root@OPX:~# opx-config-mirror create --src_intf e101-001-0 --dest_intf e101-006-0 --direction ingress --type span {'data': {'base-mirror/entry/dst-intf': bytearray(b'\\x11\\x00\\x00\\x00'), 'base-mirror/entry/type': bytearray(b'\\x01\\x00\\x00\\x00'), 'base-mirror/entry/intf': {'0': {'base-mirror/entry/intf/src': bytearray(b'\\x0c\\x00\\x00\\x00'), 'base-mirror/entry/intf/direction': bytearray(b'\\x01\\x00\\x00\\x00')}}}, 'key': '1.27.1769488.1769473.'} opx-config-mirror: Commit failed root@OPX:~# opx-config-mirror create --src_intf e101-001-0 --dest_intf e101-007-0 --direction ingress --type span {'data': {'base-mirror/entry/dst-intf': bytearray(b'\\x12\\x00\\x00\\x00'), 'base-mirror/entry/type': bytearray(b'\\x01\\x00\\x00\\x00'), 'base-mirror/entry/intf': {'0': {'base-mirror/entry/intf/src': bytearray(b'\\x0c\\x00\\x00\\x00'), 'base-mirror/entry/intf/direction': bytearray(b'\\x01\\x00\\x00\\x00')}}}, 'key': '1.27.1769488.1769473.'} opx-config-mirror: Commit failed Pretty printed version for ease of reading: { 'data': { 'base-mirror/entry/dst-intf': bytearray(b '\\x12\\x00\\x00\\x00'), 'base-mirror/entry/type': bytearray(b '\\x01\\x00\\x00\\x00'), 'base-mirror/entry/intf': { '0': { 'base-mirror/entry/intf/src': bytearray(b '\\x0c\\x00\\x00\\x00'), 'base-mirror/entry/intf/direction': bytearray(b '\\x01\\x00\\x00\\x00') } } }, 'key': '1.27.1769488.1769473.' }","title":"Mirror Port Failure After 4"},{"location":"Mulitple%20Span%20on%204112F-ON%20with%20OpenSwitch/#functioning-mirror-ports","text":"Before I caught the error, I did test the first two mirror ports I made and they worked as expected. See the below. I used tcpreplay with some traffic I captured on my desktop to test the idea. I just uploaded the PCAP and replayed it with tcpreplay -i ens224 ./test_pcap.pcap --loop 500 I then confirmed that all target ports received traffic. See screenshots below:","title":"Functioning Mirror Ports"},{"location":"Mulitple%20Span%20on%204112F-ON%20with%20OpenSwitch/#host-1_1","text":"","title":"Host 1"},{"location":"Mulitple%20Span%20on%204112F-ON%20with%20OpenSwitch/#host-2_1","text":"The host I collected the traffic on was 192.168.1.6 and as you can see from the images both hosts were able to see traffic from the tcpreplay session.","title":"Host 2"},{"location":"Mulitple%20Span%20on%204112F-ON%20with%20OpenSwitch/#attempt-2-tc","text":"","title":"Attempt 2 - tc"},{"location":"Mulitple%20Span%20on%204112F-ON%20with%20OpenSwitch/#configuration","text":"tc is a feature of modern Linux kernels designed for mirroring traffic. I found this article helpful. This Mellanox article was also useful. The first thing I did was create an ingress queue on my input interface with tc qdisc add dev e101-001-0 handle ffff: ingress If you need to delete a qdisc you can do it with tc qdisc del dev e101-001-0 [ root | ingress ] Double check your queue with handle ffff was created with tc -s qdisc ls dev e101-001-0 Next we want to mirror all traffic from the ingress port to an output port with tc filter add dev e101-001-0 parent ffff: protocol all u32 match u32 0 0 action mirred egress mirror dev e101-005-0 Check that your port mirror appeared in the config with tc -s -p filter ls dev e101-001-0 parent ffff: If you need to delete the filters you can do so with tc filter del dev e101-001-0 parent ffff: Set queue to not shape traffic with tc qdisc add dev e101-001-0 handle 1: root prio","title":"Configuration"},{"location":"Mulitple%20Span%20on%204112F-ON%20with%20OpenSwitch/#alternate-configuration","text":"I tried instead running: tc qdisc add dev e101-001-0 clsact tc filter add dev e101-001-0 ingress matchall skip_sw action mirred egress mirror dev e101-005-0","title":"Alternate Configuration"},{"location":"Mulitple%20Span%20on%204112F-ON%20with%20OpenSwitch/#other-things-tried","text":"Haven't been able to figure out why just yet, but only Layer 2 traffic is making it through the port mirror. Everything above gets dropped. I thought maybe it was MAC address learning, but the problem persisted when I ran opx-config-global-switch --mac-age-time 0 I also thought that it was the port not being set to promiscuous mode so I gave it ifconfig e101-001-0 promisc . That didn't work either.","title":"Other Things Tried"},{"location":"Mulitple%20Span%20on%204112F-ON%20with%20OpenSwitch/#conclusions","text":"I'm pretty confident that because this is a network OS for switching something funky is going on. Ex: When you run a port mirror, all the traffic passes correctly, but you won't see any of that traffic on a tcpdump session. Need to study up on the architecture. I'm pretty sure there's a way to make this particular tactic work, but for time's sake I'm going to try something else.","title":"Conclusions"},{"location":"Mulitple%20Span%20on%204112F-ON%20with%20OpenSwitch/#attempt-3-tc-on-management-interface","text":"I realized something is going on with the forwarding tables on the switch at a low level that was intercepting our traffic in attempt 2. That said, I noticed that the management interface for the switch effectively works like a standard Linux interface. I did the same thing I did in attempt 2 except I used the managament interface instead of one of the other interfaces.","title":"Attempt 3  - tc on Management Interface"},{"location":"Mulitple%20Span%20on%204112F-ON%20with%20OpenSwitch/#results_1","text":"The switch accepts the config. However, the traffic only goes out to one port at a time.","title":"Results"},{"location":"Multiple%20Span%20on%204112F-ON%20with%20OS10/","text":"Multiple Span on 4112F-ON with OS10 In this test case I am testing to see if we can configure a Dell 4112F-ON with OS10 to create a one to many port configuration using SPAN. Helpful Links ONIE Network Install Process Overview My Configuration General Configuration ONIE host is running RHEL 8 I am using a Dell S4112F-ON for testing OS10 version 10.5.0.2 PFSense running DNS and DHCP as services RHEL Release Info NAME=\"Red Hat Enterprise Linux\" VERSION=\"8.0 (Ootpa)\" ID=\"rhel\" ID_LIKE=\"fedora\" VERSION_ID=\"8.0\" PLATFORM_ID=\"platform:el8\" PRETTY_NAME=\"Red Hat Enterprise Linux 8.0 (Ootpa)\" ANSI_COLOR=\"0;31\" CPE_NAME=\"cpe:/o:redhat:enterprise_linux:8.0:GA\" HOME_URL=\"https://www.redhat.com/\" BUG_REPORT_URL=\"https://bugzilla.redhat.com/\" REDHAT_BUGZILLA_PRODUCT=\"Red Hat Enterprise Linux 8\" REDHAT_BUGZILLA_PRODUCT_VERSION=8.0 REDHAT_SUPPORT_PRODUCT=\"Red Hat Enterprise Linux\" REDHAT_SUPPORT_PRODUCT_VERSION=\"8.0\" Red Hat Enterprise Linux release 8.0 (Ootpa) Red Hat Enterprise Linux release 8.0 (Ootpa) OS10 Version Dell EMC Networking OS10 Enterprise Copyright (c) 1999-2019 by Dell Inc. All Rights Reserved. OS Version: 10.5.0.2 Build Version: 10.5.0.2.468 Build Time: 2019-10-19T00:29:00+0000 System Type: S4112F-ON Architecture: x86_64 Up Time: 00:13:11 Setup ONIE Prerequisites See ONIE Install Setup for instructions. Configure Device as TAP Physical Configuration For this configuration to work, we will use the management interface as the input interface for the tap. See image below. You will need to move your network cable over from your usual network to your traffic generator. Problem The way this worked on OPX was to use the Linux kernel module called TC. The net_sched module which supports ingress packet manipulation is not available on OS10. It could be reinstalled, but I didn't explore this option. Currently I don't have a working config on OS10. Test with Mirror Ports It looks like OS10 only supports one destination port on a port mirror. See the below. OS10(conf-mon-local-1)# do show monitor session 1 S.Id Source Destination Dir Mode Source IP Dest IP DSCP TTL Gre-Protocol State Reason --------------------------------------------------------------------------------------------------------------------------------------- 1 ethernet1/1/3 both port N/A N/A N/A N/A N/A false Destination is not configured OS10(conf-mon-local-1)# destination interface ethernet 1/1/7 OS10(conf-mon-local-1)# destination interface ethernet 1/1/8 % Error: Configuration mismatch. OS10(conf-mon-local-1)# no destination interface ethernet 1/1/7 OS10(conf-mon-local-1)# destination interface ethernet 1/1/8 OS10(conf-mon-local-1)#","title":"Multiple Span on 4112F-ON with OS10"},{"location":"Multiple%20Span%20on%204112F-ON%20with%20OS10/#multiple-span-on-4112f-on-with-os10","text":"In this test case I am testing to see if we can configure a Dell 4112F-ON with OS10 to create a one to many port configuration using SPAN.","title":"Multiple Span on 4112F-ON with OS10"},{"location":"Multiple%20Span%20on%204112F-ON%20with%20OS10/#helpful-links","text":"ONIE Network Install Process Overview","title":"Helpful Links"},{"location":"Multiple%20Span%20on%204112F-ON%20with%20OS10/#my-configuration","text":"","title":"My Configuration"},{"location":"Multiple%20Span%20on%204112F-ON%20with%20OS10/#general-configuration","text":"ONIE host is running RHEL 8 I am using a Dell S4112F-ON for testing OS10 version 10.5.0.2 PFSense running DNS and DHCP as services","title":"General Configuration"},{"location":"Multiple%20Span%20on%204112F-ON%20with%20OS10/#rhel-release-info","text":"NAME=\"Red Hat Enterprise Linux\" VERSION=\"8.0 (Ootpa)\" ID=\"rhel\" ID_LIKE=\"fedora\" VERSION_ID=\"8.0\" PLATFORM_ID=\"platform:el8\" PRETTY_NAME=\"Red Hat Enterprise Linux 8.0 (Ootpa)\" ANSI_COLOR=\"0;31\" CPE_NAME=\"cpe:/o:redhat:enterprise_linux:8.0:GA\" HOME_URL=\"https://www.redhat.com/\" BUG_REPORT_URL=\"https://bugzilla.redhat.com/\" REDHAT_BUGZILLA_PRODUCT=\"Red Hat Enterprise Linux 8\" REDHAT_BUGZILLA_PRODUCT_VERSION=8.0 REDHAT_SUPPORT_PRODUCT=\"Red Hat Enterprise Linux\" REDHAT_SUPPORT_PRODUCT_VERSION=\"8.0\" Red Hat Enterprise Linux release 8.0 (Ootpa) Red Hat Enterprise Linux release 8.0 (Ootpa)","title":"RHEL Release Info"},{"location":"Multiple%20Span%20on%204112F-ON%20with%20OS10/#os10-version","text":"Dell EMC Networking OS10 Enterprise Copyright (c) 1999-2019 by Dell Inc. All Rights Reserved. OS Version: 10.5.0.2 Build Version: 10.5.0.2.468 Build Time: 2019-10-19T00:29:00+0000 System Type: S4112F-ON Architecture: x86_64 Up Time: 00:13:11","title":"OS10 Version"},{"location":"Multiple%20Span%20on%204112F-ON%20with%20OS10/#setup-onie-prerequisites","text":"See ONIE Install Setup for instructions.","title":"Setup ONIE Prerequisites"},{"location":"Multiple%20Span%20on%204112F-ON%20with%20OS10/#configure-device-as-tap","text":"","title":"Configure Device as TAP"},{"location":"Multiple%20Span%20on%204112F-ON%20with%20OS10/#physical-configuration","text":"For this configuration to work, we will use the management interface as the input interface for the tap. See image below. You will need to move your network cable over from your usual network to your traffic generator.","title":"Physical Configuration"},{"location":"Multiple%20Span%20on%204112F-ON%20with%20OS10/#problem","text":"The way this worked on OPX was to use the Linux kernel module called TC. The net_sched module which supports ingress packet manipulation is not available on OS10. It could be reinstalled, but I didn't explore this option. Currently I don't have a working config on OS10.","title":"Problem"},{"location":"Multiple%20Span%20on%204112F-ON%20with%20OS10/#test-with-mirror-ports","text":"It looks like OS10 only supports one destination port on a port mirror. See the below. OS10(conf-mon-local-1)# do show monitor session 1 S.Id Source Destination Dir Mode Source IP Dest IP DSCP TTL Gre-Protocol State Reason --------------------------------------------------------------------------------------------------------------------------------------- 1 ethernet1/1/3 both port N/A N/A N/A N/A N/A false Destination is not configured OS10(conf-mon-local-1)# destination interface ethernet 1/1/7 OS10(conf-mon-local-1)# destination interface ethernet 1/1/8 % Error: Configuration mismatch. OS10(conf-mon-local-1)# no destination interface ethernet 1/1/7 OS10(conf-mon-local-1)# destination interface ethernet 1/1/8 OS10(conf-mon-local-1)#","title":"Test with Mirror Ports"},{"location":"NVMe%20Log%20Pages%20Explained/","text":"NVMe Log Pages Explained My Test Drive [root@r8402 ~]# nvme id-ctrl /dev/nvme0n1 NVME Identify Controller: vid : 0x8086 ssvid : 0x1028 sn : PHLN939602VB3P2BGN mn : Dell Express Flash NVMe P4610 3.2TB SFF fr : VDV1DP25 rab : 0 ieee : 5cd2e4 cmic : 0 mdts : 5 cntlid : 0 ver : 0x10200 rtd3r : 0x989680 rtd3e : 0xe4e1c0 oaes : 0x200 ctratt : 0 rrls : 0 cntrltype : 0 fguid : crdt1 : 0 crdt2 : 0 crdt3 : 0 oacs : 0x6 acl : 3 aerl : 3 frmw : 0x18 lpa : 0xe elpe : 63 npss : 0 avscc : 0 apsta : 0 wctemp : 343 cctemp : 349 mtfa : 0 hmpre : 0 hmmin : 0 tnvmcap : 3200631791616 unvmcap : 0 rpmbs : 0 edstt : 0 dsto : 0 fwug : 0 kas : 0 hctma : 0 mntmt : 0 mxtmt : 0 sanicap : 0 hmminds : 0 hmmaxd : 0 nsetidmax : 0 endgidmax : 0 anatt : 0 anacap : 0 anagrpmax : 0 nanagrpid : 0 pels : 0 sqes : 0x66 cqes : 0x44 maxcmd : 0 nn : 1 oncs : 0x6 fuses : 0 fna : 0x4 vwc : 0 awun : 0 awupf : 0 icsvscc : 0 nwpc : 0 acwu : 0 sgls : 0 mnan : 0 subnqn : ioccsz : 0 iorcsz : 0 icdoff : 0 fcatt : 0 msdbd : 0 ofcs : 0 ps 0 : mp:25.00W operational enlat:0 exlat:0 rrt:0 rrl:0 rwt:0 rwl:0 idle_power:- active_power:- Get Log Page Identifiers NVMe Express Base Specification Error Information (01h) This log page is used to describe extended error information for a command that completed with error or report an error that is not specific to a particular command. Extended error information is provided when the More (M) bit is set to \u20181\u2019 in the Status Field for the completion queue entry associated with the command that completed with error or as part of an asynchronous event with an Error status type. This log page is global to the controller. This error log may return the last n errors. If host software specifies a data transfer of the size of n error logs, then the error logs for the most recent n errors are returned. The ordering of the entries is based on the time when the error occurred, with the most recent error being returned as the first log entry. Each entry in the log page returned is defined in Figure 206. The log page is a set of 64-byte entries; the maximum number of entries supported is indicated in the ELPE field in the Identify Controller data structure (refer to Figure 275). If the log page is full when a new entry is generated, the controller should insert the new entry into the log and discard the oldest entry. The controller should clear this log page by removing all entries on power cycle and Controller Level Reset. Sample Output [root@r8402 ~]# nvme error-log /dev/nvme0n1 Error Log Entries for device:nvme0n1 entries:64 ................. Entry[ 0] ................. error_count : 0 sqid : 0 cmdid : 0 status_field : 0(SUCCESS: The command completed successfully) phase_tag : 0 parm_err_loc : 0 lba : 0 nsid : 0 vs : 0 trtype : The transport type is not indicated or the error is not transport related. cs : 0 trtype_spec_info: 0 ................. ...SNIP... Entry[63] ................. error_count : 0 sqid : 0 cmdid : 0 status_field : 0(SUCCESS: The command completed successfully) phase_tag : 0 parm_err_loc : 0 lba : 0 nsid : 0 vs : 0 trtype : The transport type is not indicated or the error is not transport related. cs : 0 trtype_spec_info: 0 ................. SMART / Health Information (02h) This log page is used to provide SMART and general health information. The information provided is over the life of the controller and is retained across power cycles. To request the controller log page, the namespace identifier specified is FFFFFFFFh or 0h. For compatibility with implementations compliant with NVM Express Base Specification revision 1.4 and earlier, hosts should use a namespace identifier of FFFFFFFFh to request the controller log page. The controller may also support requesting the log page on a per namespace basis, as indicated by bit 0 of the LPA field in the Identify Controller data structure in Figure 275. Sample Output [root@r8402 ~]# nvme smart-log /dev/nvme0n1 Smart Log for NVME device:nvme0n1 namespace-id:ffffffff critical_warning : 0 temperature : 26 C available_spare : 100% available_spare_threshold : 10% percentage_used : 0% endurance group critical warning summary: 0 data_units_read : 4,002,753 data_units_written : 255,875,492 host_read_commands : 45,714,473 host_write_commands : 1,620,770,593 controller_busy_time : 372 power_cycles : 150 power_on_hours : 5,219 unsafe_shutdowns : 99 media_errors : 0 num_err_log_entries : 0 Warning Temperature Time : 0 Critical Composite Temperature Time : 0 Thermal Management T1 Trans Count : 0 Thermal Management T2 Trans Count : 0 Thermal Management T1 Total Time : 0 Thermal Management T2 Total Time : 0 Firmware Slot Information (03h) This log page is used to describe the firmware revision stored in each firmware slot supported. The firmware revision is indicated as an ASCII string. The log page also indicates the active slot number. The log page returned is defined in Figure 209 Sample Output [root@r8402 ~]# nvme fw-log /dev/nvme0n1 Firmware Log for device:nvme0n1 afi : 0x1 frs1 : 0x3532504431564456 (VDV1DP25) frs2 : 0x3532504431564456 (VDV1DP25) Changed Namespace List (04h) NOTE This command is not currently supported because the drives currently only have one namespace. This log page is used to describe namespaces attached to the controller that have: changed information in their Identify Namespace data structures (refer to in Figure 146) since the last time the log page was read; been added; and been deleted. The log page contains a Namespace List with up to 1,024 entries. If more than 1,024 namespaces have changed attributes since the last time the log page was read, the first entry in the log page shall be set to FFFFFFFFh and the remainder of the list shall be zero filled. Commands Supported and Effects (05h) This log page is used to describe the commands that the controller supports and the effects of those commands on the state of the NVM subsystem. The log page is 4,096 bytes in size. There is one Commands Supported and Effects data structure per Admin command and one Commands Supported and Effects data structure per I/O command based on: the I/O Command Set selected in CC.CSS, if CC.CSS is not set to 110b; and the Command Set Identifier field in CDW 14, if CC.CSS is set to 110b. Sample Output nvme effects-log /dev/nvme0n1 Device Self-test (06h) This log page is used to indicate: the status of any device self-test operation in progress and the percentage complete of that operation; and the results of the last 20 device self-test operations. The Self-test Result Data Structure contained in the Newest Self-test Result Data Structure field is always the result of the last completed or aborted self-test operation. The next Self-test Result Data Structure field in the Device Self-test log page contains the results of the second newest self-test operation and so on. If fewer than 20 self-test operations have completed or been aborted, then the Device Self-test Status field shall be set to Fh in the unused Self-test Result Data Structure fields and all other fields in that Self-test Result Data Structure are ignored. Other NVMe CLI Commands List All NVMe Drives nvme list Lists all the NVMe SSDs attached: name, serial number, size, LBA format, and serial","title":"NVMe Log Pages Explained"},{"location":"NVMe%20Log%20Pages%20Explained/#nvme-log-pages-explained","text":"","title":"NVMe Log Pages Explained"},{"location":"NVMe%20Log%20Pages%20Explained/#my-test-drive","text":"[root@r8402 ~]# nvme id-ctrl /dev/nvme0n1 NVME Identify Controller: vid : 0x8086 ssvid : 0x1028 sn : PHLN939602VB3P2BGN mn : Dell Express Flash NVMe P4610 3.2TB SFF fr : VDV1DP25 rab : 0 ieee : 5cd2e4 cmic : 0 mdts : 5 cntlid : 0 ver : 0x10200 rtd3r : 0x989680 rtd3e : 0xe4e1c0 oaes : 0x200 ctratt : 0 rrls : 0 cntrltype : 0 fguid : crdt1 : 0 crdt2 : 0 crdt3 : 0 oacs : 0x6 acl : 3 aerl : 3 frmw : 0x18 lpa : 0xe elpe : 63 npss : 0 avscc : 0 apsta : 0 wctemp : 343 cctemp : 349 mtfa : 0 hmpre : 0 hmmin : 0 tnvmcap : 3200631791616 unvmcap : 0 rpmbs : 0 edstt : 0 dsto : 0 fwug : 0 kas : 0 hctma : 0 mntmt : 0 mxtmt : 0 sanicap : 0 hmminds : 0 hmmaxd : 0 nsetidmax : 0 endgidmax : 0 anatt : 0 anacap : 0 anagrpmax : 0 nanagrpid : 0 pels : 0 sqes : 0x66 cqes : 0x44 maxcmd : 0 nn : 1 oncs : 0x6 fuses : 0 fna : 0x4 vwc : 0 awun : 0 awupf : 0 icsvscc : 0 nwpc : 0 acwu : 0 sgls : 0 mnan : 0 subnqn : ioccsz : 0 iorcsz : 0 icdoff : 0 fcatt : 0 msdbd : 0 ofcs : 0 ps 0 : mp:25.00W operational enlat:0 exlat:0 rrt:0 rrl:0 rwt:0 rwl:0 idle_power:- active_power:-","title":"My Test Drive"},{"location":"NVMe%20Log%20Pages%20Explained/#get-log-page-identifiers","text":"NVMe Express Base Specification","title":"Get Log Page Identifiers"},{"location":"NVMe%20Log%20Pages%20Explained/#error-information-01h","text":"This log page is used to describe extended error information for a command that completed with error or report an error that is not specific to a particular command. Extended error information is provided when the More (M) bit is set to \u20181\u2019 in the Status Field for the completion queue entry associated with the command that completed with error or as part of an asynchronous event with an Error status type. This log page is global to the controller. This error log may return the last n errors. If host software specifies a data transfer of the size of n error logs, then the error logs for the most recent n errors are returned. The ordering of the entries is based on the time when the error occurred, with the most recent error being returned as the first log entry. Each entry in the log page returned is defined in Figure 206. The log page is a set of 64-byte entries; the maximum number of entries supported is indicated in the ELPE field in the Identify Controller data structure (refer to Figure 275). If the log page is full when a new entry is generated, the controller should insert the new entry into the log and discard the oldest entry. The controller should clear this log page by removing all entries on power cycle and Controller Level Reset.","title":"Error Information (01h)"},{"location":"NVMe%20Log%20Pages%20Explained/#sample-output","text":"[root@r8402 ~]# nvme error-log /dev/nvme0n1 Error Log Entries for device:nvme0n1 entries:64 ................. Entry[ 0] ................. error_count : 0 sqid : 0 cmdid : 0 status_field : 0(SUCCESS: The command completed successfully) phase_tag : 0 parm_err_loc : 0 lba : 0 nsid : 0 vs : 0 trtype : The transport type is not indicated or the error is not transport related. cs : 0 trtype_spec_info: 0 ................. ...SNIP... Entry[63] ................. error_count : 0 sqid : 0 cmdid : 0 status_field : 0(SUCCESS: The command completed successfully) phase_tag : 0 parm_err_loc : 0 lba : 0 nsid : 0 vs : 0 trtype : The transport type is not indicated or the error is not transport related. cs : 0 trtype_spec_info: 0 .................","title":"Sample Output"},{"location":"NVMe%20Log%20Pages%20Explained/#smart-health-information-02h","text":"This log page is used to provide SMART and general health information. The information provided is over the life of the controller and is retained across power cycles. To request the controller log page, the namespace identifier specified is FFFFFFFFh or 0h. For compatibility with implementations compliant with NVM Express Base Specification revision 1.4 and earlier, hosts should use a namespace identifier of FFFFFFFFh to request the controller log page. The controller may also support requesting the log page on a per namespace basis, as indicated by bit 0 of the LPA field in the Identify Controller data structure in Figure 275.","title":"SMART / Health Information (02h)"},{"location":"NVMe%20Log%20Pages%20Explained/#sample-output_1","text":"[root@r8402 ~]# nvme smart-log /dev/nvme0n1 Smart Log for NVME device:nvme0n1 namespace-id:ffffffff critical_warning : 0 temperature : 26 C available_spare : 100% available_spare_threshold : 10% percentage_used : 0% endurance group critical warning summary: 0 data_units_read : 4,002,753 data_units_written : 255,875,492 host_read_commands : 45,714,473 host_write_commands : 1,620,770,593 controller_busy_time : 372 power_cycles : 150 power_on_hours : 5,219 unsafe_shutdowns : 99 media_errors : 0 num_err_log_entries : 0 Warning Temperature Time : 0 Critical Composite Temperature Time : 0 Thermal Management T1 Trans Count : 0 Thermal Management T2 Trans Count : 0 Thermal Management T1 Total Time : 0 Thermal Management T2 Total Time : 0","title":"Sample Output"},{"location":"NVMe%20Log%20Pages%20Explained/#firmware-slot-information-03h","text":"This log page is used to describe the firmware revision stored in each firmware slot supported. The firmware revision is indicated as an ASCII string. The log page also indicates the active slot number. The log page returned is defined in Figure 209","title":"Firmware Slot Information (03h)"},{"location":"NVMe%20Log%20Pages%20Explained/#sample-output_2","text":"[root@r8402 ~]# nvme fw-log /dev/nvme0n1 Firmware Log for device:nvme0n1 afi : 0x1 frs1 : 0x3532504431564456 (VDV1DP25) frs2 : 0x3532504431564456 (VDV1DP25)","title":"Sample Output"},{"location":"NVMe%20Log%20Pages%20Explained/#changed-namespace-list-04h","text":"NOTE This command is not currently supported because the drives currently only have one namespace. This log page is used to describe namespaces attached to the controller that have: changed information in their Identify Namespace data structures (refer to in Figure 146) since the last time the log page was read; been added; and been deleted. The log page contains a Namespace List with up to 1,024 entries. If more than 1,024 namespaces have changed attributes since the last time the log page was read, the first entry in the log page shall be set to FFFFFFFFh and the remainder of the list shall be zero filled.","title":"Changed Namespace List (04h)"},{"location":"NVMe%20Log%20Pages%20Explained/#commands-supported-and-effects-05h","text":"This log page is used to describe the commands that the controller supports and the effects of those commands on the state of the NVM subsystem. The log page is 4,096 bytes in size. There is one Commands Supported and Effects data structure per Admin command and one Commands Supported and Effects data structure per I/O command based on: the I/O Command Set selected in CC.CSS, if CC.CSS is not set to 110b; and the Command Set Identifier field in CDW 14, if CC.CSS is set to 110b.","title":"Commands Supported and Effects (05h)"},{"location":"NVMe%20Log%20Pages%20Explained/#sample-output_3","text":"nvme effects-log /dev/nvme0n1","title":"Sample Output"},{"location":"NVMe%20Log%20Pages%20Explained/#device-self-test-06h","text":"This log page is used to indicate: the status of any device self-test operation in progress and the percentage complete of that operation; and the results of the last 20 device self-test operations. The Self-test Result Data Structure contained in the Newest Self-test Result Data Structure field is always the result of the last completed or aborted self-test operation. The next Self-test Result Data Structure field in the Device Self-test log page contains the results of the second newest self-test operation and so on. If fewer than 20 self-test operations have completed or been aborted, then the Device Self-test Status field shall be set to Fh in the unused Self-test Result Data Structure fields and all other fields in that Self-test Result Data Structure are ignored.","title":"Device Self-test (06h)"},{"location":"NVMe%20Log%20Pages%20Explained/#other-nvme-cli-commands","text":"","title":"Other NVMe CLI Commands"},{"location":"NVMe%20Log%20Pages%20Explained/#list-all-nvme-drives","text":"nvme list Lists all the NVMe SSDs attached: name, serial number, size, LBA format, and serial","title":"List All NVMe Drives"},{"location":"NVMe%20Performance%20Testing/","text":"NVMe Performance Testing Helpful Resources How fast are your disks? Find out the open source way, with fio (arstechnica) Configuration Drives OS [root@r8402 ~]# cat /etc/*-release NAME=\"Red Hat Enterprise Linux\" VERSION=\"8.2 (Ootpa)\" ID=\"rhel\" ID_LIKE=\"fedora\" VERSION_ID=\"8.2\" PLATFORM_ID=\"platform:el8\" PRETTY_NAME=\"Red Hat Enterprise Linux 8.2 (Ootpa)\" ANSI_COLOR=\"0;31\" CPE_NAME=\"cpe:/o:redhat:enterprise_linux:8.2:GA\" HOME_URL=\"https://www.redhat.com/\" BUG_REPORT_URL=\"https://bugzilla.redhat.com/\" REDHAT_BUGZILLA_PRODUCT=\"Red Hat Enterprise Linux 8\" REDHAT_BUGZILLA_PRODUCT_VERSION=8.2 REDHAT_SUPPORT_PRODUCT=\"Red Hat Enterprise Linux\" REDHAT_SUPPORT_PRODUCT_VERSION=\"8.2\" Red Hat Enterprise Linux release 8.2 (Ootpa) Red Hat Enterprise Linux release 8.2 (Ootpa) Tests Test 1 Config pvcreate /dev/nvme2n1 pvcreate /dev/nvme1n1 pvcreate /dev/nvme0n1 pvcreate /dev/nvme3n1 vgcreate data /dev/nvme3n1 /dev/nvme2n1 /dev/nvme1n1 /dev/nvme0n1 lvcreate -l 100%FREE -i4 -I128 -n data data mkfs.ext4 -F -b 4096 -E discard,stride=16,stripe-width=256 /dev/mapper/data-data mkdir /data mount -o rw,auto,discard /dev/mapper/data-data /data fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test --filename=test --bs=64k --iodepth=32 --size=50G --readwrite=randrw --rwmixread=60 Results Test: (g=0): rw=randrw, bs=(R) 64.0KiB-64.0KiB, (W) 64.0KiB-64.0KiB, (T) 64.0KiB-64.0KiB, ioengine=libaio, iodepth=32 fio-3.7 Starting 1 process Test: Laying out IO file (1 file / 51200MiB) Jobs: 1 (f=1): [m(1)][100.0%][r=217MiB/s,w=148MiB/s][r=3476,w=2371 IOPS][eta 00m:00s] Test: (groupid=0, jobs=1): err= 0: pid=3290: Mon Sep 14 11:17:39 2020 read: IOPS=3639, BW=227MiB/s (239MB/s)(29.0GiB/134968msec) bw ( KiB/s): min=211328, max=261504, per=99.99%, avg=232911.32, stdev=8921.48, samples=269 iops : min= 3302, max= 4086, avg=3639.22, stdev=139.41, samples=269 write: IOPS=2429, BW=152MiB/s (159MB/s)(20.0GiB/134968msec) bw ( KiB/s): min=140800, max=169856, per=100.00%, avg=155506.13, stdev=5852.42, samples=269 iops : min= 2200, max= 2654, avg=2429.77, stdev=91.43, samples=269 cpu : usr=2.19%, sys=8.61%, ctx=98853, majf=0, minf=13 IO depths : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=100.0%, >=64=0.0% submit : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0% complete : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.1%, 64=0.0%, >=64=0.0% issued rwts: total=491242,327958,0,0 short=0,0,0,0 dropped=0,0,0,0 latency : target=0, window=0, percentile=100.00%, depth=32 Run status group 0 (all jobs): READ: bw=227MiB/s (239MB/s), 227MiB/s-227MiB/s (239MB/s-239MB/s), io=29.0GiB (32.2GB), run=134968-134968msec WRITE: bw=152MiB/s (159MB/s), 152MiB/s-152MiB/s (159MB/s-159MB/s), io=20.0GiB (21.5GB), run=134968-134968msec Disk stats (read/write): dm-0: ios=490775/327707, merge=0/0, ticks=3019139/1204501, in_queue=4223640, util=68.01%, aggrios=491243/328000, aggrmerge=1/11, aggrticks=3031345/1210547, aggrin_queue=3827355, aggrutil=67.98% sda: ios=491243/328000, merge=1/11, ticks=3031345/1210547, in_queue=3827355, util=67.98%","title":"NVMe Performance Testing"},{"location":"NVMe%20Performance%20Testing/#nvme-performance-testing","text":"","title":"NVMe Performance Testing"},{"location":"NVMe%20Performance%20Testing/#helpful-resources","text":"How fast are your disks? Find out the open source way, with fio (arstechnica)","title":"Helpful Resources"},{"location":"NVMe%20Performance%20Testing/#configuration","text":"","title":"Configuration"},{"location":"NVMe%20Performance%20Testing/#drives","text":"","title":"Drives"},{"location":"NVMe%20Performance%20Testing/#os","text":"[root@r8402 ~]# cat /etc/*-release NAME=\"Red Hat Enterprise Linux\" VERSION=\"8.2 (Ootpa)\" ID=\"rhel\" ID_LIKE=\"fedora\" VERSION_ID=\"8.2\" PLATFORM_ID=\"platform:el8\" PRETTY_NAME=\"Red Hat Enterprise Linux 8.2 (Ootpa)\" ANSI_COLOR=\"0;31\" CPE_NAME=\"cpe:/o:redhat:enterprise_linux:8.2:GA\" HOME_URL=\"https://www.redhat.com/\" BUG_REPORT_URL=\"https://bugzilla.redhat.com/\" REDHAT_BUGZILLA_PRODUCT=\"Red Hat Enterprise Linux 8\" REDHAT_BUGZILLA_PRODUCT_VERSION=8.2 REDHAT_SUPPORT_PRODUCT=\"Red Hat Enterprise Linux\" REDHAT_SUPPORT_PRODUCT_VERSION=\"8.2\" Red Hat Enterprise Linux release 8.2 (Ootpa) Red Hat Enterprise Linux release 8.2 (Ootpa)","title":"OS"},{"location":"NVMe%20Performance%20Testing/#tests","text":"","title":"Tests"},{"location":"NVMe%20Performance%20Testing/#test-1","text":"","title":"Test 1"},{"location":"NVMe%20Performance%20Testing/#config","text":"pvcreate /dev/nvme2n1 pvcreate /dev/nvme1n1 pvcreate /dev/nvme0n1 pvcreate /dev/nvme3n1 vgcreate data /dev/nvme3n1 /dev/nvme2n1 /dev/nvme1n1 /dev/nvme0n1 lvcreate -l 100%FREE -i4 -I128 -n data data mkfs.ext4 -F -b 4096 -E discard,stride=16,stripe-width=256 /dev/mapper/data-data mkdir /data mount -o rw,auto,discard /dev/mapper/data-data /data fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test --filename=test --bs=64k --iodepth=32 --size=50G --readwrite=randrw --rwmixread=60","title":"Config"},{"location":"NVMe%20Performance%20Testing/#results","text":"Test: (g=0): rw=randrw, bs=(R) 64.0KiB-64.0KiB, (W) 64.0KiB-64.0KiB, (T) 64.0KiB-64.0KiB, ioengine=libaio, iodepth=32 fio-3.7 Starting 1 process Test: Laying out IO file (1 file / 51200MiB) Jobs: 1 (f=1): [m(1)][100.0%][r=217MiB/s,w=148MiB/s][r=3476,w=2371 IOPS][eta 00m:00s] Test: (groupid=0, jobs=1): err= 0: pid=3290: Mon Sep 14 11:17:39 2020 read: IOPS=3639, BW=227MiB/s (239MB/s)(29.0GiB/134968msec) bw ( KiB/s): min=211328, max=261504, per=99.99%, avg=232911.32, stdev=8921.48, samples=269 iops : min= 3302, max= 4086, avg=3639.22, stdev=139.41, samples=269 write: IOPS=2429, BW=152MiB/s (159MB/s)(20.0GiB/134968msec) bw ( KiB/s): min=140800, max=169856, per=100.00%, avg=155506.13, stdev=5852.42, samples=269 iops : min= 2200, max= 2654, avg=2429.77, stdev=91.43, samples=269 cpu : usr=2.19%, sys=8.61%, ctx=98853, majf=0, minf=13 IO depths : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=100.0%, >=64=0.0% submit : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0% complete : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.1%, 64=0.0%, >=64=0.0% issued rwts: total=491242,327958,0,0 short=0,0,0,0 dropped=0,0,0,0 latency : target=0, window=0, percentile=100.00%, depth=32 Run status group 0 (all jobs): READ: bw=227MiB/s (239MB/s), 227MiB/s-227MiB/s (239MB/s-239MB/s), io=29.0GiB (32.2GB), run=134968-134968msec WRITE: bw=152MiB/s (159MB/s), 152MiB/s-152MiB/s (159MB/s-159MB/s), io=20.0GiB (21.5GB), run=134968-134968msec Disk stats (read/write): dm-0: ios=490775/327707, merge=0/0, ticks=3019139/1204501, in_queue=4223640, util=68.01%, aggrios=491243/328000, aggrmerge=1/11, aggrticks=3031345/1210547, aggrin_queue=3827355, aggrutil=67.98% sda: ios=491243/328000, merge=1/11, ticks=3031345/1210547, in_queue=3827355, util=67.98%","title":"Results"},{"location":"Notes%20on%20HPC/","text":"Notes on HPC KEY TAKEAWAY Economics will drive the pooling of main memory, and whether or not customers choose the CXL way or the Gen-Z way. Considering that memory can account for half of the cost of a server at a hyperscaler, anything that allows a machine to have a minimal amount of capacity on the node and then share the rest in the rack with all of it being transparent to the operating system and all of it looking local will be adopted. There is just no question about that. Memory area networks, in one fashion or another, are going to be common in datacenters before too long, and this will be driven by economics. Load Store Architecture A load\u2013store architecture is an instruction set architecture that divides instructions into two categories: memory access (load and store between memory and registers) and ALU operations (which only occur between registers). For instance, in a load\u2013store approach both operands and destination for an ADD operation must be in registers. This differs from a register-memory architecture (for example, a CISC instruction set architecture such as x86) in which one of the operands for the ADD operation may be in memory, while the other is in a register. https://www.sciencedirect.com/topics/computer-science/load-store-architecture https://en.wikipedia.org/wiki/Load%E2%80%93store_architecture Fabric Attached Memory See Fabric Attached Memory CXL See Compute Express Link Radix https://github.com/HewlettPackard/meadowlark https://ieeexplore.ieee.org/document/6307777 Things to investigate https://www.techpowerup.com/292256/amd-details-its-3d-v-cache-design-at-isscc","title":"Notes on HPC"},{"location":"Notes%20on%20HPC/#notes-on-hpc","text":"KEY TAKEAWAY Economics will drive the pooling of main memory, and whether or not customers choose the CXL way or the Gen-Z way. Considering that memory can account for half of the cost of a server at a hyperscaler, anything that allows a machine to have a minimal amount of capacity on the node and then share the rest in the rack with all of it being transparent to the operating system and all of it looking local will be adopted. There is just no question about that. Memory area networks, in one fashion or another, are going to be common in datacenters before too long, and this will be driven by economics.","title":"Notes on HPC"},{"location":"Notes%20on%20HPC/#load-store-architecture","text":"A load\u2013store architecture is an instruction set architecture that divides instructions into two categories: memory access (load and store between memory and registers) and ALU operations (which only occur between registers). For instance, in a load\u2013store approach both operands and destination for an ADD operation must be in registers. This differs from a register-memory architecture (for example, a CISC instruction set architecture such as x86) in which one of the operands for the ADD operation may be in memory, while the other is in a register. https://www.sciencedirect.com/topics/computer-science/load-store-architecture https://en.wikipedia.org/wiki/Load%E2%80%93store_architecture","title":"Load Store Architecture"},{"location":"Notes%20on%20HPC/#fabric-attached-memory","text":"See Fabric Attached Memory","title":"Fabric Attached Memory"},{"location":"Notes%20on%20HPC/#cxl","text":"See Compute Express Link","title":"CXL"},{"location":"Notes%20on%20HPC/#radix","text":"https://github.com/HewlettPackard/meadowlark https://ieeexplore.ieee.org/document/6307777","title":"Radix"},{"location":"Notes%20on%20HPC/#things-to-investigate","text":"https://www.techpowerup.com/292256/amd-details-its-3d-v-cache-design-at-isscc","title":"Things to investigate"},{"location":"Notes%20on%20HPC/chiplet_based_systems/","text":"Chiplet-Based Systems Return to HPC Notes README.md What is the problem Moore's Low is slowing Manufacturing costs are rising You could make larger chips to increase performance but: They are more expensive to make Verification costs are higher Manufacturing defects in densely packed logic reduce wafer yield You could create specialized chips but it is difficult to make a financial case for that What are chiplet-based systems Chiplet-based systems propose the integration of multiple discrete chips within the same package via an integration technology such as a multi-chip module or silicon interposer. Ex: From: https://www.sigarch.org/chiplet-based-systems/ Why chiplets Cost Previously, chiplets weren't considered practical because you introduce multiple parts plus having to contend with on-dye communication between them. However, now the smaller chips have sufficiently cheap manufacturing costs (compared to larger) that this is a viable alternative. Flexibility If you want to move from mobile, to desktop, to server this may be a matter of just increasing the number of chiplets. It is also possible that if we develop the proper standards that we could have a future system where we have general interconnects which would allow multiple different vendor chiplets to work together. Ex: Common Heterogeneous Integration and IP Reuse Strategies (CHIPS) Questions What is process technology? A process technology is the process of creating a single chip. In this case, you could use older process technologies to create the chiplets and then put them together.","title":"Chiplet-Based Systems"},{"location":"Notes%20on%20HPC/chiplet_based_systems/#chiplet-based-systems","text":"Return to HPC Notes README.md","title":"Chiplet-Based Systems"},{"location":"Notes%20on%20HPC/chiplet_based_systems/#what-is-the-problem","text":"Moore's Low is slowing Manufacturing costs are rising You could make larger chips to increase performance but: They are more expensive to make Verification costs are higher Manufacturing defects in densely packed logic reduce wafer yield You could create specialized chips but it is difficult to make a financial case for that","title":"What is the problem"},{"location":"Notes%20on%20HPC/chiplet_based_systems/#what-are-chiplet-based-systems","text":"Chiplet-based systems propose the integration of multiple discrete chips within the same package via an integration technology such as a multi-chip module or silicon interposer. Ex: From: https://www.sigarch.org/chiplet-based-systems/","title":"What are chiplet-based systems"},{"location":"Notes%20on%20HPC/chiplet_based_systems/#why-chiplets","text":"","title":"Why chiplets"},{"location":"Notes%20on%20HPC/chiplet_based_systems/#cost","text":"Previously, chiplets weren't considered practical because you introduce multiple parts plus having to contend with on-dye communication between them. However, now the smaller chips have sufficiently cheap manufacturing costs (compared to larger) that this is a viable alternative.","title":"Cost"},{"location":"Notes%20on%20HPC/chiplet_based_systems/#flexibility","text":"If you want to move from mobile, to desktop, to server this may be a matter of just increasing the number of chiplets. It is also possible that if we develop the proper standards that we could have a future system where we have general interconnects which would allow multiple different vendor chiplets to work together. Ex: Common Heterogeneous Integration and IP Reuse Strategies (CHIPS)","title":"Flexibility"},{"location":"Notes%20on%20HPC/chiplet_based_systems/#questions","text":"","title":"Questions"},{"location":"Notes%20on%20HPC/chiplet_based_systems/#what-is-process-technology","text":"A process technology is the process of creating a single chip. In this case, you could use older process technologies to create the chiplets and then put them together.","title":"What is process technology?"},{"location":"Notes%20on%20HPC/cxl/","text":"Compute Express Link (CXL) Return to HPC Notes README.md Compute Express Link (CXL) Useful Resources Examples of Vendor Interconnects More Specific Interconnects What is the Purpose of CXL? Protocols Devices Type 1 Device Type 2 Device Type 3 Device Memory Pooling Switching Useful Resources Interesting article on roadmap: https://www.nextplatform.com/2021/09/07/the-cxl-roadmap-opens-up-the-memory-hierarchy/ Deep Dive: https://www.nextplatform.com/2019/09/18/eating-the-interconnect-alphabet-soup-with-intels-cxl/ Examples of Vendor Interconnects Intel's Compute Express Link (CXL) IBM's Coherent Accelerator Interface (CAPI) Xilinx's Cache Coherence Interconnect for Accelerators (CCIX) AMD's Infinity Fabric More Specific Interconnects Nvidia's NVLink IBM's OpenCAPI HPE's Gen-Z: It can be used to hook anything from DRAM to flash to accelerators in meshes with any manner of CPU. What is the Purpose of CXL? The primary purpose is to disaggregate I/O and memory and to effectively virtualize the motherboard to make the following malleable across clusters of components: - Compute, memory, and I/O - Computational offload to devices such as GPU and FPGA accelerators - Memory buffers and other kinds of devices such as SmartNICs CXL is a set of sub-protocols that ride on the PCI-Express bus on a single link Protocols CXL.io: This sub-protocol is functionally equivalent to the PCIe 5.0 protocol and utilizes the broad industry adoption and familiarity of PCIe. It is effectively the PCIe transaction layer reformatted to allow two sub-protocols to co-exist side by side. CXL.io is used to discover devices in systems, manage interrupts, give access to registers, handle initialization, and deal with signaling errors. CXL.cache: This sub-protocol, which is designed for more specific applications, enables accelerators to efficiently access and cache host memory for optimized performance. It allows an accelerator to access the CPU's DRAM. CXL.memory: This sub-protocol enables a host, such as a processor, to access device-attached memory using load/store commands. It is not expected that all three protocols are used in all configurations. There are three basic usage templates which represent the three usages expected: From https://www.servethehome.com/compute-express-link-cxl-2-0-specification-released-the-big-one/cxl-1-0-and-1-1-usages/ Devices Type 1 Device Accelerators such as smart NICs typically lack local memory. However, they can leverage the CXL.io protocol and CXL.cache to communicate with the host processors DDR memory. Type 2 Device The idea here is there is memory (like HBM or DDR) on the accelerator (GPUs, ASICs, FPGAs, etc) and you want the accelerator's memory to be locally available to the CPU and the CPU's memory to be locally available to the accelerator. The CXL.io protocol is used to allow the CPU to discover the device and configure it and then you use the CXL.cache to allow the processor to touch the device\u2019s memory and CXL.memory to allow the accelerator to touch the CPU's memory. This memory should be co-located in the same cache coherent domain. Type 3 Device The CXL.io and CXL.memory protocols can be leveraged for memory expansion and pooling. For example, a buffer attached to the CXL bus could be used to enable DRAM capacity expansion, augmenting memory bandwidth, or adding persistent memory without the loss of DRAM slots. In real world terms, this means the high-speed, low-latency storage devices that would have previously displaced DRAM can instead complement it with CXL-enabled devices. These could include non-volatile technologies in various form factors such as add-in cards, U.2, and EDSFF. For type 3 devices you need the CXL.io sub-protocol to discover and configure the device and the CXL.memory sub-protocol to allow the CPU to reach into the memory attached to your memory buffer. WHERE I LEFT OFF : Left off studying symmetric cache coherency protocols vs asymmetric on this article: https://www.nextplatform.com/2019/09/18/eating-the-interconnect-alphabet-soup-with-intels-cxl/ Memory Pooling CXL 2.0 supports switching to enable memory pooling. With a CXL 2.0 switch, a host can access one or more devices from the pool. Although the hosts must be CXL 2.0-enabled to leverage this capability, the memory devices can be a mix of CXL 1.0, 1.1, and 2.0-enabled hardware. At 1.0/1.1, a device is limited to behaving as a single logical device accessible by only one host at a time. However, a 2.0 level device can be partitioned as multiple logical devices, allowing up to 16 hosts to simultaneously access different portions of the memory. As an example, a host 1 (H1) can use half the memory in device 1 (D1) and a quarter of the memory in device 2 (D2) to finely match the memory requirements of its workload to the available capacity in the memory pool. The remaining capacity in devices D1 and D2 can be used by one or more of the other hosts up to a maximum of 16. Devices D3 and D4, CXL 1.0 and 1.1-enabled respectively, can be used by only one host at a time. Switching By moving to a CXL 2.0 direct-connect architecture, data centers can achieve the performance benefits of main memory expansion\ufffdand the efficiency and total cost of ownership (TCO) benefits of pooled memory. Assuming all hosts and devices are CXL 2.0-enabled, \ufffdswitching is incorporated into the memory devices via a crossbar in the CXL memory pooling chip. This keeps latency low but requires a more powerful chip since it is now responsible for the control plane functionality performed by the switch. With low-latency direct connections, attached memory devices can employ DDR DRAM to provide expansion of host main memory. This can be done on a very flexible basis, as a host is able to access all\ufffdor portions of\ufffdthe capacity of as many devices as needed to tackle a specific workload.","title":"Compute Express Link (CXL)"},{"location":"Notes%20on%20HPC/cxl/#compute-express-link-cxl","text":"Return to HPC Notes README.md Compute Express Link (CXL) Useful Resources Examples of Vendor Interconnects More Specific Interconnects What is the Purpose of CXL? Protocols Devices Type 1 Device Type 2 Device Type 3 Device Memory Pooling Switching","title":"Compute Express Link (CXL)"},{"location":"Notes%20on%20HPC/cxl/#useful-resources","text":"Interesting article on roadmap: https://www.nextplatform.com/2021/09/07/the-cxl-roadmap-opens-up-the-memory-hierarchy/ Deep Dive: https://www.nextplatform.com/2019/09/18/eating-the-interconnect-alphabet-soup-with-intels-cxl/","title":"Useful Resources"},{"location":"Notes%20on%20HPC/cxl/#examples-of-vendor-interconnects","text":"Intel's Compute Express Link (CXL) IBM's Coherent Accelerator Interface (CAPI) Xilinx's Cache Coherence Interconnect for Accelerators (CCIX) AMD's Infinity Fabric","title":"Examples of Vendor Interconnects"},{"location":"Notes%20on%20HPC/cxl/#more-specific-interconnects","text":"Nvidia's NVLink IBM's OpenCAPI HPE's Gen-Z: It can be used to hook anything from DRAM to flash to accelerators in meshes with any manner of CPU.","title":"More Specific Interconnects"},{"location":"Notes%20on%20HPC/cxl/#what-is-the-purpose-of-cxl","text":"The primary purpose is to disaggregate I/O and memory and to effectively virtualize the motherboard to make the following malleable across clusters of components: - Compute, memory, and I/O - Computational offload to devices such as GPU and FPGA accelerators - Memory buffers and other kinds of devices such as SmartNICs CXL is a set of sub-protocols that ride on the PCI-Express bus on a single link","title":"What is the Purpose of CXL?"},{"location":"Notes%20on%20HPC/cxl/#protocols","text":"CXL.io: This sub-protocol is functionally equivalent to the PCIe 5.0 protocol and utilizes the broad industry adoption and familiarity of PCIe. It is effectively the PCIe transaction layer reformatted to allow two sub-protocols to co-exist side by side. CXL.io is used to discover devices in systems, manage interrupts, give access to registers, handle initialization, and deal with signaling errors. CXL.cache: This sub-protocol, which is designed for more specific applications, enables accelerators to efficiently access and cache host memory for optimized performance. It allows an accelerator to access the CPU's DRAM. CXL.memory: This sub-protocol enables a host, such as a processor, to access device-attached memory using load/store commands. It is not expected that all three protocols are used in all configurations. There are three basic usage templates which represent the three usages expected: From https://www.servethehome.com/compute-express-link-cxl-2-0-specification-released-the-big-one/cxl-1-0-and-1-1-usages/","title":"Protocols"},{"location":"Notes%20on%20HPC/cxl/#devices","text":"","title":"Devices"},{"location":"Notes%20on%20HPC/cxl/#type-1-device","text":"Accelerators such as smart NICs typically lack local memory. However, they can leverage the CXL.io protocol and CXL.cache to communicate with the host processors DDR memory.","title":"Type 1 Device"},{"location":"Notes%20on%20HPC/cxl/#type-2-device","text":"The idea here is there is memory (like HBM or DDR) on the accelerator (GPUs, ASICs, FPGAs, etc) and you want the accelerator's memory to be locally available to the CPU and the CPU's memory to be locally available to the accelerator. The CXL.io protocol is used to allow the CPU to discover the device and configure it and then you use the CXL.cache to allow the processor to touch the device\u2019s memory and CXL.memory to allow the accelerator to touch the CPU's memory. This memory should be co-located in the same cache coherent domain.","title":"Type 2 Device"},{"location":"Notes%20on%20HPC/cxl/#type-3-device","text":"The CXL.io and CXL.memory protocols can be leveraged for memory expansion and pooling. For example, a buffer attached to the CXL bus could be used to enable DRAM capacity expansion, augmenting memory bandwidth, or adding persistent memory without the loss of DRAM slots. In real world terms, this means the high-speed, low-latency storage devices that would have previously displaced DRAM can instead complement it with CXL-enabled devices. These could include non-volatile technologies in various form factors such as add-in cards, U.2, and EDSFF. For type 3 devices you need the CXL.io sub-protocol to discover and configure the device and the CXL.memory sub-protocol to allow the CPU to reach into the memory attached to your memory buffer. WHERE I LEFT OFF : Left off studying symmetric cache coherency protocols vs asymmetric on this article: https://www.nextplatform.com/2019/09/18/eating-the-interconnect-alphabet-soup-with-intels-cxl/","title":"Type 3 Device"},{"location":"Notes%20on%20HPC/cxl/#memory-pooling","text":"CXL 2.0 supports switching to enable memory pooling. With a CXL 2.0 switch, a host can access one or more devices from the pool. Although the hosts must be CXL 2.0-enabled to leverage this capability, the memory devices can be a mix of CXL 1.0, 1.1, and 2.0-enabled hardware. At 1.0/1.1, a device is limited to behaving as a single logical device accessible by only one host at a time. However, a 2.0 level device can be partitioned as multiple logical devices, allowing up to 16 hosts to simultaneously access different portions of the memory. As an example, a host 1 (H1) can use half the memory in device 1 (D1) and a quarter of the memory in device 2 (D2) to finely match the memory requirements of its workload to the available capacity in the memory pool. The remaining capacity in devices D1 and D2 can be used by one or more of the other hosts up to a maximum of 16. Devices D3 and D4, CXL 1.0 and 1.1-enabled respectively, can be used by only one host at a time.","title":"Memory Pooling"},{"location":"Notes%20on%20HPC/cxl/#switching","text":"By moving to a CXL 2.0 direct-connect architecture, data centers can achieve the performance benefits of main memory expansion\ufffdand the efficiency and total cost of ownership (TCO) benefits of pooled memory. Assuming all hosts and devices are CXL 2.0-enabled, \ufffdswitching is incorporated into the memory devices via a crossbar in the CXL memory pooling chip. This keeps latency low but requires a more powerful chip since it is now responsible for the control plane functionality performed by the switch. With low-latency direct connections, attached memory devices can employ DDR DRAM to provide expansion of host main memory. This can be done on a very flexible basis, as a host is able to access all\ufffdor portions of\ufffdthe capacity of as many devices as needed to tackle a specific workload.","title":"Switching"},{"location":"Notes%20on%20HPC/fabric_attached_memory/","text":"Fabric Attached Memory Return to README.md Fabric Attached Memory Resources What is it What Problem is it Solving The Future of FAM Questions: Resources Broad overview: https://itigic.com/fabric-attached-memory-is-not-ram-or-cache-in-cpu/ The Machine background information: https://github.com/FabricAttachedMemory/Emulation/wiki How the emulation for the Machine works: https://github.com/FabricAttachedMemory/Emulation/wiki/Emulation-via-Virtual-Machines What is it FAM is a type of scratchpad memory. Scratchpad memory is memory that resides inside the processor (usually). This brings with it two obvious benefits: Programs that run inside scratchpad memory run faster due to the low distance to the processor and with lower power consumption Due to its proximity to the processor, a cache system is not needed to access said memory The difference between regular scratchpad memory and FAM is that FAM uses some network interface to communicate. What Problem is it Solving The main problem with main memory is that the increased wire distance leads to a large increase in power. FAM seeks to solve this by moving it closer to the processor. It also allows processors to directly share information (or at least not have to write it to main memory and then recover it) by instead writing to FAM which is located between the last level cache and the interface to RAM for each of them. The Future of FAM To understand this part you will need to understand chiplets The best solution is a chiplet-based system where the Northbridge is disconnected from the rest of the system, as is the case in AMD\u2018s Ryzen 3000 and Ryzen 5000 CPUs. FAM, by its nature should have more capacity than the fastest cache but less than RAM. With the northbridge on a separate chip you can integrate FAM into it. However this is difficult to do on 2D chip so instead it would be preferable to use a 3D chip with the northbridge on one level and FAM on the others. Questions: Why is it difficult to integrate FAM onto a 2D chip?","title":"Fabric Attached Memory"},{"location":"Notes%20on%20HPC/fabric_attached_memory/#fabric-attached-memory","text":"Return to README.md Fabric Attached Memory Resources What is it What Problem is it Solving The Future of FAM Questions:","title":"Fabric Attached Memory"},{"location":"Notes%20on%20HPC/fabric_attached_memory/#resources","text":"Broad overview: https://itigic.com/fabric-attached-memory-is-not-ram-or-cache-in-cpu/ The Machine background information: https://github.com/FabricAttachedMemory/Emulation/wiki How the emulation for the Machine works: https://github.com/FabricAttachedMemory/Emulation/wiki/Emulation-via-Virtual-Machines","title":"Resources"},{"location":"Notes%20on%20HPC/fabric_attached_memory/#what-is-it","text":"FAM is a type of scratchpad memory. Scratchpad memory is memory that resides inside the processor (usually). This brings with it two obvious benefits: Programs that run inside scratchpad memory run faster due to the low distance to the processor and with lower power consumption Due to its proximity to the processor, a cache system is not needed to access said memory The difference between regular scratchpad memory and FAM is that FAM uses some network interface to communicate.","title":"What is it"},{"location":"Notes%20on%20HPC/fabric_attached_memory/#what-problem-is-it-solving","text":"The main problem with main memory is that the increased wire distance leads to a large increase in power. FAM seeks to solve this by moving it closer to the processor. It also allows processors to directly share information (or at least not have to write it to main memory and then recover it) by instead writing to FAM which is located between the last level cache and the interface to RAM for each of them.","title":"What Problem is it Solving"},{"location":"Notes%20on%20HPC/fabric_attached_memory/#the-future-of-fam","text":"To understand this part you will need to understand chiplets The best solution is a chiplet-based system where the Northbridge is disconnected from the rest of the system, as is the case in AMD\u2018s Ryzen 3000 and Ryzen 5000 CPUs. FAM, by its nature should have more capacity than the fastest cache but less than RAM. With the northbridge on a separate chip you can integrate FAM into it. However this is difficult to do on 2D chip so instead it would be preferable to use a 3D chip with the northbridge on one level and FAM on the others.","title":"The Future of FAM"},{"location":"Notes%20on%20HPC/fabric_attached_memory/#questions","text":"Why is it difficult to integrate FAM onto a 2D chip?","title":"Questions:"},{"location":"Notes%20on%20Improving%20Drive%20Performance/","text":"Located NUMA Nodes https://community.mellanox.com/s/article/understanding-numa-node-for-performance-benchmarks Definition of Drive Stats","title":"Index"},{"location":"Notes%20on%20Improving%20Drive%20Performance/#located-numa-nodes","text":"https://community.mellanox.com/s/article/understanding-numa-node-for-performance-benchmarks","title":"Located NUMA Nodes"},{"location":"Notes%20on%20Improving%20Drive%20Performance/#definition-of-drive-stats","text":"","title":"Definition of Drive Stats"},{"location":"Notes%20on%20PCIe/","text":"Notes on PCIe Notes on PCIe PCIe Basics and Background How multiple root complexes are handled Interpreting PCIe Device to CPU Locality Information What is the PCIe PHY Human readable overview of how PCIe works How does ID-based Ordering (IDO) Work? How does transaction ordering work? What is a PCIe Root Complex? How does PCIe Enumeration Work? NVMe over PCIe vs Other Protocols What is a PCIe Function? PCIe-Bus and NUMA Node Correlation How does the root complex work? What is PCIe P2P? What is Relaxed Ordering What is a traffic class (TC)? PCIe BAR Register How NVMe Drive Opcodes Work How does SR-IOV work? PCIe Bridge vs Switch PCIe Configuration Space PCIe Switches PCIe Basics and Background https://pcisig.com/sites/default/files/files/PCI_Express_Basics_Background.pdf#page=26 How multiple root complexes are handled https://codywu2010.wordpress.com/2015/11/29/how-modern-multi-processor-multi-root-complex-system-assigns-pci-bus-number/ Interpreting PCIe Device to CPU Locality Information https://dshcherb.github.io/2019/02/02/interpreting-pcie-device-to-cpu-locality-information.html What is the PCIe PHY https://www.linkedin.com/pulse/pci-express-depth-physical-layer-luigi-c-filho-/ Human readable overview of how PCIe works http://xillybus.com/tutorials/pci-express-tlp-pcie-primer-tutorial-guide-1/ http://xillybus.com/tutorials/pci-express-tlp-pcie-primer-tutorial-guide-2 How does ID-based Ordering (IDO) Work? https://blog.csdn.net/weixin_48180416/article/details/115790068 How does transaction ordering work? https://blog.csdn.net/weixin_40357487/article/details/120162461?spm=1001.2101.3001.6661.1&utm_medium=distribute.pc_relevant_t0.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-1.no_search_link&depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-1.no_search_link&utm_relevant_index=1 What is a PCIe Root Complex? https://www.quora.com/What-is-a-PCIe-root-complex?share=1 How does PCIe Enumeration Work? https://www.quora.com/What-is-PCIE-enumeration/answer/Satish-Kumar-525?ch=15&oid=31389493&share=44585235&target_type=answer NVMe over PCIe vs Other Protocols https://www.quora.com/Is-NVMe-faster-than-PCIe/answer/Mike-Jones-169?ch=15&oid=193548046&share=a587ff45&target_type=answer What is a PCIe Function? https://www.quora.com/What-is-a-PCIe-function/answer/Udit-Khanna-2?ch=15&oid=58319695&share=c7f066e5&target_type=answer PCIe-Bus and NUMA Node Correlation https://social.msdn.microsoft.com/Forums/en-US/fabb05b7-eb3f-4a7c-91c5-1ced90af3d0c/pciebus-and-numanode-correlation How does the root complex work? https://codywu2010.wordpress.com/2015/11/29/how-modern-multi-processor-multi-root-complex-system-assigns-pci-bus-number/ What is PCIe P2P? https://xilinx.github.io/XRT/master/html/p2p.html What is Relaxed Ordering https://qr.ae/pG6SWe What is a traffic class (TC)? https://www.oreilly.com/library/view/pci-express-system/0321156307/0321156307_ch06lev1sec6.html PCIe BAR Register https://github.com/cirosantilli/linux-kernel-module-cheat/blob/366b1c1af269f56d6a7e6464f2862ba2bc368062/kernel_module/pci.c How NVMe Drive Opcodes Work https://stackoverflow.com/questions/30190050/what-is-the-base-address-register-bar-in-pcie https://stackoverflow.com/questions/19006632/how-is-a-pci-pcie-bar-size-determined BIOS/OS discovers whether PCIe device exists Places the addresses for mmio or I/O port addresses in NVMe drive\u200b's BAR registers (which it figures out from the configuration registers) It seems from the documentation I found NVMe does this through 64bit mmio Driver establishes the admin queue via BAR0. The admin queue's base addresses are in ASQ and ACQ respectively I submit commands to the admin submission queue to establish I/O queues. Send/receive data via I/O queues. How does SR-IOV work? https://docs.microsoft.com/en-us/windows-hardware/drivers/network/overview-of-single-root-i-o-virtualization--sr-iov- Architecture: https://docs.microsoft.com/en-us/windows-hardware/drivers/network/sr-iov-architecture PCIe Bridge vs Switch wke...@gmail.com wrote: I would appreciate of someone can explain the difference between a PCI bridge and a PCI switch. With good ol' PCI, a single bus can have many devices. A PCI bridge is a device that connects multiple buses together, which is something that was very seldom needed. PCI Express looks, for software, very similar to PCI, but is electrically a point-to-point connection, i.e., a PCIe bus has exactly two devices. To connect PCIe with PCI, you need a PCI/PCIe or PCIe/PCI bridge. If you have a single PCIe connector and multiple PCIe devices, you need a PCIe switch. A single PCIe connection still is between exactly two devices, so a PCIe switch consists of a (virtual) PCI bridge for the upstream PCIe connection, and one (virtual) PCI bridge for each downstream PCIe connection. Regards, Clemens PCIe Configuration Space https://docs.oracle.com/cd/E19683-01/806-5222/hwovr-22/#:~:text=The%20PCI%20host%20bridge%20provides,of%20other%20PCI%20bus%20masters. https://bitwiseanne.wordpress.com/2020/05/15/pcie-101-the-root-complex-and-the-endpoint/ PCIe Switches https://linuxhint.com/pcie-switch/#:~:text=PCIe%20switches%20are%20devices%20that,the%20CPU%20alone%20can%20handle.","title":"Notes on PCIe"},{"location":"Notes%20on%20PCIe/#notes-on-pcie","text":"Notes on PCIe PCIe Basics and Background How multiple root complexes are handled Interpreting PCIe Device to CPU Locality Information What is the PCIe PHY Human readable overview of how PCIe works How does ID-based Ordering (IDO) Work? How does transaction ordering work? What is a PCIe Root Complex? How does PCIe Enumeration Work? NVMe over PCIe vs Other Protocols What is a PCIe Function? PCIe-Bus and NUMA Node Correlation How does the root complex work? What is PCIe P2P? What is Relaxed Ordering What is a traffic class (TC)? PCIe BAR Register How NVMe Drive Opcodes Work How does SR-IOV work? PCIe Bridge vs Switch PCIe Configuration Space PCIe Switches","title":"Notes on PCIe"},{"location":"Notes%20on%20PCIe/#pcie-basics-and-background","text":"https://pcisig.com/sites/default/files/files/PCI_Express_Basics_Background.pdf#page=26","title":"PCIe Basics and Background"},{"location":"Notes%20on%20PCIe/#how-multiple-root-complexes-are-handled","text":"https://codywu2010.wordpress.com/2015/11/29/how-modern-multi-processor-multi-root-complex-system-assigns-pci-bus-number/","title":"How multiple root complexes are handled"},{"location":"Notes%20on%20PCIe/#interpreting-pcie-device-to-cpu-locality-information","text":"https://dshcherb.github.io/2019/02/02/interpreting-pcie-device-to-cpu-locality-information.html","title":"Interpreting PCIe Device to CPU Locality Information"},{"location":"Notes%20on%20PCIe/#what-is-the-pcie-phy","text":"https://www.linkedin.com/pulse/pci-express-depth-physical-layer-luigi-c-filho-/","title":"What is the PCIe PHY"},{"location":"Notes%20on%20PCIe/#human-readable-overview-of-how-pcie-works","text":"http://xillybus.com/tutorials/pci-express-tlp-pcie-primer-tutorial-guide-1/ http://xillybus.com/tutorials/pci-express-tlp-pcie-primer-tutorial-guide-2","title":"Human readable overview of how PCIe works"},{"location":"Notes%20on%20PCIe/#how-does-id-based-ordering-ido-work","text":"https://blog.csdn.net/weixin_48180416/article/details/115790068","title":"How does ID-based Ordering (IDO) Work?"},{"location":"Notes%20on%20PCIe/#how-does-transaction-ordering-work","text":"https://blog.csdn.net/weixin_40357487/article/details/120162461?spm=1001.2101.3001.6661.1&utm_medium=distribute.pc_relevant_t0.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-1.no_search_link&depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-1.no_search_link&utm_relevant_index=1","title":"How does transaction ordering work?"},{"location":"Notes%20on%20PCIe/#what-is-a-pcie-root-complex","text":"https://www.quora.com/What-is-a-PCIe-root-complex?share=1","title":"What is a PCIe Root Complex?"},{"location":"Notes%20on%20PCIe/#how-does-pcie-enumeration-work","text":"https://www.quora.com/What-is-PCIE-enumeration/answer/Satish-Kumar-525?ch=15&oid=31389493&share=44585235&target_type=answer","title":"How does PCIe Enumeration Work?"},{"location":"Notes%20on%20PCIe/#nvme-over-pcie-vs-other-protocols","text":"https://www.quora.com/Is-NVMe-faster-than-PCIe/answer/Mike-Jones-169?ch=15&oid=193548046&share=a587ff45&target_type=answer","title":"NVMe over PCIe vs Other Protocols"},{"location":"Notes%20on%20PCIe/#what-is-a-pcie-function","text":"https://www.quora.com/What-is-a-PCIe-function/answer/Udit-Khanna-2?ch=15&oid=58319695&share=c7f066e5&target_type=answer","title":"What is a PCIe Function?"},{"location":"Notes%20on%20PCIe/#pcie-bus-and-numa-node-correlation","text":"https://social.msdn.microsoft.com/Forums/en-US/fabb05b7-eb3f-4a7c-91c5-1ced90af3d0c/pciebus-and-numanode-correlation","title":"PCIe-Bus and NUMA Node Correlation"},{"location":"Notes%20on%20PCIe/#how-does-the-root-complex-work","text":"https://codywu2010.wordpress.com/2015/11/29/how-modern-multi-processor-multi-root-complex-system-assigns-pci-bus-number/","title":"How does the root complex work?"},{"location":"Notes%20on%20PCIe/#what-is-pcie-p2p","text":"https://xilinx.github.io/XRT/master/html/p2p.html","title":"What is PCIe P2P?"},{"location":"Notes%20on%20PCIe/#what-is-relaxed-ordering","text":"https://qr.ae/pG6SWe","title":"What is Relaxed Ordering"},{"location":"Notes%20on%20PCIe/#what-is-a-traffic-class-tc","text":"https://www.oreilly.com/library/view/pci-express-system/0321156307/0321156307_ch06lev1sec6.html","title":"What is a traffic class (TC)?"},{"location":"Notes%20on%20PCIe/#pcie-bar-register","text":"https://github.com/cirosantilli/linux-kernel-module-cheat/blob/366b1c1af269f56d6a7e6464f2862ba2bc368062/kernel_module/pci.c","title":"PCIe BAR Register"},{"location":"Notes%20on%20PCIe/#how-nvme-drive-opcodes-work","text":"https://stackoverflow.com/questions/30190050/what-is-the-base-address-register-bar-in-pcie https://stackoverflow.com/questions/19006632/how-is-a-pci-pcie-bar-size-determined BIOS/OS discovers whether PCIe device exists Places the addresses for mmio or I/O port addresses in NVMe drive\u200b's BAR registers (which it figures out from the configuration registers) It seems from the documentation I found NVMe does this through 64bit mmio Driver establishes the admin queue via BAR0. The admin queue's base addresses are in ASQ and ACQ respectively I submit commands to the admin submission queue to establish I/O queues. Send/receive data via I/O queues.","title":"How NVMe Drive Opcodes Work"},{"location":"Notes%20on%20PCIe/#how-does-sr-iov-work","text":"https://docs.microsoft.com/en-us/windows-hardware/drivers/network/overview-of-single-root-i-o-virtualization--sr-iov- Architecture: https://docs.microsoft.com/en-us/windows-hardware/drivers/network/sr-iov-architecture","title":"How does SR-IOV work?"},{"location":"Notes%20on%20PCIe/#pcie-bridge-vs-switch","text":"wke...@gmail.com wrote: I would appreciate of someone can explain the difference between a PCI bridge and a PCI switch. With good ol' PCI, a single bus can have many devices. A PCI bridge is a device that connects multiple buses together, which is something that was very seldom needed. PCI Express looks, for software, very similar to PCI, but is electrically a point-to-point connection, i.e., a PCIe bus has exactly two devices. To connect PCIe with PCI, you need a PCI/PCIe or PCIe/PCI bridge. If you have a single PCIe connector and multiple PCIe devices, you need a PCIe switch. A single PCIe connection still is between exactly two devices, so a PCIe switch consists of a (virtual) PCI bridge for the upstream PCIe connection, and one (virtual) PCI bridge for each downstream PCIe connection. Regards, Clemens","title":"PCIe Bridge vs Switch"},{"location":"Notes%20on%20PCIe/#pcie-configuration-space","text":"https://docs.oracle.com/cd/E19683-01/806-5222/hwovr-22/#:~:text=The%20PCI%20host%20bridge%20provides,of%20other%20PCI%20bus%20masters. https://bitwiseanne.wordpress.com/2020/05/15/pcie-101-the-root-complex-and-the-endpoint/","title":"PCIe Configuration Space"},{"location":"Notes%20on%20PCIe/#pcie-switches","text":"https://linuxhint.com/pcie-switch/#:~:text=PCIe%20switches%20are%20devices%20that,the%20CPU%20alone%20can%20handle.","title":"PCIe Switches"},{"location":"Notes%20on%20mdraid%20Performance%20Testing/","text":"Notes on mdraid Performance Testing Notes on mdraid Performance Testing Helpful Resources Helpful Commands Check I/O Scheduler Check Available CPU Governors RAW IO vs Direct IO My Notes What are NUMAs per socket? What is rq_affinity Configuration My Hardware My Configuration FIO Command libaio I/O Depth Direct Ramp Time Time Based readwrite (rw) Name numjobs Blocksize NUMA Memory Policy NUMA CPU Nodes Raw I/O Testing Research P-states and C-States Power performance states (ACPI P states) Processor idle sleep states (ACPI C states) I/O Models Blocking I/O Nonblocking I/O I/O Multiplexing Model Signal Driven I/O Model Asynchronous I/O Model What is aqu-sz From Understanding the Linux Kernel How does VFS Work? The superblock object The inode object The file object The dentry object Block Devices Handling Block Device Sizes Sectors Blocks Segments Generic Block layer My Questions Helpful Resources https://www.amd.com/system/files/TechDocs/56163-PUB.pdf https://www.computerworld.com/article/2785965/raw-disk-i-o.html https://www.cloudbees.com/blog/linux-io-scheduler-tuning https://wiki.ubuntu.com/Kernel/Reference/IOSchedulers http://developer.amd.com/wp-content/resources/56420.pdf https://infohub.delltechnologies.com/l/cpu-best-practices-3/poweredge-numa-nodes-per-socket-1#:~:text=AMD%20servers%20provide%20the%20ability,bank%20into%20two%20equal%20parts. Helpful Commands Check I/O Scheduler # cat /sys/block/sda/queue/scheduler noop [deadline] cfq Check Available CPU Governors cpupower frequency-info --governors analyzing CPU 0: available cpufreq governors: performance powersave RAW IO vs Direct IO Raw I/O is issued directly to disk offsets, bypassing the file system altogether. It has been used by some applications, especially databases, that can manage and cache their own data better than the file system cache. A drawback is more complexity in the software. From Oracle\u2019s official website, input/output (I/O) to a raw partition offers approximately a 5% to 10% performance improvement over I/O to a partition with a file system on it. Direct I/O allows applications to use a file system but bypass the file system cache, for example, by using the O_DIRECT open(2) flag on Linux. This is similar to synchronous writes (but without the guarantees that O_SYNC offers), and it works for reads as well. It isn\u2019t as direct as raw device I/O, since mapping of file offsets to disk offsets must still be performed by file system code, and I/O may also be resized to match the size used by the file system for on-disk layout (its record size) or it may error (EINVAL). My Notes What are NUMAs per socket? What are NUMAs per socket What is rq_affinity https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/performance_tuning_guide/sect-red_hat_enterprise_linux-performance_tuning_guide-storage_and_file_systems-configuration_tools By default, I/O completions can be processed on a different processor than the processor that issued the I/O request. Set rq_affinity to 1 to disable this ability and perform completions only on the processor that issued the I/O request. This can improve the effectiveness of processor data caching. Configuration Initial driver: /lib/modules/4.18.0-305.el8.x86_64/kernel/drivers/md/raid456.ko.xz I think this has been changed? mdraid looks like it has two stripes 12 NVMe drives in one and 12 in the other all in RAID5. They are all numa aligned by some program called map_numa.sh. Must check drive My Hardware Dell R840 12 Intel P4610 NVMe drives are only attached to processors three and four in the split backplane configuartion. My Configuration I checked firmware rev with nvme list to make sure that all drives were the same. If not need to update TODO still need to do Set the CPU governor to performance. You can check the governors with cpupower frequency-info --governors and then set it with cpupower frequency-set --governor performance . You can check the current governor with cat /sys/devices/system/cpu/cpu0/cpufreq/scaling_governor (substitute the CPU number accordingly) FIO Command libaio Linux native asynchronous I/O. Note that Linux may only support queued behavior with non-buffered I/O (set direct=1 or buffered=0). This engine defines engine specific options. I/O Depth Number of I/O units to keep in flight against the file. Note that increasing iodepth beyond 1 will not affect synchronous ioengines (except for small degrees when verify_async is in use). Even async engines may impose OS restrictions causing the desired depth not to be achieved. This may happen on Linux when using libaio and not setting direct=1, since buffered I/O is not async on that OS. Keep an eye on the I/O depth distribution in the fio output to verify that the achieved depth is as expected. Default: 1. Direct If value is true, use non-buffered I/O. This is usually O_DIRECT. Note that OpenBSD and ZFS on Solaris don\u2019t support direct I/O. On Windows the synchronous ioengines don\u2019t support direct I/O. Default: false. See https://stackoverflow.com/a/49462406/4427375 Ramp Time If set, fio will run the specified workload for this amount of time before logging any performance numbers. Useful for letting performance settle before logging results, thus minimizing the runtime required for stable results. Note that the ramp_time is considered lead in time for a job, thus it will increase the total runtime if a special timeout or runtime is specified. When the unit is omitted, the value is given in seconds. Time Based If set, fio will run for the duration of the runtime specified even if the file(s) are completely read or written. It will simply loop over the same workload as many times as the runtime allows. readwrite (rw) Type of I/O pattern. Fio defaults to read if the option is not specified. For the mixed I/O types, the default is to split them 50/50. For certain types of I/O the result may still be skewed a bit, since the speed may be different. It is possible to specify the number of I/Os to do before getting a new offset by appending : to the end of the string given. For a random read, it would look like rw=randread:8 for passing in an offset modifier with a value of 8. If the suffix is used with a sequential I/O pattern, then the value specified will be added to the generated offset for each I/O turning sequential I/O into sequential I/O with holes. For instance, using rw=write:4k will skip 4k for every write. Also see the rw_sequencer option. Name ASCII name of the job. This may be used to override the name printed by fio for this job. Otherwise the job name is used. On the command line this parameter has the special purpose of also signaling the start of a new job. numjobs Create the specified number of clones of this job. Each clone of job is spawned as an independent thread or process. May be used to setup a larger number of threads/processes doing the same thing. Each thread is reported separately; to see statistics for all clones as a whole, use group_reporting in conjunction with new_group. See --max-jobs. Default: 1. Blocksize The block size in bytes used for I/O units. Default: 4096. A single value applies to reads, writes, and trims. Comma-separated values may be specified for reads, writes, and trims. A value not terminated in a comma applies to subsequent types. NUMA Memory Policy Set this job\u2019s memory policy and corresponding NUMA nodes. Format of the arguments: <mode>[:<nodelist>] mode is one of the following memory policies: default, prefer, bind, interleave or local. For default and local memory policies, no node needs to be specified. For prefer, only one node is allowed. For bind and interleave the nodelist may be as follows: a comma delimited list of numbers, A-B ranges, or all. NUMA CPU Nodes Set this job running on specified NUMA nodes\u2019 CPUs. The arguments allow comma delimited list of cpu numbers, A-B ranges, or all. Note, to enable NUMA options support, fio must be built on a system with libnuma-dev(el) installed. Raw I/O Testing Research P-states and C-States These are defined in the ACPI specification. Power performance states (ACPI P states) P-states provide a way to scale the frequency and voltage at which the processor runs so as to reduce the power consumption of the CPU. The number of available P-states can be different for each model of CPU, even those from the same family. Processor idle sleep states (ACPI C states) C-states are states when the CPU has reduced or turned off selected functions. Different processors support different numbers of C-states in which various parts of the CPU are turned off. To better understand the C-states that are supported and exposed, contact the CPU vendor. Generally, higher C-states turn off more parts of the CPU, which significantly reduce power consumption. Processors may have deeper C-states that are not exposed to the operating system. I/O Models Blocking I/O In networking, there is a call to recvfrom which will then lead to a system call into the kernel which will block until data is available. Nonblocking I/O Assuming UDP a call is made to recvfrom and if ther is no data available the kernel sends back EWOULDBLOCK saying no data is available and this is repeated until a datagram is available. This is polling. I/O Multiplexing Model With I/O multiplexing you call select which will block until data is available and then when data is available it ill return that there is return readable. After this you can call recvfrom. The difference is select can read from multiple potential sockets. Signal Driven I/O Model In this model we register a signal handler using the sigaction system call. This will listen for the SIGIO signal. When data is ready our SIGIO handler will be called at which point we have two options. We can call recvfrom from the handler and then pass that data to the main thread OR we can alert the main thread that data is waiting and let it handle it. Asynchronous I/O Model This is the same as signal driven I/O except the thread notifies us when the data has been copied from kernel space to user space. We call aio_read and pass the kernel the fdescriptor, buffer pointer, buffer size (the same three arguments for read), file offset (similar to lseek), and how to notify us when the entire operation is complete. When the copy is complete our signal handler is notified. What is aqu-sz The average queue length of the requests that were issued to the device. From Understanding the Linux Kernel How does VFS Work? The idea behind the Virtual Filesystem is to put a wide range of information in the kernel to represent many different types of filesystems ; there is a field or function to support each operation provided by all real filesystems supported by Linux. For each read, write, or other function called, the kernel substitutes the actual function that supports a native Linux filesystem, the NTFS filesystem, or whatever other filesystem the file is on. Bovet, Daniel P.. Understanding the Linux Kernel (Kindle Locations 14260-14263). O'Reilly Media. Kindle Edition. $ cp /floppy/ TEST /tmp/ test where /floppy is the mount point of an MS-DOS diskette and /tmp is a normal Second Extended Filesystem (Ext2) directory. The VFS is an abstraction layer between the application program and the filesystem implementations (see Figure 12-1( a)). Therefore, the cp program is not required to know the filesystem types of /floppy/ TEST and /tmp/ test. Instead, cp interacts with the VFS by means of generic system calls known to anyone who has done Unix programming (see the section \"File-Handling System Calls\" in Chapter 1); the code executed by cp is shown in Figure 12-1( b). More essentially, the Linux kernel cannot hardcode a particular function to handle an operation such as read( ) or ioctl( ) . Instead, it must use a pointer for each operation; the pointer is made to point to the proper function for the particular filesystem being accessed. Let\u2019s illustrate this concept by showing how the read( ) shown in Figure 12-1 would be translated by the kernel into a call specific to the MS-DOS filesystem. The application\u2019s call to read( ) makes the kernel invoke the corresponding sys_read( ) service routine, like every other system call. The file is represented by a file data structure in kernel memory, as we\u2019ll see later in this chapter. This data structure contains a field called f_op that contains pointers to functions specific to MS-DOS files, including a function that reads a file. sys_read( ) finds the pointer to this function and invokes it. Thus, the application\u2019s read( ) is turned into the rather indirect call: file-> f_op-> read(...); One can think of the common file model as object-oriented, where an object is a software construct that defines both a data structure and the methods that operate on it. For reasons of efficiency, Linux is not coded in an object-oriented language such as C ++. Objects are therefore implemented as plain C data structures with some fields pointing to functions that correspond to the object\u2019s methods. The common file model consists of the following object types: The superblock object Stores information concerning a mounted filesystem. For disk-based filesystems, this object usually corresponds to a filesystem control block stored on disk. The inode object Stores general information about a specific file. For disk-based filesystems, this object usually corresponds to a file control block stored on disk. Each inode object is associated with an inode number, which uniquely identifies the file within the filesystem. The file object Stores information about the interaction between an open file and a process. This information exists only in kernel memory during the period when a process has the file open. The dentry object Stores information about the linking of a directory entry (that is, a particular name of the file) with the corresponding file. Each disk-based filesystem stores this information in its own particular way on disk. The picture below illustrates with a simple example how processes interact with files. Three different processes have opened the same file, two of them using the same hard link. In this case, each of the three processes uses its own file object, while only two dentry objects are required \u2014 one for each hard link. Both dentry objects refer to the same inode object, which identifies the superblock object and, together with the latter, the common disk file. Block Devices Handling We will follow the path of a call to read() through the kernel. The service routine of the read( ) system call activates a suitable VFS function, passing to it a file descriptor and an offset inside the file. The Virtual Filesystem is the upper layer of the block device handling architecture, and it provides a common file model adopted by all filesystems supported by Linux. The VFS function determines if the requested data is already available and, if necessary, how to perform the read operation. Sometimes there is no need to access the data on disk, because the kernel keeps in RAM the data most recently read from \u2014 or written to \u2014 a block device. TODO - investigate pacge cache in chapter 15 and how VFS talks to the cache in chapter 16. Let\u2019s assume that the kernel must read the data from the block device, thus it must determine the physical location of that data. To do this, the kernel relies on the mapping layer , which typically executes two steps: It determines the block size of the filesystem including the file and computes the extent of the requested data in terms of file block numbers . Essentially, the file is seen as split in many blocks, and the kernel determines the numbers (indices relative to the beginning of file) of the blocks containing the requested data. Next, the mapping layer invokes a filesystem-specific function that accesses the file\u2019s disk inode and determines the position of the requested data on disk in terms of logical block numbers. Essentially, the disk is seen as split in blocks, and the kernel determines the numbers (indices relative to the beginning of the disk or partition) corresponding to the blocks storing the requested data. Because a file may be stored in nonadjacent blocks on disk, a data structure stored in the disk inode maps each file block number to a logical block number.[*] However, if the read access was done on a raw block device file, the mapping layer does not invoke a filesystem-specific method; rather, it translates the offset in the block device file to a position inside the disk \u2014 or disk partition \u2014 corresponding to the device file. The kernel can now issue the read operation on the block device. It makes use of the generic block layer , which starts the I/ O operations that transfer the requested data. In general, each I/ O operation involves a group of blocks that are adjacent on disk. Because the requested data is not necessarily adjacent on disk, the generic block layer might start several I/ O operations. Each I/ O operation is represented by a \u201cblock I/ O\u201d (in short, \u201cbio\u201d) structure, which collects all information needed by the lower components to satisfy the request. The generic block layer hides the peculiarities of each hardware block device, thus offering an abstract view of the block devices. Because almost all block devices are disks, the generic block layer also provides some general data structures that describe \u201cdisks\u201d and \u201cdisk partitions.\u201d Below the generic block layer, the \u201cI/ O scheduler \" sorts the pending I/ O data transfer requests according to predefined kernel policies. The purpose of the scheduler is to group requests of data that lie near each other on the physical medium. Finally, the block device drivers take care of the actual data transfer by sending suitable commands to the hardware interfaces of the disk controllers. Block Device Sizes There are many kernel components that are concerned with data stored in block devices; each of them manages the disk data using chunks of different length: The controllers of the hardware block devices transfer data in chunks of fixed length called \u201csectors.\u201d Therefore, the I/ O scheduler and the block device drivers must manage sectors of data. The Virtual Filesystem, the mapping layer, and the filesystems group the disk data in logical units called \u201cblocks.\u201d A block corresponds to the minimal disk storage unit inside a filesystem. Block device drivers should be able to cope with \u201csegments\u201d of data: each segment is a memory page \u2014 or a portion of a memory page \u2014 including chunks of data that are physically adjacent on disk. The disk caches work on \u201cpages\u201d of disk data, each of which fits in a page frame. The generic block layer glues together all the upper and lower components, thus it knows about sectors , blocks, segments, and pages of data. Even if there are many different chunks of data, they usually share the same physical RAM cells. For instance, Figure 14-2 shows the layout of a 4,096-byte page. The upper kernel components see the page as composed of four block buffers of 1,024 bytes each. The last three blocks of the page are being transferred by the block device driver, thus they are inserted in a segment covering the last 3,072 bytes of the page. The hard disk controller considers the segment as composed of six 512-byte sectors. Sectors To achieve acceptable performance, hard disks and similar devices transfer several adjacent bytes at once. Each data transfer operation for a block device acts on a group of adjacent bytes called a sector. In the following discussion, we say that groups of bytes are adjacent when they are recorded on the disk surface in such a manner that a single seek operation can access them. Although the physical geometry of a disk is usually very complicated, the hard disk controller accepts commands that refer to the disk as a large array of sectors. In most disk devices, the size of a sector is 512 bytes, although there are devices that use larger sectors (1,024 and 2,048 bytes). Notice that the sector should be considered as the basic unit of data transfer; it is never possible to transfer less than one sector, although most disk devices are capable of transferring several adjacent sectors at once. In Linux, the size of a sector is conventionally set to 512 bytes; if a block device uses larger sectors, the corresponding low-level block device driver will do the necessary conversions. Thus, a group of data stored in a block device is identified on disk by its position \u2014 the index of the first 512-byte sector \u2014 and its length as number of 512-byte sectors. Sector indices are stored in 32- or 64-bit variables of type sector_t. Blocks While the sector is the basic unit of data transfer for the hardware devices, the block is the basic unit of data transfer for the VFS and, consequently, for the filesystems. For example, when the kernel accesses the contents of a file, it must first read from disk a block containing the disk inode of the file (see the section \"Inode Objects\" in Chapter 12). This block on disk corresponds to one or more adjacent sectors, which are looked at by the VFS as a single data unit. In Linux, the block size must be a power of 2 and cannot be larger than a page frame. Moreover, it must be a multiple of the sector size, because each block must include an integral number of sectors. Therefore, on 80 \u00d7 86 architecture, the permitted block sizes are 512, 1,024, 2,048, and 4,096 bytes. The block size is not specific to a block device. When creating a disk-based filesystem, the administrator may select the proper block size. Thus, several partitions on the same disk might make use of different block sizes. Furthermore, each read or write operation issued on a block device file is a \u201craw\u201d access that bypasses the disk-based filesystem; the kernel executes it by using blocks of largest size (4,096 bytes). Each block requires its own block buffer, which is a RAM memory area used by the kernel to store the block\u2019s content. When the kernel reads a block from disk, it fills the corresponding block buffer with the values obtained from the hardware device; similarly, when the kernel writes a block on disk, it updates the corresponding group of adjacent bytes on the hardware device with the actual values of the associated block buffer. The size of a block buffer always matches the size of the corresponding block. Each buffer has a \u201cbuffer head\u201d descriptor of type buffer_head. This descriptor contains all the information needed by the kernel to know how to handle the buffer; thus, before operating on each buffer, the kernel checks its buffer head. We will give a detailed explanation of all fields of the buffer head in Chapter 15; in the present chapter, however, we will only consider a few fields: b_page, b_data, b_blocknr, and b_bdev. The b_page field stores the page descriptor address of the page frame that includes the block buffer. If the page frame is in high memory, the b_data field stores the offset of the block buffer inside the page; otherwise, it stores the starting linear address of the block buffer itself. The b_blocknr field stores the logical block number (i.e., the index of the block inside the disk partition). Finally, the b_bdev field identifies the block device that is using the buffer head Segments We know that each disk I/ O operation consists of transferring the contents of some adjacent sectors from \u2014 or to \u2014 some RAM locations. In almost all cases, the data transfer is directly performed by the disk controller with a DMA operation (see the section \"Direct Memory Access (DMA)\" in Chapter 13). The block device driver simply triggers the data transfer by sending suitable commands to the disk controller; once the data transfer is finished, the controller raises an interrupt to notify the block device driver. The data transferred by a single DMA operation must belong to sectors that are adjacent on disk. This is a physical constraint: a disk controller that allows DMA transfers to non-adjacent sectors would have a poor transfer rate, because moving a read/ write head on the disk surface is quite a slow operation. Older disk controllers support \u201csimple\u201d DMA operations only: in each such operation, data is transferred from or to memory cells that are physically contiguous in RAM. Recent disk controllers, however, may also support the so-called scatter-gather DMA transfers : in each such operation, the data can be transferred from or to several noncontiguous memory areas. For each scatter-gather DMA transfer, the block device driver must send to the disk controller: The initial disk sector number and the total number of sectors to be transferred A list of descriptors of memory areas, each of which consists of an address and a length. The disk controller takes care of the whole data transfer; for instance, in a read operation the controller fetches the data from the adjacent disk sectors and scatters it into the various memory areas. To make use of scatter-gather DMA operations, block device drivers must handle the data in units called segments . A segment is simply a memory page \u2014 or a portion of a memory page \u2014 that includes the data of some adjacent disk sectors. Thus, a scatter-gather DMA operation may involve several segments at once. Notice that a block device driver does not need to know about blocks, block sizes, and block buffers. Thus, even if a segment is seen by the higher levels as a page composed of several block buffers, the block device driver does not care about it. As we\u2019ll see, the generic block layer can merge different segments if the corresponding page frames happen to be contiguous in RAM and the corresponding chunks of disk data are adjacent on disk. The larger memory area resulting from this merge operation is called physical segment. Yet another merge operation is allowed on architectures that handle the mapping between bus addresses and physical addresses through a dedicated bus circuitry (the IO-MMU; see the section \"Direct Memory Access (DMA)\" in Chapter 13). The memory area resulting from this kind of merge operation is called hardware segment . Because we will focus on the 80 \u00d7 86 architecture, which has no such dynamic mapping between bus addresses and physical addresses, we will assume in the rest of this chapter that hardware segments always coincide with physical segments . TODO - need to go read about how DMA works Generic Block layer My Questions - When running FIO, to what extent is disk caching engaged?","title":"Notes on mdraid Performance Testing"},{"location":"Notes%20on%20mdraid%20Performance%20Testing/#notes-on-mdraid-performance-testing","text":"Notes on mdraid Performance Testing Helpful Resources Helpful Commands Check I/O Scheduler Check Available CPU Governors RAW IO vs Direct IO My Notes What are NUMAs per socket? What is rq_affinity Configuration My Hardware My Configuration FIO Command libaio I/O Depth Direct Ramp Time Time Based readwrite (rw) Name numjobs Blocksize NUMA Memory Policy NUMA CPU Nodes Raw I/O Testing Research P-states and C-States Power performance states (ACPI P states) Processor idle sleep states (ACPI C states) I/O Models Blocking I/O Nonblocking I/O I/O Multiplexing Model Signal Driven I/O Model Asynchronous I/O Model What is aqu-sz From Understanding the Linux Kernel How does VFS Work? The superblock object The inode object The file object The dentry object Block Devices Handling Block Device Sizes Sectors Blocks Segments Generic Block layer My Questions","title":"Notes on mdraid Performance Testing"},{"location":"Notes%20on%20mdraid%20Performance%20Testing/#helpful-resources","text":"https://www.amd.com/system/files/TechDocs/56163-PUB.pdf https://www.computerworld.com/article/2785965/raw-disk-i-o.html https://www.cloudbees.com/blog/linux-io-scheduler-tuning https://wiki.ubuntu.com/Kernel/Reference/IOSchedulers http://developer.amd.com/wp-content/resources/56420.pdf https://infohub.delltechnologies.com/l/cpu-best-practices-3/poweredge-numa-nodes-per-socket-1#:~:text=AMD%20servers%20provide%20the%20ability,bank%20into%20two%20equal%20parts.","title":"Helpful Resources"},{"location":"Notes%20on%20mdraid%20Performance%20Testing/#helpful-commands","text":"","title":"Helpful Commands"},{"location":"Notes%20on%20mdraid%20Performance%20Testing/#check-io-scheduler","text":"# cat /sys/block/sda/queue/scheduler noop [deadline] cfq","title":"Check I/O Scheduler"},{"location":"Notes%20on%20mdraid%20Performance%20Testing/#check-available-cpu-governors","text":"cpupower frequency-info --governors analyzing CPU 0: available cpufreq governors: performance powersave","title":"Check Available CPU Governors"},{"location":"Notes%20on%20mdraid%20Performance%20Testing/#raw-io-vs-direct-io","text":"Raw I/O is issued directly to disk offsets, bypassing the file system altogether. It has been used by some applications, especially databases, that can manage and cache their own data better than the file system cache. A drawback is more complexity in the software. From Oracle\u2019s official website, input/output (I/O) to a raw partition offers approximately a 5% to 10% performance improvement over I/O to a partition with a file system on it. Direct I/O allows applications to use a file system but bypass the file system cache, for example, by using the O_DIRECT open(2) flag on Linux. This is similar to synchronous writes (but without the guarantees that O_SYNC offers), and it works for reads as well. It isn\u2019t as direct as raw device I/O, since mapping of file offsets to disk offsets must still be performed by file system code, and I/O may also be resized to match the size used by the file system for on-disk layout (its record size) or it may error (EINVAL).","title":"RAW IO vs Direct IO"},{"location":"Notes%20on%20mdraid%20Performance%20Testing/#my-notes","text":"","title":"My Notes"},{"location":"Notes%20on%20mdraid%20Performance%20Testing/#what-are-numas-per-socket","text":"What are NUMAs per socket","title":"What are NUMAs per socket?"},{"location":"Notes%20on%20mdraid%20Performance%20Testing/#what-is-rq_affinity","text":"https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/performance_tuning_guide/sect-red_hat_enterprise_linux-performance_tuning_guide-storage_and_file_systems-configuration_tools By default, I/O completions can be processed on a different processor than the processor that issued the I/O request. Set rq_affinity to 1 to disable this ability and perform completions only on the processor that issued the I/O request. This can improve the effectiveness of processor data caching.","title":"What is rq_affinity"},{"location":"Notes%20on%20mdraid%20Performance%20Testing/#configuration","text":"Initial driver: /lib/modules/4.18.0-305.el8.x86_64/kernel/drivers/md/raid456.ko.xz I think this has been changed? mdraid looks like it has two stripes 12 NVMe drives in one and 12 in the other all in RAID5. They are all numa aligned by some program called map_numa.sh. Must check drive","title":"Configuration"},{"location":"Notes%20on%20mdraid%20Performance%20Testing/#my-hardware","text":"Dell R840 12 Intel P4610 NVMe drives are only attached to processors three and four in the split backplane configuartion.","title":"My Hardware"},{"location":"Notes%20on%20mdraid%20Performance%20Testing/#my-configuration","text":"I checked firmware rev with nvme list to make sure that all drives were the same. If not need to update TODO still need to do Set the CPU governor to performance. You can check the governors with cpupower frequency-info --governors and then set it with cpupower frequency-set --governor performance . You can check the current governor with cat /sys/devices/system/cpu/cpu0/cpufreq/scaling_governor (substitute the CPU number accordingly)","title":"My Configuration"},{"location":"Notes%20on%20mdraid%20Performance%20Testing/#fio-command","text":"","title":"FIO Command"},{"location":"Notes%20on%20mdraid%20Performance%20Testing/#libaio","text":"Linux native asynchronous I/O. Note that Linux may only support queued behavior with non-buffered I/O (set direct=1 or buffered=0). This engine defines engine specific options.","title":"libaio"},{"location":"Notes%20on%20mdraid%20Performance%20Testing/#io-depth","text":"Number of I/O units to keep in flight against the file. Note that increasing iodepth beyond 1 will not affect synchronous ioengines (except for small degrees when verify_async is in use). Even async engines may impose OS restrictions causing the desired depth not to be achieved. This may happen on Linux when using libaio and not setting direct=1, since buffered I/O is not async on that OS. Keep an eye on the I/O depth distribution in the fio output to verify that the achieved depth is as expected. Default: 1.","title":"I/O Depth"},{"location":"Notes%20on%20mdraid%20Performance%20Testing/#direct","text":"If value is true, use non-buffered I/O. This is usually O_DIRECT. Note that OpenBSD and ZFS on Solaris don\u2019t support direct I/O. On Windows the synchronous ioengines don\u2019t support direct I/O. Default: false. See https://stackoverflow.com/a/49462406/4427375","title":"Direct"},{"location":"Notes%20on%20mdraid%20Performance%20Testing/#ramp-time","text":"If set, fio will run the specified workload for this amount of time before logging any performance numbers. Useful for letting performance settle before logging results, thus minimizing the runtime required for stable results. Note that the ramp_time is considered lead in time for a job, thus it will increase the total runtime if a special timeout or runtime is specified. When the unit is omitted, the value is given in seconds.","title":"Ramp Time"},{"location":"Notes%20on%20mdraid%20Performance%20Testing/#time-based","text":"If set, fio will run for the duration of the runtime specified even if the file(s) are completely read or written. It will simply loop over the same workload as many times as the runtime allows.","title":"Time Based"},{"location":"Notes%20on%20mdraid%20Performance%20Testing/#readwrite-rw","text":"Type of I/O pattern. Fio defaults to read if the option is not specified. For the mixed I/O types, the default is to split them 50/50. For certain types of I/O the result may still be skewed a bit, since the speed may be different. It is possible to specify the number of I/Os to do before getting a new offset by appending : to the end of the string given. For a random read, it would look like rw=randread:8 for passing in an offset modifier with a value of 8. If the suffix is used with a sequential I/O pattern, then the value specified will be added to the generated offset for each I/O turning sequential I/O into sequential I/O with holes. For instance, using rw=write:4k will skip 4k for every write. Also see the rw_sequencer option.","title":"readwrite (rw)"},{"location":"Notes%20on%20mdraid%20Performance%20Testing/#name","text":"ASCII name of the job. This may be used to override the name printed by fio for this job. Otherwise the job name is used. On the command line this parameter has the special purpose of also signaling the start of a new job.","title":"Name"},{"location":"Notes%20on%20mdraid%20Performance%20Testing/#numjobs","text":"Create the specified number of clones of this job. Each clone of job is spawned as an independent thread or process. May be used to setup a larger number of threads/processes doing the same thing. Each thread is reported separately; to see statistics for all clones as a whole, use group_reporting in conjunction with new_group. See --max-jobs. Default: 1.","title":"numjobs"},{"location":"Notes%20on%20mdraid%20Performance%20Testing/#blocksize","text":"The block size in bytes used for I/O units. Default: 4096. A single value applies to reads, writes, and trims. Comma-separated values may be specified for reads, writes, and trims. A value not terminated in a comma applies to subsequent types.","title":"Blocksize"},{"location":"Notes%20on%20mdraid%20Performance%20Testing/#numa-memory-policy","text":"Set this job\u2019s memory policy and corresponding NUMA nodes. Format of the arguments: <mode>[:<nodelist>] mode is one of the following memory policies: default, prefer, bind, interleave or local. For default and local memory policies, no node needs to be specified. For prefer, only one node is allowed. For bind and interleave the nodelist may be as follows: a comma delimited list of numbers, A-B ranges, or all.","title":"NUMA Memory Policy"},{"location":"Notes%20on%20mdraid%20Performance%20Testing/#numa-cpu-nodes","text":"Set this job running on specified NUMA nodes\u2019 CPUs. The arguments allow comma delimited list of cpu numbers, A-B ranges, or all. Note, to enable NUMA options support, fio must be built on a system with libnuma-dev(el) installed.","title":"NUMA CPU Nodes"},{"location":"Notes%20on%20mdraid%20Performance%20Testing/#raw-io-testing","text":"","title":"Raw I/O Testing"},{"location":"Notes%20on%20mdraid%20Performance%20Testing/#research","text":"","title":"Research"},{"location":"Notes%20on%20mdraid%20Performance%20Testing/#p-states-and-c-states","text":"These are defined in the ACPI specification.","title":"P-states and C-States"},{"location":"Notes%20on%20mdraid%20Performance%20Testing/#power-performance-states-acpi-p-states","text":"P-states provide a way to scale the frequency and voltage at which the processor runs so as to reduce the power consumption of the CPU. The number of available P-states can be different for each model of CPU, even those from the same family.","title":"Power performance states (ACPI P states)"},{"location":"Notes%20on%20mdraid%20Performance%20Testing/#processor-idle-sleep-states-acpi-c-states","text":"C-states are states when the CPU has reduced or turned off selected functions. Different processors support different numbers of C-states in which various parts of the CPU are turned off. To better understand the C-states that are supported and exposed, contact the CPU vendor. Generally, higher C-states turn off more parts of the CPU, which significantly reduce power consumption. Processors may have deeper C-states that are not exposed to the operating system.","title":"Processor idle sleep states (ACPI C states)"},{"location":"Notes%20on%20mdraid%20Performance%20Testing/#io-models","text":"","title":"I/O Models"},{"location":"Notes%20on%20mdraid%20Performance%20Testing/#blocking-io","text":"In networking, there is a call to recvfrom which will then lead to a system call into the kernel which will block until data is available.","title":"Blocking I/O"},{"location":"Notes%20on%20mdraid%20Performance%20Testing/#nonblocking-io","text":"Assuming UDP a call is made to recvfrom and if ther is no data available the kernel sends back EWOULDBLOCK saying no data is available and this is repeated until a datagram is available. This is polling.","title":"Nonblocking I/O"},{"location":"Notes%20on%20mdraid%20Performance%20Testing/#io-multiplexing-model","text":"With I/O multiplexing you call select which will block until data is available and then when data is available it ill return that there is return readable. After this you can call recvfrom. The difference is select can read from multiple potential sockets.","title":"I/O Multiplexing Model"},{"location":"Notes%20on%20mdraid%20Performance%20Testing/#signal-driven-io-model","text":"In this model we register a signal handler using the sigaction system call. This will listen for the SIGIO signal. When data is ready our SIGIO handler will be called at which point we have two options. We can call recvfrom from the handler and then pass that data to the main thread OR we can alert the main thread that data is waiting and let it handle it.","title":"Signal Driven I/O Model"},{"location":"Notes%20on%20mdraid%20Performance%20Testing/#asynchronous-io-model","text":"This is the same as signal driven I/O except the thread notifies us when the data has been copied from kernel space to user space. We call aio_read and pass the kernel the fdescriptor, buffer pointer, buffer size (the same three arguments for read), file offset (similar to lseek), and how to notify us when the entire operation is complete. When the copy is complete our signal handler is notified.","title":"Asynchronous I/O Model"},{"location":"Notes%20on%20mdraid%20Performance%20Testing/#what-is-aqu-sz","text":"The average queue length of the requests that were issued to the device.","title":"What is aqu-sz"},{"location":"Notes%20on%20mdraid%20Performance%20Testing/#from-understanding-the-linux-kernel","text":"","title":"From Understanding the Linux Kernel"},{"location":"Notes%20on%20mdraid%20Performance%20Testing/#how-does-vfs-work","text":"The idea behind the Virtual Filesystem is to put a wide range of information in the kernel to represent many different types of filesystems ; there is a field or function to support each operation provided by all real filesystems supported by Linux. For each read, write, or other function called, the kernel substitutes the actual function that supports a native Linux filesystem, the NTFS filesystem, or whatever other filesystem the file is on. Bovet, Daniel P.. Understanding the Linux Kernel (Kindle Locations 14260-14263). O'Reilly Media. Kindle Edition. $ cp /floppy/ TEST /tmp/ test where /floppy is the mount point of an MS-DOS diskette and /tmp is a normal Second Extended Filesystem (Ext2) directory. The VFS is an abstraction layer between the application program and the filesystem implementations (see Figure 12-1( a)). Therefore, the cp program is not required to know the filesystem types of /floppy/ TEST and /tmp/ test. Instead, cp interacts with the VFS by means of generic system calls known to anyone who has done Unix programming (see the section \"File-Handling System Calls\" in Chapter 1); the code executed by cp is shown in Figure 12-1( b). More essentially, the Linux kernel cannot hardcode a particular function to handle an operation such as read( ) or ioctl( ) . Instead, it must use a pointer for each operation; the pointer is made to point to the proper function for the particular filesystem being accessed. Let\u2019s illustrate this concept by showing how the read( ) shown in Figure 12-1 would be translated by the kernel into a call specific to the MS-DOS filesystem. The application\u2019s call to read( ) makes the kernel invoke the corresponding sys_read( ) service routine, like every other system call. The file is represented by a file data structure in kernel memory, as we\u2019ll see later in this chapter. This data structure contains a field called f_op that contains pointers to functions specific to MS-DOS files, including a function that reads a file. sys_read( ) finds the pointer to this function and invokes it. Thus, the application\u2019s read( ) is turned into the rather indirect call: file-> f_op-> read(...); One can think of the common file model as object-oriented, where an object is a software construct that defines both a data structure and the methods that operate on it. For reasons of efficiency, Linux is not coded in an object-oriented language such as C ++. Objects are therefore implemented as plain C data structures with some fields pointing to functions that correspond to the object\u2019s methods. The common file model consists of the following object types:","title":"How does VFS Work?"},{"location":"Notes%20on%20mdraid%20Performance%20Testing/#the-superblock-object","text":"Stores information concerning a mounted filesystem. For disk-based filesystems, this object usually corresponds to a filesystem control block stored on disk.","title":"The superblock object"},{"location":"Notes%20on%20mdraid%20Performance%20Testing/#the-inode-object","text":"Stores general information about a specific file. For disk-based filesystems, this object usually corresponds to a file control block stored on disk. Each inode object is associated with an inode number, which uniquely identifies the file within the filesystem.","title":"The inode object"},{"location":"Notes%20on%20mdraid%20Performance%20Testing/#the-file-object","text":"Stores information about the interaction between an open file and a process. This information exists only in kernel memory during the period when a process has the file open.","title":"The file object"},{"location":"Notes%20on%20mdraid%20Performance%20Testing/#the-dentry-object","text":"Stores information about the linking of a directory entry (that is, a particular name of the file) with the corresponding file. Each disk-based filesystem stores this information in its own particular way on disk. The picture below illustrates with a simple example how processes interact with files. Three different processes have opened the same file, two of them using the same hard link. In this case, each of the three processes uses its own file object, while only two dentry objects are required \u2014 one for each hard link. Both dentry objects refer to the same inode object, which identifies the superblock object and, together with the latter, the common disk file.","title":"The dentry object"},{"location":"Notes%20on%20mdraid%20Performance%20Testing/#block-devices-handling","text":"We will follow the path of a call to read() through the kernel. The service routine of the read( ) system call activates a suitable VFS function, passing to it a file descriptor and an offset inside the file. The Virtual Filesystem is the upper layer of the block device handling architecture, and it provides a common file model adopted by all filesystems supported by Linux. The VFS function determines if the requested data is already available and, if necessary, how to perform the read operation. Sometimes there is no need to access the data on disk, because the kernel keeps in RAM the data most recently read from \u2014 or written to \u2014 a block device. TODO - investigate pacge cache in chapter 15 and how VFS talks to the cache in chapter 16. Let\u2019s assume that the kernel must read the data from the block device, thus it must determine the physical location of that data. To do this, the kernel relies on the mapping layer , which typically executes two steps: It determines the block size of the filesystem including the file and computes the extent of the requested data in terms of file block numbers . Essentially, the file is seen as split in many blocks, and the kernel determines the numbers (indices relative to the beginning of file) of the blocks containing the requested data. Next, the mapping layer invokes a filesystem-specific function that accesses the file\u2019s disk inode and determines the position of the requested data on disk in terms of logical block numbers. Essentially, the disk is seen as split in blocks, and the kernel determines the numbers (indices relative to the beginning of the disk or partition) corresponding to the blocks storing the requested data. Because a file may be stored in nonadjacent blocks on disk, a data structure stored in the disk inode maps each file block number to a logical block number.[*] However, if the read access was done on a raw block device file, the mapping layer does not invoke a filesystem-specific method; rather, it translates the offset in the block device file to a position inside the disk \u2014 or disk partition \u2014 corresponding to the device file. The kernel can now issue the read operation on the block device. It makes use of the generic block layer , which starts the I/ O operations that transfer the requested data. In general, each I/ O operation involves a group of blocks that are adjacent on disk. Because the requested data is not necessarily adjacent on disk, the generic block layer might start several I/ O operations. Each I/ O operation is represented by a \u201cblock I/ O\u201d (in short, \u201cbio\u201d) structure, which collects all information needed by the lower components to satisfy the request. The generic block layer hides the peculiarities of each hardware block device, thus offering an abstract view of the block devices. Because almost all block devices are disks, the generic block layer also provides some general data structures that describe \u201cdisks\u201d and \u201cdisk partitions.\u201d Below the generic block layer, the \u201cI/ O scheduler \" sorts the pending I/ O data transfer requests according to predefined kernel policies. The purpose of the scheduler is to group requests of data that lie near each other on the physical medium. Finally, the block device drivers take care of the actual data transfer by sending suitable commands to the hardware interfaces of the disk controllers.","title":"Block Devices Handling"},{"location":"Notes%20on%20mdraid%20Performance%20Testing/#block-device-sizes","text":"There are many kernel components that are concerned with data stored in block devices; each of them manages the disk data using chunks of different length: The controllers of the hardware block devices transfer data in chunks of fixed length called \u201csectors.\u201d Therefore, the I/ O scheduler and the block device drivers must manage sectors of data. The Virtual Filesystem, the mapping layer, and the filesystems group the disk data in logical units called \u201cblocks.\u201d A block corresponds to the minimal disk storage unit inside a filesystem. Block device drivers should be able to cope with \u201csegments\u201d of data: each segment is a memory page \u2014 or a portion of a memory page \u2014 including chunks of data that are physically adjacent on disk. The disk caches work on \u201cpages\u201d of disk data, each of which fits in a page frame. The generic block layer glues together all the upper and lower components, thus it knows about sectors , blocks, segments, and pages of data. Even if there are many different chunks of data, they usually share the same physical RAM cells. For instance, Figure 14-2 shows the layout of a 4,096-byte page. The upper kernel components see the page as composed of four block buffers of 1,024 bytes each. The last three blocks of the page are being transferred by the block device driver, thus they are inserted in a segment covering the last 3,072 bytes of the page. The hard disk controller considers the segment as composed of six 512-byte sectors.","title":"Block Device Sizes"},{"location":"Notes%20on%20mdraid%20Performance%20Testing/#sectors","text":"To achieve acceptable performance, hard disks and similar devices transfer several adjacent bytes at once. Each data transfer operation for a block device acts on a group of adjacent bytes called a sector. In the following discussion, we say that groups of bytes are adjacent when they are recorded on the disk surface in such a manner that a single seek operation can access them. Although the physical geometry of a disk is usually very complicated, the hard disk controller accepts commands that refer to the disk as a large array of sectors. In most disk devices, the size of a sector is 512 bytes, although there are devices that use larger sectors (1,024 and 2,048 bytes). Notice that the sector should be considered as the basic unit of data transfer; it is never possible to transfer less than one sector, although most disk devices are capable of transferring several adjacent sectors at once. In Linux, the size of a sector is conventionally set to 512 bytes; if a block device uses larger sectors, the corresponding low-level block device driver will do the necessary conversions. Thus, a group of data stored in a block device is identified on disk by its position \u2014 the index of the first 512-byte sector \u2014 and its length as number of 512-byte sectors. Sector indices are stored in 32- or 64-bit variables of type sector_t.","title":"Sectors"},{"location":"Notes%20on%20mdraid%20Performance%20Testing/#blocks","text":"While the sector is the basic unit of data transfer for the hardware devices, the block is the basic unit of data transfer for the VFS and, consequently, for the filesystems. For example, when the kernel accesses the contents of a file, it must first read from disk a block containing the disk inode of the file (see the section \"Inode Objects\" in Chapter 12). This block on disk corresponds to one or more adjacent sectors, which are looked at by the VFS as a single data unit. In Linux, the block size must be a power of 2 and cannot be larger than a page frame. Moreover, it must be a multiple of the sector size, because each block must include an integral number of sectors. Therefore, on 80 \u00d7 86 architecture, the permitted block sizes are 512, 1,024, 2,048, and 4,096 bytes. The block size is not specific to a block device. When creating a disk-based filesystem, the administrator may select the proper block size. Thus, several partitions on the same disk might make use of different block sizes. Furthermore, each read or write operation issued on a block device file is a \u201craw\u201d access that bypasses the disk-based filesystem; the kernel executes it by using blocks of largest size (4,096 bytes). Each block requires its own block buffer, which is a RAM memory area used by the kernel to store the block\u2019s content. When the kernel reads a block from disk, it fills the corresponding block buffer with the values obtained from the hardware device; similarly, when the kernel writes a block on disk, it updates the corresponding group of adjacent bytes on the hardware device with the actual values of the associated block buffer. The size of a block buffer always matches the size of the corresponding block. Each buffer has a \u201cbuffer head\u201d descriptor of type buffer_head. This descriptor contains all the information needed by the kernel to know how to handle the buffer; thus, before operating on each buffer, the kernel checks its buffer head. We will give a detailed explanation of all fields of the buffer head in Chapter 15; in the present chapter, however, we will only consider a few fields: b_page, b_data, b_blocknr, and b_bdev. The b_page field stores the page descriptor address of the page frame that includes the block buffer. If the page frame is in high memory, the b_data field stores the offset of the block buffer inside the page; otherwise, it stores the starting linear address of the block buffer itself. The b_blocknr field stores the logical block number (i.e., the index of the block inside the disk partition). Finally, the b_bdev field identifies the block device that is using the buffer head","title":"Blocks"},{"location":"Notes%20on%20mdraid%20Performance%20Testing/#segments","text":"We know that each disk I/ O operation consists of transferring the contents of some adjacent sectors from \u2014 or to \u2014 some RAM locations. In almost all cases, the data transfer is directly performed by the disk controller with a DMA operation (see the section \"Direct Memory Access (DMA)\" in Chapter 13). The block device driver simply triggers the data transfer by sending suitable commands to the disk controller; once the data transfer is finished, the controller raises an interrupt to notify the block device driver. The data transferred by a single DMA operation must belong to sectors that are adjacent on disk. This is a physical constraint: a disk controller that allows DMA transfers to non-adjacent sectors would have a poor transfer rate, because moving a read/ write head on the disk surface is quite a slow operation. Older disk controllers support \u201csimple\u201d DMA operations only: in each such operation, data is transferred from or to memory cells that are physically contiguous in RAM. Recent disk controllers, however, may also support the so-called scatter-gather DMA transfers : in each such operation, the data can be transferred from or to several noncontiguous memory areas. For each scatter-gather DMA transfer, the block device driver must send to the disk controller: The initial disk sector number and the total number of sectors to be transferred A list of descriptors of memory areas, each of which consists of an address and a length. The disk controller takes care of the whole data transfer; for instance, in a read operation the controller fetches the data from the adjacent disk sectors and scatters it into the various memory areas. To make use of scatter-gather DMA operations, block device drivers must handle the data in units called segments . A segment is simply a memory page \u2014 or a portion of a memory page \u2014 that includes the data of some adjacent disk sectors. Thus, a scatter-gather DMA operation may involve several segments at once. Notice that a block device driver does not need to know about blocks, block sizes, and block buffers. Thus, even if a segment is seen by the higher levels as a page composed of several block buffers, the block device driver does not care about it. As we\u2019ll see, the generic block layer can merge different segments if the corresponding page frames happen to be contiguous in RAM and the corresponding chunks of disk data are adjacent on disk. The larger memory area resulting from this merge operation is called physical segment. Yet another merge operation is allowed on architectures that handle the mapping between bus addresses and physical addresses through a dedicated bus circuitry (the IO-MMU; see the section \"Direct Memory Access (DMA)\" in Chapter 13). The memory area resulting from this kind of merge operation is called hardware segment . Because we will focus on the 80 \u00d7 86 architecture, which has no such dynamic mapping between bus addresses and physical addresses, we will assume in the rest of this chapter that hardware segments always coincide with physical segments . TODO - need to go read about how DMA works","title":"Segments"},{"location":"Notes%20on%20mdraid%20Performance%20Testing/#generic-block-layer","text":"","title":"Generic Block layer"},{"location":"Notes%20on%20mdraid%20Performance%20Testing/#my-questions","text":"","title":"My Questions"},{"location":"Notes%20on%20mdraid%20Performance%20Testing/#-when-running-fio-to-what-extent-is-disk-caching-engaged","text":"","title":"- When running FIO, to what extent is disk caching engaged?"},{"location":"Notes%20on%20mdraid%20Performance%20Testing/lstopo/","text":"[root@r8402 ~]# lstopo Machine (187GB total) Package L#0 NUMANode L#0 (P#0 45GB) L3 L#0 (14MB) L2 L#0 (1024KB) + L1d L#0 (32KB) + L1i L#0 (32KB) + Core L#0 PU L#0 (P#0) PU L#1 (P#40) L2 L#1 (1024KB) + L1d L#1 (32KB) + L1i L#1 (32KB) + Core L#1 PU L#2 (P#4) PU L#3 (P#44) L2 L#2 (1024KB) + L1d L#2 (32KB) + L1i L#2 (32KB) + Core L#2 PU L#4 (P#8) PU L#5 (P#48) L2 L#3 (1024KB) + L1d L#3 (32KB) + L1i L#3 (32KB) + Core L#3 PU L#6 (P#12) PU L#7 (P#52) L2 L#4 (1024KB) + L1d L#4 (32KB) + L1i L#4 (32KB) + Core L#4 PU L#8 (P#16) PU L#9 (P#56) L2 L#5 (1024KB) + L1d L#5 (32KB) + L1i L#5 (32KB) + Core L#5 PU L#10 (P#20) PU L#11 (P#60) L2 L#6 (1024KB) + L1d L#6 (32KB) + L1i L#6 (32KB) + Core L#6 PU L#12 (P#24) PU L#13 (P#64) L2 L#7 (1024KB) + L1d L#7 (32KB) + L1i L#7 (32KB) + Core L#7 PU L#14 (P#28) PU L#15 (P#68) L2 L#8 (1024KB) + L1d L#8 (32KB) + L1i L#8 (32KB) + Core L#8 PU L#16 (P#32) PU L#17 (P#72) L2 L#9 (1024KB) + L1d L#9 (32KB) + L1i L#9 (32KB) + Core L#9 PU L#18 (P#36) PU L#19 (P#76) HostBridge PCI 00:11.5 (RAID) PCI 00:17.0 (RAID) PCIBridge PCI 01:00.0 (Ethernet) Net \"eno99\" PCI 01:00.1 (Ethernet) Net \"eno100\" PCIBridge PCIBridge PCI 03:00.0 (VGA) HostBridge PCIBridge PCI 17:00.0 (Ethernet) Net \"eno145\" PCI 17:00.1 (Ethernet) Net \"eno146\" HostBridge PCIBridge PCI 25:00.0 (SATA) Block(Disk) \"sda\" Package L#1 NUMANode L#1 (P#1 47GB) L3 L#1 (14MB) L2 L#10 (1024KB) + L1d L#10 (32KB) + L1i L#10 (32KB) + Core L#10 PU L#20 (P#1) PU L#21 (P#41) L2 L#11 (1024KB) + L1d L#11 (32KB) + L1i L#11 (32KB) + Core L#11 PU L#22 (P#5) PU L#23 (P#45) L2 L#12 (1024KB) + L1d L#12 (32KB) + L1i L#12 (32KB) + Core L#12 PU L#24 (P#9) PU L#25 (P#49) L2 L#13 (1024KB) + L1d L#13 (32KB) + L1i L#13 (32KB) + Core L#13 PU L#26 (P#13) PU L#27 (P#53) L2 L#14 (1024KB) + L1d L#14 (32KB) + L1i L#14 (32KB) + Core L#14 PU L#28 (P#17) PU L#29 (P#57) L2 L#15 (1024KB) + L1d L#15 (32KB) + L1i L#15 (32KB) + Core L#15 PU L#30 (P#21) PU L#31 (P#61) L2 L#16 (1024KB) + L1d L#16 (32KB) + L1i L#16 (32KB) + Core L#16 PU L#32 (P#25) PU L#33 (P#65) L2 L#17 (1024KB) + L1d L#17 (32KB) + L1i L#17 (32KB) + Core L#17 PU L#34 (P#29) PU L#35 (P#69) L2 L#18 (1024KB) + L1d L#18 (32KB) + L1i L#18 (32KB) + Core L#18 PU L#36 (P#33) PU L#37 (P#73) L2 L#19 (1024KB) + L1d L#19 (32KB) + L1i L#19 (32KB) + Core L#19 PU L#38 (P#37) PU L#39 (P#77) HostBridge PCIBridge PCI 48:00.0 (RAID) Package L#2 NUMANode L#2 (P#2 47GB) L3 L#2 (14MB) L2 L#20 (1024KB) + L1d L#20 (32KB) + L1i L#20 (32KB) + Core L#20 PU L#40 (P#2) PU L#41 (P#42) L2 L#21 (1024KB) + L1d L#21 (32KB) + L1i L#21 (32KB) + Core L#21 PU L#42 (P#6) PU L#43 (P#46) L2 L#22 (1024KB) + L1d L#22 (32KB) + L1i L#22 (32KB) + Core L#22 PU L#44 (P#10) PU L#45 (P#50) L2 L#23 (1024KB) + L1d L#23 (32KB) + L1i L#23 (32KB) + Core L#23 PU L#46 (P#14) PU L#47 (P#54) L2 L#24 (1024KB) + L1d L#24 (32KB) + L1i L#24 (32KB) + Core L#24 PU L#48 (P#18) PU L#49 (P#58) L2 L#25 (1024KB) + L1d L#25 (32KB) + L1i L#25 (32KB) + Core L#25 PU L#50 (P#22) PU L#51 (P#62) L2 L#26 (1024KB) + L1d L#26 (32KB) + L1i L#26 (32KB) + Core L#26 PU L#52 (P#26) PU L#53 (P#66) L2 L#27 (1024KB) + L1d L#27 (32KB) + L1i L#27 (32KB) + Core L#27 PU L#54 (P#30) PU L#55 (P#70) L2 L#28 (1024KB) + L1d L#28 (32KB) + L1i L#28 (32KB) + Core L#28 PU L#56 (P#34) PU L#57 (P#74) L2 L#29 (1024KB) + L1d L#29 (32KB) + L1i L#29 (32KB) + Core L#29 PU L#58 (P#38) PU L#59 (P#78) HostBridge PCIBridge PCI 88:00.0 (NVMExp) Block(Disk) \"nvme0n1\" PCIBridge PCI 89:00.0 (NVMExp) Block(Disk) \"nvme1n1\" HostBridge PCIBridge PCI 9b:00.0 (NVMExp) Block(Disk) \"nvme2n1\" PCIBridge PCI 9c:00.0 (NVMExp) Block(Disk) \"nvme3n1\" PCIBridge PCI 9d:00.0 (NVMExp) Block(Disk) \"nvme4n1\" PCIBridge PCI 9e:00.0 (NVMExp) Block(Disk) \"nvme5n1\" Package L#3 NUMANode L#3 (P#3 47GB) L3 L#3 (14MB) L2 L#30 (1024KB) + L1d L#30 (32KB) + L1i L#30 (32KB) + Core L#30 PU L#60 (P#3) PU L#61 (P#43) L2 L#31 (1024KB) + L1d L#31 (32KB) + L1i L#31 (32KB) + Core L#31 PU L#62 (P#7) PU L#63 (P#47) L2 L#32 (1024KB) + L1d L#32 (32KB) + L1i L#32 (32KB) + Core L#32 PU L#64 (P#11) PU L#65 (P#51) L2 L#33 (1024KB) + L1d L#33 (32KB) + L1i L#33 (32KB) + Core L#33 PU L#66 (P#15) PU L#67 (P#55) L2 L#34 (1024KB) + L1d L#34 (32KB) + L1i L#34 (32KB) + Core L#34 PU L#68 (P#19) PU L#69 (P#59) L2 L#35 (1024KB) + L1d L#35 (32KB) + L1i L#35 (32KB) + Core L#35 PU L#70 (P#23) PU L#71 (P#63) L2 L#36 (1024KB) + L1d L#36 (32KB) + L1i L#36 (32KB) + Core L#36 PU L#72 (P#27) PU L#73 (P#67) L2 L#37 (1024KB) + L1d L#37 (32KB) + L1i L#37 (32KB) + Core L#37 PU L#74 (P#31) PU L#75 (P#71) L2 L#38 (1024KB) + L1d L#38 (32KB) + L1i L#38 (32KB) + Core L#38 PU L#76 (P#35) PU L#77 (P#75) L2 L#39 (1024KB) + L1d L#39 (32KB) + L1i L#39 (32KB) + Core L#39 PU L#78 (P#39) PU L#79 (P#79) HostBridge PCIBridge PCI c8:00.0 (NVMExp) Block(Disk) \"nvme6n1\" PCIBridge PCI c9:00.0 (NVMExp) Block(Disk) \"nvme7n1\" PCIBridge PCI ca:00.0 (NVMExp) Block(Disk) \"nvme8n1\" PCIBridge PCI cb:00.0 (NVMExp) Block(Disk) \"nvme9n1\" HostBridge PCIBridge PCI db:00.0 (NVMExp) Block(Disk) \"nvme10n1\" PCIBridge PCI dc:00.0 (NVMExp) Block(Disk) \"nvme11n1\" Misc(MemoryModule) Misc(MemoryModule) Misc(MemoryModule) Misc(MemoryModule) Misc(MemoryModule) Misc(MemoryModule) Misc(MemoryModule) Misc(MemoryModule) Misc(MemoryModule) Misc(MemoryModule) Misc(MemoryModule) Misc(MemoryModule) Misc(MemoryModule) Misc(MemoryModule) Misc(MemoryModule) Misc(MemoryModule) Misc(MemoryModule) Misc(MemoryModule) Misc(MemoryModule) Misc(MemoryModule) Misc(MemoryModule) Misc(MemoryModule) Misc(MemoryModule) Misc(MemoryModule)","title":"Lstopo"},{"location":"Nvidia%20GPUDirect/","text":"Nvidia GPUDirect Nvidia GPUDirect Background Research Helpful Links Source Code Description of the Problem Lab Configuration Hardware Configuration RHEL Version GCC Version Nvidia SMI CUDA Version Setting Up the Code Environment CUDA Development Packages Prepare the Code Compiling and Running the App Debugging Background Research See Background Research for background information I studied before doing this. Helpful Links RDMA Aware Programming Typical Application Flow Source Code See the test file Description of the Problem What we are doing in the below steps is playing a packet from one Mellanox card directly into another. Before playing the packet, we write to a region in GPU memory with a specific pattern and in the packet we send we have a different pattern. As a proof of concept we expect that the packet's data overwrites this memory buffer. See this post for the original description of the problem. A queue pair and its associated resources are established exactly as described in the (generic application flow)[https://docs.nvidia.com/networking/display/RDMAAwareProgrammingv17/Typical+Application] Lines 0-192 of the attached code Register a region of host memory and fill it with a known pattern lines 192-195 \u200bRegister a region of GPU memory Lines 197-223 Send a packet containing a known pattern from one Mellanox device to another Lines 223-375 Copy the data from the GPU device's memory region into the host system memory which we expect to overwrite the host system memory's bit pattern with the one we just sent Line 375-380 Confirm that the memory patterns match. The idea being that we just sent a new pattern from one Mellanox device to the other and then told it to overwrite the pattern that was already in system memory with what the GPU received. The logic being that we expect the pattern which was in system memory to be overwritten by what was just sent. This happens in lines 391-396\u200b At no point does the CUDA toolkit issue any errors. Everything returns as a success however, the pattern in system memory is not overwritten. Need to determine why this simple POC does not work in order to move forward with customer. Lab Configuration Hardware Configuration Dell R750 with a Mellanox MLX6 as the transmitting device and a MLX5 as the receiving device. Worth noting is that at the time of writing there is no special MLX6 driver. All device names will appear is MLX5. RHEL Version NAME=\"Red Hat Enterprise Linux\" VERSION=\"8.5 (Ootpa)\" ID=\"rhel\" ID_LIKE=\"fedora\" VERSION_ID=\"8.5\" PLATFORM_ID=\"platform:el8\" PRETTY_NAME=\"Red Hat Enterprise Linux 8.5 (Ootpa)\" ANSI_COLOR=\"0;31\" CPE_NAME=\"cpe:/o:redhat:enterprise_linux:8::baseos\" HOME_URL=\"https://www.redhat.com/\" DOCUMENTATION_URL=\"https://access.redhat.com/documentation/red_hat_enterprise_linux/8/\" BUG_REPORT_URL=\"https://bugzilla.redhat.com/\" REDHAT_BUGZILLA_PRODUCT=\"Red Hat Enterprise Linux 8\" REDHAT_BUGZILLA_PRODUCT_VERSION=8.5 REDHAT_SUPPORT_PRODUCT=\"Red Hat Enterprise Linux\" REDHAT_SUPPORT_PRODUCT_VERSION=\"8.5\" Red Hat Enterprise Linux release 8.5 (Ootpa) Red Hat Enterprise Linux release 8.5 (Ootpa) GCC Version [root@gputest ~]# gcc --version gcc (GCC) 8.5.0 20210514 (Red Hat 8.5.0-4) Copyright (C) 2018 Free Software Foundation, Inc. This is free software; see the source for copying conditions. There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. Nvidia SMI +-----------------------------------------------------------------------------+ | NVIDIA-SMI 495.29.05 Driver Version: 495.29.05 CUDA Version: 11.5 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 NVIDIA A100-PCI... Off | 00000000:CA:00.0 Off | 0 | | N/A 26C P0 35W / 250W | 541MiB / 40536MiB | 0% Default | | | | Disabled | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | 0 N/A N/A 4969 G /usr/libexec/Xorg 26MiB | | 0 N/A N/A 5345 G /usr/bin/gnome-shell 98MiB | | 0 N/A N/A 606994 C /tmp/test/rdma-loopback 413MiB | +-----------------------------------------------------------------------------+ CUDA Version [root@gputest ~]# nvcc --version nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 2005-2021 NVIDIA Corporation Built on Thu_Nov_18_09:45:30_PST_2021 Cuda compilation tools, release 11.5, V11.5.119 Build cuda_11.5.r11.5/compiler.30672275_0 ` ### MLX Config See [MLX5_0](images/mlx5_0.log) and [MLX5_2](images/mlx5_2.log) ## Installation ### MLNX_OFED 1. Download MLNX_OFED drivers from https://www.mellanox.com/products/infiniband-drivers/linux/mlnx_ofed 1. MLNX_OFED is version dependent. I suggest you use `subscription-manager release --set=8.4` to ensure your version of RHEL stays at the version for which MLNX_OFED was compiled 2. Upload the ISO to the target box 3. Run: ```bash dnf group install \"Development Tools\" -y dnf install -y tk tcsh tcl gcc-gfortran kernel-modules-extra gcc-g++ gdb rsync ninja-build make zip mount MLNX* /mnt cd /mnt ./mlnxofedinstall Setting Up the Code Environment CUDA Development Packages Make sure you have an Nvidia GPU that shows on the device with lspci | grep -i nvidia Make sure you have the kernel dev headers for your kernel. with rpm -qa | grep devel | grep kernel && uname -r Run the following (See Nvidia's instructions for details): dnf install -y https://dl.fedoraproject.org/pub/epel/epel-release-latest-8.noarch.rpm subscription-manager repos --enable=rhel-8-for-x86_64-appstream-rpms subscription-manager repos --enable=rhel-8-for-x86_64-baseos-rpms subscription-manager repos --enable=codeready-builder-for-rhel-8-x86_64-rpms sudo rpm -i cuda-repo-rhel8-11-5-local-11.5.1_495.29.05-1.x86_64.rpm sudo dnf config-manager --add-repo https://developer.download.nvidia.com/compute/cuda/repos/rhel8/x86_64/cuda-rhel8.repo sudo dnf clean expire-cache sudo dnf module install -y nvidia-driver:latest-dkms sudo dnf install -y cuda export PATH=/usr/local/cuda-11.5/bin${PATH:+:${PATH}} echo 'export PATH=/usr/local/cuda-11.5/bin${PATH:+:${PATH}}' >> /root/.bashrc modprobe nvidia-peermem # YOU MUST RUN THIS MANUALLY # Below is just for debugging. You don't have to install them. # Make sure, even though it is RHEL 8, you use the word yum here. yum debuginfo-install libgcc-8.5.0-4.el8_5.x86_64 libibverbs-55mlnx37-1.55103.x86_64 libnl3-3.5.0-1.el8.x86_64 libstdc++-8.5.0-4.el8_5.x86_64 nvidia-driver-cuda-libs-495.29.05-1.el8.x86_64 WARNING Whenever you want to run this code you must manually load the nvidia-peermem module. See Nvidia peermem . Load with modprobe nvidia-peermem Prepare the Code First we need to make some manual adjustments to some parameters in the code. For this you need the MAC addresses My first challenge was that the system was entirely remote so I had to figure out how to determine exactly which interfaces belonged to which card. To find the MAC addresses remotely you can use lspci -v | grep -i ethernet and compare that with the output of ethtool -i <interface_name> . This will allow you to corelate the model name of the NIC to the interface/MAC using the PCIe bus number. Ex: [root@gputest ~]# ip a s 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2: eno8303: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP group default qlen 1000 link/ether b0:7b:25:f8:44:d2 brd ff:ff:ff:ff:ff:ff inet 172.28.1.40/24 brd 172.28.1.255 scope global dynamic noprefixroute eno8303 valid_lft 71017sec preferred_lft 71017sec inet6 fe80::b27b:25ff:fef8:44d2/64 scope link noprefixroute valid_lft forever preferred_lft forever 3: eno8403: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc mq state DOWN group default qlen 1000 link/ether b0:7b:25:f8:44:d3 brd ff:ff:ff:ff:ff:ff 4: eno12399: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN group default qlen 1000 link/ether b4:96:91:cd:e8:ac brd ff:ff:ff:ff:ff:ff 5: eno12409: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN group default qlen 1000 link/ether b4:96:91:cd:e8:ad brd ff:ff:ff:ff:ff:ff 6: ens6f0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc mq state DOWN group default qlen 1000 link/ether b8:ce:f6:cc:9e:dc brd ff:ff:ff:ff:ff:ff 7: ens6f1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP group default qlen 1000 link/ether b8:ce:f6:cc:9e:dd brd ff:ff:ff:ff:ff:ff 8: ens5f0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP group default qlen 1000 link/ether 0c:42:a1:73:8d:e6 brd ff:ff:ff:ff:ff:ff 9: ens5f1: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc mq state DOWN group default qlen 1000 link/ether 0c:42:a1:73:8d:e7 brd ff:ff:ff:ff:ff:ff 10: virbr0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default qlen 1000 link/ether 52:54:00:b7:e9:a7 brd ff:ff:ff:ff:ff:ff inet 192.168.122.1/24 brd 192.168.122.255 scope global virbr0 valid_lft forever preferred_lft forever 11: virbr0-nic: <BROADCAST,MULTICAST> mtu 1500 qdisc fq_codel master virbr0 state DOWN group default qlen 1000 link/ether 52:54:00:b7:e9:a7 brd ff:ff:ff:ff:ff:ff [root@gputest test_code]# lspci -v | grep -i ethernet 04:00.0 Ethernet controller: Broadcom Inc. and subsidiaries NetXtreme BCM5720 Gigabit Ethernet PCIe 04:00.1 Ethernet controller: Broadcom Inc. and subsidiaries NetXtreme BCM5720 Gigabit Ethernet PCIe 31:00.0 Ethernet controller: Intel Corporation Ethernet Controller E810-XXV for SFP (rev 02) Subsystem: Intel Corporation Ethernet 25G 2P E810-XXV OCP 31:00.1 Ethernet controller: Intel Corporation Ethernet Controller E810-XXV for SFP (rev 02) Subsystem: Intel Corporation Ethernet 25G 2P E810-XXV OCP 98:00.0 Ethernet controller: Mellanox Technologies MT2892 Family [ConnectX-6 Dx] 98:00.1 Ethernet controller: Mellanox Technologies MT2892 Family [ConnectX-6 Dx] b1:00.0 Ethernet controller: Mellanox Technologies MT28800 Family [ConnectX-5 Ex] b1:00.1 Ethernet controller: Mellanox Technologies MT28800 Family [ConnectX-5 Ex] [root@gputest test_code]# ethtool -i ens6f1 driver: mlx5_core version: 5.5-1.0.3 firmware-version: 22.31.1014 (DEL0000000027) expansion-rom-version: bus-info: 0000:98:00.1 supports-statistics: yes supports-test: yes supports-eeprom-access: no supports-register-dump: no supports-priv-flags: yes [root@gputest ~]# ethtool -i ens5f0 driver: mlx5_core version: 5.5-1.0.3 firmware-version: 16.27.6106 (DEL0000000004) expansion-rom-version: bus-info: 0000:b1:00.0 supports-statistics: yes supports-test: yes supports-eeprom-access: no supports-register-dump: no supports-priv-flags: yes So here we can see from the bus numbers that in my case MLX6 device is ens6f0/ens6f1 and the MLX5 is ens5f0/ensf1. My transmit interface will be b8:ce:f6:cc:9e:dd/ens6f1 and my receive is 0c:42:a1:73:8d:e6/ens5f0. Next you have to figure out the Mellanox device numbers. You can do this using mlxconfig. To get this listing run mlxconfig -d mlx5_0 q . In the header of each block you should see something like this: Device #1: ---------- Device type: ConnectX6DX Name: 0F6FXM_08P2T2_Ax Description: Mellanox ConnectX-6 Dx Dual Port 100 GbE QSFP56 Network Adapter Device: mlx5_0 From my experimentation they are in order. So mlx5_0 is the first interface on the MLX6 device which lines up with ens6f0. This means mlx5_1 is the second interface and then mlx5_3 would be the first interface of the MLX5 device which we can confirm with mlxconfig -d mlx5_3 q Device #1: ---------- Device type: ConnectX5 Name: 09FTMY_071C1T_Ax Description: Mellanox ConnectX-5 Ex Dual Port 100 GbE QSFP Network Adapter Device: mlx5_3 Compiling and Running the App g++ rdma-loopback.cc -o rdma-loopback -libverbs -I/usr/local/cuda/include -L/usr/local/cuda/lib64 -lcudart Debugging gdb --args rdma-loopback 0 set print pretty on is helpful To get the config of the Mellanox devices run mlxconfig -d mlx5_0 q > mlx5_0.log . Replace mlx5 with your device name.","title":"Nvidia GPUDirect"},{"location":"Nvidia%20GPUDirect/#nvidia-gpudirect","text":"Nvidia GPUDirect Background Research Helpful Links Source Code Description of the Problem Lab Configuration Hardware Configuration RHEL Version GCC Version Nvidia SMI CUDA Version Setting Up the Code Environment CUDA Development Packages Prepare the Code Compiling and Running the App Debugging","title":"Nvidia GPUDirect"},{"location":"Nvidia%20GPUDirect/#background-research","text":"See Background Research for background information I studied before doing this.","title":"Background Research"},{"location":"Nvidia%20GPUDirect/#helpful-links","text":"RDMA Aware Programming Typical Application Flow","title":"Helpful Links"},{"location":"Nvidia%20GPUDirect/#source-code","text":"See the test file","title":"Source Code"},{"location":"Nvidia%20GPUDirect/#description-of-the-problem","text":"What we are doing in the below steps is playing a packet from one Mellanox card directly into another. Before playing the packet, we write to a region in GPU memory with a specific pattern and in the packet we send we have a different pattern. As a proof of concept we expect that the packet's data overwrites this memory buffer. See this post for the original description of the problem. A queue pair and its associated resources are established exactly as described in the (generic application flow)[https://docs.nvidia.com/networking/display/RDMAAwareProgrammingv17/Typical+Application] Lines 0-192 of the attached code Register a region of host memory and fill it with a known pattern lines 192-195 \u200bRegister a region of GPU memory Lines 197-223 Send a packet containing a known pattern from one Mellanox device to another Lines 223-375 Copy the data from the GPU device's memory region into the host system memory which we expect to overwrite the host system memory's bit pattern with the one we just sent Line 375-380 Confirm that the memory patterns match. The idea being that we just sent a new pattern from one Mellanox device to the other and then told it to overwrite the pattern that was already in system memory with what the GPU received. The logic being that we expect the pattern which was in system memory to be overwritten by what was just sent. This happens in lines 391-396\u200b At no point does the CUDA toolkit issue any errors. Everything returns as a success however, the pattern in system memory is not overwritten. Need to determine why this simple POC does not work in order to move forward with customer.","title":"Description of the Problem"},{"location":"Nvidia%20GPUDirect/#lab-configuration","text":"","title":"Lab Configuration"},{"location":"Nvidia%20GPUDirect/#hardware-configuration","text":"Dell R750 with a Mellanox MLX6 as the transmitting device and a MLX5 as the receiving device. Worth noting is that at the time of writing there is no special MLX6 driver. All device names will appear is MLX5.","title":"Hardware Configuration"},{"location":"Nvidia%20GPUDirect/#rhel-version","text":"NAME=\"Red Hat Enterprise Linux\" VERSION=\"8.5 (Ootpa)\" ID=\"rhel\" ID_LIKE=\"fedora\" VERSION_ID=\"8.5\" PLATFORM_ID=\"platform:el8\" PRETTY_NAME=\"Red Hat Enterprise Linux 8.5 (Ootpa)\" ANSI_COLOR=\"0;31\" CPE_NAME=\"cpe:/o:redhat:enterprise_linux:8::baseos\" HOME_URL=\"https://www.redhat.com/\" DOCUMENTATION_URL=\"https://access.redhat.com/documentation/red_hat_enterprise_linux/8/\" BUG_REPORT_URL=\"https://bugzilla.redhat.com/\" REDHAT_BUGZILLA_PRODUCT=\"Red Hat Enterprise Linux 8\" REDHAT_BUGZILLA_PRODUCT_VERSION=8.5 REDHAT_SUPPORT_PRODUCT=\"Red Hat Enterprise Linux\" REDHAT_SUPPORT_PRODUCT_VERSION=\"8.5\" Red Hat Enterprise Linux release 8.5 (Ootpa) Red Hat Enterprise Linux release 8.5 (Ootpa)","title":"RHEL Version"},{"location":"Nvidia%20GPUDirect/#gcc-version","text":"[root@gputest ~]# gcc --version gcc (GCC) 8.5.0 20210514 (Red Hat 8.5.0-4) Copyright (C) 2018 Free Software Foundation, Inc. This is free software; see the source for copying conditions. There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.","title":"GCC Version"},{"location":"Nvidia%20GPUDirect/#nvidia-smi","text":"+-----------------------------------------------------------------------------+ | NVIDIA-SMI 495.29.05 Driver Version: 495.29.05 CUDA Version: 11.5 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 NVIDIA A100-PCI... Off | 00000000:CA:00.0 Off | 0 | | N/A 26C P0 35W / 250W | 541MiB / 40536MiB | 0% Default | | | | Disabled | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | 0 N/A N/A 4969 G /usr/libexec/Xorg 26MiB | | 0 N/A N/A 5345 G /usr/bin/gnome-shell 98MiB | | 0 N/A N/A 606994 C /tmp/test/rdma-loopback 413MiB | +-----------------------------------------------------------------------------+","title":"Nvidia SMI"},{"location":"Nvidia%20GPUDirect/#cuda-version","text":"[root@gputest ~]# nvcc --version nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 2005-2021 NVIDIA Corporation Built on Thu_Nov_18_09:45:30_PST_2021 Cuda compilation tools, release 11.5, V11.5.119 Build cuda_11.5.r11.5/compiler.30672275_0 ` ### MLX Config See [MLX5_0](images/mlx5_0.log) and [MLX5_2](images/mlx5_2.log) ## Installation ### MLNX_OFED 1. Download MLNX_OFED drivers from https://www.mellanox.com/products/infiniband-drivers/linux/mlnx_ofed 1. MLNX_OFED is version dependent. I suggest you use `subscription-manager release --set=8.4` to ensure your version of RHEL stays at the version for which MLNX_OFED was compiled 2. Upload the ISO to the target box 3. Run: ```bash dnf group install \"Development Tools\" -y dnf install -y tk tcsh tcl gcc-gfortran kernel-modules-extra gcc-g++ gdb rsync ninja-build make zip mount MLNX* /mnt cd /mnt ./mlnxofedinstall","title":"CUDA Version"},{"location":"Nvidia%20GPUDirect/#setting-up-the-code-environment","text":"","title":"Setting Up the Code Environment"},{"location":"Nvidia%20GPUDirect/#cuda-development-packages","text":"Make sure you have an Nvidia GPU that shows on the device with lspci | grep -i nvidia Make sure you have the kernel dev headers for your kernel. with rpm -qa | grep devel | grep kernel && uname -r Run the following (See Nvidia's instructions for details): dnf install -y https://dl.fedoraproject.org/pub/epel/epel-release-latest-8.noarch.rpm subscription-manager repos --enable=rhel-8-for-x86_64-appstream-rpms subscription-manager repos --enable=rhel-8-for-x86_64-baseos-rpms subscription-manager repos --enable=codeready-builder-for-rhel-8-x86_64-rpms sudo rpm -i cuda-repo-rhel8-11-5-local-11.5.1_495.29.05-1.x86_64.rpm sudo dnf config-manager --add-repo https://developer.download.nvidia.com/compute/cuda/repos/rhel8/x86_64/cuda-rhel8.repo sudo dnf clean expire-cache sudo dnf module install -y nvidia-driver:latest-dkms sudo dnf install -y cuda export PATH=/usr/local/cuda-11.5/bin${PATH:+:${PATH}} echo 'export PATH=/usr/local/cuda-11.5/bin${PATH:+:${PATH}}' >> /root/.bashrc modprobe nvidia-peermem # YOU MUST RUN THIS MANUALLY # Below is just for debugging. You don't have to install them. # Make sure, even though it is RHEL 8, you use the word yum here. yum debuginfo-install libgcc-8.5.0-4.el8_5.x86_64 libibverbs-55mlnx37-1.55103.x86_64 libnl3-3.5.0-1.el8.x86_64 libstdc++-8.5.0-4.el8_5.x86_64 nvidia-driver-cuda-libs-495.29.05-1.el8.x86_64 WARNING Whenever you want to run this code you must manually load the nvidia-peermem module. See Nvidia peermem . Load with modprobe nvidia-peermem","title":"CUDA Development Packages"},{"location":"Nvidia%20GPUDirect/#prepare-the-code","text":"First we need to make some manual adjustments to some parameters in the code. For this you need the MAC addresses My first challenge was that the system was entirely remote so I had to figure out how to determine exactly which interfaces belonged to which card. To find the MAC addresses remotely you can use lspci -v | grep -i ethernet and compare that with the output of ethtool -i <interface_name> . This will allow you to corelate the model name of the NIC to the interface/MAC using the PCIe bus number. Ex: [root@gputest ~]# ip a s 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2: eno8303: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP group default qlen 1000 link/ether b0:7b:25:f8:44:d2 brd ff:ff:ff:ff:ff:ff inet 172.28.1.40/24 brd 172.28.1.255 scope global dynamic noprefixroute eno8303 valid_lft 71017sec preferred_lft 71017sec inet6 fe80::b27b:25ff:fef8:44d2/64 scope link noprefixroute valid_lft forever preferred_lft forever 3: eno8403: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc mq state DOWN group default qlen 1000 link/ether b0:7b:25:f8:44:d3 brd ff:ff:ff:ff:ff:ff 4: eno12399: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN group default qlen 1000 link/ether b4:96:91:cd:e8:ac brd ff:ff:ff:ff:ff:ff 5: eno12409: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN group default qlen 1000 link/ether b4:96:91:cd:e8:ad brd ff:ff:ff:ff:ff:ff 6: ens6f0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc mq state DOWN group default qlen 1000 link/ether b8:ce:f6:cc:9e:dc brd ff:ff:ff:ff:ff:ff 7: ens6f1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP group default qlen 1000 link/ether b8:ce:f6:cc:9e:dd brd ff:ff:ff:ff:ff:ff 8: ens5f0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP group default qlen 1000 link/ether 0c:42:a1:73:8d:e6 brd ff:ff:ff:ff:ff:ff 9: ens5f1: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc mq state DOWN group default qlen 1000 link/ether 0c:42:a1:73:8d:e7 brd ff:ff:ff:ff:ff:ff 10: virbr0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default qlen 1000 link/ether 52:54:00:b7:e9:a7 brd ff:ff:ff:ff:ff:ff inet 192.168.122.1/24 brd 192.168.122.255 scope global virbr0 valid_lft forever preferred_lft forever 11: virbr0-nic: <BROADCAST,MULTICAST> mtu 1500 qdisc fq_codel master virbr0 state DOWN group default qlen 1000 link/ether 52:54:00:b7:e9:a7 brd ff:ff:ff:ff:ff:ff [root@gputest test_code]# lspci -v | grep -i ethernet 04:00.0 Ethernet controller: Broadcom Inc. and subsidiaries NetXtreme BCM5720 Gigabit Ethernet PCIe 04:00.1 Ethernet controller: Broadcom Inc. and subsidiaries NetXtreme BCM5720 Gigabit Ethernet PCIe 31:00.0 Ethernet controller: Intel Corporation Ethernet Controller E810-XXV for SFP (rev 02) Subsystem: Intel Corporation Ethernet 25G 2P E810-XXV OCP 31:00.1 Ethernet controller: Intel Corporation Ethernet Controller E810-XXV for SFP (rev 02) Subsystem: Intel Corporation Ethernet 25G 2P E810-XXV OCP 98:00.0 Ethernet controller: Mellanox Technologies MT2892 Family [ConnectX-6 Dx] 98:00.1 Ethernet controller: Mellanox Technologies MT2892 Family [ConnectX-6 Dx] b1:00.0 Ethernet controller: Mellanox Technologies MT28800 Family [ConnectX-5 Ex] b1:00.1 Ethernet controller: Mellanox Technologies MT28800 Family [ConnectX-5 Ex] [root@gputest test_code]# ethtool -i ens6f1 driver: mlx5_core version: 5.5-1.0.3 firmware-version: 22.31.1014 (DEL0000000027) expansion-rom-version: bus-info: 0000:98:00.1 supports-statistics: yes supports-test: yes supports-eeprom-access: no supports-register-dump: no supports-priv-flags: yes [root@gputest ~]# ethtool -i ens5f0 driver: mlx5_core version: 5.5-1.0.3 firmware-version: 16.27.6106 (DEL0000000004) expansion-rom-version: bus-info: 0000:b1:00.0 supports-statistics: yes supports-test: yes supports-eeprom-access: no supports-register-dump: no supports-priv-flags: yes So here we can see from the bus numbers that in my case MLX6 device is ens6f0/ens6f1 and the MLX5 is ens5f0/ensf1. My transmit interface will be b8:ce:f6:cc:9e:dd/ens6f1 and my receive is 0c:42:a1:73:8d:e6/ens5f0. Next you have to figure out the Mellanox device numbers. You can do this using mlxconfig. To get this listing run mlxconfig -d mlx5_0 q . In the header of each block you should see something like this: Device #1: ---------- Device type: ConnectX6DX Name: 0F6FXM_08P2T2_Ax Description: Mellanox ConnectX-6 Dx Dual Port 100 GbE QSFP56 Network Adapter Device: mlx5_0 From my experimentation they are in order. So mlx5_0 is the first interface on the MLX6 device which lines up with ens6f0. This means mlx5_1 is the second interface and then mlx5_3 would be the first interface of the MLX5 device which we can confirm with mlxconfig -d mlx5_3 q Device #1: ---------- Device type: ConnectX5 Name: 09FTMY_071C1T_Ax Description: Mellanox ConnectX-5 Ex Dual Port 100 GbE QSFP Network Adapter Device: mlx5_3","title":"Prepare the Code"},{"location":"Nvidia%20GPUDirect/#compiling-and-running-the-app","text":"g++ rdma-loopback.cc -o rdma-loopback -libverbs -I/usr/local/cuda/include -L/usr/local/cuda/lib64 -lcudart","title":"Compiling and Running the App"},{"location":"Nvidia%20GPUDirect/#debugging","text":"gdb --args rdma-loopback 0 set print pretty on is helpful To get the config of the Mellanox devices run mlxconfig -d mlx5_0 q > mlx5_0.log . Replace mlx5 with your device name.","title":"Debugging"},{"location":"Nvidia%20GPUDirect/background_research/","text":"Helpful Links Nvidia GPUDirect Overview Nvidia System Management Interface GPUDirect RDMA Access Example Code Mellanox GPUDirect RDMA Example NVIDIA CUDA Basics GPUDirect Benchmark Tests RDMA Aware Programming User Manual (has a great RDMA architecture overview) NOTE This also includes how Infiniband works. Packet Capture with GPUDirect How Infiniband Works https://stackoverflow.com/questions/52125610/visual-studio-remote-linux-headers Using Nvidia SMI Run from Windows: c:\\Windows\\System32\\DriverStore\\FileRepository\\nvdm.inf_amd64_3349a8117b680632>nvidia-smi.exe OUTPUT c:\\Windows\\System32\\DriverStore\\FileRepository\\nvdm.inf_amd64_3349a8117b680632>nvidia-smi.exe Wed Nov 10 15:47:38 2021 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 471.35 Driver Version: 471.35 CUDA Version: 11.4 | |-------------------------------+----------------------+----------------------+ | GPU Name TCC/WDDM | Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 Quadro T1000 WDDM | 00000000:01:00.0 Off | N/A | | N/A 52C P8 2W / N/A | 287MiB / 4096MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | 0 N/A N/A 3360 C+G ...ser\\Application\\brave.exe N/A | +-----------------------------------------------------------------------------+ Processes is a list of processes having compute or graphics context on the device. Compute processes are reported on all the fully supported products. Reporting for Graphics processes is limited to the supported products starting with Kepler architecture. Each Entry is of format <GPU Index> <PID> <Type> <Process Name> <GPUMemory Usage>\" GPU Index - Represents NVML index of the device? PID Represents process ID corresponding to the active compute or graphics context? Type - Displayed as \"C\" for compute process, \"G\" for graphics process, and \"C+G\" for the procses having both compute and graphics contexts Process Name - Represents process name for the compute or graphics process GPU Memory Usage - Amount of memory used on the device by the context. Not available on Windows when running in WDDM mode because Windows KMD manages all memory not Nvidia driver Notes on CUDA Programming See CUDA C Programming Guide Overview At its core CUDA has three key abstractions: - a hierarchy of thread groups - shared memories - barrier synchronization This decomposition preserves language expressivity by allowing threads to cooperate when solving each sub-problem, and at the same time enables automatic scalability. Indeed, each block of threads can be scheduled on any of the available multiprocessors within a GPU, in any order, concurrently or sequentially, so that a compiled CUDA program can execute on any number of multiprocessors as illustrated by Figure 3, and only the runtime system needs to know the physical multiprocessor count. Note : A GPU is built around an array of Streaming Multiprocessors (SMs) (see Hardware Implementation for more details). A multithreaded program is partitioned into blocks of threads that execute independently from each other, so that a GPU with more multiprocessors will automatically execute the program in less time than a GPU with fewer multiprocessors. Kernels CUDA C++ extends C++ by allowing the programmer to define C++ functions, called kernels, that, when called, are executed N times in parallel by N different CUDA threads, as opposed to only once like regular C++ functions. The following sample code, using the built-in variable threadIdx, adds two vectors A and B of size N and stores the result into vector C. // Kernel definition __global__ void VecAdd(float* A, float* B, float* C) { int i = threadIdx.x; C[i] = A[i] + B[i]; } int main() { ... // Kernel invocation with N threads VecAdd<<<1, N>>>(A, B, C); ... } Global Keyword The global keyword indicates a function that runs on the device and is called from host code How nvcc separates code nvcc separates source code into host and device components. Device functions (e.g. mykernel()) are proccesed by the Nvidia compiler while host functions (e.g. main()) are processed by standard host compiler (gcc, cl, etc) Thread Hierarchy For convenience, threadIdx is a 3-component vector, so that threads can be identified using a one-dimensional, two-dimensional, or three-dimensional thread index, forming a one-dimensional, two-dimensional, or three-dimensional block of threads, called a thread block. This provides a natural way to invoke computation across the elements in a domain such as a vector, matrix, or volume. CUDA Runtime The runtime is introduced in CUDA Runtime. It provides C and C++ functions that execute on the host to allocate and deallocate device memory, transfer data between host memory and device memory, manage systems with multiple devices, etc. A complete description of the runtime can be found in the CUDA reference manual. The runtime is built on top of a lower-level C API, the CUDA driver API, which is also accessible by the application. The driver API provides an additional level of control by exposing lower-level concepts such as CUDA contexts - the analogue of host processes for the device - and CUDA modules - the analogue of dynamically loaded libraries for the device. Most applications do not use the driver API as they do not need this additional level of control and when using the runtime, context and module management are implicit, resulting in more concise code. As the runtime is interoperable with the driver API, most applications that need some driver API features can default to use the runtime API and only use the driver API where needed. The driver API is introduced in Driver API and fully described in the reference manual. The runtime is implemented in the cudart library, which is linked to the application, either statically via cudart.lib or libcudart.a, or dynamically via cudart.dll or libcudart.so. Applications that require cudart.dll and/or cudart.so for dynamic linking typically include them as part of the application installation package. It is only safe to pass the address of CUDA runtime symbols between components that link to the same instance of the CUDA runtime. Questions What is NVML Index? What is WDDM mode? What is Windows KMD? What is a Graphics Context? See https://stackoverflow.com/a/23087951/4427375 A GPU context is described here . It represents all the state (data, variables, conditions, etc.) that are collectively required and instantiated to perform certain tasks (e.g. CUDA compute, graphics, H.264 encode, etc). A CUDA context is instantiated to perform CUDA compute activities on the GPU, either implicitly by the CUDA runtime API, or explicitly by the CUDA device API. What is a PCIe Root Complex? https://www.quora.com/What-is-a-PCIe-root-complex?share=1 How does PCIe Enumeration Work? https://www.quora.com/What-is-PCIE-enumeration/answer/Satish-Kumar-525?ch=15&oid=31389493&share=44585235&target_type=answer NVMe over PCIe vs Other Protocols https://www.quora.com/Is-NVMe-faster-than-PCIe/answer/Mike-Jones-169?ch=15&oid=193548046&share=a587ff45&target_type=answer What is a PCIe Function? https://www.quora.com/What-is-a-PCIe-function/answer/Udit-Khanna-2?ch=15&oid=58319695&share=c7f066e5&target_type=answer PCIe-Bus and NUMA Node Correlation https://social.msdn.microsoft.com/Forums/en-US/fabb05b7-eb3f-4a7c-91c5-1ced90af3d0c/pciebus-and-numanode-correlation How does the root complex work? https://codywu2010.wordpress.com/2015/11/29/how-modern-multi-processor-multi-root-complex-system-assigns-pci-bus-number/ What is PCIe P2P? https://xilinx.github.io/XRT/master/html/p2p.html What is the difference between a thread and a block? https://stackoverflow.com/questions/16635587/whats-the-difference-between-a-thread-in-a-block-and-a-warp32-threads A block is used to run things in parallel and for each block there is one thread. Each parallel invocation of a kernel is a block and a set of blocks is a grid. You can also run multiple threads per block.","title":"Background research"},{"location":"Nvidia%20GPUDirect/background_research/#helpful-links","text":"Nvidia GPUDirect Overview Nvidia System Management Interface GPUDirect RDMA Access Example Code Mellanox GPUDirect RDMA Example NVIDIA CUDA Basics GPUDirect Benchmark Tests RDMA Aware Programming User Manual (has a great RDMA architecture overview) NOTE This also includes how Infiniband works. Packet Capture with GPUDirect How Infiniband Works https://stackoverflow.com/questions/52125610/visual-studio-remote-linux-headers","title":"Helpful Links"},{"location":"Nvidia%20GPUDirect/background_research/#using-nvidia-smi","text":"Run from Windows: c:\\Windows\\System32\\DriverStore\\FileRepository\\nvdm.inf_amd64_3349a8117b680632>nvidia-smi.exe OUTPUT c:\\Windows\\System32\\DriverStore\\FileRepository\\nvdm.inf_amd64_3349a8117b680632>nvidia-smi.exe Wed Nov 10 15:47:38 2021 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 471.35 Driver Version: 471.35 CUDA Version: 11.4 | |-------------------------------+----------------------+----------------------+ | GPU Name TCC/WDDM | Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 Quadro T1000 WDDM | 00000000:01:00.0 Off | N/A | | N/A 52C P8 2W / N/A | 287MiB / 4096MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | 0 N/A N/A 3360 C+G ...ser\\Application\\brave.exe N/A | +-----------------------------------------------------------------------------+ Processes is a list of processes having compute or graphics context on the device. Compute processes are reported on all the fully supported products. Reporting for Graphics processes is limited to the supported products starting with Kepler architecture. Each Entry is of format <GPU Index> <PID> <Type> <Process Name> <GPUMemory Usage>\" GPU Index - Represents NVML index of the device? PID Represents process ID corresponding to the active compute or graphics context? Type - Displayed as \"C\" for compute process, \"G\" for graphics process, and \"C+G\" for the procses having both compute and graphics contexts Process Name - Represents process name for the compute or graphics process GPU Memory Usage - Amount of memory used on the device by the context. Not available on Windows when running in WDDM mode because Windows KMD manages all memory not Nvidia driver","title":"Using Nvidia SMI"},{"location":"Nvidia%20GPUDirect/background_research/#notes-on-cuda-programming","text":"See CUDA C Programming Guide","title":"Notes on CUDA Programming"},{"location":"Nvidia%20GPUDirect/background_research/#overview","text":"At its core CUDA has three key abstractions: - a hierarchy of thread groups - shared memories - barrier synchronization This decomposition preserves language expressivity by allowing threads to cooperate when solving each sub-problem, and at the same time enables automatic scalability. Indeed, each block of threads can be scheduled on any of the available multiprocessors within a GPU, in any order, concurrently or sequentially, so that a compiled CUDA program can execute on any number of multiprocessors as illustrated by Figure 3, and only the runtime system needs to know the physical multiprocessor count. Note : A GPU is built around an array of Streaming Multiprocessors (SMs) (see Hardware Implementation for more details). A multithreaded program is partitioned into blocks of threads that execute independently from each other, so that a GPU with more multiprocessors will automatically execute the program in less time than a GPU with fewer multiprocessors.","title":"Overview"},{"location":"Nvidia%20GPUDirect/background_research/#kernels","text":"CUDA C++ extends C++ by allowing the programmer to define C++ functions, called kernels, that, when called, are executed N times in parallel by N different CUDA threads, as opposed to only once like regular C++ functions. The following sample code, using the built-in variable threadIdx, adds two vectors A and B of size N and stores the result into vector C. // Kernel definition __global__ void VecAdd(float* A, float* B, float* C) { int i = threadIdx.x; C[i] = A[i] + B[i]; } int main() { ... // Kernel invocation with N threads VecAdd<<<1, N>>>(A, B, C); ... }","title":"Kernels"},{"location":"Nvidia%20GPUDirect/background_research/#global-keyword","text":"The global keyword indicates a function that runs on the device and is called from host code","title":"Global Keyword"},{"location":"Nvidia%20GPUDirect/background_research/#how-nvcc-separates-code","text":"nvcc separates source code into host and device components. Device functions (e.g. mykernel()) are proccesed by the Nvidia compiler while host functions (e.g. main()) are processed by standard host compiler (gcc, cl, etc)","title":"How nvcc separates code"},{"location":"Nvidia%20GPUDirect/background_research/#thread-hierarchy","text":"For convenience, threadIdx is a 3-component vector, so that threads can be identified using a one-dimensional, two-dimensional, or three-dimensional thread index, forming a one-dimensional, two-dimensional, or three-dimensional block of threads, called a thread block. This provides a natural way to invoke computation across the elements in a domain such as a vector, matrix, or volume.","title":"Thread Hierarchy"},{"location":"Nvidia%20GPUDirect/background_research/#cuda-runtime","text":"The runtime is introduced in CUDA Runtime. It provides C and C++ functions that execute on the host to allocate and deallocate device memory, transfer data between host memory and device memory, manage systems with multiple devices, etc. A complete description of the runtime can be found in the CUDA reference manual. The runtime is built on top of a lower-level C API, the CUDA driver API, which is also accessible by the application. The driver API provides an additional level of control by exposing lower-level concepts such as CUDA contexts - the analogue of host processes for the device - and CUDA modules - the analogue of dynamically loaded libraries for the device. Most applications do not use the driver API as they do not need this additional level of control and when using the runtime, context and module management are implicit, resulting in more concise code. As the runtime is interoperable with the driver API, most applications that need some driver API features can default to use the runtime API and only use the driver API where needed. The driver API is introduced in Driver API and fully described in the reference manual. The runtime is implemented in the cudart library, which is linked to the application, either statically via cudart.lib or libcudart.a, or dynamically via cudart.dll or libcudart.so. Applications that require cudart.dll and/or cudart.so for dynamic linking typically include them as part of the application installation package. It is only safe to pass the address of CUDA runtime symbols between components that link to the same instance of the CUDA runtime.","title":"CUDA Runtime"},{"location":"Nvidia%20GPUDirect/background_research/#questions","text":"What is NVML Index? What is WDDM mode? What is Windows KMD?","title":"Questions"},{"location":"Nvidia%20GPUDirect/background_research/#what-is-a-graphics-context","text":"See https://stackoverflow.com/a/23087951/4427375 A GPU context is described here . It represents all the state (data, variables, conditions, etc.) that are collectively required and instantiated to perform certain tasks (e.g. CUDA compute, graphics, H.264 encode, etc). A CUDA context is instantiated to perform CUDA compute activities on the GPU, either implicitly by the CUDA runtime API, or explicitly by the CUDA device API.","title":"What is a Graphics Context?"},{"location":"Nvidia%20GPUDirect/background_research/#what-is-a-pcie-root-complex","text":"https://www.quora.com/What-is-a-PCIe-root-complex?share=1","title":"What is a PCIe Root Complex?"},{"location":"Nvidia%20GPUDirect/background_research/#how-does-pcie-enumeration-work","text":"https://www.quora.com/What-is-PCIE-enumeration/answer/Satish-Kumar-525?ch=15&oid=31389493&share=44585235&target_type=answer","title":"How does PCIe Enumeration Work?"},{"location":"Nvidia%20GPUDirect/background_research/#nvme-over-pcie-vs-other-protocols","text":"https://www.quora.com/Is-NVMe-faster-than-PCIe/answer/Mike-Jones-169?ch=15&oid=193548046&share=a587ff45&target_type=answer","title":"NVMe over PCIe vs Other Protocols"},{"location":"Nvidia%20GPUDirect/background_research/#what-is-a-pcie-function","text":"https://www.quora.com/What-is-a-PCIe-function/answer/Udit-Khanna-2?ch=15&oid=58319695&share=c7f066e5&target_type=answer","title":"What is a PCIe Function?"},{"location":"Nvidia%20GPUDirect/background_research/#pcie-bus-and-numa-node-correlation","text":"https://social.msdn.microsoft.com/Forums/en-US/fabb05b7-eb3f-4a7c-91c5-1ced90af3d0c/pciebus-and-numanode-correlation","title":"PCIe-Bus and NUMA Node Correlation"},{"location":"Nvidia%20GPUDirect/background_research/#how-does-the-root-complex-work","text":"https://codywu2010.wordpress.com/2015/11/29/how-modern-multi-processor-multi-root-complex-system-assigns-pci-bus-number/","title":"How does the root complex work?"},{"location":"Nvidia%20GPUDirect/background_research/#what-is-pcie-p2p","text":"https://xilinx.github.io/XRT/master/html/p2p.html","title":"What is PCIe P2P?"},{"location":"Nvidia%20GPUDirect/background_research/#what-is-the-difference-between-a-thread-and-a-block","text":"https://stackoverflow.com/questions/16635587/whats-the-difference-between-a-thread-in-a-block-and-a-warp32-threads A block is used to run things in parallel and for each block there is one thread. Each parallel invocation of a kernel is a block and a set of blocks is a grid. You can also run multiple threads per block.","title":"What is the difference between a thread and a block?"},{"location":"Nvidia%20GRID%20Notes/","text":"Nvidia GRID Notes Nvidia GRID Notes Useful Resources GPUs which Support vGPU Creation on VMWare vGPU Architecture Diagram How does SR-IOV work? Multi-Instance GPU Nvidia Professional Technologies How Does GRID Manager Integrate with the Hypervisor Where do profiles fit in the stack? What is a Profile Selecting a GPU Two General Types of Users Which Type of GPU? Dell GPUs and Upgrade Path Nvidia GPUs GPUs for Power Users Desktop Hosting Models vGPU License Decision Tree Useful Resources Nvidia vGPU Validated Solutions Nvidia Product Support Matrix for Operating Systems Add an NVIDIA GRID vGPU to a Virtual Machine Introduction to Nvidia Virtual GPU - Part 1 - Intro, Which GPU & License? NVIDIA Virtual GPU Software Packaging, Pricing, and Licensing Guide GPUs which Support vGPU Creation on VMWare Here are all the GPUs which support creating vGPU instances on VMWare: https://docs.nvidia.com/grid/13.0/grid-vgpu-release-notes-vmware-vsphere/index.html#hardware-configuration vGPU Architecture Diagram vGPU is exposed via SR-IOV . SR-IOV is an extension of the PCIe specification. It allows you to take one physical entity like a GPU or NIC and split it up into multiple virtual devices. How does SR-IOV work? In the case of SR-IOV, things are split into a Physical Function (PF) and a PCIe Virtual Function (VF) which represent the physical device and a virtual instance of that device which exposes some subset of the physical resources respectively. Usually a PCIe device has a single Requester ID (RID) which allows it to communicate over PCIe. This functions more or less like an IP address. However, with SR-IOV each physical function and virtual function gets its own RID. This allows the I/O Memory Management Unit (IOMMU) to differentiate between the different VFs. Note: The IOMMU connects any device with DMA capability (ex: NIC/GPU) to main memory directly instead of routing it through the CPU. This system allows the hypervisor to deliver IO from the VF directly to a VM without going through any software switching in the hypervisor. Multi-Instance GPU Nvidia calls this segmentation of the GPU Multi-Instance GPU (MIG). MIG enables a physical GPU to be securely partitioned into multiple separate GPU instances, providing multiple users with separate GPU resources to accelerate their applications. Nvidia Professional Technologies This is from Introduction to NVIDIA Virtual GPU - Part 1 - Intro, Which GPU & License? While a bit out of date it does a good job of showing where what technologies fit and what licenses are relevant. How Does GRID Manager Integrate with the Hypervisor Each vGPU in a VM gets a time slice on the actual GPU along with its own dedicated memory (the frame buffer). You\u2019ll have the aforementioned GPU manager that\u2019s part of the hypervisor and then each VM will run a standard Nvidia driver. Nvidia says that doing full PCIe passthrough vs giving the same resources via a vGPU is a negligible performance difference - a couple of percentage points max. Nvidia maintains a compatibility matrix on their website with what platforms and card combinations are available Where do profiles fit in the stack? This images is helpful because it shows where GPU profiles fit in the stack. What is a Profile Profiles are the means by which we define how much resources and the types of capabilities a vGPU has. Selecting a GPU Two General Types of Users Which Type of GPU? Dell GPUs and Upgrade Path Nvidia GPUs NOTE : These are from the aforementioned YouTube lecture which is a bit out of date but I thought it was helpful to see it broken out. GPUs for Power Users Desktop Hosting Models There are two general ways to present virtual desktops XenApp you have one instance of Windows Server and multiple people using it. You generally need a large frame buffer for this. Everyone else uses this model where you have multiple desktops running simultaneously. Licensing: There\u2019s an Nvidia license server that runs and as you create virtual desktops it will consume licenses. Which license depends on what you\u2019re doing and the type of physical GPU. vGPU License Decision Tree","title":"Nvidia GRID Notes"},{"location":"Nvidia%20GRID%20Notes/#nvidia-grid-notes","text":"Nvidia GRID Notes Useful Resources GPUs which Support vGPU Creation on VMWare vGPU Architecture Diagram How does SR-IOV work? Multi-Instance GPU Nvidia Professional Technologies How Does GRID Manager Integrate with the Hypervisor Where do profiles fit in the stack? What is a Profile Selecting a GPU Two General Types of Users Which Type of GPU? Dell GPUs and Upgrade Path Nvidia GPUs GPUs for Power Users Desktop Hosting Models vGPU License Decision Tree","title":"Nvidia GRID Notes"},{"location":"Nvidia%20GRID%20Notes/#useful-resources","text":"Nvidia vGPU Validated Solutions Nvidia Product Support Matrix for Operating Systems Add an NVIDIA GRID vGPU to a Virtual Machine Introduction to Nvidia Virtual GPU - Part 1 - Intro, Which GPU & License? NVIDIA Virtual GPU Software Packaging, Pricing, and Licensing Guide","title":"Useful Resources"},{"location":"Nvidia%20GRID%20Notes/#gpus-which-support-vgpu-creation-on-vmware","text":"Here are all the GPUs which support creating vGPU instances on VMWare: https://docs.nvidia.com/grid/13.0/grid-vgpu-release-notes-vmware-vsphere/index.html#hardware-configuration","title":"GPUs which Support vGPU Creation on VMWare"},{"location":"Nvidia%20GRID%20Notes/#vgpu-architecture-diagram","text":"vGPU is exposed via SR-IOV . SR-IOV is an extension of the PCIe specification. It allows you to take one physical entity like a GPU or NIC and split it up into multiple virtual devices.","title":"vGPU Architecture Diagram"},{"location":"Nvidia%20GRID%20Notes/#how-does-sr-iov-work","text":"In the case of SR-IOV, things are split into a Physical Function (PF) and a PCIe Virtual Function (VF) which represent the physical device and a virtual instance of that device which exposes some subset of the physical resources respectively. Usually a PCIe device has a single Requester ID (RID) which allows it to communicate over PCIe. This functions more or less like an IP address. However, with SR-IOV each physical function and virtual function gets its own RID. This allows the I/O Memory Management Unit (IOMMU) to differentiate between the different VFs. Note: The IOMMU connects any device with DMA capability (ex: NIC/GPU) to main memory directly instead of routing it through the CPU. This system allows the hypervisor to deliver IO from the VF directly to a VM without going through any software switching in the hypervisor.","title":"How does SR-IOV work?"},{"location":"Nvidia%20GRID%20Notes/#multi-instance-gpu","text":"Nvidia calls this segmentation of the GPU Multi-Instance GPU (MIG). MIG enables a physical GPU to be securely partitioned into multiple separate GPU instances, providing multiple users with separate GPU resources to accelerate their applications.","title":"Multi-Instance GPU"},{"location":"Nvidia%20GRID%20Notes/#nvidia-professional-technologies","text":"This is from Introduction to NVIDIA Virtual GPU - Part 1 - Intro, Which GPU & License? While a bit out of date it does a good job of showing where what technologies fit and what licenses are relevant.","title":"Nvidia Professional Technologies"},{"location":"Nvidia%20GRID%20Notes/#how-does-grid-manager-integrate-with-the-hypervisor","text":"Each vGPU in a VM gets a time slice on the actual GPU along with its own dedicated memory (the frame buffer). You\u2019ll have the aforementioned GPU manager that\u2019s part of the hypervisor and then each VM will run a standard Nvidia driver. Nvidia says that doing full PCIe passthrough vs giving the same resources via a vGPU is a negligible performance difference - a couple of percentage points max. Nvidia maintains a compatibility matrix on their website with what platforms and card combinations are available","title":"How Does GRID Manager Integrate with the Hypervisor"},{"location":"Nvidia%20GRID%20Notes/#where-do-profiles-fit-in-the-stack","text":"This images is helpful because it shows where GPU profiles fit in the stack.","title":"Where do profiles fit in the stack?"},{"location":"Nvidia%20GRID%20Notes/#what-is-a-profile","text":"Profiles are the means by which we define how much resources and the types of capabilities a vGPU has.","title":"What is a Profile"},{"location":"Nvidia%20GRID%20Notes/#selecting-a-gpu","text":"","title":"Selecting a GPU"},{"location":"Nvidia%20GRID%20Notes/#two-general-types-of-users","text":"","title":"Two General Types of Users"},{"location":"Nvidia%20GRID%20Notes/#which-type-of-gpu","text":"","title":"Which Type of GPU?"},{"location":"Nvidia%20GRID%20Notes/#dell-gpus-and-upgrade-path","text":"","title":"Dell GPUs and Upgrade Path"},{"location":"Nvidia%20GRID%20Notes/#nvidia-gpus","text":"NOTE : These are from the aforementioned YouTube lecture which is a bit out of date but I thought it was helpful to see it broken out.","title":"Nvidia GPUs"},{"location":"Nvidia%20GRID%20Notes/#gpus-for-power-users","text":"","title":"GPUs for Power Users"},{"location":"Nvidia%20GRID%20Notes/#desktop-hosting-models","text":"There are two general ways to present virtual desktops XenApp you have one instance of Windows Server and multiple people using it. You generally need a large frame buffer for this. Everyone else uses this model where you have multiple desktops running simultaneously. Licensing: There\u2019s an Nvidia license server that runs and as you create virtual desktops it will consume licenses. Which license depends on what you\u2019re doing and the type of physical GPU.","title":"Desktop Hosting Models"},{"location":"Nvidia%20GRID%20Notes/#vgpu-license-decision-tree","text":"","title":"vGPU License Decision Tree"},{"location":"OME%20Bug/","text":"OME Bug OME Version 3.7.0 (Build 82) Description OME Does not correctly handle the @odata.nextLink attribute for /api/DeviceService/Devices when passed specific IDs. For example: https://10.55.160.130/api/DeviceService/Devices?Id=13878,16669,16697,16698,16700,16701,16705,16707,16711,16712,16803,16846,16852,16975,16976,16982,16983,17002,17006,17007,17008,17020,17021,17255,17288,17289,17750,17752,17753,17755,17780,17781,17821,17822,17824,17825,17826,17829,17834,17835,17836,17838,17847,17848,17850,17851,17852,17856,17857,17864,17865,17868,17869,17870,17892,17893,17895,17896,17897,17902,17903,17904,17924,17934,17956,17970,17971,17992,18016,18017,18021,18023,18027,18075,18167,18417,18418,18419,18420,18421,18422,18423,18442,18443,18444,18445,18446,18447,18448,18449,18450,18451,18452,18453,18454,18455,18456,18457,18458,18459,18460,18461,18462,18463,18464,18465,18466,18467,18468,18469,18470,18471,18472,18473,18474,18475,18476,18477,18478,18479,18480,18481,18482,18483,18484,18485,18486,18487,18488,18489,18490,18491,18492,18493,18494,18495,18496,18497,18498,18499,18500,18501,18502,18503,18504,18505,18506,18507,18509,18510,18511,18512,18513,18514 The first page of 50 results works as expected. However, after retrieval of the first set of 50 machines the nextLink url is trunkated to '/api/DeviceService/Devices?$skip=50&$top=50' instead of properly including the IDs listed in the original URL. This leads to automated code looping over the totality of the OME inventory instead of some list of specified IDs.","title":"OME Bug"},{"location":"OME%20Bug/#ome-bug","text":"","title":"OME Bug"},{"location":"OME%20Bug/#ome-version","text":"3.7.0 (Build 82)","title":"OME Version"},{"location":"OME%20Bug/#description","text":"OME Does not correctly handle the @odata.nextLink attribute for /api/DeviceService/Devices when passed specific IDs. For example: https://10.55.160.130/api/DeviceService/Devices?Id=13878,16669,16697,16698,16700,16701,16705,16707,16711,16712,16803,16846,16852,16975,16976,16982,16983,17002,17006,17007,17008,17020,17021,17255,17288,17289,17750,17752,17753,17755,17780,17781,17821,17822,17824,17825,17826,17829,17834,17835,17836,17838,17847,17848,17850,17851,17852,17856,17857,17864,17865,17868,17869,17870,17892,17893,17895,17896,17897,17902,17903,17904,17924,17934,17956,17970,17971,17992,18016,18017,18021,18023,18027,18075,18167,18417,18418,18419,18420,18421,18422,18423,18442,18443,18444,18445,18446,18447,18448,18449,18450,18451,18452,18453,18454,18455,18456,18457,18458,18459,18460,18461,18462,18463,18464,18465,18466,18467,18468,18469,18470,18471,18472,18473,18474,18475,18476,18477,18478,18479,18480,18481,18482,18483,18484,18485,18486,18487,18488,18489,18490,18491,18492,18493,18494,18495,18496,18497,18498,18499,18500,18501,18502,18503,18504,18505,18506,18507,18509,18510,18511,18512,18513,18514 The first page of 50 results works as expected. However, after retrieval of the first set of 50 machines the nextLink url is trunkated to '/api/DeviceService/Devices?$skip=50&$top=50' instead of properly including the IDs listed in the original URL. This leads to automated code looping over the totality of the OME inventory instead of some list of specified IDs.","title":"Description"},{"location":"OME%20Integration%20for%20VMWare/","text":"OME Integration for VMWare Installation Download from https://www.dell.com/support/kbdoc/en-us/000176981/openmanage-integration-for-vmware-vcenter#Downloads Open the ZIP file and run the installer. This is a self unpacking executable","title":"OME Integration for VMWare"},{"location":"OME%20Integration%20for%20VMWare/#ome-integration-for-vmware","text":"","title":"OME Integration for VMWare"},{"location":"OME%20Integration%20for%20VMWare/#installation","text":"Download from https://www.dell.com/support/kbdoc/en-us/000176981/openmanage-integration-for-vmware-vcenter#Downloads Open the ZIP file and run the installer. This is a self unpacking executable","title":"Installation"},{"location":"OS10%20Password%20Recovery%20Bug/","text":"OS10 Password Recovery Bug YouTube Reproduction https://youtu.be/b5MJiLTl9KE Update It looks like the instructions are set up to take care of this, but they need to be updated - step 8 tells you to run a sed command that looks like it is targeted at reloving the problem, but it only tells you to run it on 10.5.1.0. When I went through this procedure I just ignored it because the customer was on 10.5.1.3 Description Follow Password recovery instructions to get into the switch at boot. During step 9: /opt/dell/os10/bin/recover_linuxadmin_password.sh produces mount: special device /dev/mapper/OS10-CONFIG does not exist . This is due to a previous configuration of OS10 when tho configuration was mounted on its own logical volume. It has since been moved to SYSROOT. Removing the below lines resolves the issue:","title":"OS10 Password Recovery Bug"},{"location":"OS10%20Password%20Recovery%20Bug/#os10-password-recovery-bug","text":"","title":"OS10 Password Recovery Bug"},{"location":"OS10%20Password%20Recovery%20Bug/#youtube-reproduction","text":"https://youtu.be/b5MJiLTl9KE","title":"YouTube Reproduction"},{"location":"OS10%20Password%20Recovery%20Bug/#update","text":"It looks like the instructions are set up to take care of this, but they need to be updated - step 8 tells you to run a sed command that looks like it is targeted at reloving the problem, but it only tells you to run it on 10.5.1.0. When I went through this procedure I just ignored it because the customer was on 10.5.1.3","title":"Update"},{"location":"OS10%20Password%20Recovery%20Bug/#description","text":"Follow Password recovery instructions to get into the switch at boot. During step 9: /opt/dell/os10/bin/recover_linuxadmin_password.sh produces mount: special device /dev/mapper/OS10-CONFIG does not exist . This is due to a previous configuration of OS10 when tho configuration was mounted on its own logical volume. It has since been moved to SYSROOT. Removing the below lines resolves the issue:","title":"Description"},{"location":"Offline%20Updates%20with%20OpenManage%20Enterprise/","text":"Offline Updates with OpenManage Enterprise Versions My Operating System [root@dellrepo html]# cat /etc/*-release CentOS Linux release 8.3.2011 NAME=\"CentOS Linux\" VERSION=\"8\" ID=\"centos\" ID_LIKE=\"rhel fedora\" VERSION_ID=\"8\" PLATFORM_ID=\"platform:el8\" PRETTY_NAME=\"CentOS Linux 8\" ANSI_COLOR=\"0;31\" CPE_NAME=\"cpe:/o:centos:centos:8\" HOME_URL=\"https://centos.org/\" BUG_REPORT_URL=\"https://bugs.centos.org/\" CENTOS_MANTISBT_PROJECT=\"CentOS-8\" CENTOS_MANTISBT_PROJECT_VERSION=\"8\" CentOS Linux release 8.3.2011 CentOS Linux release 8.3.2011 OME Version Instructions Download Dell Repository Manager Run it with /opt/dell/dellemcrepositorymanager/drm.sh NOTE: Running it with root does not work! You will get an error: GUI interface is not supported by this operating system. Click add repository Select the systems for which you want to download updates under select systems Make sure you select Windows-64 as one of the types which will be available chmod +x <binary name> then run with ./<binary_name> . I chose to distribute the repository using HTTP with Apache dnf install httpd sudo systemctl start --now httpd Next you have to synchronize the repository by clicking download. I downloaded my files to /opt/dell/catalogs/fc640 You then have to go open the Dell EMC Repository Manager -> Export -> Export. You will need to select which repositories you want to export and then you will want to select share and a save location. WARNING You have to download the Windows 64 bit versions of the updates for it to work! Even if you are using Linux the idrac only accepts the Windows EXE files. The export will generate a catalog file when you export. This is what OME will need to reference when you add the catalog. I have included a copy of mine so you can see what it looks like. You can see the progress of the export in the jobs manager: Now go to OME catalog management and hit add (Firmware compliance -> catalog Management -> Add) Configure your repository Share Address: (nothing else) Catalog File Path: /catalog.xml (cannot have anything else) NOTE: The catalog my have a different name depending on how you exported it! Go back to Firmware Compliance -> Create Baseline Select your local catalog Give it a name Add the hosts you discovered Here is what it looks like in action https://youtu.be/p7pxMX-UAJw Example With Subfolder I wanted to confirm an old bug had been cleared out so I also ran it using a subfolder. See https://www.youtube.com/watch?v=iKuCgkBAzu0 to see a BIOS upgrade from start to finish. Note : In all instances the baseLocation field for me was empty.","title":"Offline Updates with OpenManage Enterprise"},{"location":"Offline%20Updates%20with%20OpenManage%20Enterprise/#offline-updates-with-openmanage-enterprise","text":"","title":"Offline Updates with OpenManage Enterprise"},{"location":"Offline%20Updates%20with%20OpenManage%20Enterprise/#versions","text":"","title":"Versions"},{"location":"Offline%20Updates%20with%20OpenManage%20Enterprise/#my-operating-system","text":"[root@dellrepo html]# cat /etc/*-release CentOS Linux release 8.3.2011 NAME=\"CentOS Linux\" VERSION=\"8\" ID=\"centos\" ID_LIKE=\"rhel fedora\" VERSION_ID=\"8\" PLATFORM_ID=\"platform:el8\" PRETTY_NAME=\"CentOS Linux 8\" ANSI_COLOR=\"0;31\" CPE_NAME=\"cpe:/o:centos:centos:8\" HOME_URL=\"https://centos.org/\" BUG_REPORT_URL=\"https://bugs.centos.org/\" CENTOS_MANTISBT_PROJECT=\"CentOS-8\" CENTOS_MANTISBT_PROJECT_VERSION=\"8\" CentOS Linux release 8.3.2011 CentOS Linux release 8.3.2011","title":"My Operating System"},{"location":"Offline%20Updates%20with%20OpenManage%20Enterprise/#ome-version","text":"","title":"OME Version"},{"location":"Offline%20Updates%20with%20OpenManage%20Enterprise/#instructions","text":"Download Dell Repository Manager Run it with /opt/dell/dellemcrepositorymanager/drm.sh NOTE: Running it with root does not work! You will get an error: GUI interface is not supported by this operating system. Click add repository Select the systems for which you want to download updates under select systems Make sure you select Windows-64 as one of the types which will be available chmod +x <binary name> then run with ./<binary_name> . I chose to distribute the repository using HTTP with Apache dnf install httpd sudo systemctl start --now httpd Next you have to synchronize the repository by clicking download. I downloaded my files to /opt/dell/catalogs/fc640 You then have to go open the Dell EMC Repository Manager -> Export -> Export. You will need to select which repositories you want to export and then you will want to select share and a save location. WARNING You have to download the Windows 64 bit versions of the updates for it to work! Even if you are using Linux the idrac only accepts the Windows EXE files. The export will generate a catalog file when you export. This is what OME will need to reference when you add the catalog. I have included a copy of mine so you can see what it looks like. You can see the progress of the export in the jobs manager: Now go to OME catalog management and hit add (Firmware compliance -> catalog Management -> Add) Configure your repository Share Address: (nothing else) Catalog File Path: /catalog.xml (cannot have anything else) NOTE: The catalog my have a different name depending on how you exported it! Go back to Firmware Compliance -> Create Baseline Select your local catalog Give it a name Add the hosts you discovered Here is what it looks like in action https://youtu.be/p7pxMX-UAJw","title":"Instructions"},{"location":"Offline%20Updates%20with%20OpenManage%20Enterprise/#example-with-subfolder","text":"I wanted to confirm an old bug had been cleared out so I also ran it using a subfolder. See https://www.youtube.com/watch?v=iKuCgkBAzu0 to see a BIOS upgrade from start to finish. Note : In all instances the baseLocation field for me was empty.","title":"Example With Subfolder"},{"location":"OpenFlow%20on%204112F-ON/","text":"OpenFlow on 4112F-ON Create OpenFlow Load Balancer Files Reading Material Overview My Configuration Switch Version Info Setup Setup Controller On Host Workstation Setup OpenFlow on the Switch Enable OpenFlow Configure Management Configure OpenFlow Controller Running the Code Supported Protocols Helpful Commands Personal Notes Things We Want Protocols Things to mention Use Cases Problems Files See here for a listing of files and source code. Reading Material Open Flow Switch Specification v1.3.1 Dell OpenFlow Deployment and User Guide 3.0 OS10 Setup Instructions Overview My Configuration Controller is running on Windows in PyCharm while I'm testing. I'll move it to RHEL when I'm done. I am using a S4112F-ON I am using a Ryu OpenFlow controller Switch Version Info Dell EMC Networking OS10 Enterprise Copyright (c) 1999-2020 by Dell Inc. All Rights Reserved. OS Version: 10.5.1.0 Build Version: 10.5.1.0.124 Build Time: 2020-02-12T09:05:20+0000 System Type: S4112F-ON Architecture: x86_64 Up Time: 00:03:52 Setup Setup Controller pip install -r requirements.txt On Host Workstation Make sure you use sudo or things will go wrong curl -sL https://deb.nodesource.com/setup_14.x | sudo -E bash - sudo apt-get install -y nodejs sudo npm install -g @angular/cli sudo ng add @angular/material You can drop the -g if you want to install angular locally in the directory instead of globally. You will have to prefix your commands with npx -p @angular/cli ng To setup debugging do the following: Go to https://marketplace.visualstudio.com/items?itemName=msjsdiag.debugger-for-chrome and install the addon for Visual Studio Code Go to the debugging tab in Visual Studio code, hit the down arrow next to launch program and click launch Chrome. I used the following configuration: { // Use IntelliSense to learn about possible attributes. // Hover to view descriptions of existing attributes. // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387 \"version\": \"0.2.0\", \"configurations\": [ { \"type\": \"chrome\", \"request\": \"launch\", \"name\": \"Launch Chrome against localhost\", \"url\": \"http://localhost:4200\", \"webRoot\": \"c:\\\\Users\\\\grant\\\\Documents\\\\trafficshaper\\\\angular\" } ] } Setup OpenFlow on the Switch Enable OpenFlow On the switch run: OS10# configure terminal OS10(config)# openflow OS10(config-openflow)# mode openflow-only Configurations not relevant to openflow mode will be removed from the startup-configuration and system will be rebooted. Do you want to proceed? [confirm yes/no]:yes Configure Management OS10(conf-if-ma-1/1/1)# interface mgmt 1/1/1 OS10(conf-if-ma-1/1/1)# ip address <SOME MANAGEMENT IP>/24 OS10(conf-if-ma-1/1/1)# no shutdown OS10(conf-if-ma-1/1/1)# exit Configure OpenFlow Controller OS10# configure terminal OS10(config)# openflow OS10(config-openflow)# switch of-switch-1 OS10(config-openflow-switch)# controller ipv4 <YOUR_CONTROLLER_IP> port 6633 OS10(config-openflow-switch)# protocol-version 1.3 OS10(config-openflow-switch)# no shutdown Running the Code python main.py Supported Protocols TCP UDP ICMP Helpful Commands Personal Notes Things We Want Protocols HTTP TLS DNS SSH Things to mention Inline decryption possibilities Use Cases I want to tie a sensor directly to a DC. So all things for that DC go to one sensor A couple of dropdown boxes in a statement and an execute button. One of those things could be an IP address, or a port, or a protocol, physical port Problems need to make sure we don't receive a reject message need to make it so outports and inports persist if something is an input port do we want to stop them from using redirect port I need to go back and make sure that when compressed tiles move to the next line I need to handle getting flows for the openflow controller's interface Need to add error handling if the server is unavailable Need to update the getPorts documentation","title":"OpenFlow on 4112F-ON"},{"location":"OpenFlow%20on%204112F-ON/#openflow-on-4112f-on","text":"Create OpenFlow Load Balancer Files Reading Material Overview My Configuration Switch Version Info Setup Setup Controller On Host Workstation Setup OpenFlow on the Switch Enable OpenFlow Configure Management Configure OpenFlow Controller Running the Code Supported Protocols Helpful Commands Personal Notes Things We Want Protocols Things to mention Use Cases Problems","title":"OpenFlow on 4112F-ON"},{"location":"OpenFlow%20on%204112F-ON/#files","text":"See here for a listing of files and source code.","title":"Files"},{"location":"OpenFlow%20on%204112F-ON/#reading-material","text":"Open Flow Switch Specification v1.3.1 Dell OpenFlow Deployment and User Guide 3.0 OS10 Setup Instructions","title":"Reading Material"},{"location":"OpenFlow%20on%204112F-ON/#overview","text":"","title":"Overview"},{"location":"OpenFlow%20on%204112F-ON/#my-configuration","text":"Controller is running on Windows in PyCharm while I'm testing. I'll move it to RHEL when I'm done. I am using a S4112F-ON I am using a Ryu OpenFlow controller","title":"My Configuration"},{"location":"OpenFlow%20on%204112F-ON/#switch-version-info","text":"Dell EMC Networking OS10 Enterprise Copyright (c) 1999-2020 by Dell Inc. All Rights Reserved. OS Version: 10.5.1.0 Build Version: 10.5.1.0.124 Build Time: 2020-02-12T09:05:20+0000 System Type: S4112F-ON Architecture: x86_64 Up Time: 00:03:52","title":"Switch Version Info"},{"location":"OpenFlow%20on%204112F-ON/#setup","text":"","title":"Setup"},{"location":"OpenFlow%20on%204112F-ON/#setup-controller","text":"pip install -r requirements.txt","title":"Setup Controller"},{"location":"OpenFlow%20on%204112F-ON/#on-host-workstation","text":"Make sure you use sudo or things will go wrong curl -sL https://deb.nodesource.com/setup_14.x | sudo -E bash - sudo apt-get install -y nodejs sudo npm install -g @angular/cli sudo ng add @angular/material You can drop the -g if you want to install angular locally in the directory instead of globally. You will have to prefix your commands with npx -p @angular/cli ng To setup debugging do the following: Go to https://marketplace.visualstudio.com/items?itemName=msjsdiag.debugger-for-chrome and install the addon for Visual Studio Code Go to the debugging tab in Visual Studio code, hit the down arrow next to launch program and click launch Chrome. I used the following configuration: { // Use IntelliSense to learn about possible attributes. // Hover to view descriptions of existing attributes. // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387 \"version\": \"0.2.0\", \"configurations\": [ { \"type\": \"chrome\", \"request\": \"launch\", \"name\": \"Launch Chrome against localhost\", \"url\": \"http://localhost:4200\", \"webRoot\": \"c:\\\\Users\\\\grant\\\\Documents\\\\trafficshaper\\\\angular\" } ] }","title":"On Host Workstation"},{"location":"OpenFlow%20on%204112F-ON/#setup-openflow-on-the-switch","text":"","title":"Setup OpenFlow on the Switch"},{"location":"OpenFlow%20on%204112F-ON/#enable-openflow","text":"On the switch run: OS10# configure terminal OS10(config)# openflow OS10(config-openflow)# mode openflow-only Configurations not relevant to openflow mode will be removed from the startup-configuration and system will be rebooted. Do you want to proceed? [confirm yes/no]:yes","title":"Enable OpenFlow"},{"location":"OpenFlow%20on%204112F-ON/#configure-management","text":"OS10(conf-if-ma-1/1/1)# interface mgmt 1/1/1 OS10(conf-if-ma-1/1/1)# ip address <SOME MANAGEMENT IP>/24 OS10(conf-if-ma-1/1/1)# no shutdown OS10(conf-if-ma-1/1/1)# exit","title":"Configure Management"},{"location":"OpenFlow%20on%204112F-ON/#configure-openflow-controller","text":"OS10# configure terminal OS10(config)# openflow OS10(config-openflow)# switch of-switch-1 OS10(config-openflow-switch)# controller ipv4 <YOUR_CONTROLLER_IP> port 6633 OS10(config-openflow-switch)# protocol-version 1.3 OS10(config-openflow-switch)# no shutdown","title":"Configure OpenFlow Controller"},{"location":"OpenFlow%20on%204112F-ON/#running-the-code","text":"python main.py","title":"Running the Code"},{"location":"OpenFlow%20on%204112F-ON/#supported-protocols","text":"TCP UDP ICMP","title":"Supported Protocols"},{"location":"OpenFlow%20on%204112F-ON/#helpful-commands","text":"","title":"Helpful Commands"},{"location":"OpenFlow%20on%204112F-ON/#personal-notes","text":"","title":"Personal Notes"},{"location":"OpenFlow%20on%204112F-ON/#things-we-want","text":"","title":"Things We Want"},{"location":"OpenFlow%20on%204112F-ON/#protocols","text":"HTTP TLS DNS SSH","title":"Protocols"},{"location":"OpenFlow%20on%204112F-ON/#things-to-mention","text":"Inline decryption possibilities","title":"Things to mention"},{"location":"OpenFlow%20on%204112F-ON/#use-cases","text":"I want to tie a sensor directly to a DC. So all things for that DC go to one sensor A couple of dropdown boxes in a statement and an execute button. One of those things could be an IP address, or a port, or a protocol, physical port","title":"Use Cases"},{"location":"OpenFlow%20on%204112F-ON/#problems","text":"need to make sure we don't receive a reject message need to make it so outports and inports persist if something is an input port do we want to stop them from using redirect port I need to go back and make sure that when compressed tiles move to the next line I need to handle getting flows for the openflow controller's interface Need to add error handling if the server is unavailable Need to update the getPorts documentation","title":"Problems"},{"location":"OpenFlow%20on%204112F-ON/angular/","text":"Trafficshapergui This project was generated with Angular CLI version 9.1.1. Development server Run ng serve for a dev server. Navigate to http://localhost:4200/ . The app will automatically reload if you change any of the source files. Code scaffolding Run ng generate component component-name to generate a new component. You can also use ng generate directive|pipe|service|class|guard|interface|enum|module . Build Run ng build to build the project. The build artifacts will be stored in the dist/ directory. Use the --prod flag for a production build. Running unit tests Run ng test to execute the unit tests via Karma . Running end-to-end tests Run ng e2e to execute the end-to-end tests via Protractor . Further help To get more help on the Angular CLI use ng help or go check out the Angular CLI README .","title":"Trafficshapergui"},{"location":"OpenFlow%20on%204112F-ON/angular/#trafficshapergui","text":"This project was generated with Angular CLI version 9.1.1.","title":"Trafficshapergui"},{"location":"OpenFlow%20on%204112F-ON/angular/#development-server","text":"Run ng serve for a dev server. Navigate to http://localhost:4200/ . The app will automatically reload if you change any of the source files.","title":"Development server"},{"location":"OpenFlow%20on%204112F-ON/angular/#code-scaffolding","text":"Run ng generate component component-name to generate a new component. You can also use ng generate directive|pipe|service|class|guard|interface|enum|module .","title":"Code scaffolding"},{"location":"OpenFlow%20on%204112F-ON/angular/#build","text":"Run ng build to build the project. The build artifacts will be stored in the dist/ directory. Use the --prod flag for a production build.","title":"Build"},{"location":"OpenFlow%20on%204112F-ON/angular/#running-unit-tests","text":"Run ng test to execute the unit tests via Karma .","title":"Running unit tests"},{"location":"OpenFlow%20on%204112F-ON/angular/#running-end-to-end-tests","text":"Run ng e2e to execute the end-to-end tests via Protractor .","title":"Running end-to-end tests"},{"location":"OpenFlow%20on%204112F-ON/angular/#further-help","text":"To get more help on the Angular CLI use ng help or go check out the Angular CLI README .","title":"Further help"},{"location":"OpenFlow%20on%204112F-ON/openflow_bug/","text":"Bug in Ryu datapath_id Overview Problem Ryu incorrectly truncates datapath_id from 16 characters to 15. I found the problem while testing this Ryu example . Proof of Concept Running the code in debug mode produces the below: You can see that the switch ID 150013889525632 which is only 15 characters instead of the required 16. To confirm that the problem was not with what the switch was sending I captured the response in Wireshark. You can see the switch correctly adheres to the 64bit datapath_id requirement. Detailed Troubleshooting I found the problem when none of the routes ran when browsing to the address: http://127.0.0.1:8080/simpleswitch/mactable/150013889525632 I eventually realized it is because of the following line: @route('/simpleswitch', url, methods=['PUT'], requirements={'dpid': dpid_lib.DPID_PATTERN}) DPID_PATTERN's definition is as follows: _DPID_LEN = 16 _DPID_FMT = '%0{0}x'.format(_DPID_LEN) DPID_PATTERN = r'[0-9a-f]{%d}' % _DPID_LEN You can see this more directly by looking at the regex as it is used in the WSGI call produced from the above line. As you can see from {16} the switch ID Ryu produces does not match because it is a character short. You can fix the problem by using the URL: http://127.0.0.1:8080/simpleswitch/mactable/0150013889525632 However, that then causes other code to fail because it is is looking for the original switch ID of 150013889525632. Reproducing My Configuration Controller is running on Windows in PyCharm Controller: Ryu Switch: 4112F-ON Switch Version Info Dell EMC Networking OS10 Enterprise Copyright (c) 1999-2020 by Dell Inc. All Rights Reserved. OS Version: 10.5.1.0 Build Version: 10.5.1.0.124 Build Time: 2020-02-12T09:05:20+0000 System Type: S4112F-ON Architecture: x86_64 Up Time: 4 days 09:16:43 Setup Enable OpenFlow on the Switch On the switch run: OS10# configure terminal OS10(config)# openflow OS10(config-openflow)# mode openflow-only Configurations not relevant to openflow mode will be removed from the startup-configuration and system will be rebooted. Do you want to proceed? [confirm yes/no]:yes Configure OpenFlow OS10# configure terminal OS10(config)# openflow OS10(config-openflow)# switch of-switch-1 OS10(config-openflow-switch)# controller ipv4 <YOUR_CONTROLLER_IP> port 6633 OS10(config-openflow-switch)# no shutdown See the switch config for details. Run the Code Run pip install ryu to install Ryu and its dependencies. I have included my Ryu app as it currently was when I found the bug in the file main.py . I used PyCharm to perform debugging which required me to adjust the debug configuration to the below: This will allow you to use PyCharm's debugger. Alternatively, you can delete everything after line 358 in main.py and use ryu-manager to run the application. To run the code there is an application called ryu-manager . To run the code you have to run ryu-manager main.py .","title":"Bug in Ryu datapath_id"},{"location":"OpenFlow%20on%204112F-ON/openflow_bug/#bug-in-ryu-datapath_id","text":"","title":"Bug in Ryu datapath_id"},{"location":"OpenFlow%20on%204112F-ON/openflow_bug/#overview","text":"","title":"Overview"},{"location":"OpenFlow%20on%204112F-ON/openflow_bug/#problem","text":"Ryu incorrectly truncates datapath_id from 16 characters to 15. I found the problem while testing this Ryu example .","title":"Problem"},{"location":"OpenFlow%20on%204112F-ON/openflow_bug/#proof-of-concept","text":"Running the code in debug mode produces the below: You can see that the switch ID 150013889525632 which is only 15 characters instead of the required 16. To confirm that the problem was not with what the switch was sending I captured the response in Wireshark. You can see the switch correctly adheres to the 64bit datapath_id requirement.","title":"Proof of Concept"},{"location":"OpenFlow%20on%204112F-ON/openflow_bug/#detailed-troubleshooting","text":"I found the problem when none of the routes ran when browsing to the address: http://127.0.0.1:8080/simpleswitch/mactable/150013889525632 I eventually realized it is because of the following line: @route('/simpleswitch', url, methods=['PUT'], requirements={'dpid': dpid_lib.DPID_PATTERN}) DPID_PATTERN's definition is as follows: _DPID_LEN = 16 _DPID_FMT = '%0{0}x'.format(_DPID_LEN) DPID_PATTERN = r'[0-9a-f]{%d}' % _DPID_LEN You can see this more directly by looking at the regex as it is used in the WSGI call produced from the above line. As you can see from {16} the switch ID Ryu produces does not match because it is a character short. You can fix the problem by using the URL: http://127.0.0.1:8080/simpleswitch/mactable/0150013889525632 However, that then causes other code to fail because it is is looking for the original switch ID of 150013889525632.","title":"Detailed Troubleshooting"},{"location":"OpenFlow%20on%204112F-ON/openflow_bug/#reproducing","text":"","title":"Reproducing"},{"location":"OpenFlow%20on%204112F-ON/openflow_bug/#my-configuration","text":"Controller is running on Windows in PyCharm Controller: Ryu Switch: 4112F-ON","title":"My Configuration"},{"location":"OpenFlow%20on%204112F-ON/openflow_bug/#switch-version-info","text":"Dell EMC Networking OS10 Enterprise Copyright (c) 1999-2020 by Dell Inc. All Rights Reserved. OS Version: 10.5.1.0 Build Version: 10.5.1.0.124 Build Time: 2020-02-12T09:05:20+0000 System Type: S4112F-ON Architecture: x86_64 Up Time: 4 days 09:16:43","title":"Switch Version Info"},{"location":"OpenFlow%20on%204112F-ON/openflow_bug/#setup","text":"","title":"Setup"},{"location":"OpenFlow%20on%204112F-ON/openflow_bug/#enable-openflow-on-the-switch","text":"On the switch run: OS10# configure terminal OS10(config)# openflow OS10(config-openflow)# mode openflow-only Configurations not relevant to openflow mode will be removed from the startup-configuration and system will be rebooted. Do you want to proceed? [confirm yes/no]:yes","title":"Enable OpenFlow on the Switch"},{"location":"OpenFlow%20on%204112F-ON/openflow_bug/#configure-openflow","text":"OS10# configure terminal OS10(config)# openflow OS10(config-openflow)# switch of-switch-1 OS10(config-openflow-switch)# controller ipv4 <YOUR_CONTROLLER_IP> port 6633 OS10(config-openflow-switch)# no shutdown See the switch config for details.","title":"Configure OpenFlow"},{"location":"OpenFlow%20on%204112F-ON/openflow_bug/#run-the-code","text":"Run pip install ryu to install Ryu and its dependencies. I have included my Ryu app as it currently was when I found the bug in the file main.py . I used PyCharm to perform debugging which required me to adjust the debug configuration to the below: This will allow you to use PyCharm's debugger. Alternatively, you can delete everything after line 358 in main.py and use ryu-manager to run the application. To run the code there is an application called ryu-manager . To run the code you have to run ryu-manager main.py .","title":"Run the Code"},{"location":"PCIev3%20vs%20v4/","text":"PCIev3 vs v4 PCIe v3 vs v4 PCIev3 Speed Per lane: 1GB/s unidirectional PCIev3 Max unidirectional speed: 16GB/s PCIev4 Speed per lane: 2GB/s unidirectional PCIev4 Max unidirectional speed: 32GB/s Samsung NF1 Drive According to this article a single NF1 drive runs at 3,000MB/s sequential read speed and a 1900 MB/s sequential write speed. Xeon Second Gen Scalable Procs According to this article a second gen xeon proc provides 48 PCIe v3 lanes. Two procs would mean 96. This means that a 2 proc server with these chips could theoretically run up to 96GB/s unidirectional speed. 96-36 = 60 64 SuperMicro 1029P-NMR36L Dual socket Xeon gen 2 procs with space for 32 hot-swap pci-e3 nf1 drives plus 4 SATA 3 M.2 drive bays. In a scenario where only drives were in the box and under theoretical perfect conditions, you could see a 96GB/s read speed. (Bascially impossible you'd ever actually see that)","title":"PCIev3 vs v4"},{"location":"PCIev3%20vs%20v4/#pciev3-vs-v4","text":"","title":"PCIev3 vs v4"},{"location":"PCIev3%20vs%20v4/#pcie-v3-vs-v4","text":"PCIev3 Speed Per lane: 1GB/s unidirectional PCIev3 Max unidirectional speed: 16GB/s PCIev4 Speed per lane: 2GB/s unidirectional PCIev4 Max unidirectional speed: 32GB/s","title":"PCIe v3 vs v4"},{"location":"PCIev3%20vs%20v4/#samsung-nf1-drive","text":"According to this article a single NF1 drive runs at 3,000MB/s sequential read speed and a 1900 MB/s sequential write speed.","title":"Samsung NF1 Drive"},{"location":"PCIev3%20vs%20v4/#xeon-second-gen-scalable-procs","text":"According to this article a second gen xeon proc provides 48 PCIe v3 lanes. Two procs would mean 96. This means that a 2 proc server with these chips could theoretically run up to 96GB/s unidirectional speed. 96-36 = 60 64","title":"Xeon Second Gen Scalable Procs"},{"location":"PCIev3%20vs%20v4/#supermicro-1029p-nmr36l","text":"Dual socket Xeon gen 2 procs with space for 32 hot-swap pci-e3 nf1 drives plus 4 SATA 3 M.2 drive bays. In a scenario where only drives were in the box and under theoretical perfect conditions, you could see a 96GB/s read speed. (Bascially impossible you'd ever actually see that)","title":"SuperMicro 1029P-NMR36L"},{"location":"Playing%20with%20virsh/","text":"Playing with virsh VMs List all VMs virsh list --all Network Stuff Get network info virsh net-info Dump network info virsh net-dumpxml xhubnet List all networks virsh net-list --all IPables Heads up, iptables -L does not truly list all the rules. It just lists the rules in the current table. If you want to see the NAT rules you can run: iptables -t nat -L Heads up, iptables will by default try to resolve names. To skip this do: iptables -t nat -vnL Adding a destination NAT rule: iptables -t nat -A PREROUTING -p tcp --dport 8000 -j DNAT --to 10.125.120.21 Delete a rule from iptables: iptables -t nat -D POSTROUTING -p tcp --dport 50000 -j SNAT --to 5.136.13.37","title":"Playing with virsh"},{"location":"Playing%20with%20virsh/#playing-with-virsh","text":"","title":"Playing with virsh"},{"location":"Playing%20with%20virsh/#vms","text":"List all VMs virsh list --all","title":"VMs"},{"location":"Playing%20with%20virsh/#network-stuff","text":"Get network info virsh net-info Dump network info virsh net-dumpxml xhubnet List all networks virsh net-list --all","title":"Network Stuff"},{"location":"Playing%20with%20virsh/#ipables","text":"Heads up, iptables -L does not truly list all the rules. It just lists the rules in the current table. If you want to see the NAT rules you can run: iptables -t nat -L Heads up, iptables will by default try to resolve names. To skip this do: iptables -t nat -vnL Adding a destination NAT rule: iptables -t nat -A PREROUTING -p tcp --dport 8000 -j DNAT --to 10.125.120.21 Delete a rule from iptables: iptables -t nat -D POSTROUTING -p tcp --dport 50000 -j SNAT --to 5.136.13.37","title":"IPables"},{"location":"Reset%20OS10%20Admin%20Password/","text":"Reset OS10 Admin Password Video: https://youtu.be/0VfJCa8s7yo Connect to the serial console port. The serial settings are 115,200 baud, 8 data bits, and no parity. Reboot or power up the system. Press ESC at the Grub prompt to view the boot menu. The OS10-A partition is selected by default. +-------------------------------------+ |*OS10-A | | OS10-B | | ONIE | +-------------------------------------+ Press e to open the OS10 GRUB editor. Use the arrow keys to navigate to the end of the line that has set os_debug_args= and then add init=/bin/bash. +---------------------------------------------------------+ |setparams 'OS10-A' | | | | set os_debug_args=\"init=/bin/bash\" | | select_image A | | boot_os | | | +---------------------------------------------------------+ Press Alt + 0. The system boots to a root shell without a password. At the root prompt run passwd admin and set the password to whatever you want Run reboot -f to reboot the system When the system reboots log into the admin account with your new password. That password is temporary and is not permanently written to the system . Enter configuration mode with configure terminal and then run username admin password <YOURPASSWORD> role sysadmin to change the password Run write memory to make the changes permanent. Startup Config Location /config/etc/opt/dell/os10/db_init/startup.xml","title":"Reset OS10 Admin Password"},{"location":"Reset%20OS10%20Admin%20Password/#reset-os10-admin-password","text":"Video: https://youtu.be/0VfJCa8s7yo Connect to the serial console port. The serial settings are 115,200 baud, 8 data bits, and no parity. Reboot or power up the system. Press ESC at the Grub prompt to view the boot menu. The OS10-A partition is selected by default. +-------------------------------------+ |*OS10-A | | OS10-B | | ONIE | +-------------------------------------+ Press e to open the OS10 GRUB editor. Use the arrow keys to navigate to the end of the line that has set os_debug_args= and then add init=/bin/bash. +---------------------------------------------------------+ |setparams 'OS10-A' | | | | set os_debug_args=\"init=/bin/bash\" | | select_image A | | boot_os | | | +---------------------------------------------------------+ Press Alt + 0. The system boots to a root shell without a password. At the root prompt run passwd admin and set the password to whatever you want Run reboot -f to reboot the system When the system reboots log into the admin account with your new password. That password is temporary and is not permanently written to the system . Enter configuration mode with configure terminal and then run username admin password <YOURPASSWORD> role sysadmin to change the password Run write memory to make the changes permanent.","title":"Reset OS10 Admin Password"},{"location":"Reset%20OS10%20Admin%20Password/#startup-config-location","text":"/config/etc/opt/dell/os10/db_init/startup.xml","title":"Startup Config Location"},{"location":"Reset%20OS10%20Admin%20Password/problems/","text":"Resetting Dell OS10 Password Working Instructions Connect to the serial console port. The serial settings are 115,200 baud, 8 data bits, and no parity. Reboot or power up the system. Press ESC at the Grub prompt to view the boot menu. The OS10-A partition is selected by default. +-------------------------------------+ |*OS10-A | | OS10-B | | ONIE | +-------------------------------------+ Press e to open the OS10 GRUB editor. Use the arrow keys to navigate to the end of the line that has set os_debug_args= and then add init=/bin/bash. +---------------------------------------------------------+ |setparams 'OS10-A' | | | | set os_debug_args=\"init=/bin/bash\" | | select_image A | | boot_os | | | +---------------------------------------------------------+ Press Alt + 0. The system boots to a root shell without a password. At the root prompt run passwd admin and set the password to whatever you want Run reboot -f to reboot the system When the system reboots log into the admin account with your new password. That password is temporary and is not permanently written to the system . Enter configuration mode with configure terminal and then run username admin password <YOURPASSWORD> role sysadmin to change the password Run write memory to make the changes permanent. Problems with online documentation Based on official instructions here Instructions are for resetting the linuxadmin account. Majority of audience are network engineers who have little understanding of Linux. Even for me, a Linux engineer who also does network engineering, and has done extensive work with OS10 had no idea a linuxadmin account even existed much less what it was used for. Having worked extensively with customers in the field, I have never met one that actually knew that OS10 is just Debian Linux much less understood the relationship between the Linux command line and the OS10 shell. The instructions seem to assume that you understand the relationship between the OS10 shell, the Linux command line, and the Linux users. The vast majority of the users of OS10 will not understand these distinctions. In my case the customer was very confused as to what linuxadmin was. They had never used it and after resetting the password for linuxadmin tried logging into the admin account and was confused when it didn't work They were then further confused because they tried to log into OS10 B thinking it was some sort of synchronized backup. Since it was identical to OS10 A they thought changes made there would synchronize between the two. The instructions do not tell you that the changes made to the password are temporary and will not persist through reboot. As far as I can tell, we do not have any documentation describing how to reset the admin account for the OS10 command line which is what virtually all customers will want to do. Suggestions Ensure that instructions for resetting the admin account password are easily accessible online Clarify the purpose of the linuxadmin account If there are plans to expose the Linux nature of OS10 to the customer, provide documentation clearly articulating the relationship between Linux and OS10 from the perspective of what is relevant to someone with a network engineering background. I am happy to help with this.","title":"Resetting Dell OS10 Password"},{"location":"Reset%20OS10%20Admin%20Password/problems/#resetting-dell-os10-password","text":"","title":"Resetting Dell OS10 Password"},{"location":"Reset%20OS10%20Admin%20Password/problems/#working-instructions","text":"Connect to the serial console port. The serial settings are 115,200 baud, 8 data bits, and no parity. Reboot or power up the system. Press ESC at the Grub prompt to view the boot menu. The OS10-A partition is selected by default. +-------------------------------------+ |*OS10-A | | OS10-B | | ONIE | +-------------------------------------+ Press e to open the OS10 GRUB editor. Use the arrow keys to navigate to the end of the line that has set os_debug_args= and then add init=/bin/bash. +---------------------------------------------------------+ |setparams 'OS10-A' | | | | set os_debug_args=\"init=/bin/bash\" | | select_image A | | boot_os | | | +---------------------------------------------------------+ Press Alt + 0. The system boots to a root shell without a password. At the root prompt run passwd admin and set the password to whatever you want Run reboot -f to reboot the system When the system reboots log into the admin account with your new password. That password is temporary and is not permanently written to the system . Enter configuration mode with configure terminal and then run username admin password <YOURPASSWORD> role sysadmin to change the password Run write memory to make the changes permanent.","title":"Working Instructions"},{"location":"Reset%20OS10%20Admin%20Password/problems/#problems-with-online-documentation","text":"Based on official instructions here Instructions are for resetting the linuxadmin account. Majority of audience are network engineers who have little understanding of Linux. Even for me, a Linux engineer who also does network engineering, and has done extensive work with OS10 had no idea a linuxadmin account even existed much less what it was used for. Having worked extensively with customers in the field, I have never met one that actually knew that OS10 is just Debian Linux much less understood the relationship between the Linux command line and the OS10 shell. The instructions seem to assume that you understand the relationship between the OS10 shell, the Linux command line, and the Linux users. The vast majority of the users of OS10 will not understand these distinctions. In my case the customer was very confused as to what linuxadmin was. They had never used it and after resetting the password for linuxadmin tried logging into the admin account and was confused when it didn't work They were then further confused because they tried to log into OS10 B thinking it was some sort of synchronized backup. Since it was identical to OS10 A they thought changes made there would synchronize between the two. The instructions do not tell you that the changes made to the password are temporary and will not persist through reboot. As far as I can tell, we do not have any documentation describing how to reset the admin account for the OS10 command line which is what virtually all customers will want to do.","title":"Problems with online documentation"},{"location":"Reset%20OS10%20Admin%20Password/problems/#suggestions","text":"Ensure that instructions for resetting the admin account password are easily accessible online Clarify the purpose of the linuxadmin account If there are plans to expose the Linux nature of OS10 to the customer, provide documentation clearly articulating the relationship between Linux and OS10 from the perspective of what is relevant to someone with a network engineering background. I am happy to help with this.","title":"Suggestions"},{"location":"Run%20VPN%20on%20OS10/","text":"Run VPN on OS10 My Configuration Dell 4112F-ON Dell EMC Networking OS10 Enterprise Copyright (c) 1999-2020 by Dell Inc. All Rights Reserved. OS Version: 10.5.0.4 Build Version: 10.5.0.4.638 Build Time: 2020-01-30T21:08:56+0000 System Type: S4112F-ON Architecture: x86_64 Up Time: 2 days 03:54:07 CentOS [root@centos ~]# cat /etc/*-release CentOS Linux release 7.6.1810 (Core) NAME=\"CentOS Linux\" VERSION=\"7 (Core)\" ID=\"centos\" ID_LIKE=\"rhel fedora\" VERSION_ID=\"7\" PRETTY_NAME=\"CentOS Linux 7 (Core)\" ANSI_COLOR=\"0;31\" CPE_NAME=\"cpe:/o:centos:centos:7\" HOME_URL=\"https://www.centos.org/\" BUG_REPORT_URL=\"https://bugs.centos.org/\" CENTOS_MANTISBT_PROJECT=\"CentOS-7\" CENTOS_MANTISBT_PROJECT_VERSION=\"7\" REDHAT_SUPPORT_PRODUCT=\"centos\" REDHAT_SUPPORT_PRODUCT_VERSION=\"7\" CentOS Linux release 7.6.1810 (Core) CentOS Linux release 7.6.1810 (Core) Testing Topology Switch Configuration Research Sources Helpful Book Basics of Network Processor Packet Processing How Network Processors Work NPU Interface Problem The one big gotcha with doing this is that when you drop to the command line in OS10 and do a ip a s , the interfaces you see that look like physical interfaces ex: 13: e101-001-0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc multiq master br32 state UP group default qlen 1000 link/ether 50:9a:4c:d6:0a:71 brd ff:ff:ff:ff:ff:ff 14: e101-002-0: <BROADCAST,MULTICAST> mtu 1500 qdisc noop master br1 state DOWN group default qlen 1000 link/ether 50:9a:4c:d6:0a:72 brd ff:ff:ff:ff:ff:ff 15: e101-003-0: <BROADCAST,MULTICAST> mtu 1500 qdisc noop master br1 state DOWN group default qlen 1000 link/ether 50:9a:4c:d6:0a:73 brd ff:ff:ff:ff:ff:ff 16: e101-004-0: <BROADCAST,MULTICAST> mtu 1500 qdisc noop master br1 state DOWN group default qlen 1000 link/ether 50:9a:4c:d6:0a:74 brd ff:ff:ff:ff:ff:ff are not actual physical interfaces. Under the hood the operating system is actually using tap interfaces. Inside the switch there are two processors - a regular x86 processor and a separate processor called the Network Processing Unit (NPU). The interfaces are connected to the NPU. Most traffic that comes in on the physical interfaces managed by the NPU does not flow up to the x86 chip. This means that if you do a tcpdump on one of the interfaces you see in ip a s you will see very little. In fact, the only traffic you will see is management traffic which is handled by the Linux kernel. This means if you want to set up a VPN you have to have a way to make sure all the traffic is visible to the Linux kernel. Fortunately, there is a way to make this happen. VLAN interfaces are virtual and subsequently are handled entirely by the Linux kernel. In fact, VLAN interfaces actually show up under the hood as bridge interfaces: 29: br32: <BROADCAST,MULTICAST,PROMISC,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default qlen 1000 link/ether 50:9a:4c:d6:0a:a1 brd ff:ff:ff:ff:ff:ff inet 192.168.32.1/24 brd 255.168.32.255 scope global br32 valid_lft forever preferred_lft forever inet6 fe80::529a:4cff:fed6:aa1/64 scope link valid_lft forever preferred_lft forever 41: br33: <BROADCAST,MULTICAST,PROMISC,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default qlen 1000 link/ether 50:9a:4c:d6:0a:a1 brd ff:ff:ff:ff:ff:ff inet 192.168.33.1/24 brd 255.168.33.255 scope global br33 valid_lft forever preferred_lft forever inet6 fe80::529a:4cff:fed6:aa1/64 scope link valid_lft forever preferred_lft forever These two correspond to interface vlans 32 and 33. By using VLAN interfaces you can tie the VPN to these interfaces and everything works just fine. You just place any associated physical interfaces as access VLANs or trunks with the appropriate allowed VLANs. Installing OpenVPN as a Server on the 4112F-ON Note: I ran a VPN server on my switch, but you could just as easily make the switch a point to point VPN gateway connecting to a PFSense instance such that anything that can reach the switch could participate in a multipoint network. On the 4112F-ON: Enter configuration mode from user mode by running en and then config <enter> Run ip name-server 192.168.1.1 to add a name server. Run write mem in enable mode to save your configuration changes. Run system bash Before continuing, make sure that the time is correct on the device. WARNING If you do not do this and you generate certificates, none of the encryption will work and you will have to recreate all of your certificates! Run sudo apt-get install -y openvpn vim . I installed vim because I don't hate myself. I used this script from git.io/vpn to install OpenVPN. Having done the entire thing manually before, I can tell you this saves a huge amount of time. To run the script run wget https://git.io/vpn -O openvpn-install.sh && chmod +x openvpn-install.sh && ./openvpn-install.sh Fill in the options as needed. I did find some things you have to tweak with their script. Perform the below to clean things up. Run vim /lib/systemd/system/openvpn@.service . Where it says --config /etc/openvpn/%i.conf , change that to --config /etc/openvpn/%i/%i.conf . For details on specifies work see this post . When you are done run systemctl daemon-reload to reload the systemd daemon. If you used my version of the script then you do not need to do this. Otherwise you need to run vim /etc/openvpn/server/server.conf and you need to prepend /etc/openvpn/server/ on several of the paths or the service won't start. See my config below: local 192.168.32.1 port 1194 proto udp dev tun ca /etc/openvpn/server/ca.crt cert /etc/openvpn/server/server.crt key /etc/openvpn/server/server.key dh /etc/openvpn/server/dh.pem auth SHA512 tls-crypt /etc/openvpn/server/tc.key topology subnet server 10.8.0.0 255.255.255.0 ifconfig-pool-persist ipp.txt push \"redirect-gateway def1 bypass-dhcp\" push \"dhcp-option DNS 192.168.1.1\" keepalive 10 120 cipher AES-256-CBC user nobody group nogroup persist-key persist-tun status openvpn-status.log verb 3 crl-verify /etc/openvpn/server/crl.pem explicit-exit-notify You may want to add something like push route 192.168.1.0 255.255.255.0 to your server config. This allows the server to push routes to the client. For example, in my case the 192.168.1.0/24 network is behind my server, so I have to push a route so that the clients know how to get to it. Just keep in mind, that hosts on your distant network must have a route back to your VPN network. Run systemctl start openvpn@server to start the server. Rerun the script to add clients. Your output should look like the below. In my case I added one client to perform the test. ``` Looks like OpenVPN is already installed. What do you want to do? 1) Add a new user 2) Revoke an existing user 3) Remove OpenVPN 4) Exit Select an option: 1 Tell me a name for the client certificate. Client name: test-client Using SSL: openssl OpenSSL 1.1.0l 10 Sep 2019 Generating a RSA private key ........+++++ .......+++++ writing new private key to '/etc/openvpn/server/easy-rsa/pki/private/test-client.key.WONcIB6m1N' ----- Using configuration from ./safessl-easyrsa.cnf Check that the request matches the signature Signature ok The Subject's Distinguished Name is as follows commonName :ASN.1 12:'test-client' Certificate is to be certified until Mar 9 00:13:09 2030 GMT (3650 days) Write out database with 1 new entries Data Base Updated Client test-client added, configuration is available at: /root/test-client.ovpn ``` Copy the contents of your client config. In my case this was from /root/test-client.ovpn and it looked like: client dev tun proto udp remote <SERVER ADDRESS> 1194 resolv-retry infinite nobind persist-key persist-tun remote-cert-tls server auth SHA512 cipher AES-256-CBC ignore-unknown-option block-outside-dns block-outside-dns verb 3 <ca> -----BEGIN CERTIFICATE----- MIIDKzCCAhOgAwIBAgIJANmH49pJjiOUMA0GCSqGSIb3DQEBCwUAMBMxETAPBgNV BAMMCENoYW5nZU1lMB4XDTIwMDMxMTAwMTA1M1oXDTMwMDMwOTAwMTA1M1owEzER MA8GA1UEAwwIQ2hhbmdlTWUwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIB AQD5FZJN5STAXRX7ZBq8CVf7DntSQTgnVVqwntKJwggTPHgwn8uMUWRdaIpXZVN5 MYTGPCICDoxdlF/2KUgH9n/L1Rlmm9RW4beXMwFJUR8NIExf5vQy03gk6JpEO1DA Pu+x0/EhXGvGo/lAEpF4rk0ZPpNEkFM71bIqhKAMAe9M5c2ZrAxqplyTz/Zl4nRm YQSsqnx3ikN+SkxdnifIBlF3MzCHqCCV9QaOkrztXHs9XFhnWpyu+OLqyP5+ipOZ gYsTDA4otjv6D9MX+BoWCZ6zSzo/kMSkM7ByZt5jjyp1lQQaYnZe8LmRkB3vcBb4 lWlN8Gu3tvunXSlKJWp7Fh7VAgMBAAGjgYEwfzAdBgNVHQ4EFgQUSDkx6kENF55m RsJZip/xOrv2E2EwQwYDVR0jBDwwOoAUSDkx6kENF55mRsJZip/xOrv2E2GhF6QV MBMxETAPBgNVBAMMCENoYW5nZU1lggkA2Yfj2kmOI5QwDAYDVR0TBAUwAwEB/zAL BgNVHQ8EBAMCAQYwDQYJKoZIhvcNAQELBQADggEBADwKrP9NcTakAbQnd+x+lBzv co0I2XOJrsm6N1r8MKVjEq9Ti5quGtoDLNQDlORnKAaWVzSg6oAFNVrItVJU5GRe J+XI+t2pXqo/OBlVoXcwG52m2rXd9e5wjdmrYwpzijvj//FjjfIZysJJiLW8xSA9 t+3/BCCGqy6uBy2KNvuYMQHr2BdHU05haXtp/mrsalSTlvLFwJeUbHDrqCKoFlDj tXkzcF4sIOfF0dzQXdXT5qerZGOMsXBQ8ALFoHd/wvS5cJvI8nWywEg3w3vWCSO1 zLdcNmvIqYEYrLZBhtLlwBnjKuHSsXorfJsUcmdKsgwIw1KtMBF2bBMyd8twBn8= -----END CERTIFICATE----- </ca> <cert> -----BEGIN CERTIFICATE----- MIIDSTCCAjGgAwIBAgIRAODlLyd7mnQoRNC4oqxJm5AwDQYJKoZIhvcNAQELBQAw EzERMA8GA1UEAwwIQ2hhbmdlTWUwHhcNMjAwMzExMDAxMzA5WhcNMzAwMzA5MDAx MzA5WjAWMRQwEgYDVQQDDAt0ZXN0LWNsaWVudDCCASIwDQYJKoZIhvcNAQEBBQAD ggEPADCCAQoCggEBAOP2megEI8f/e0Xxi6n+EKQwaLZweYFTVg25vT2X6a2HHJfg 8tXznih0NxGJFyITmpl+lddBXEnm/ZqSH6HBGujyd8aWHZ1algvbpyzU0qNXRoAu AjknbkcQ4/m+28/1ocGukY2aKYjQXddp4HzquSQupza/3JcJ+5roWte1PzLZCC74 yfdzhdBwHHOfG4B7SfYOuT7eXQwisCrTFZmtK1FoONhwSlhqcEbMBaEjT9ZP7K7p WSmx82c7xyYhdD4JMZ79qiIm/pbeszu1SpUqd3682mVwmZZOCUWf3pRKwcwEyJnk YKS9ksKTh0F9B9VibfvNw2harR3471qwt6pbSXUCAwEAAaOBlDCBkTAJBgNVHRME AjAAMB0GA1UdDgQWBBTv4I3fmPShB7U6scRReENGsLkiQDBDBgNVHSMEPDA6gBRI OTHqQQ0XnmZGwlmKn/E6u/YTYaEXpBUwEzERMA8GA1UEAwwIQ2hhbmdlTWWCCQDZ h+PaSY4jlDATBgNVHSUEDDAKBggrBgEFBQcDAjALBgNVHQ8EBAMCB4AwDQYJKoZI hvcNAQELBQADggEBACKCvwckhCZ7w5j79gYvRhujm02z2Bah7aggZ9uoyYFw3EVi 1GmyU6aoa3ui2UKciWglm8R21TuhnPsUopbWNniHDlFqOOrVxFST11FD02Qfae8P 6YWhkbUoaS3IwF7NOPg56Q7VaU1P8+GI2fR5kjHrb9pBPTCFX+1gSpiA0TE3DHj4 zO7NFRq+hE17QqeE1+W7pq4uyZYQFpbC6n+VsCJWBXDm/8WR97uJpjWUjFCNPm71 PD5YN6cSa9iasBQVvBWbKkMaf+aFvtLHGteYrVUGkvpnw9DquYFxMnHpwegU4DQh PRL2TL8szw7751o2v2CHZ+zLJbDaq26thdoIh64= -----END CERTIFICATE----- </cert> <key> -----BEGIN PRIVATE KEY----- MIIEvgIBADANBgkqhkiG9w0BAQEFAASCBKgwggSkAgEAAoIBAQDj9pnoBCPH/3tF 8Yup/hCkMGi2cHmBU1YNub09l+mthxyX4PLV854odDcRiRciE5qZfpXXQVxJ5v2a kh+hwRro8nfGlh2dWpYL26cs1NKjV0aALgI5J25HEOP5vtvP9aHBrpGNmimI0F3X aeB86rkkLqc2v9yXCfua6FrXtT8y2Qgu+Mn3c4XQcBxznxuAe0n2Drk+3l0MIrAq 0xWZrStRaDjYcEpYanBGzAWhI0/WT+yu6VkpsfNnO8cmIXQ+CTGe/aoiJv6W3rM7 tUqVKnd+vNplcJmWTglFn96USsHMBMiZ5GCkvZLCk4dBfQfVYm37zcNoWq0d+O9a sLeqW0l1AgMBAAECggEBAKU7AscG6SB3b1R9BWxLeKhpZhyGXat9Sexc6muQhpF+ Ux1KsPiewc40ng2Zvii26OHEvLru5wOx57N3onHN08FwrZxFBmYdWJBzvzJhd+No yPLzZi0jBW2BMpy81/pd4cbOzzVBvkUqMjqGxW4Fe/hb0FuAqVTYqYPYUq/y8UHa atIehY3jNc46pSRmmFIDDdyh6K5lmFVZntpRKg9RzUibQxBkLZZwnRwFLf58wJbr Os9OT2QZsaSDIIK4mtL3xTVbT9ORC/ADY6XXO+Yb6IyLqD6WD5Yqh7wEpp/Gv4Ob BvlObULOEZnjeAK9FIPs9gFuimBjcJK5kX3an8yok6kCgYEA/cqQIMR8ORRdTBaj v2CK8RtQOJ2VPEpINcxPHK8vh38CrKNmCjETXqhkCwI1wOT/WKA4IUHBLfOhgC00 cHYn6k2JfossQGh8DvjyY+JtdmSamzeecQ4i13RcnSj5G+kY/iEQogTaSALpB1Uo cugU116HiHSvcz+FK3Ia4lAHnjcCgYEA5fJ+lg3lCCd1Cq4UpzGWLMWpo5VBX9Eu QhWRC2uIGkO4BAXVlkU/1TOvzonfoHLcyVUlLjE//p6djyVezkdVHTYYXQwrWIYE oinC4YnxV1Pcvii7WaBw9t3s5REYdgyvT0Wh7GIm+o6TMnfBTvVV/DMU6K9z59f+ wLXfMZaZH7MCgYBOXhdlVub5BTXOAgusU9ZznziFUvu7M0DbA+zF8b6ee3TK9GXU 7dSKXTsPPy50EwJaTpcmhdRuKRYMq2jO9V1b93dmkPkoJltwkCTg/RFKBsTK+0C8 rl3J5A+ZJAbQPIlQJ8uoDBGPPP7SGdS0rr+IxZLaaxWmY83uXXy5t3ayvwKBgQCM YMrovljI7pWkTHvtSfddI9qZNAAyB5jO3S2sJBx1tEu9oPYwg9whQymb1E3CPP0O qD1HgueHgLu9bNoA4klSyPh8rXY017Qyb346hCTi5B6JtIITiEAOZZM+kH43ay0H HwJoNc+H/Mxd7gAEPQAeM+0a1CnVKuaqLR2xvzeBwwKBgC+vydOv2Fqu58b+/cWi 52/stI11Y+xkdQ+/SP+cAucN05xVrfFzEbv90/Tintk2G+oCb5lWxM2uGIfSMCMA CUHg03a0oZdTTapUs+i0fahuhR/ojK5i4COTHM0jF3ryr1Gjo0RUgbJe/RlnRY5v bbOS07Ao554/jPNrXGzImnQz -----END PRIVATE KEY----- </key> <tls-crypt> -----BEGIN OpenVPN Static key V1----- 470a961d29e78b8f4884b46741587ecf 6008c6bb16acf2eae299f68df994133d 7fbe5dbacd187c21ac9e61bc2aab3de0 c88f39674dec40ef4844dddb80884ad4 652542876fdadd98ca95cf4e9f4ed6e8 2b2f6315aa77c0ae9fc5dca6df687622 82f629e230990b340b1b95f6f7ca18a4 185176cf29c04d5d0a9f9c19083fe3b6 24e55a25f5e5ccf2a48f33373d56792a 20f60074f9e6ef855e0b0ceca0a07300 294718d41af0a97da641053397fdc944 d21f5a9a702a118de21440fce772ab17 11a575acc9ce0097e2fdefc1233ea2e6 01e49032eaf2aa3e0898c3f5b334839f f8c69c80614a45cfb0ba7d43d3476e37 a22a4d43b0dbc96430b1115a6b1f6aac -----END OpenVPN Static key V1----- </tls-crypt> NOTE: The script automatically accounts for NAT. Notice in your client config that it sets the remote server as whatever your external address is. You may not want this behavior. If that is the case you will need to go in and edit the remote line with the IP address of your VPN server. On CentOS 7 Make sure everything is up to date. yum update -y && reboot . The reboot is important because if your kernel might update. If this happens you need to reboot to load the new kernel. Run yum install -y epel-release && yum update -y && yum install -y openvpn easy-rsa chrony && systemctl enable chronyd && chronyc makestep This is a long series of commands, but it installs openvpn and chrony. You need chrony to ensure your time is synched. WARNING : If the time is not synched between the server and your clients, the VPN will fail to connect! You should have copied your client config to your client already. If you haven't, do that now. To run the VPN, run openvpn <client_config_name>","title":"Run VPN on OS10"},{"location":"Run%20VPN%20on%20OS10/#run-vpn-on-os10","text":"","title":"Run VPN on OS10"},{"location":"Run%20VPN%20on%20OS10/#my-configuration","text":"","title":"My Configuration"},{"location":"Run%20VPN%20on%20OS10/#dell-4112f-on","text":"Dell EMC Networking OS10 Enterprise Copyright (c) 1999-2020 by Dell Inc. All Rights Reserved. OS Version: 10.5.0.4 Build Version: 10.5.0.4.638 Build Time: 2020-01-30T21:08:56+0000 System Type: S4112F-ON Architecture: x86_64 Up Time: 2 days 03:54:07","title":"Dell 4112F-ON"},{"location":"Run%20VPN%20on%20OS10/#centos","text":"[root@centos ~]# cat /etc/*-release CentOS Linux release 7.6.1810 (Core) NAME=\"CentOS Linux\" VERSION=\"7 (Core)\" ID=\"centos\" ID_LIKE=\"rhel fedora\" VERSION_ID=\"7\" PRETTY_NAME=\"CentOS Linux 7 (Core)\" ANSI_COLOR=\"0;31\" CPE_NAME=\"cpe:/o:centos:centos:7\" HOME_URL=\"https://www.centos.org/\" BUG_REPORT_URL=\"https://bugs.centos.org/\" CENTOS_MANTISBT_PROJECT=\"CentOS-7\" CENTOS_MANTISBT_PROJECT_VERSION=\"7\" REDHAT_SUPPORT_PRODUCT=\"centos\" REDHAT_SUPPORT_PRODUCT_VERSION=\"7\" CentOS Linux release 7.6.1810 (Core) CentOS Linux release 7.6.1810 (Core)","title":"CentOS"},{"location":"Run%20VPN%20on%20OS10/#testing-topology","text":"Switch Configuration","title":"Testing Topology"},{"location":"Run%20VPN%20on%20OS10/#research-sources","text":"Helpful Book Basics of Network Processor Packet Processing How Network Processors Work","title":"Research Sources"},{"location":"Run%20VPN%20on%20OS10/#npu-interface-problem","text":"The one big gotcha with doing this is that when you drop to the command line in OS10 and do a ip a s , the interfaces you see that look like physical interfaces ex: 13: e101-001-0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc multiq master br32 state UP group default qlen 1000 link/ether 50:9a:4c:d6:0a:71 brd ff:ff:ff:ff:ff:ff 14: e101-002-0: <BROADCAST,MULTICAST> mtu 1500 qdisc noop master br1 state DOWN group default qlen 1000 link/ether 50:9a:4c:d6:0a:72 brd ff:ff:ff:ff:ff:ff 15: e101-003-0: <BROADCAST,MULTICAST> mtu 1500 qdisc noop master br1 state DOWN group default qlen 1000 link/ether 50:9a:4c:d6:0a:73 brd ff:ff:ff:ff:ff:ff 16: e101-004-0: <BROADCAST,MULTICAST> mtu 1500 qdisc noop master br1 state DOWN group default qlen 1000 link/ether 50:9a:4c:d6:0a:74 brd ff:ff:ff:ff:ff:ff are not actual physical interfaces. Under the hood the operating system is actually using tap interfaces. Inside the switch there are two processors - a regular x86 processor and a separate processor called the Network Processing Unit (NPU). The interfaces are connected to the NPU. Most traffic that comes in on the physical interfaces managed by the NPU does not flow up to the x86 chip. This means that if you do a tcpdump on one of the interfaces you see in ip a s you will see very little. In fact, the only traffic you will see is management traffic which is handled by the Linux kernel. This means if you want to set up a VPN you have to have a way to make sure all the traffic is visible to the Linux kernel. Fortunately, there is a way to make this happen. VLAN interfaces are virtual and subsequently are handled entirely by the Linux kernel. In fact, VLAN interfaces actually show up under the hood as bridge interfaces: 29: br32: <BROADCAST,MULTICAST,PROMISC,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default qlen 1000 link/ether 50:9a:4c:d6:0a:a1 brd ff:ff:ff:ff:ff:ff inet 192.168.32.1/24 brd 255.168.32.255 scope global br32 valid_lft forever preferred_lft forever inet6 fe80::529a:4cff:fed6:aa1/64 scope link valid_lft forever preferred_lft forever 41: br33: <BROADCAST,MULTICAST,PROMISC,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default qlen 1000 link/ether 50:9a:4c:d6:0a:a1 brd ff:ff:ff:ff:ff:ff inet 192.168.33.1/24 brd 255.168.33.255 scope global br33 valid_lft forever preferred_lft forever inet6 fe80::529a:4cff:fed6:aa1/64 scope link valid_lft forever preferred_lft forever These two correspond to interface vlans 32 and 33. By using VLAN interfaces you can tie the VPN to these interfaces and everything works just fine. You just place any associated physical interfaces as access VLANs or trunks with the appropriate allowed VLANs.","title":"NPU Interface Problem"},{"location":"Run%20VPN%20on%20OS10/#installing-openvpn-as-a-server-on-the-4112f-on","text":"Note: I ran a VPN server on my switch, but you could just as easily make the switch a point to point VPN gateway connecting to a PFSense instance such that anything that can reach the switch could participate in a multipoint network. On the 4112F-ON: Enter configuration mode from user mode by running en and then config <enter> Run ip name-server 192.168.1.1 to add a name server. Run write mem in enable mode to save your configuration changes. Run system bash Before continuing, make sure that the time is correct on the device. WARNING If you do not do this and you generate certificates, none of the encryption will work and you will have to recreate all of your certificates! Run sudo apt-get install -y openvpn vim . I installed vim because I don't hate myself. I used this script from git.io/vpn to install OpenVPN. Having done the entire thing manually before, I can tell you this saves a huge amount of time. To run the script run wget https://git.io/vpn -O openvpn-install.sh && chmod +x openvpn-install.sh && ./openvpn-install.sh Fill in the options as needed. I did find some things you have to tweak with their script. Perform the below to clean things up. Run vim /lib/systemd/system/openvpn@.service . Where it says --config /etc/openvpn/%i.conf , change that to --config /etc/openvpn/%i/%i.conf . For details on specifies work see this post . When you are done run systemctl daemon-reload to reload the systemd daemon. If you used my version of the script then you do not need to do this. Otherwise you need to run vim /etc/openvpn/server/server.conf and you need to prepend /etc/openvpn/server/ on several of the paths or the service won't start. See my config below: local 192.168.32.1 port 1194 proto udp dev tun ca /etc/openvpn/server/ca.crt cert /etc/openvpn/server/server.crt key /etc/openvpn/server/server.key dh /etc/openvpn/server/dh.pem auth SHA512 tls-crypt /etc/openvpn/server/tc.key topology subnet server 10.8.0.0 255.255.255.0 ifconfig-pool-persist ipp.txt push \"redirect-gateway def1 bypass-dhcp\" push \"dhcp-option DNS 192.168.1.1\" keepalive 10 120 cipher AES-256-CBC user nobody group nogroup persist-key persist-tun status openvpn-status.log verb 3 crl-verify /etc/openvpn/server/crl.pem explicit-exit-notify You may want to add something like push route 192.168.1.0 255.255.255.0 to your server config. This allows the server to push routes to the client. For example, in my case the 192.168.1.0/24 network is behind my server, so I have to push a route so that the clients know how to get to it. Just keep in mind, that hosts on your distant network must have a route back to your VPN network. Run systemctl start openvpn@server to start the server. Rerun the script to add clients. Your output should look like the below. In my case I added one client to perform the test. ``` Looks like OpenVPN is already installed. What do you want to do? 1) Add a new user 2) Revoke an existing user 3) Remove OpenVPN 4) Exit Select an option: 1 Tell me a name for the client certificate. Client name: test-client Using SSL: openssl OpenSSL 1.1.0l 10 Sep 2019 Generating a RSA private key ........+++++ .......+++++ writing new private key to '/etc/openvpn/server/easy-rsa/pki/private/test-client.key.WONcIB6m1N' ----- Using configuration from ./safessl-easyrsa.cnf Check that the request matches the signature Signature ok The Subject's Distinguished Name is as follows commonName :ASN.1 12:'test-client' Certificate is to be certified until Mar 9 00:13:09 2030 GMT (3650 days) Write out database with 1 new entries Data Base Updated Client test-client added, configuration is available at: /root/test-client.ovpn ``` Copy the contents of your client config. In my case this was from /root/test-client.ovpn and it looked like: client dev tun proto udp remote <SERVER ADDRESS> 1194 resolv-retry infinite nobind persist-key persist-tun remote-cert-tls server auth SHA512 cipher AES-256-CBC ignore-unknown-option block-outside-dns block-outside-dns verb 3 <ca> -----BEGIN CERTIFICATE----- MIIDKzCCAhOgAwIBAgIJANmH49pJjiOUMA0GCSqGSIb3DQEBCwUAMBMxETAPBgNV BAMMCENoYW5nZU1lMB4XDTIwMDMxMTAwMTA1M1oXDTMwMDMwOTAwMTA1M1owEzER MA8GA1UEAwwIQ2hhbmdlTWUwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIB AQD5FZJN5STAXRX7ZBq8CVf7DntSQTgnVVqwntKJwggTPHgwn8uMUWRdaIpXZVN5 MYTGPCICDoxdlF/2KUgH9n/L1Rlmm9RW4beXMwFJUR8NIExf5vQy03gk6JpEO1DA Pu+x0/EhXGvGo/lAEpF4rk0ZPpNEkFM71bIqhKAMAe9M5c2ZrAxqplyTz/Zl4nRm YQSsqnx3ikN+SkxdnifIBlF3MzCHqCCV9QaOkrztXHs9XFhnWpyu+OLqyP5+ipOZ gYsTDA4otjv6D9MX+BoWCZ6zSzo/kMSkM7ByZt5jjyp1lQQaYnZe8LmRkB3vcBb4 lWlN8Gu3tvunXSlKJWp7Fh7VAgMBAAGjgYEwfzAdBgNVHQ4EFgQUSDkx6kENF55m RsJZip/xOrv2E2EwQwYDVR0jBDwwOoAUSDkx6kENF55mRsJZip/xOrv2E2GhF6QV MBMxETAPBgNVBAMMCENoYW5nZU1lggkA2Yfj2kmOI5QwDAYDVR0TBAUwAwEB/zAL BgNVHQ8EBAMCAQYwDQYJKoZIhvcNAQELBQADggEBADwKrP9NcTakAbQnd+x+lBzv co0I2XOJrsm6N1r8MKVjEq9Ti5quGtoDLNQDlORnKAaWVzSg6oAFNVrItVJU5GRe J+XI+t2pXqo/OBlVoXcwG52m2rXd9e5wjdmrYwpzijvj//FjjfIZysJJiLW8xSA9 t+3/BCCGqy6uBy2KNvuYMQHr2BdHU05haXtp/mrsalSTlvLFwJeUbHDrqCKoFlDj tXkzcF4sIOfF0dzQXdXT5qerZGOMsXBQ8ALFoHd/wvS5cJvI8nWywEg3w3vWCSO1 zLdcNmvIqYEYrLZBhtLlwBnjKuHSsXorfJsUcmdKsgwIw1KtMBF2bBMyd8twBn8= -----END CERTIFICATE----- </ca> <cert> -----BEGIN CERTIFICATE----- MIIDSTCCAjGgAwIBAgIRAODlLyd7mnQoRNC4oqxJm5AwDQYJKoZIhvcNAQELBQAw EzERMA8GA1UEAwwIQ2hhbmdlTWUwHhcNMjAwMzExMDAxMzA5WhcNMzAwMzA5MDAx MzA5WjAWMRQwEgYDVQQDDAt0ZXN0LWNsaWVudDCCASIwDQYJKoZIhvcNAQEBBQAD ggEPADCCAQoCggEBAOP2megEI8f/e0Xxi6n+EKQwaLZweYFTVg25vT2X6a2HHJfg 8tXznih0NxGJFyITmpl+lddBXEnm/ZqSH6HBGujyd8aWHZ1algvbpyzU0qNXRoAu AjknbkcQ4/m+28/1ocGukY2aKYjQXddp4HzquSQupza/3JcJ+5roWte1PzLZCC74 yfdzhdBwHHOfG4B7SfYOuT7eXQwisCrTFZmtK1FoONhwSlhqcEbMBaEjT9ZP7K7p WSmx82c7xyYhdD4JMZ79qiIm/pbeszu1SpUqd3682mVwmZZOCUWf3pRKwcwEyJnk YKS9ksKTh0F9B9VibfvNw2harR3471qwt6pbSXUCAwEAAaOBlDCBkTAJBgNVHRME AjAAMB0GA1UdDgQWBBTv4I3fmPShB7U6scRReENGsLkiQDBDBgNVHSMEPDA6gBRI OTHqQQ0XnmZGwlmKn/E6u/YTYaEXpBUwEzERMA8GA1UEAwwIQ2hhbmdlTWWCCQDZ h+PaSY4jlDATBgNVHSUEDDAKBggrBgEFBQcDAjALBgNVHQ8EBAMCB4AwDQYJKoZI hvcNAQELBQADggEBACKCvwckhCZ7w5j79gYvRhujm02z2Bah7aggZ9uoyYFw3EVi 1GmyU6aoa3ui2UKciWglm8R21TuhnPsUopbWNniHDlFqOOrVxFST11FD02Qfae8P 6YWhkbUoaS3IwF7NOPg56Q7VaU1P8+GI2fR5kjHrb9pBPTCFX+1gSpiA0TE3DHj4 zO7NFRq+hE17QqeE1+W7pq4uyZYQFpbC6n+VsCJWBXDm/8WR97uJpjWUjFCNPm71 PD5YN6cSa9iasBQVvBWbKkMaf+aFvtLHGteYrVUGkvpnw9DquYFxMnHpwegU4DQh PRL2TL8szw7751o2v2CHZ+zLJbDaq26thdoIh64= -----END CERTIFICATE----- </cert> <key> -----BEGIN PRIVATE KEY----- MIIEvgIBADANBgkqhkiG9w0BAQEFAASCBKgwggSkAgEAAoIBAQDj9pnoBCPH/3tF 8Yup/hCkMGi2cHmBU1YNub09l+mthxyX4PLV854odDcRiRciE5qZfpXXQVxJ5v2a kh+hwRro8nfGlh2dWpYL26cs1NKjV0aALgI5J25HEOP5vtvP9aHBrpGNmimI0F3X aeB86rkkLqc2v9yXCfua6FrXtT8y2Qgu+Mn3c4XQcBxznxuAe0n2Drk+3l0MIrAq 0xWZrStRaDjYcEpYanBGzAWhI0/WT+yu6VkpsfNnO8cmIXQ+CTGe/aoiJv6W3rM7 tUqVKnd+vNplcJmWTglFn96USsHMBMiZ5GCkvZLCk4dBfQfVYm37zcNoWq0d+O9a sLeqW0l1AgMBAAECggEBAKU7AscG6SB3b1R9BWxLeKhpZhyGXat9Sexc6muQhpF+ Ux1KsPiewc40ng2Zvii26OHEvLru5wOx57N3onHN08FwrZxFBmYdWJBzvzJhd+No yPLzZi0jBW2BMpy81/pd4cbOzzVBvkUqMjqGxW4Fe/hb0FuAqVTYqYPYUq/y8UHa atIehY3jNc46pSRmmFIDDdyh6K5lmFVZntpRKg9RzUibQxBkLZZwnRwFLf58wJbr Os9OT2QZsaSDIIK4mtL3xTVbT9ORC/ADY6XXO+Yb6IyLqD6WD5Yqh7wEpp/Gv4Ob BvlObULOEZnjeAK9FIPs9gFuimBjcJK5kX3an8yok6kCgYEA/cqQIMR8ORRdTBaj v2CK8RtQOJ2VPEpINcxPHK8vh38CrKNmCjETXqhkCwI1wOT/WKA4IUHBLfOhgC00 cHYn6k2JfossQGh8DvjyY+JtdmSamzeecQ4i13RcnSj5G+kY/iEQogTaSALpB1Uo cugU116HiHSvcz+FK3Ia4lAHnjcCgYEA5fJ+lg3lCCd1Cq4UpzGWLMWpo5VBX9Eu QhWRC2uIGkO4BAXVlkU/1TOvzonfoHLcyVUlLjE//p6djyVezkdVHTYYXQwrWIYE oinC4YnxV1Pcvii7WaBw9t3s5REYdgyvT0Wh7GIm+o6TMnfBTvVV/DMU6K9z59f+ wLXfMZaZH7MCgYBOXhdlVub5BTXOAgusU9ZznziFUvu7M0DbA+zF8b6ee3TK9GXU 7dSKXTsPPy50EwJaTpcmhdRuKRYMq2jO9V1b93dmkPkoJltwkCTg/RFKBsTK+0C8 rl3J5A+ZJAbQPIlQJ8uoDBGPPP7SGdS0rr+IxZLaaxWmY83uXXy5t3ayvwKBgQCM YMrovljI7pWkTHvtSfddI9qZNAAyB5jO3S2sJBx1tEu9oPYwg9whQymb1E3CPP0O qD1HgueHgLu9bNoA4klSyPh8rXY017Qyb346hCTi5B6JtIITiEAOZZM+kH43ay0H HwJoNc+H/Mxd7gAEPQAeM+0a1CnVKuaqLR2xvzeBwwKBgC+vydOv2Fqu58b+/cWi 52/stI11Y+xkdQ+/SP+cAucN05xVrfFzEbv90/Tintk2G+oCb5lWxM2uGIfSMCMA CUHg03a0oZdTTapUs+i0fahuhR/ojK5i4COTHM0jF3ryr1Gjo0RUgbJe/RlnRY5v bbOS07Ao554/jPNrXGzImnQz -----END PRIVATE KEY----- </key> <tls-crypt> -----BEGIN OpenVPN Static key V1----- 470a961d29e78b8f4884b46741587ecf 6008c6bb16acf2eae299f68df994133d 7fbe5dbacd187c21ac9e61bc2aab3de0 c88f39674dec40ef4844dddb80884ad4 652542876fdadd98ca95cf4e9f4ed6e8 2b2f6315aa77c0ae9fc5dca6df687622 82f629e230990b340b1b95f6f7ca18a4 185176cf29c04d5d0a9f9c19083fe3b6 24e55a25f5e5ccf2a48f33373d56792a 20f60074f9e6ef855e0b0ceca0a07300 294718d41af0a97da641053397fdc944 d21f5a9a702a118de21440fce772ab17 11a575acc9ce0097e2fdefc1233ea2e6 01e49032eaf2aa3e0898c3f5b334839f f8c69c80614a45cfb0ba7d43d3476e37 a22a4d43b0dbc96430b1115a6b1f6aac -----END OpenVPN Static key V1----- </tls-crypt> NOTE: The script automatically accounts for NAT. Notice in your client config that it sets the remote server as whatever your external address is. You may not want this behavior. If that is the case you will need to go in and edit the remote line with the IP address of your VPN server.","title":"Installing OpenVPN as a Server on the 4112F-ON"},{"location":"Run%20VPN%20on%20OS10/#on-centos-7","text":"Make sure everything is up to date. yum update -y && reboot . The reboot is important because if your kernel might update. If this happens you need to reboot to load the new kernel. Run yum install -y epel-release && yum update -y && yum install -y openvpn easy-rsa chrony && systemctl enable chronyd && chronyc makestep This is a long series of commands, but it installs openvpn and chrony. You need chrony to ensure your time is synched. WARNING : If the time is not synched between the server and your clients, the VPN will fail to connect! You should have copied your client config to your client already. If you haven't, do that now. To run the VPN, run openvpn <client_config_name>","title":"On CentOS 7"},{"location":"Run%20VPN%20on%20OS10/README_wireguard_%28unfinished%29/","text":"Run VPN on OS10 My Configuration Dell 4112F-ON Dell EMC Networking OS10 Enterprise Copyright (c) 1999-2020 by Dell Inc. All Rights Reserved. OS Version: 10.5.0.4 Build Version: 10.5.0.4.638 Build Time: 2020-01-30T21:08:56+0000 System Type: S4112F-ON Architecture: x86_64 Up Time: 2 days 03:54:07 CentOS [root@centos ~]# cat /etc/*-release CentOS Linux release 7.6.1810 (Core) NAME=\"CentOS Linux\" VERSION=\"7 (Core)\" ID=\"centos\" ID_LIKE=\"rhel fedora\" VERSION_ID=\"7\" PRETTY_NAME=\"CentOS Linux 7 (Core)\" ANSI_COLOR=\"0;31\" CPE_NAME=\"cpe:/o:centos:centos:7\" HOME_URL=\"https://www.centos.org/\" BUG_REPORT_URL=\"https://bugs.centos.org/\" CENTOS_MANTISBT_PROJECT=\"CentOS-7\" CENTOS_MANTISBT_PROJECT_VERSION=\"7\" REDHAT_SUPPORT_PRODUCT=\"centos\" REDHAT_SUPPORT_PRODUCT_VERSION=\"7\" CentOS Linux release 7.6.1810 (Core) CentOS Linux release 7.6.1810 (Core) Physical Configuration Interface ethernet 1/1/12 on the 4112F-ON plugged directly into a server running ESXi. That interface was assigned as an uplink associated with a vswitch when then was tied to a portgroup running on VLAN 32. ethernet 1/1/12 was configured as follows: OS10(conf-if-eth1/1/12)# show configuration ! interface ethernet1/1/12 no shutdown switchport mode trunk switchport access vlan 1 switchport trunk allowed vlan 32 flowcontrol receive on interface vlan 32 was configured as follows: OS10(conf-if-vl-32)# show configuration ! interface vlan32 no shutdown ip address 192.168.32.1/24 The full switch config is here Interface ethernet 1/1/1 was configured as an access port in VLAN Research Sources Helpful Book Basics of Network Processor Packet Processing How Network Processors Work NPU Problem Explanation Installing WireGuard On the 4112F-ON: Enter configuration mode from user mode by running en and then config <enter> Run ip name-server 192.168.1.1 to add a name server. Run write mem in enable mode to save your configuration changes. Run system bash If you don't have a default route on your switch, you will need to add one with sudo ip route add 0.0.0.0/0 via 192.168.1.1 . Before installing WireGuard you will need the latest kernel headers. The kernel headers on the system will work a bit differently than regular systems. I found the relevant kernel headers with apt search linux-headers | grep $(uname -r) You should see an entry mentioning all - something like linux-headers-4.9.0-11-all . You want to install that package with sudo apt-get install -y <LINUX HEADER NAME> bc At this point you will need to reboot. Do this by exiting the command line and in enable mode running reload If you do not have a permanent default route set, when you log back in you will need to run system bash and then readd the default route with sudo ip route add 0.0.0.0/0 via 192.168.1.1 mkdir /opt/wireguard && cd /opt/wireguard Now you will either need to run all of the following as sudo or you will need to add a password to the root account with sudo passwd and then su - to become root. After you have done the above run: echo \"deb http://deb.debian.org/debian/ unstable main\" > /etc/apt/sources.list.d/unstable-wireguard.list printf 'Package: *\\nPin: release a=unstable\\nPin-Priority: 90\\n' > /etc/apt/preferences.d/limit-unstable apt update apt install wireguard Generate key pairs wg genkey | tee wg-private.key | wg pubkey > wg-public.key On CentOS 7 Make sure everything is up to date. yum update -y && reboot . The reboot is important because if your kernel might update. If this happens you need to reboot to load the new kernel. Wireguard requires kernel 3.10 or higher - I noticed if you haven't updated CentOS for a bit than your kernel might be to old and you'll get RTNETLINK answers: Operation not supported . Run: sudo yum install https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm sudo curl -o /etc/yum.repos.d/jdoss-wireguard-epel-7.repo https://copr.fedorainfracloud.org/coprs/jdoss/wireguard/repo/epel-7/jdoss-wireguard-epel-7.repo sudo yum install wireguard-dkms wireguard-tools mkdir /opt/wireguard && cd /opt/wireguard Generate key pairs wg genkey | tee wg-private.key | wg pubkey > wg-public.key ip link add wg0 type wireguard Set Wireguard interface IP ip address add dev wg0 10.0.0.2/24 . This IP address is the tunnel IP address. If there are only two peers together you can do something like ip address add dev wg0 10.0.0.2 peer 10.0.0.1 wg set wg0 listen-port 51820 private-key /opt/wireguard/wg-private.key peer OQiSLUOd3YWCfEkHazJHuuaJVNc++8QOb2+sOOZl/2c= endpoint 192.168.32.1:8172 listen-port: the port this host will listen on private-key: the private key for this host peer: I thought this was a bit misleading - you want the public key of the other host to which you are connecting endpoint: the other endpoint of the VPN and the port on which it is listening Strange Behavior OS10(config)# management route 192.168.1.0/24 192.168.1.1 % Error: Overlapping route for Management interface OS10(config)# ip route 0.0.0.0/0 192.168.1.1 interface ethernet 1/1/1 % Error: Network unreachable OS10(config)# ping 192.168.1.1 PING 192.168.1.1 (192.168.1.1) 56(84) bytes of data. 64 bytes from 192.168.1.1: icmp_seq=1 ttl=64 time=0.452 ms 64 bytes from 192.168.1.1: icmp_seq=2 ttl=64 time=0.388 ms ^C --- 192.168.1.1 ping statistics --- 2 packets transmitted, 2 received, 0% packet loss, time 1005ms rtt min/avg/max/mdev = 0.388/0.420/0.452/0.032 ms OS10(config)# do system bash admin@OS10:~$ su - Password: root@OS10:~# ip route 192.168.1.0/24 dev eth0 proto kernel scope link src 192.168.1.20 192.168.4.0/24 dev br32 proto none scope link root@OS10:~# ip route add 0.0.0.0/0 via 192.168.1.1 root@OS10:~# ip route default via 192.168.1.1 dev eth0 192.168.1.0/24 dev eth0 proto kernel scope link src 192.168.1.20 192.168.4.0/24 dev br32 proto none scope link root@OS10:~# ping google.com PING google.com (216.58.193.142) 56(84) bytes of data. 64 bytes from dfw25s34-in-f14.1e100.net (216.58.193.142): icmp_seq=1 ttl=55 time=25.4 ms 64 bytes from dfw25s34-in-f14.1e100.net (216.58.193.142): icmp_seq=2 ttl=55 time=22.9 ms 64 bytes from dfw25s34-in-f14.1e100.net (216.58.193.142): icmp_seq=3 ttl=55 time=22.1 ms 64 bytes from dfw25s34-in-f14.1e100.net (216.58.193.142): icmp_seq=4 ttl=55 time=22.2 ms ^C --- google.com ping statistics --- 4 packets transmitted, 4 received, 0% packet loss, time 3002ms rtt min/avg/max/mdev = 22.149/23.185/25.408/1.329 ms","title":"Run VPN on OS10"},{"location":"Run%20VPN%20on%20OS10/README_wireguard_%28unfinished%29/#run-vpn-on-os10","text":"","title":"Run VPN on OS10"},{"location":"Run%20VPN%20on%20OS10/README_wireguard_%28unfinished%29/#my-configuration","text":"","title":"My Configuration"},{"location":"Run%20VPN%20on%20OS10/README_wireguard_%28unfinished%29/#dell-4112f-on","text":"Dell EMC Networking OS10 Enterprise Copyright (c) 1999-2020 by Dell Inc. All Rights Reserved. OS Version: 10.5.0.4 Build Version: 10.5.0.4.638 Build Time: 2020-01-30T21:08:56+0000 System Type: S4112F-ON Architecture: x86_64 Up Time: 2 days 03:54:07","title":"Dell 4112F-ON"},{"location":"Run%20VPN%20on%20OS10/README_wireguard_%28unfinished%29/#centos","text":"[root@centos ~]# cat /etc/*-release CentOS Linux release 7.6.1810 (Core) NAME=\"CentOS Linux\" VERSION=\"7 (Core)\" ID=\"centos\" ID_LIKE=\"rhel fedora\" VERSION_ID=\"7\" PRETTY_NAME=\"CentOS Linux 7 (Core)\" ANSI_COLOR=\"0;31\" CPE_NAME=\"cpe:/o:centos:centos:7\" HOME_URL=\"https://www.centos.org/\" BUG_REPORT_URL=\"https://bugs.centos.org/\" CENTOS_MANTISBT_PROJECT=\"CentOS-7\" CENTOS_MANTISBT_PROJECT_VERSION=\"7\" REDHAT_SUPPORT_PRODUCT=\"centos\" REDHAT_SUPPORT_PRODUCT_VERSION=\"7\" CentOS Linux release 7.6.1810 (Core) CentOS Linux release 7.6.1810 (Core)","title":"CentOS"},{"location":"Run%20VPN%20on%20OS10/README_wireguard_%28unfinished%29/#physical-configuration","text":"Interface ethernet 1/1/12 on the 4112F-ON plugged directly into a server running ESXi. That interface was assigned as an uplink associated with a vswitch when then was tied to a portgroup running on VLAN 32. ethernet 1/1/12 was configured as follows: OS10(conf-if-eth1/1/12)# show configuration ! interface ethernet1/1/12 no shutdown switchport mode trunk switchport access vlan 1 switchport trunk allowed vlan 32 flowcontrol receive on interface vlan 32 was configured as follows: OS10(conf-if-vl-32)# show configuration ! interface vlan32 no shutdown ip address 192.168.32.1/24 The full switch config is here Interface ethernet 1/1/1 was configured as an access port in VLAN","title":"Physical Configuration"},{"location":"Run%20VPN%20on%20OS10/README_wireguard_%28unfinished%29/#research","text":"","title":"Research"},{"location":"Run%20VPN%20on%20OS10/README_wireguard_%28unfinished%29/#sources","text":"Helpful Book Basics of Network Processor Packet Processing How Network Processors Work","title":"Sources"},{"location":"Run%20VPN%20on%20OS10/README_wireguard_%28unfinished%29/#npu-problem","text":"Explanation","title":"NPU Problem"},{"location":"Run%20VPN%20on%20OS10/README_wireguard_%28unfinished%29/#installing-wireguard","text":"On the 4112F-ON: Enter configuration mode from user mode by running en and then config <enter> Run ip name-server 192.168.1.1 to add a name server. Run write mem in enable mode to save your configuration changes. Run system bash If you don't have a default route on your switch, you will need to add one with sudo ip route add 0.0.0.0/0 via 192.168.1.1 . Before installing WireGuard you will need the latest kernel headers. The kernel headers on the system will work a bit differently than regular systems. I found the relevant kernel headers with apt search linux-headers | grep $(uname -r) You should see an entry mentioning all - something like linux-headers-4.9.0-11-all . You want to install that package with sudo apt-get install -y <LINUX HEADER NAME> bc At this point you will need to reboot. Do this by exiting the command line and in enable mode running reload If you do not have a permanent default route set, when you log back in you will need to run system bash and then readd the default route with sudo ip route add 0.0.0.0/0 via 192.168.1.1 mkdir /opt/wireguard && cd /opt/wireguard Now you will either need to run all of the following as sudo or you will need to add a password to the root account with sudo passwd and then su - to become root. After you have done the above run: echo \"deb http://deb.debian.org/debian/ unstable main\" > /etc/apt/sources.list.d/unstable-wireguard.list printf 'Package: *\\nPin: release a=unstable\\nPin-Priority: 90\\n' > /etc/apt/preferences.d/limit-unstable apt update apt install wireguard Generate key pairs wg genkey | tee wg-private.key | wg pubkey > wg-public.key On CentOS 7 Make sure everything is up to date. yum update -y && reboot . The reboot is important because if your kernel might update. If this happens you need to reboot to load the new kernel. Wireguard requires kernel 3.10 or higher - I noticed if you haven't updated CentOS for a bit than your kernel might be to old and you'll get RTNETLINK answers: Operation not supported . Run: sudo yum install https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm sudo curl -o /etc/yum.repos.d/jdoss-wireguard-epel-7.repo https://copr.fedorainfracloud.org/coprs/jdoss/wireguard/repo/epel-7/jdoss-wireguard-epel-7.repo sudo yum install wireguard-dkms wireguard-tools mkdir /opt/wireguard && cd /opt/wireguard Generate key pairs wg genkey | tee wg-private.key | wg pubkey > wg-public.key ip link add wg0 type wireguard Set Wireguard interface IP ip address add dev wg0 10.0.0.2/24 . This IP address is the tunnel IP address. If there are only two peers together you can do something like ip address add dev wg0 10.0.0.2 peer 10.0.0.1 wg set wg0 listen-port 51820 private-key /opt/wireguard/wg-private.key peer OQiSLUOd3YWCfEkHazJHuuaJVNc++8QOb2+sOOZl/2c= endpoint 192.168.32.1:8172 listen-port: the port this host will listen on private-key: the private key for this host peer: I thought this was a bit misleading - you want the public key of the other host to which you are connecting endpoint: the other endpoint of the VPN and the port on which it is listening","title":"Installing WireGuard"},{"location":"Run%20VPN%20on%20OS10/README_wireguard_%28unfinished%29/#strange-behavior","text":"OS10(config)# management route 192.168.1.0/24 192.168.1.1 % Error: Overlapping route for Management interface OS10(config)# ip route 0.0.0.0/0 192.168.1.1 interface ethernet 1/1/1 % Error: Network unreachable OS10(config)# ping 192.168.1.1 PING 192.168.1.1 (192.168.1.1) 56(84) bytes of data. 64 bytes from 192.168.1.1: icmp_seq=1 ttl=64 time=0.452 ms 64 bytes from 192.168.1.1: icmp_seq=2 ttl=64 time=0.388 ms ^C --- 192.168.1.1 ping statistics --- 2 packets transmitted, 2 received, 0% packet loss, time 1005ms rtt min/avg/max/mdev = 0.388/0.420/0.452/0.032 ms OS10(config)# do system bash admin@OS10:~$ su - Password: root@OS10:~# ip route 192.168.1.0/24 dev eth0 proto kernel scope link src 192.168.1.20 192.168.4.0/24 dev br32 proto none scope link root@OS10:~# ip route add 0.0.0.0/0 via 192.168.1.1 root@OS10:~# ip route default via 192.168.1.1 dev eth0 192.168.1.0/24 dev eth0 proto kernel scope link src 192.168.1.20 192.168.4.0/24 dev br32 proto none scope link root@OS10:~# ping google.com PING google.com (216.58.193.142) 56(84) bytes of data. 64 bytes from dfw25s34-in-f14.1e100.net (216.58.193.142): icmp_seq=1 ttl=55 time=25.4 ms 64 bytes from dfw25s34-in-f14.1e100.net (216.58.193.142): icmp_seq=2 ttl=55 time=22.9 ms 64 bytes from dfw25s34-in-f14.1e100.net (216.58.193.142): icmp_seq=3 ttl=55 time=22.1 ms 64 bytes from dfw25s34-in-f14.1e100.net (216.58.193.142): icmp_seq=4 ttl=55 time=22.2 ms ^C --- google.com ping statistics --- 4 packets transmitted, 4 received, 0% packet loss, time 3002ms rtt min/avg/max/mdev = 22.149/23.185/25.408/1.329 ms","title":"Strange Behavior"},{"location":"Running%20DNS%20from%20OS10/","text":"Running DNS from OS10 Physical Configuration OS10# show version Dell EMC Networking OS10 Enterprise Copyright (c) 1999-2020 by Dell Inc. All Rights Reserved. OS Version: 10.5.1.3 Build Version: 10.5.1.3.190 Build Time: 2020-06-19T21:48:07+0000 System Type: S4112F-ON Architecture: x86_64 Up Time: 00:30:27 Software Configuration OS10(config)# configure terminal OS10(config)# ip name-server 192.168.1.6 OS10(config)# interface vlan 1 OS10(conf-if-vl-1)# ip address 192.168.1.26/24 OS10(conf-if-vl-1)# exit OS10(config)# ip route 0.0.0.0/0 192.168.1.6 interface vlan1 OS10(config)# end OS10# system bash sudo passwd root su - sudo apt-get update -y && sudo apt-get install dnsmasq vim dnsutils vim /etc/hosts Inside of /etc/hosts add some random entry. I added 192.168.1.26 testrouter.lan Run :wq! to save and exit vim Inside of my config /etc/resolv.conf had an erroneous search entry. I had to clear that out to ensure it was valid. Now run systemctl start dnsmasq Test on another host Warning! I have only tried this with VLAN interfaces which show up as a bridge with an IP under the hood. I'm not sure if you apply an IP address directly to an interface that it would still work. Would have to test it.","title":"Running DNS from OS10"},{"location":"Running%20DNS%20from%20OS10/#running-dns-from-os10","text":"","title":"Running DNS from OS10"},{"location":"Running%20DNS%20from%20OS10/#physical-configuration","text":"OS10# show version Dell EMC Networking OS10 Enterprise Copyright (c) 1999-2020 by Dell Inc. All Rights Reserved. OS Version: 10.5.1.3 Build Version: 10.5.1.3.190 Build Time: 2020-06-19T21:48:07+0000 System Type: S4112F-ON Architecture: x86_64 Up Time: 00:30:27","title":"Physical Configuration"},{"location":"Running%20DNS%20from%20OS10/#software-configuration","text":"OS10(config)# configure terminal OS10(config)# ip name-server 192.168.1.6 OS10(config)# interface vlan 1 OS10(conf-if-vl-1)# ip address 192.168.1.26/24 OS10(conf-if-vl-1)# exit OS10(config)# ip route 0.0.0.0/0 192.168.1.6 interface vlan1 OS10(config)# end OS10# system bash sudo passwd root su - sudo apt-get update -y && sudo apt-get install dnsmasq vim dnsutils vim /etc/hosts Inside of /etc/hosts add some random entry. I added 192.168.1.26 testrouter.lan Run :wq! to save and exit vim Inside of my config /etc/resolv.conf had an erroneous search entry. I had to clear that out to ensure it was valid. Now run systemctl start dnsmasq","title":"Software Configuration"},{"location":"Running%20DNS%20from%20OS10/#test-on-another-host","text":"","title":"Test on another host"},{"location":"Running%20DNS%20from%20OS10/#warning","text":"I have only tried this with VLAN interfaces which show up as a bridge with an IP under the hood. I'm not sure if you apply an IP address directly to an interface that it would still work. Would have to test it.","title":"Warning!"},{"location":"SONiC%20-%20Fully%20Automated%20Build/","text":"SONiC - Fully Automated Build See Install Workflow for an overview of how this process flows. Install the Operating System Download SONiC OS for whatever manufacturer. In my case I pulled Dell's stable image. Note: My testing is on a Z9264F-ON. Follow the instructions for setting up and running ONIE Configure ZTP The official ZTP documentation for SONiC is here . Configure DHCP for ZTP The first thing you will need to do is configure the DHCP server servicing the devices to provide option 67 which will point to initial boot file used by SONiC's ZTP agent. The options for URL are defined here . I used PFSense to provide DHCP. On the DHCP server section I added option 67 with type string and value \"http://192.168.1.95:80/initial.json\". The name of the JSON file doesn't matter. Configure Your HTTP Server Next on the HTTP server you will need to add two files. The first is the aforementioned initial configuration file. A copy of mine is below. { \"ztp\": { \"configdb-json\" : { \"url\": { \"source\": \"http://192.168.1.95:80/config_db.json\" } } } } The source field points to the actual configuration file you want to deploy to the networking device. On your web server you will also need to provide this configuration file. In my case I only had it update the device's management IP address: { \"MGMT_INTERFACE\": { \"eth0|192.168.1.96/24\": { \"gwaddr\": \"10.11.12.1\" } } } This file's sections are identical to what is in /etc/sonic/config_db.json . If you want to see what something should look like you can look there and then copy/paste. Running ZTP ZTP will run by default on a new device but if you want to force it to run you can make sure it is on with ztp enable and then ztp run to force it to run. Get Command Line Run sonic-cli to get an OS-10 style command line. TODO First I want to get it working with a fixed file, then we'll do dynamic content Build out a templating system for provisioning multiple switches in Ansible","title":"SONiC - Fully Automated Build"},{"location":"SONiC%20-%20Fully%20Automated%20Build/#sonic-fully-automated-build","text":"See Install Workflow for an overview of how this process flows.","title":"SONiC - Fully Automated Build"},{"location":"SONiC%20-%20Fully%20Automated%20Build/#install-the-operating-system","text":"Download SONiC OS for whatever manufacturer. In my case I pulled Dell's stable image. Note: My testing is on a Z9264F-ON. Follow the instructions for setting up and running ONIE","title":"Install the Operating System"},{"location":"SONiC%20-%20Fully%20Automated%20Build/#configure-ztp","text":"The official ZTP documentation for SONiC is here .","title":"Configure ZTP"},{"location":"SONiC%20-%20Fully%20Automated%20Build/#configure-dhcp-for-ztp","text":"The first thing you will need to do is configure the DHCP server servicing the devices to provide option 67 which will point to initial boot file used by SONiC's ZTP agent. The options for URL are defined here . I used PFSense to provide DHCP. On the DHCP server section I added option 67 with type string and value \"http://192.168.1.95:80/initial.json\". The name of the JSON file doesn't matter.","title":"Configure DHCP for ZTP"},{"location":"SONiC%20-%20Fully%20Automated%20Build/#configure-your-http-server","text":"Next on the HTTP server you will need to add two files. The first is the aforementioned initial configuration file. A copy of mine is below. { \"ztp\": { \"configdb-json\" : { \"url\": { \"source\": \"http://192.168.1.95:80/config_db.json\" } } } } The source field points to the actual configuration file you want to deploy to the networking device. On your web server you will also need to provide this configuration file. In my case I only had it update the device's management IP address: { \"MGMT_INTERFACE\": { \"eth0|192.168.1.96/24\": { \"gwaddr\": \"10.11.12.1\" } } } This file's sections are identical to what is in /etc/sonic/config_db.json . If you want to see what something should look like you can look there and then copy/paste.","title":"Configure Your HTTP Server"},{"location":"SONiC%20-%20Fully%20Automated%20Build/#running-ztp","text":"ZTP will run by default on a new device but if you want to force it to run you can make sure it is on with ztp enable and then ztp run to force it to run.","title":"Running ZTP"},{"location":"SONiC%20-%20Fully%20Automated%20Build/#get-command-line","text":"Run sonic-cli to get an OS-10 style command line.","title":"Get Command Line"},{"location":"SONiC%20-%20Fully%20Automated%20Build/#todo","text":"First I want to get it working with a fixed file, then we'll do dynamic content Build out a templating system for provisioning multiple switches in Ansible","title":"TODO"},{"location":"SONiC%20-%20Fully%20Automated%20Build/webserver/","text":"Configuring the Webserver Requirements: - Python 3 - flask (install with pip install flask ) Run the config server from this directory with python config_server.py --host 0.0.0.0 --port 80","title":"Configuring the Webserver"},{"location":"SONiC%20-%20Fully%20Automated%20Build/webserver/#configuring-the-webserver","text":"Requirements: - Python 3 - flask (install with pip install flask ) Run the config server from this directory with python config_server.py --host 0.0.0.0 --port 80","title":"Configuring the Webserver"},{"location":"Sample%20Datacenter%20Automation%20Architecture/","text":"Sample Datacenter Automation Architecture Sample Datacenter Automation Architecture (PDF) Sample Datacenter Automation Architecture (Visio)","title":"Sample Datacenter Automation Architecture"},{"location":"Sample%20Datacenter%20Automation%20Architecture/#sample-datacenter-automation-architecture","text":"Sample Datacenter Automation Architecture (PDF) Sample Datacenter Automation Architecture (Visio)","title":"Sample Datacenter Automation Architecture"},{"location":"Set%20Up%20RSPAN%20on%20OS10/","text":"Set Up RSPAN on OS10 Setup Source port for span is Switch 1, 1/1/9 Our goal is to move the traffic through switch 2, which is just an intermediary switch, to port 1/1/11 on switch 3 which will finally move it out port 1/1/12 where our laptop is listening. We will use VLAN 99 to transport our RSPAN traffic Explanation One of the things that's different about the Dell configuration is the use of ACLs in the configuration. The way OS10 sees RSPAN is on the source side you grab whatever source you want and then you push that to a special \"remote\" VLAN. This is indicated only on the source switch itself. Then, each other intermediate and destination switch uses an ACL to indicate what traffic should be captured from that VLAN and subsequently forwarded. Finally, on the destination a regular local monitor session is used to pull the traffic from the RSPAN VLAN and push it t o network sensors. Switch 1 Version OS10# show version Dell EMC Networking OS10 Enterprise Copyright (c) 1999-2020 by Dell Inc. All Rights Reserved. OS Version: 10.5.1.3 Build Version: 10.5.1.3.190 Build Time: 2020-06-19T21:48:07+0000 System Type: S4112F-ON Architecture: x86_64 Up Time: 18 weeks 3 days 11:10:52 Setup configure terminal interface vlan 99 description remote_span remote-span exit interface ethernet 1/1/13:1 no shutdown switchport mode trunk switchport trunk allowed vlan 99 exit monitor session 18 type rpm-source source interface ethernet 1/1/9 both destination remote-vlan 99 no shut Switch 2 Version Dell EMC Networking OS10 Enterprise Copyright (c) 1999-2020 by Dell Inc. All Rights Reserved. OS Version: 10.5.1.3 Build Version: 10.5.1.3.190 Build Time: 2020-06-19T21:48:07+0000 System Type: S4112F-ON Architecture: x86_64 Up Time: 00:48:32 Setup configure terminal interface vlan 99 exit interface range ethernet 1/1/11 switchport mode trunk switchport trunk allowed vlan 99 exit interface ethernet 1/1/12 mac access-group rspan in switchport mode trunk switchport trunk allowed vlan 99 exit monitor session 1 destination interface ethernet1/1/11 flow-based enable source interface ethernet1/1/12 no shut mac access-list rspan seq 10 permit any any capture session 1 vlan 99 Switch 3 Version OS10# show version Dell EMC Networking OS10 Enterprise Copyright (c) 1999-2020 by Dell Inc. All Rights Reserved. OS Version: 10.5.1.3 Build Version: 10.5.1.3.190 Build Time: 2020-06-19T21:48:07+0000 System Type: S4112F-ON Architecture: x86_64 Up Time: 03:00:15 Setup configure terminal interface vlan 99 no shut exit mac access-list rpan seq 10 permit any any capture session 1 vlan 99 interface ethernet 1/1/11 switchport mode trunk switchport trunk allowed vlan 99 mac access-group rpan in exit interface ethernet 1/1/12 no shut exit monitor session 1 destination interface ethernet1/1/12 flow-based enable source interface ethernet1/1/11 no shut Notice here that there is an access list which captures the traffic on the inbound trunk interface. This access list is required. Also notice that on our monitor session we have flow-based enable which is also a requirement. How RSPAN Works Under the Hood When you run commands in OS10 what you're really doing is calling into wrapper functions which ultimately do two things: configure the NPU and configure Debian. The commands you run are actually governed by a series of YANG files which exhaustively define what commands are acceptable and in what contexts. For example, the rpm-source command is defined in an enum here in /alt/opt/dell/os10/share/yang-models/dell-span.yang : typedef monitoring-types { type enumeration { enum \"local\" { value 1; description \"PM local session.\"; } enum \"rpm-source\" { value 2; description \"RPM source session.\"; } enum \"erpm-source\" { value 3; description \"ERPM source session.\"; } } description \"Monitor session types.\"; } These definitions are combined with XML files which ultimately define the exact syntax of each command. For example, /alt/opt/dell/os10/clisystem/command-tree/span.xml defines the syntax for SPAN commands (note how the yang_name key references ): <!-- monitor session <id> type [local|rpm-source|rpm-destination|erpm-source]--> <PARAM name=\"type\" help=\"Monitor session type\" optional=\"true\" ptype=\"SUBCOMMAND\" mode=\"subcommand\"> <PARAM name=\"spantype\" help=\"PM/RPM/ERPM sessions\" default=\"local\" ptype=\"SPANMODE\" yang_name=\"/sessions/session/monitortype=${spantype}\"> </PARAM> </PARAM> I didn't exhaustively reverse engineer this part but at some point those commands and their arguments are interpreted and pushed to a series of Python modules. You'll see below how VLANs are set up using brctl . That setup seems to be handled by /alt/opt/dell/os10/lib/python/dn_base_br_tool.py . Ex: def create_br(bname): \"\"\"Method to create a bridge. Args: bname (str): Name of the Bridge Returns: bool: The return value. True for success, False otherwise \"\"\" res = [] if run_command([BRCTL_CMD, 'addbr', bname], res) == 0: return True return False Remote SPAN VLAN gets setup as a bridge and a new dummy interface is created which is a member of that bridge: admin@OS10:/home/admin$ ip a s ..... 38: br99: <BROADCAST,MULTICAST,PROMISC,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default qlen 1000 link/ether 50:9a:4c:d6:0a:a1 brd ff:ff:ff:ff:ff:ff inet6 fe80::529a:4cff:fed6:aa1/64 scope link valid_lft forever preferred_lft forever 39: e101-013-1.99@e101-013-1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue master br99 state UP group default qlen 1000 link/ether 50:9a:4c:d6:0a:7d brd ff:ff:ff:ff:ff:ff root@OS10:~# brctl show br99 bridge name bridge id STP enabled interfaces br99 8000.509a4cd60aa1 no e101-013-1.99 Under the hood the NPU is instructed to push traffic coming in on port 1/1/9 to this bridge which will then forward it. If you really want to dig under the covers this is how that works. CPS (the userland tool which interacts with a database for configuring the switch) will receive the command. You can drop to command line with system bash and then go take a look at /alt/opt/dell/os10/bin/cps_config_mirror.py and actually see where the call to NAS is made. Note: The NAS (Network Abstraction Service) is responsible for abstracting the chipset (broadcom) API from the rest of the OS above. This is the definition for nas_mirror_add_source and nas_mirror_op: def nas_mirror_op(op, data_dict,commit=True): obj = cps_utils.CPSObject( module=\"base-mirror/entry\", data=data_dict) if commit: nas_common.get_cb_method(op)(obj) else: return obj def nas_mirror_add_source(obj,intf,direction): l = [\"intf\",\"0\",\"src\"] obj.add_embed_attr(l,nas_os_utils.if_nametoindex(intf)) l[2]=\"direction\" obj.add_embed_attr(l,direction_type[direction]) if commit is referring to a commit to the CPS database. The definition for get_cb_method is in /alt/opt/dell/os10/lib/python/nas_common_utils.py str_to_cps_cb = { \"create\": create, \"set\": set, \"delete\": delete, \"get\": get, \"rpc\": rpc, } def get_cb_method(op): return str_to_cps_cb[op] What's happening here is it's taking the CPS object which has our instructions which basically say \"make me an RSPAN\" and the argument (op) will be either create or set. What this is going to do is update the CPS database with our new configuration. This is the NAS' Northbound CPS APi being called here. on the southbound end it will call into the SAI which is the actual manufacturer's (Broadcom in our case) API. This process is described in detail here . The NDI represents this call to the SAI API. For remote mirrors that is defined here . You can explore it function by function but the TLDR version is it's going to create a struct with the info relevant to creating the monitor session and it's going to feed that info the SAI. That magic happens in this function: t_std_error ndi_mirror_create_session(ndi_mirror_entry_t * entry){ if(entry == NULL ){ NDI_MIRROR_LOG(ERR,0,\"NDI Mirror entry passed to create Mirror session is NULL\"); return STD_ERR(MIRROR,PARAM,0); } sai_status_t sai_ret; sai_attribute_t sai_mirror_attr_list[MAX_MIRROR_SAI_ATTR]; unsigned int ndi_mirror_attr_count = 0; if(!ndi_mirror_fill_common_attr(entry,sai_mirror_attr_list,ndi_mirror_attr_count)){ return STD_ERR(MIRROR,PARAM,0); } if(entry->mode == BASE_MIRROR_MODE_RSPAN || entry->mode == BASE_MIRROR_MODE_ERSPAN){ ndi_mirror_fill_rspan_attr(entry,sai_mirror_attr_list,ndi_mirror_attr_count); } if(entry->mode == BASE_MIRROR_MODE_ERSPAN){ ndi_mirror_fill_erspan_attr(entry,sai_mirror_attr_list,ndi_mirror_attr_count); } nas_ndi_db_t *ndi_db_ptr = ndi_db_ptr_get(entry->dst_port.npu_id); if ((sai_ret = ndi_mirror_api_get(ndi_db_ptr)->create_mirror_session((sai_object_id_t *) &entry->ndi_mirror_id,ndi_switch_id_get(),ndi_mirror_attr_count,sai_mirror_attr_list)) != SAI_STATUS_SUCCESS) { NDI_MIRROR_LOG(ERR,0,\"Failed to create a new Mirroring Session\"); return STD_ERR(MIRROR, FAIL, sai_ret); } NDI_MIRROR_LOG(INFO,3,\"Created new mirroring session with Id %\" PRIu64 \" \",entry->ndi_mirror_id); return STD_ERR_OK; } What's going on here is the function ndi_mirror_create_session is going to receive a pointer to a struct of type ndi_mirror_entry_t . That struct contains all the relevant info for creating the mirror, port ID, mode (RSPAN), VLAN, etc. That struct is going to be filled out by two functions - ndi_mirror_fill_common_attr and ndi_mirror_fill_rspan_attr . Finally it will be fed into the code at line 199 and this is where we lose track of it. This is where we drop off because below this is Broadcom's API and they don't publicly publish that stuff. At this point though you're calling into Broadcom's SDK which is going to program the NPU to behave a certain way.","title":"Set Up RSPAN on OS10"},{"location":"Set%20Up%20RSPAN%20on%20OS10/#set-up-rspan-on-os10","text":"","title":"Set Up RSPAN on OS10"},{"location":"Set%20Up%20RSPAN%20on%20OS10/#setup","text":"Source port for span is Switch 1, 1/1/9 Our goal is to move the traffic through switch 2, which is just an intermediary switch, to port 1/1/11 on switch 3 which will finally move it out port 1/1/12 where our laptop is listening. We will use VLAN 99 to transport our RSPAN traffic","title":"Setup"},{"location":"Set%20Up%20RSPAN%20on%20OS10/#explanation","text":"One of the things that's different about the Dell configuration is the use of ACLs in the configuration. The way OS10 sees RSPAN is on the source side you grab whatever source you want and then you push that to a special \"remote\" VLAN. This is indicated only on the source switch itself. Then, each other intermediate and destination switch uses an ACL to indicate what traffic should be captured from that VLAN and subsequently forwarded. Finally, on the destination a regular local monitor session is used to pull the traffic from the RSPAN VLAN and push it t o network sensors.","title":"Explanation"},{"location":"Set%20Up%20RSPAN%20on%20OS10/#switch-1","text":"","title":"Switch 1"},{"location":"Set%20Up%20RSPAN%20on%20OS10/#version","text":"OS10# show version Dell EMC Networking OS10 Enterprise Copyright (c) 1999-2020 by Dell Inc. All Rights Reserved. OS Version: 10.5.1.3 Build Version: 10.5.1.3.190 Build Time: 2020-06-19T21:48:07+0000 System Type: S4112F-ON Architecture: x86_64 Up Time: 18 weeks 3 days 11:10:52","title":"Version"},{"location":"Set%20Up%20RSPAN%20on%20OS10/#setup_1","text":"configure terminal interface vlan 99 description remote_span remote-span exit interface ethernet 1/1/13:1 no shutdown switchport mode trunk switchport trunk allowed vlan 99 exit monitor session 18 type rpm-source source interface ethernet 1/1/9 both destination remote-vlan 99 no shut","title":"Setup"},{"location":"Set%20Up%20RSPAN%20on%20OS10/#switch-2","text":"","title":"Switch 2"},{"location":"Set%20Up%20RSPAN%20on%20OS10/#version_1","text":"Dell EMC Networking OS10 Enterprise Copyright (c) 1999-2020 by Dell Inc. All Rights Reserved. OS Version: 10.5.1.3 Build Version: 10.5.1.3.190 Build Time: 2020-06-19T21:48:07+0000 System Type: S4112F-ON Architecture: x86_64 Up Time: 00:48:32","title":"Version"},{"location":"Set%20Up%20RSPAN%20on%20OS10/#setup_2","text":"configure terminal interface vlan 99 exit interface range ethernet 1/1/11 switchport mode trunk switchport trunk allowed vlan 99 exit interface ethernet 1/1/12 mac access-group rspan in switchport mode trunk switchport trunk allowed vlan 99 exit monitor session 1 destination interface ethernet1/1/11 flow-based enable source interface ethernet1/1/12 no shut mac access-list rspan seq 10 permit any any capture session 1 vlan 99","title":"Setup"},{"location":"Set%20Up%20RSPAN%20on%20OS10/#switch-3","text":"","title":"Switch 3"},{"location":"Set%20Up%20RSPAN%20on%20OS10/#version_2","text":"OS10# show version Dell EMC Networking OS10 Enterprise Copyright (c) 1999-2020 by Dell Inc. All Rights Reserved. OS Version: 10.5.1.3 Build Version: 10.5.1.3.190 Build Time: 2020-06-19T21:48:07+0000 System Type: S4112F-ON Architecture: x86_64 Up Time: 03:00:15","title":"Version"},{"location":"Set%20Up%20RSPAN%20on%20OS10/#setup_3","text":"configure terminal interface vlan 99 no shut exit mac access-list rpan seq 10 permit any any capture session 1 vlan 99 interface ethernet 1/1/11 switchport mode trunk switchport trunk allowed vlan 99 mac access-group rpan in exit interface ethernet 1/1/12 no shut exit monitor session 1 destination interface ethernet1/1/12 flow-based enable source interface ethernet1/1/11 no shut Notice here that there is an access list which captures the traffic on the inbound trunk interface. This access list is required. Also notice that on our monitor session we have flow-based enable which is also a requirement.","title":"Setup"},{"location":"Set%20Up%20RSPAN%20on%20OS10/#how-rspan-works-under-the-hood","text":"When you run commands in OS10 what you're really doing is calling into wrapper functions which ultimately do two things: configure the NPU and configure Debian. The commands you run are actually governed by a series of YANG files which exhaustively define what commands are acceptable and in what contexts. For example, the rpm-source command is defined in an enum here in /alt/opt/dell/os10/share/yang-models/dell-span.yang : typedef monitoring-types { type enumeration { enum \"local\" { value 1; description \"PM local session.\"; } enum \"rpm-source\" { value 2; description \"RPM source session.\"; } enum \"erpm-source\" { value 3; description \"ERPM source session.\"; } } description \"Monitor session types.\"; } These definitions are combined with XML files which ultimately define the exact syntax of each command. For example, /alt/opt/dell/os10/clisystem/command-tree/span.xml defines the syntax for SPAN commands (note how the yang_name key references ): <!-- monitor session <id> type [local|rpm-source|rpm-destination|erpm-source]--> <PARAM name=\"type\" help=\"Monitor session type\" optional=\"true\" ptype=\"SUBCOMMAND\" mode=\"subcommand\"> <PARAM name=\"spantype\" help=\"PM/RPM/ERPM sessions\" default=\"local\" ptype=\"SPANMODE\" yang_name=\"/sessions/session/monitortype=${spantype}\"> </PARAM> </PARAM> I didn't exhaustively reverse engineer this part but at some point those commands and their arguments are interpreted and pushed to a series of Python modules. You'll see below how VLANs are set up using brctl . That setup seems to be handled by /alt/opt/dell/os10/lib/python/dn_base_br_tool.py . Ex: def create_br(bname): \"\"\"Method to create a bridge. Args: bname (str): Name of the Bridge Returns: bool: The return value. True for success, False otherwise \"\"\" res = [] if run_command([BRCTL_CMD, 'addbr', bname], res) == 0: return True return False Remote SPAN VLAN gets setup as a bridge and a new dummy interface is created which is a member of that bridge: admin@OS10:/home/admin$ ip a s ..... 38: br99: <BROADCAST,MULTICAST,PROMISC,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default qlen 1000 link/ether 50:9a:4c:d6:0a:a1 brd ff:ff:ff:ff:ff:ff inet6 fe80::529a:4cff:fed6:aa1/64 scope link valid_lft forever preferred_lft forever 39: e101-013-1.99@e101-013-1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue master br99 state UP group default qlen 1000 link/ether 50:9a:4c:d6:0a:7d brd ff:ff:ff:ff:ff:ff root@OS10:~# brctl show br99 bridge name bridge id STP enabled interfaces br99 8000.509a4cd60aa1 no e101-013-1.99 Under the hood the NPU is instructed to push traffic coming in on port 1/1/9 to this bridge which will then forward it. If you really want to dig under the covers this is how that works. CPS (the userland tool which interacts with a database for configuring the switch) will receive the command. You can drop to command line with system bash and then go take a look at /alt/opt/dell/os10/bin/cps_config_mirror.py and actually see where the call to NAS is made. Note: The NAS (Network Abstraction Service) is responsible for abstracting the chipset (broadcom) API from the rest of the OS above. This is the definition for nas_mirror_add_source and nas_mirror_op: def nas_mirror_op(op, data_dict,commit=True): obj = cps_utils.CPSObject( module=\"base-mirror/entry\", data=data_dict) if commit: nas_common.get_cb_method(op)(obj) else: return obj def nas_mirror_add_source(obj,intf,direction): l = [\"intf\",\"0\",\"src\"] obj.add_embed_attr(l,nas_os_utils.if_nametoindex(intf)) l[2]=\"direction\" obj.add_embed_attr(l,direction_type[direction]) if commit is referring to a commit to the CPS database. The definition for get_cb_method is in /alt/opt/dell/os10/lib/python/nas_common_utils.py str_to_cps_cb = { \"create\": create, \"set\": set, \"delete\": delete, \"get\": get, \"rpc\": rpc, } def get_cb_method(op): return str_to_cps_cb[op] What's happening here is it's taking the CPS object which has our instructions which basically say \"make me an RSPAN\" and the argument (op) will be either create or set. What this is going to do is update the CPS database with our new configuration. This is the NAS' Northbound CPS APi being called here. on the southbound end it will call into the SAI which is the actual manufacturer's (Broadcom in our case) API. This process is described in detail here . The NDI represents this call to the SAI API. For remote mirrors that is defined here . You can explore it function by function but the TLDR version is it's going to create a struct with the info relevant to creating the monitor session and it's going to feed that info the SAI. That magic happens in this function: t_std_error ndi_mirror_create_session(ndi_mirror_entry_t * entry){ if(entry == NULL ){ NDI_MIRROR_LOG(ERR,0,\"NDI Mirror entry passed to create Mirror session is NULL\"); return STD_ERR(MIRROR,PARAM,0); } sai_status_t sai_ret; sai_attribute_t sai_mirror_attr_list[MAX_MIRROR_SAI_ATTR]; unsigned int ndi_mirror_attr_count = 0; if(!ndi_mirror_fill_common_attr(entry,sai_mirror_attr_list,ndi_mirror_attr_count)){ return STD_ERR(MIRROR,PARAM,0); } if(entry->mode == BASE_MIRROR_MODE_RSPAN || entry->mode == BASE_MIRROR_MODE_ERSPAN){ ndi_mirror_fill_rspan_attr(entry,sai_mirror_attr_list,ndi_mirror_attr_count); } if(entry->mode == BASE_MIRROR_MODE_ERSPAN){ ndi_mirror_fill_erspan_attr(entry,sai_mirror_attr_list,ndi_mirror_attr_count); } nas_ndi_db_t *ndi_db_ptr = ndi_db_ptr_get(entry->dst_port.npu_id); if ((sai_ret = ndi_mirror_api_get(ndi_db_ptr)->create_mirror_session((sai_object_id_t *) &entry->ndi_mirror_id,ndi_switch_id_get(),ndi_mirror_attr_count,sai_mirror_attr_list)) != SAI_STATUS_SUCCESS) { NDI_MIRROR_LOG(ERR,0,\"Failed to create a new Mirroring Session\"); return STD_ERR(MIRROR, FAIL, sai_ret); } NDI_MIRROR_LOG(INFO,3,\"Created new mirroring session with Id %\" PRIu64 \" \",entry->ndi_mirror_id); return STD_ERR_OK; } What's going on here is the function ndi_mirror_create_session is going to receive a pointer to a struct of type ndi_mirror_entry_t . That struct contains all the relevant info for creating the mirror, port ID, mode (RSPAN), VLAN, etc. That struct is going to be filled out by two functions - ndi_mirror_fill_common_attr and ndi_mirror_fill_rspan_attr . Finally it will be fed into the code at line 199 and this is where we lose track of it. This is where we drop off because below this is Broadcom's API and they don't publicly publish that stuff. At this point though you're calling into Broadcom's SDK which is going to program the NPU to behave a certain way.","title":"How RSPAN Works Under the Hood"},{"location":"Setting%20Up%20Breakout%20Cables/","text":"Setting Up Breakout Cables Helpful Links Dell OS10 Manual Platform Information WARNING If you are using QSFPs you must use Dell branded QSFPs! For breakout mode to work it requires physical modification of the QSFP. It is possible other vendors may make these modifications, but most of the time they do not. When I tested with Intel QSFPs it did not work. Physical Configuration OS Information OS10# show version Dell EMC Networking OS10-Enterprise Copyright (c) 1999-2019 by Dell Inc. All Rights Reserved. OS Version: 10.4.2.2 Build Version: 10.4.2.2.265 Build Time: 2019-01-14T15:15:14-0800 System Type: S4112F-ON Architecture: x86_64 Up Time: 00:13:36 Configure Management Port See Configure Management Interface on Dell OS10 Configure Breakout Port Automatically Run: OS10(config)# feature auto-breakout OS10(config)# do write memory You can then access the interfaces individually using the subport: OS10(config)# interface ethernet 1/1/13:1 Configure Breakout Port Manually Run: OS10(config)# interface breakout 1/1/13 map 10g-4x OS10(config)# do write memory","title":"Setting Up Breakout Cables"},{"location":"Setting%20Up%20Breakout%20Cables/#setting-up-breakout-cables","text":"","title":"Setting Up Breakout Cables"},{"location":"Setting%20Up%20Breakout%20Cables/#helpful-links","text":"Dell OS10 Manual","title":"Helpful Links"},{"location":"Setting%20Up%20Breakout%20Cables/#platform-information","text":"WARNING If you are using QSFPs you must use Dell branded QSFPs! For breakout mode to work it requires physical modification of the QSFP. It is possible other vendors may make these modifications, but most of the time they do not. When I tested with Intel QSFPs it did not work.","title":"Platform Information"},{"location":"Setting%20Up%20Breakout%20Cables/#physical-configuration","text":"","title":"Physical Configuration"},{"location":"Setting%20Up%20Breakout%20Cables/#os-information","text":"OS10# show version Dell EMC Networking OS10-Enterprise Copyright (c) 1999-2019 by Dell Inc. All Rights Reserved. OS Version: 10.4.2.2 Build Version: 10.4.2.2.265 Build Time: 2019-01-14T15:15:14-0800 System Type: S4112F-ON Architecture: x86_64 Up Time: 00:13:36","title":"OS Information"},{"location":"Setting%20Up%20Breakout%20Cables/#configure-management-port","text":"See Configure Management Interface on Dell OS10","title":"Configure Management Port"},{"location":"Setting%20Up%20Breakout%20Cables/#configure-breakout-port-automatically","text":"Run: OS10(config)# feature auto-breakout OS10(config)# do write memory You can then access the interfaces individually using the subport: OS10(config)# interface ethernet 1/1/13:1","title":"Configure Breakout Port Automatically"},{"location":"Setting%20Up%20Breakout%20Cables/#configure-breakout-port-manually","text":"Run: OS10(config)# interface breakout 1/1/13 map 10g-4x OS10(config)# do write memory","title":"Configure Breakout Port Manually"},{"location":"Setting%20Up%20SmartFabric%20Director/","text":"Setting Up SmartFabric Director Helpful Links SmartFabric Director Download Page Dell OS10 User's Guide Dell SmartFabric Director User's Guide My Configuration Dell 4112F-ON Dell EMC Networking OS10 Enterprise Copyright (c) 1999-2020 by Dell Inc. All Rights Reserved. OS Version: 10.5.1.0 Build Version: 10.5.1.0.124 Build Time: 2020-02-12T09:05:20+0000 System Type: S4112F-ON Architecture: x86_64 Up Time: 01:06:30 Dell OS10 Running in GNS3 I used the GNS3 VM and tied the switch with a virtual cloud into my actual infrastructure. Dell EMC Networking OS10 Enterprise Copyright (c) 1999-2019 by Dell Inc. All Rights Reserved. OS Version: 10.5.0.0 Build Version: 10.5.0.0.326 Build Time: 2019-08-07T00:12:30+0000 System Type: S5248F-VM Architecture: x86_64 Up Time: 00:32:14 Setup Download, extract, upload to vCenter (or ESXi) Fill in network settings. When it finishes importing, keep in mind that the username and password is: Username: admin@sfd.local Password: The password you set I had problems when I used a DNS name for NTP. See: BUG Go to the Fabric Designer online. Enter your design in the Fabric Designer. The point of the Fabric Designer is to determine all of those materials you will need in order to build out your stack. It will provide you a lot of useful info, for example a bill of materials: ![](images/bom.PNG) It will also give you a logical view of your network ![](images/logical_view.PNG) A view of your networking ![](images/network_view.PNG) It will also allow you to download the configurations for each switch - either as a regular config or as a zero touch configuration. You can also save the overall design. Problems Encountered On the 4112F-ON These were taken from the physical switch running 10.5.0.4.638 I have confirmed I am in full switch mode: OS10# show switch-operating-mode Switch-Operating-Mode : Full Switch Mode Page 10 step 6 of the manual does not work. There is no cert command. Page 11 step 1 of the manual does not work. There is no switch-port-profile command. With the VM See bug notes .","title":"Setting Up SmartFabric Director"},{"location":"Setting%20Up%20SmartFabric%20Director/#setting-up-smartfabric-director","text":"","title":"Setting Up SmartFabric Director"},{"location":"Setting%20Up%20SmartFabric%20Director/#helpful-links","text":"SmartFabric Director Download Page Dell OS10 User's Guide Dell SmartFabric Director User's Guide","title":"Helpful Links"},{"location":"Setting%20Up%20SmartFabric%20Director/#my-configuration","text":"","title":"My Configuration"},{"location":"Setting%20Up%20SmartFabric%20Director/#dell-4112f-on","text":"Dell EMC Networking OS10 Enterprise Copyright (c) 1999-2020 by Dell Inc. All Rights Reserved. OS Version: 10.5.1.0 Build Version: 10.5.1.0.124 Build Time: 2020-02-12T09:05:20+0000 System Type: S4112F-ON Architecture: x86_64 Up Time: 01:06:30","title":"Dell 4112F-ON"},{"location":"Setting%20Up%20SmartFabric%20Director/#dell-os10-running-in-gns3","text":"I used the GNS3 VM and tied the switch with a virtual cloud into my actual infrastructure. Dell EMC Networking OS10 Enterprise Copyright (c) 1999-2019 by Dell Inc. All Rights Reserved. OS Version: 10.5.0.0 Build Version: 10.5.0.0.326 Build Time: 2019-08-07T00:12:30+0000 System Type: S5248F-VM Architecture: x86_64 Up Time: 00:32:14","title":"Dell OS10 Running in GNS3"},{"location":"Setting%20Up%20SmartFabric%20Director/#setup","text":"Download, extract, upload to vCenter (or ESXi) Fill in network settings. When it finishes importing, keep in mind that the username and password is: Username: admin@sfd.local Password: The password you set I had problems when I used a DNS name for NTP. See: BUG Go to the Fabric Designer online. Enter your design in the Fabric Designer. The point of the Fabric Designer is to determine all of those materials you will need in order to build out your stack. It will provide you a lot of useful info, for example a bill of materials: ![](images/bom.PNG) It will also give you a logical view of your network ![](images/logical_view.PNG) A view of your networking ![](images/network_view.PNG) It will also allow you to download the configurations for each switch - either as a regular config or as a zero touch configuration. You can also save the overall design.","title":"Setup"},{"location":"Setting%20Up%20SmartFabric%20Director/#problems-encountered","text":"","title":"Problems Encountered"},{"location":"Setting%20Up%20SmartFabric%20Director/#on-the-4112f-on","text":"These were taken from the physical switch running 10.5.0.4.638 I have confirmed I am in full switch mode: OS10# show switch-operating-mode Switch-Operating-Mode : Full Switch Mode Page 10 step 6 of the manual does not work. There is no cert command. Page 11 step 1 of the manual does not work. There is no switch-port-profile command.","title":"On the 4112F-ON"},{"location":"Setting%20Up%20SmartFabric%20Director/#with-the-vm","text":"See bug notes .","title":"With the VM"},{"location":"Setting%20Up%20SmartFabric%20Director/BUG/","text":"Bug Write Up sfd_init.sh fails because it incorrectly checks ntp status. The failure condition can be replicated by using an ntp server of time.google.com on template import for the OVA. Version SFD SFD version is 1.1.0. Ubuntu admin@sfd.local@sfd:~$ cat /etc/*-release DISTRIB_ID=Ubuntu DISTRIB_RELEASE=16.04 DISTRIB_CODENAME=xenial DISTRIB_DESCRIPTION=\"Ubuntu 16.04.6 LTS\" NAME=\"Ubuntu\" VERSION=\"16.04.6 LTS (Xenial Xerus)\" ID=ubuntu ID_LIKE=debian PRETTY_NAME=\"Ubuntu 16.04.6 LTS\" VERSION_ID=\"16.04\" HOME_URL=\"http://www.ubuntu.com/\" SUPPORT_URL=\"http://help.ubuntu.com/\" BUG_REPORT_URL=\"http://bugs.launchpad.net/ubuntu/\" VERSION_CODENAME=xenial UBUNTU_CODENAME=xenial Suggested Patch Patch admin@sfd.local@sfd:~$ diff -c sfd_init.sh.original sfd_init.sh.patch *** sfd_init.sh.original 2020-02-04 21:58:22.327755522 +0000 --- sfd_init.sh.patch 2020-02-04 21:59:38.651751964 +0000 *************** *** 188,194 **** echo \"Checking if NTP is configured.\" until [[ $cnt -ge $max_tries ]] do ! ntpq -pn | awk '{print $2}' | xargs | grep -E -o \"([0-9]{1,3}[\\.]){3}[0-9]{1,3}|PPS|GPS|LOCAL|LOCL\" [[ \"$?\" -eq 0 ]] && echo \"NTP synchronized successfully.\" && return 0 cnt=$[$cnt+1] echo \"Waiting for NTP to get configured. Sleeping for 10 seconds...\" --- 188,194 ---- echo \"Checking if NTP is configured.\" until [[ $cnt -ge $max_tries ]] do ! ntpq -pn | awk '{print $1}' | sed -n '/^\\*/p' [[ \"$?\" -eq 0 ]] && echo \"NTP synchronized successfully.\" && return 0 cnt=$[$cnt+1] echo \"Waiting for NTP to get configured. Sleeping for 10 seconds...\" Explanation See below for a detailed explanation of my troubleshooting and additional information. Based on the regex looking for an IP I'm guessing the intent was to verify that the server is synched to a remote NTP server. However there are two problems. The first is that it is looking at the refid (column 2) which is not the server you are synched to, but the server that the server you are pointing at is synched to. Second, there is no guarentee that an IP address will be in this column (or PPS/GPS/LOCAL/LOCL). This was my case. Instead, I would check for the line that has the synchronized server (starts with *). Using ntpq -pn | awk '{print $1}' | sed -n '/^\\*/p' . You don't really need to check beyond that because it won't synchronize unless everything is working. Though if you want to take it a step further you could do: ntpq -pn | awk '{print $1}' | sed -n '/^\\*/p' | sed 's/\\*//g' | grep -E -o \"([0-9]{1,3}[\\.]){3}[0-9]{1,3}|PPS|GPS|LOCAL|LOCL\" . Detailed Explanation Page 17 step 1 of the manual doesn't work. There is no web server listening. It looks like on the first run the SFD service failed. There is no obvious log information output from the service. Suggest that sfd_init.sh print the log path to stdout on failure so that getting the service status will more obviously provide direction on where the problem is. # Dumped listening ports admin@sfd.local@sfd:~$ ss -ltn State Recv-Q Send-Q Local Address:Port Peer Address:Port LISTEN 0 128 127.0.0.1:39125 *:* LISTEN 0 128 *:22 *:* LISTEN 0 128 :::9100 :::* LISTEN 0 128 :::5555 :::* LISTEN 0 128 :::22 :::* # Checked service status after figuring out SFD ran a service admin@sfd.local@sfd:~$ systemctl status sfd \u25cf sfd.service - NFC Bootstrap Service Loaded: loaded (/etc/systemd/system/sfd.service; enabled; vendor preset: enabled) Active: failed (Result: exit-code) since Tue 2020-02-04 20:38:10 UTC; 18min ago Main PID: 1327 (code=exited, status=1/FAILURE) Entire system will hard fail if NTP cannot start. I realized this is the cause of the failure above. This should not be the default behavior. Should continue and issue a warning. Most customers working on servers would not know how to troubleshoot this. \u25cf ntp.service - LSB: Start NTP daemon Loaded: loaded (/etc/init.d/ntp; bad; vendor preset: enabled) Active: active (running) since Wed 2020-02-05 03:50:34 UTC; 10s ago Docs: man:systemd-sysv-generator(8) Process: 2235 ExecStop=/etc/init.d/ntp stop (code=exited, status=0/SUCCESS) Process: 2248 ExecStart=/etc/init.d/ntp start (code=exited, status=0/SUCCESS) Tasks: 2 Memory: 1.5M CPU: 12ms CGroup: /system.slice/ntp.service \u2514\u25002260 /usr/sbin/ntpd -p /var/run/ntpd.pid -g -u 112:117 Feb 05 03:50:34 sfd ntpd[2260]: Listen and drop on 1 v4wildcard 0.0.0.0:123 Feb 05 03:50:34 sfd ntpd[2260]: Listen normally on 2 lo 127.0.0.1:123 Feb 05 03:50:34 sfd ntpd[2260]: Listen normally on 3 ens192 192.168.1.31:123 Feb 05 03:50:34 sfd ntpd[2260]: Listen normally on 4 lo [::1]:123 Feb 05 03:50:34 sfd ntpd[2260]: Listening on routing socket on fd #21 for interface updates Feb 05 03:50:35 sfd ntpd[2260]: Soliciting pool server 216.239.35.12 Feb 05 03:50:36 sfd ntpd[2260]: Soliciting pool server 216.239.35.4 Feb 05 03:50:37 sfd ntpd[2260]: Soliciting pool server 216.239.35.0 Feb 05 03:50:38 sfd ntpd[2260]: Soliciting pool server 216.239.35.8 Feb 05 03:50:39 sfd ntpd[2260]: Soliciting pool server 2001:4860:4806:: -----RESTART NTP - END----- Checking if NTP service is running. NTP service is running. Checking if NTP is configured. Waiting for NTP to get configured. Sleeping for 10 seconds... ... SNIP ... Waiting for NTP to get configured. Sleeping for 10 seconds... NTP failed to get synced. NTP config failed # This failure seems to be erroneous. Checking the NTP service confirms it is working. admin@sfd.local@sfd:~$ systemctl status ntp \u25cf ntp.service - LSB: Start NTP daemon Loaded: loaded (/etc/init.d/ntp; bad; vendor preset: enabled) Active: active (running) since Wed 2020-02-05 03:50:34 UTC; 6h left Docs: man:systemd-sysv-generator(8) Process: 2235 ExecStop=/etc/init.d/ntp stop (code=exited, status=0/SUCCESS) Process: 2248 ExecStart=/etc/init.d/ntp start (code=exited, status=0/SUCCESS) Tasks: 2 Memory: 1.5M CPU: 81ms CGroup: /system.slice/ntp.service \u2514\u25002260 /usr/sbin/ntpd -p /var/run/ntpd.pid -g -u 112:117 admin@sfd.local@sfd:~$ ntpstat The program 'ntpstat' is currently not installed. You can install it by typing: sudo apt install ntpstat admin@sfd.local@sfd:~$ ntp -qn No command 'ntp' found, but there are 21 similar ones ntp: command not found admin@sfd.local@sfd:~$ ntpq -pn remote refid st t when poll reach delay offset jitter ============================================================================== time.google.com .POOL. 16 p - 64 0 0.000 0.000 0.000 +216.239.35.12 .GOOG. 1 u 54 64 375 64.340 -91.086 8.997 *216.239.35.4 .GOOG. 1 u 62 64 377 62.412 -83.944 8.385 +216.239.35.0 .GOOG. 1 u 42 64 377 63.484 -83.978 10.080 -216.239.35.8 .GOOG. 1 u 21 64 377 72.245 -73.087 15.674 Problem isolated to: max_tries=30 # check counter till 30. (total 300 seconds) echo \"Checking if NTP is configured.\" until [[ $cnt -ge $max_tries ]] do ntpq -pn | awk '{print $2}' | xargs | grep -E -o \"([0-9]{1,3}[\\.]){3}[0-9]{1,3}|PPS|GPS|LOCAL|LOCL\" [[ \"$?\" -eq 0 ]] && echo \"NTP synchronized successfully.\" && return 0 cnt=$[$cnt+1] echo \"Waiting for NTP to get configured. Sleeping for 10 seconds...\" sleep 10 done The offending line is ntpq -pn | awk '{print $2}' | xargs | grep -E -o \"([0-9]{1,3}[\\.]){3}[0-9]{1,3}|PPS|GPS|LOCAL|LOCL\" . ntpq -pn has output formatted like this: admin@sfd.local@sfd:~$ ntpq -pn remote refid st t when poll reach delay offset jitter ============================================================================== time.google.com .POOL. 16 p - 64 0 0.000 0.000 0.000 +216.239.35.12 .GOOG. 1 u 54 64 375 64.340 -91.086 8.997 *216.239.35.4 .GOOG. 1 u 62 64 377 62.412 -83.944 8.385 +216.239.35.0 .GOOG. 1 u 42 64 377 63.484 -83.978 10.080 -216.239.35.8 .GOOG. 1 u 21 64 377 72.245 -73.087 15.674 Based on the regex looking for an IP I'm guessing the intent was to verify that the server is synched to a remote NTP server. However there are two problems. The first is that it is looking at the refid which is not the server you are synched to, but the server that the server you are pointing at is synched to. Second, there is no guarentee that an IP address will be in this column (or PPS/GPS/LOCAL/LOCL). This was my case. See below for my ntpq -pn results. Instead, I would check for the line that has the synchronized server (starts with *). Using ntpq -pn | awk '{print $1}' | sed -n '/^\\*/p' . You don't really need to check beyond that because it won't synchronize unless everything is working. Though if you want to take it a step further you could do: ntpq -pn | awk '{print $1}' | sed -n '/^\\*/p' | sed 's/\\*//g' | grep -E -o \"([0-9]{1,3}[\\.]){3}[0-9]{1,3}|PPS|GPS|LOCAL|LOCL\" . My ntpq -pn with time.google.com as my server admin@sfd.local@sfd:~$ ntpq -pn remote refid st t when poll reach delay offset jitter ============================================================================== time.google.com .POOL. 16 p - 64 0 0.000 0.000 0.000 *216.239.35.12 .GOOG. 1 u 38 64 377 67.370 -0.549 23.161 +216.239.35.4 .GOOG. 1 u 29 64 377 75.466 -1.691 22.089 +216.239.35.0 .GOOG. 1 u 36 64 377 67.731 -1.434 24.201 +216.239.35.8 .GOOG. 1 u 38 64 377 76.705 -0.042 22.436","title":"Bug Write Up"},{"location":"Setting%20Up%20SmartFabric%20Director/BUG/#bug-write-up","text":"sfd_init.sh fails because it incorrectly checks ntp status. The failure condition can be replicated by using an ntp server of time.google.com on template import for the OVA.","title":"Bug Write Up"},{"location":"Setting%20Up%20SmartFabric%20Director/BUG/#version","text":"","title":"Version"},{"location":"Setting%20Up%20SmartFabric%20Director/BUG/#sfd","text":"SFD version is 1.1.0.","title":"SFD"},{"location":"Setting%20Up%20SmartFabric%20Director/BUG/#ubuntu","text":"admin@sfd.local@sfd:~$ cat /etc/*-release DISTRIB_ID=Ubuntu DISTRIB_RELEASE=16.04 DISTRIB_CODENAME=xenial DISTRIB_DESCRIPTION=\"Ubuntu 16.04.6 LTS\" NAME=\"Ubuntu\" VERSION=\"16.04.6 LTS (Xenial Xerus)\" ID=ubuntu ID_LIKE=debian PRETTY_NAME=\"Ubuntu 16.04.6 LTS\" VERSION_ID=\"16.04\" HOME_URL=\"http://www.ubuntu.com/\" SUPPORT_URL=\"http://help.ubuntu.com/\" BUG_REPORT_URL=\"http://bugs.launchpad.net/ubuntu/\" VERSION_CODENAME=xenial UBUNTU_CODENAME=xenial","title":"Ubuntu"},{"location":"Setting%20Up%20SmartFabric%20Director/BUG/#suggested-patch","text":"","title":"Suggested Patch"},{"location":"Setting%20Up%20SmartFabric%20Director/BUG/#patch","text":"admin@sfd.local@sfd:~$ diff -c sfd_init.sh.original sfd_init.sh.patch *** sfd_init.sh.original 2020-02-04 21:58:22.327755522 +0000 --- sfd_init.sh.patch 2020-02-04 21:59:38.651751964 +0000 *************** *** 188,194 **** echo \"Checking if NTP is configured.\" until [[ $cnt -ge $max_tries ]] do ! ntpq -pn | awk '{print $2}' | xargs | grep -E -o \"([0-9]{1,3}[\\.]){3}[0-9]{1,3}|PPS|GPS|LOCAL|LOCL\" [[ \"$?\" -eq 0 ]] && echo \"NTP synchronized successfully.\" && return 0 cnt=$[$cnt+1] echo \"Waiting for NTP to get configured. Sleeping for 10 seconds...\" --- 188,194 ---- echo \"Checking if NTP is configured.\" until [[ $cnt -ge $max_tries ]] do ! ntpq -pn | awk '{print $1}' | sed -n '/^\\*/p' [[ \"$?\" -eq 0 ]] && echo \"NTP synchronized successfully.\" && return 0 cnt=$[$cnt+1] echo \"Waiting for NTP to get configured. Sleeping for 10 seconds...\"","title":"Patch"},{"location":"Setting%20Up%20SmartFabric%20Director/BUG/#explanation","text":"See below for a detailed explanation of my troubleshooting and additional information. Based on the regex looking for an IP I'm guessing the intent was to verify that the server is synched to a remote NTP server. However there are two problems. The first is that it is looking at the refid (column 2) which is not the server you are synched to, but the server that the server you are pointing at is synched to. Second, there is no guarentee that an IP address will be in this column (or PPS/GPS/LOCAL/LOCL). This was my case. Instead, I would check for the line that has the synchronized server (starts with *). Using ntpq -pn | awk '{print $1}' | sed -n '/^\\*/p' . You don't really need to check beyond that because it won't synchronize unless everything is working. Though if you want to take it a step further you could do: ntpq -pn | awk '{print $1}' | sed -n '/^\\*/p' | sed 's/\\*//g' | grep -E -o \"([0-9]{1,3}[\\.]){3}[0-9]{1,3}|PPS|GPS|LOCAL|LOCL\" .","title":"Explanation"},{"location":"Setting%20Up%20SmartFabric%20Director/BUG/#detailed-explanation","text":"Page 17 step 1 of the manual doesn't work. There is no web server listening. It looks like on the first run the SFD service failed. There is no obvious log information output from the service. Suggest that sfd_init.sh print the log path to stdout on failure so that getting the service status will more obviously provide direction on where the problem is. # Dumped listening ports admin@sfd.local@sfd:~$ ss -ltn State Recv-Q Send-Q Local Address:Port Peer Address:Port LISTEN 0 128 127.0.0.1:39125 *:* LISTEN 0 128 *:22 *:* LISTEN 0 128 :::9100 :::* LISTEN 0 128 :::5555 :::* LISTEN 0 128 :::22 :::* # Checked service status after figuring out SFD ran a service admin@sfd.local@sfd:~$ systemctl status sfd \u25cf sfd.service - NFC Bootstrap Service Loaded: loaded (/etc/systemd/system/sfd.service; enabled; vendor preset: enabled) Active: failed (Result: exit-code) since Tue 2020-02-04 20:38:10 UTC; 18min ago Main PID: 1327 (code=exited, status=1/FAILURE) Entire system will hard fail if NTP cannot start. I realized this is the cause of the failure above. This should not be the default behavior. Should continue and issue a warning. Most customers working on servers would not know how to troubleshoot this. \u25cf ntp.service - LSB: Start NTP daemon Loaded: loaded (/etc/init.d/ntp; bad; vendor preset: enabled) Active: active (running) since Wed 2020-02-05 03:50:34 UTC; 10s ago Docs: man:systemd-sysv-generator(8) Process: 2235 ExecStop=/etc/init.d/ntp stop (code=exited, status=0/SUCCESS) Process: 2248 ExecStart=/etc/init.d/ntp start (code=exited, status=0/SUCCESS) Tasks: 2 Memory: 1.5M CPU: 12ms CGroup: /system.slice/ntp.service \u2514\u25002260 /usr/sbin/ntpd -p /var/run/ntpd.pid -g -u 112:117 Feb 05 03:50:34 sfd ntpd[2260]: Listen and drop on 1 v4wildcard 0.0.0.0:123 Feb 05 03:50:34 sfd ntpd[2260]: Listen normally on 2 lo 127.0.0.1:123 Feb 05 03:50:34 sfd ntpd[2260]: Listen normally on 3 ens192 192.168.1.31:123 Feb 05 03:50:34 sfd ntpd[2260]: Listen normally on 4 lo [::1]:123 Feb 05 03:50:34 sfd ntpd[2260]: Listening on routing socket on fd #21 for interface updates Feb 05 03:50:35 sfd ntpd[2260]: Soliciting pool server 216.239.35.12 Feb 05 03:50:36 sfd ntpd[2260]: Soliciting pool server 216.239.35.4 Feb 05 03:50:37 sfd ntpd[2260]: Soliciting pool server 216.239.35.0 Feb 05 03:50:38 sfd ntpd[2260]: Soliciting pool server 216.239.35.8 Feb 05 03:50:39 sfd ntpd[2260]: Soliciting pool server 2001:4860:4806:: -----RESTART NTP - END----- Checking if NTP service is running. NTP service is running. Checking if NTP is configured. Waiting for NTP to get configured. Sleeping for 10 seconds... ... SNIP ... Waiting for NTP to get configured. Sleeping for 10 seconds... NTP failed to get synced. NTP config failed # This failure seems to be erroneous. Checking the NTP service confirms it is working. admin@sfd.local@sfd:~$ systemctl status ntp \u25cf ntp.service - LSB: Start NTP daemon Loaded: loaded (/etc/init.d/ntp; bad; vendor preset: enabled) Active: active (running) since Wed 2020-02-05 03:50:34 UTC; 6h left Docs: man:systemd-sysv-generator(8) Process: 2235 ExecStop=/etc/init.d/ntp stop (code=exited, status=0/SUCCESS) Process: 2248 ExecStart=/etc/init.d/ntp start (code=exited, status=0/SUCCESS) Tasks: 2 Memory: 1.5M CPU: 81ms CGroup: /system.slice/ntp.service \u2514\u25002260 /usr/sbin/ntpd -p /var/run/ntpd.pid -g -u 112:117 admin@sfd.local@sfd:~$ ntpstat The program 'ntpstat' is currently not installed. You can install it by typing: sudo apt install ntpstat admin@sfd.local@sfd:~$ ntp -qn No command 'ntp' found, but there are 21 similar ones ntp: command not found admin@sfd.local@sfd:~$ ntpq -pn remote refid st t when poll reach delay offset jitter ============================================================================== time.google.com .POOL. 16 p - 64 0 0.000 0.000 0.000 +216.239.35.12 .GOOG. 1 u 54 64 375 64.340 -91.086 8.997 *216.239.35.4 .GOOG. 1 u 62 64 377 62.412 -83.944 8.385 +216.239.35.0 .GOOG. 1 u 42 64 377 63.484 -83.978 10.080 -216.239.35.8 .GOOG. 1 u 21 64 377 72.245 -73.087 15.674 Problem isolated to: max_tries=30 # check counter till 30. (total 300 seconds) echo \"Checking if NTP is configured.\" until [[ $cnt -ge $max_tries ]] do ntpq -pn | awk '{print $2}' | xargs | grep -E -o \"([0-9]{1,3}[\\.]){3}[0-9]{1,3}|PPS|GPS|LOCAL|LOCL\" [[ \"$?\" -eq 0 ]] && echo \"NTP synchronized successfully.\" && return 0 cnt=$[$cnt+1] echo \"Waiting for NTP to get configured. Sleeping for 10 seconds...\" sleep 10 done The offending line is ntpq -pn | awk '{print $2}' | xargs | grep -E -o \"([0-9]{1,3}[\\.]){3}[0-9]{1,3}|PPS|GPS|LOCAL|LOCL\" . ntpq -pn has output formatted like this: admin@sfd.local@sfd:~$ ntpq -pn remote refid st t when poll reach delay offset jitter ============================================================================== time.google.com .POOL. 16 p - 64 0 0.000 0.000 0.000 +216.239.35.12 .GOOG. 1 u 54 64 375 64.340 -91.086 8.997 *216.239.35.4 .GOOG. 1 u 62 64 377 62.412 -83.944 8.385 +216.239.35.0 .GOOG. 1 u 42 64 377 63.484 -83.978 10.080 -216.239.35.8 .GOOG. 1 u 21 64 377 72.245 -73.087 15.674 Based on the regex looking for an IP I'm guessing the intent was to verify that the server is synched to a remote NTP server. However there are two problems. The first is that it is looking at the refid which is not the server you are synched to, but the server that the server you are pointing at is synched to. Second, there is no guarentee that an IP address will be in this column (or PPS/GPS/LOCAL/LOCL). This was my case. See below for my ntpq -pn results. Instead, I would check for the line that has the synchronized server (starts with *). Using ntpq -pn | awk '{print $1}' | sed -n '/^\\*/p' . You don't really need to check beyond that because it won't synchronize unless everything is working. Though if you want to take it a step further you could do: ntpq -pn | awk '{print $1}' | sed -n '/^\\*/p' | sed 's/\\*//g' | grep -E -o \"([0-9]{1,3}[\\.]){3}[0-9]{1,3}|PPS|GPS|LOCAL|LOCL\" .","title":"Detailed Explanation"},{"location":"Setting%20Up%20SmartFabric%20Director/BUG/#my-ntpq-pn-with-timegooglecom-as-my-server","text":"admin@sfd.local@sfd:~$ ntpq -pn remote refid st t when poll reach delay offset jitter ============================================================================== time.google.com .POOL. 16 p - 64 0 0.000 0.000 0.000 *216.239.35.12 .GOOG. 1 u 38 64 377 67.370 -0.549 23.161 +216.239.35.4 .GOOG. 1 u 29 64 377 75.466 -1.691 22.089 +216.239.35.0 .GOOG. 1 u 36 64 377 67.731 -1.434 24.201 +216.239.35.8 .GOOG. 1 u 38 64 377 76.705 -0.042 22.436","title":"My ntpq -pn with time.google.com as my server"},{"location":"Setting%20Up%20iDRAC%20Telemetry%20with%20Splunk/","text":"Setting Up iDRAC Telemetry with Splunk Helpful Links Dell API Docs: https://developer.dell.com/apis/2978/versions/5.xx/docs/0WhatsNew.md Redfish Telemetry Whitepaper: https://www.dmtf.org/sites/default/files/standards/documents/DSP2051_1.0.0.pdf Description of the AMQP Messaging Protocol: https://www.ionos.com/digitalguide/websites/web-development/advanced-message-queuing-protocol-amqp/ Setting Up Splunk for the First Time: https://docs.splunk.com/Documentation/Splunk/8.2.4/Installation/StartSplunkforthefirsttime Integrate iDRAC Telemetry Data Into Splunk: Link to PDF My Test Environment RHEL NAME=\"Red Hat Enterprise Linux\" VERSION=\"8.5 (Ootpa)\" ID=\"rhel\" ID_LIKE=\"fedora\" VERSION_ID=\"8.5\" PLATFORM_ID=\"platform:el8\" PRETTY_NAME=\"Red Hat Enterprise Linux 8.5 (Ootpa)\" ANSI_COLOR=\"0;31\" CPE_NAME=\"cpe:/o:redhat:enterprise_linux:8::baseos\" HOME_URL=\"https://www.redhat.com/\" DOCUMENTATION_URL=\"https://access.redhat.com/documentation/red_hat_enterprise_linux/8/\" BUG_REPORT_URL=\"https://bugzilla.redhat.com/\" REDHAT_BUGZILLA_PRODUCT=\"Red Hat Enterprise Linux 8\" REDHAT_BUGZILLA_PRODUCT_VERSION=8.5 REDHAT_SUPPORT_PRODUCT=\"Red Hat Enterprise Linux\" REDHAT_SUPPORT_PRODUCT_VERSION=\"8.5\" Red Hat Enterprise Linux release 8.5 (Ootpa) Red Hat Enterprise Linux release 8.5 (Ootpa) Installation Setup Splunk Download trial of Splunk Follow Splunk installation instructions By default it will install to /opt/splunk. Run /opt/splunk/bin/splunk start (I suggest you do this in tmux or another terminal emulator) Run firewall-cmd --permanent --zone public --add-port=8000/tcp && firewall-cmd --reload Make splunk start on boot with /opt/splunk/bin/splunk enable boot-start Using Syslog Following the instructions here Install podman with dnf install -y podman Follow the instructions here NOTE: When adding the HTTP input in Splunk it failed out because the token weren't enabled. I had to manually edit /opt/splunk/etc/apps/splunk_httpinput/default/inputs.conf and set disabled to 0 then do a systemctl restart splunk Run systemctl stop rsyslog && systemctl disable rsyslog Using ActiveMQ and splunkpump dnf install -y podman mkdir -p mkdir -p /opt/activemq/data && /opt/activemq/conf Run the following to generate default configs: bash podman run --user root --rm -ti -p 61616:61616 -p 8161:8161 -v /opt/activemq/conf:/mnt/conf:z -v /opt/activemq/data:/mnt/data:z rmohr/activemq /bin/sh chown activemq:activemq /mnt/conf chown activemq:activemq /mnt/data cp -a /opt/activemq/conf/* /mnt/conf/ cp -a /opt/activemq/data/* /mnt/data/ exit podman run -p 61616:61616 -p 8161:8161 -v /opt/activemq/conf:/opt/activemq/conf -v /opt/activemq/data:/opt/activemq/data rmohr/activemq Configure the iDRAC Download this script which will enable telemetry reports. Run EnableOrDisableAllTelemetryReports.py -ip $target -u $user -p $password This enables telemetry on the target server Using ActiveMQ and splunkpump Using Syslog Next you will need to enable Redfish alerting which will publish the events to Splunk. Download this script Run the following command SubscriptionManagementREDFISH.py -ip $target -u $user -p $password -c y -D https://$splunkserver/services/collector/raw -E Alert -V Event $target is the ip address or DNS name of the iDRAC $user/$password are the username and password for iDRAC $splunkserver is the IP address or DNS name of your Splunk HTTP event collector instance On the command line (racadm) SSH to the iDRAC Run ``` racadm set idrac.telemetry.RsyslogServer1 \"<splunk_ip/fqdn>\" racadm set idrac.telemetry.RsyslogServer1port \"514\" racadm testrsyslogconnection ```","title":"Setting Up iDRAC Telemetry with Splunk"},{"location":"Setting%20Up%20iDRAC%20Telemetry%20with%20Splunk/#setting-up-idrac-telemetry-with-splunk","text":"","title":"Setting Up iDRAC Telemetry with Splunk"},{"location":"Setting%20Up%20iDRAC%20Telemetry%20with%20Splunk/#helpful-links","text":"Dell API Docs: https://developer.dell.com/apis/2978/versions/5.xx/docs/0WhatsNew.md Redfish Telemetry Whitepaper: https://www.dmtf.org/sites/default/files/standards/documents/DSP2051_1.0.0.pdf Description of the AMQP Messaging Protocol: https://www.ionos.com/digitalguide/websites/web-development/advanced-message-queuing-protocol-amqp/ Setting Up Splunk for the First Time: https://docs.splunk.com/Documentation/Splunk/8.2.4/Installation/StartSplunkforthefirsttime Integrate iDRAC Telemetry Data Into Splunk: Link to PDF","title":"Helpful Links"},{"location":"Setting%20Up%20iDRAC%20Telemetry%20with%20Splunk/#my-test-environment","text":"","title":"My Test Environment"},{"location":"Setting%20Up%20iDRAC%20Telemetry%20with%20Splunk/#rhel","text":"NAME=\"Red Hat Enterprise Linux\" VERSION=\"8.5 (Ootpa)\" ID=\"rhel\" ID_LIKE=\"fedora\" VERSION_ID=\"8.5\" PLATFORM_ID=\"platform:el8\" PRETTY_NAME=\"Red Hat Enterprise Linux 8.5 (Ootpa)\" ANSI_COLOR=\"0;31\" CPE_NAME=\"cpe:/o:redhat:enterprise_linux:8::baseos\" HOME_URL=\"https://www.redhat.com/\" DOCUMENTATION_URL=\"https://access.redhat.com/documentation/red_hat_enterprise_linux/8/\" BUG_REPORT_URL=\"https://bugzilla.redhat.com/\" REDHAT_BUGZILLA_PRODUCT=\"Red Hat Enterprise Linux 8\" REDHAT_BUGZILLA_PRODUCT_VERSION=8.5 REDHAT_SUPPORT_PRODUCT=\"Red Hat Enterprise Linux\" REDHAT_SUPPORT_PRODUCT_VERSION=\"8.5\" Red Hat Enterprise Linux release 8.5 (Ootpa) Red Hat Enterprise Linux release 8.5 (Ootpa)","title":"RHEL"},{"location":"Setting%20Up%20iDRAC%20Telemetry%20with%20Splunk/#installation","text":"","title":"Installation"},{"location":"Setting%20Up%20iDRAC%20Telemetry%20with%20Splunk/#setup-splunk","text":"Download trial of Splunk Follow Splunk installation instructions By default it will install to /opt/splunk. Run /opt/splunk/bin/splunk start (I suggest you do this in tmux or another terminal emulator) Run firewall-cmd --permanent --zone public --add-port=8000/tcp && firewall-cmd --reload Make splunk start on boot with /opt/splunk/bin/splunk enable boot-start","title":"Setup Splunk"},{"location":"Setting%20Up%20iDRAC%20Telemetry%20with%20Splunk/#using-syslog","text":"Following the instructions here Install podman with dnf install -y podman Follow the instructions here NOTE: When adding the HTTP input in Splunk it failed out because the token weren't enabled. I had to manually edit /opt/splunk/etc/apps/splunk_httpinput/default/inputs.conf and set disabled to 0 then do a systemctl restart splunk Run systemctl stop rsyslog && systemctl disable rsyslog","title":"Using Syslog"},{"location":"Setting%20Up%20iDRAC%20Telemetry%20with%20Splunk/#using-activemq-and-splunkpump","text":"dnf install -y podman mkdir -p mkdir -p /opt/activemq/data && /opt/activemq/conf Run the following to generate default configs: bash podman run --user root --rm -ti -p 61616:61616 -p 8161:8161 -v /opt/activemq/conf:/mnt/conf:z -v /opt/activemq/data:/mnt/data:z rmohr/activemq /bin/sh chown activemq:activemq /mnt/conf chown activemq:activemq /mnt/data cp -a /opt/activemq/conf/* /mnt/conf/ cp -a /opt/activemq/data/* /mnt/data/ exit podman run -p 61616:61616 -p 8161:8161 -v /opt/activemq/conf:/opt/activemq/conf -v /opt/activemq/data:/opt/activemq/data rmohr/activemq","title":"Using ActiveMQ and splunkpump"},{"location":"Setting%20Up%20iDRAC%20Telemetry%20with%20Splunk/#configure-the-idrac","text":"Download this script which will enable telemetry reports. Run EnableOrDisableAllTelemetryReports.py -ip $target -u $user -p $password This enables telemetry on the target server","title":"Configure the iDRAC"},{"location":"Setting%20Up%20iDRAC%20Telemetry%20with%20Splunk/#using-activemq-and-splunkpump_1","text":"","title":"Using ActiveMQ and splunkpump"},{"location":"Setting%20Up%20iDRAC%20Telemetry%20with%20Splunk/#using-syslog_1","text":"Next you will need to enable Redfish alerting which will publish the events to Splunk. Download this script Run the following command SubscriptionManagementREDFISH.py -ip $target -u $user -p $password -c y -D https://$splunkserver/services/collector/raw -E Alert -V Event $target is the ip address or DNS name of the iDRAC $user/$password are the username and password for iDRAC $splunkserver is the IP address or DNS name of your Splunk HTTP event collector instance On the command line (racadm) SSH to the iDRAC Run ``` racadm set idrac.telemetry.RsyslogServer1 \"<splunk_ip/fqdn>\" racadm set idrac.telemetry.RsyslogServer1port \"514\" racadm testrsyslogconnection ```","title":"Using Syslog"},{"location":"Setup%20IDPA/","text":"Setup IDPA Version DP4400 Current Documentation Manual Setup Guide Notes IDPA DP4400 model is a hyperconverged, 2U system that a user can install and configure onsite. The DP4400 includes a virtual edition of Avamar server (AVE) as the Backup Server node, a virtual edition of Data Domain system (DDVE) as the Protection Storage node, Cloud Disaster Recovery, IDPA System Manager as a centralized system management, an Appliance Configuration Manager(ACM) for simplified configuration and upgrades, Search, Reporting and Analytics, and a compute node that hosts the virtual components and the software. Components Appliance administration The ACM provides a web-based interface for configuring, monitoring, and upgrading the appliance.The ACM dashboard displays a summary of the configuration of the individual components. It also enables the administrators to monitor the appliance, modify configuration details such as expanding the Data Domain disk capacity, change the common password for the appliance, change LDAP settings, update customer information, and change the values in the General Settings panel. The ACM dashboard enables you to upgrade the system and its components. It also displays the health information of the Appliance Server and VMware components. Backup administration The IDPA uses Avamar Virtual Edition (AVE) servers fo-r the DP5xxx and DP4xxx models and a physical Avamar for DP8xxxx to perform backup operations, with the data being stored in a Data Domain system. Generally, when using the Avamar Administrator Management Console, all Avamar servers look and behave the same. The main differences among the Avamar server configurations are the number of nodes and disk drives that are reported in the Server Monitor console. You can also add an Avamar NDMP Accelerator (one NDMP Accelerator node is supported in DP4400 and DP5800) to enable backup and recovery of NAS systems. For more information about the configuration details, see Table 3. Configuration options for each model on page 9. The Avamar NDMP Accelerator uses the network data management protocol (NDMP) to enable backup and recovery of network attached storage (NAS) systems. The accelerator performs NDMP processing and then sends the data directly to the Data Domain Server (Data Domain Virtual Edition Storage). Instructions Network Setup Set up 12 continuous IP addresses in DNS plus idrac. They must be in the same subnet. idrac can be separate TODO Register the 13 IP addresses in DNS with forward and reverse lookup entries for each address. Ensure that the router for the 13 IP addresses can be pinged. Cannot use _ in the hostname Addresses should cover the following: My Configuration 192.168.2.64 acm.lan 192.168.2.65 idpa-esxi.lan 192.168.2.66 idpa-ddve-backup-1.lan 192.168.2.67 idpa-ddve-backup-2.lan 192.168.2.68 idpa-ave-server.lan 192.168.2.69 idpa-proxy.lan 192.168.2.70 idpa-system-manager.lan 192.168.2.71 idpa-application-server.lan 192.168.2.72 datastore-server-host.lan 192.168.2.73 index-master-node.lan 192.168.2.74 cdra.lan Configuration On the switch: switch(config)# interface range ethernet 1/1/1 switch(config)# interface range ethernet 1/1/1-1/1/5 switch(conf-range-eth1/1/1-1/1/5)# switchport mode access switch(conf-range-eth1/1/1-1/1/5)# switchport access vlan 2 On a jump box set an IP of 192.168.100.98 Verify you can ping 192.168.100.100 Browse to: https://192.168.100.100:8543/ Log in with default creds: User root, password Idpa_1234 Follow the prompts and set up the networking. After you finish setting up the networking the UI will wait while the network settings are applied. Afterwards, it usually autoreconnects with updated IP information. Otherwise you can browse to `https:// :8543 Provide the information in the following screens: Proxy does image level backups. The system will backup the local VMs. Adding Backups Do not use change block tracking with vCenter Static vs dynamic - dynamic is dependent on it being in a folder. If the VMs are in a sub folder and you select the folder itself, then when you import the folder, the folder will be purple. That means dynamic, if someone creates another VM it will automatically add it. You can do rule based filtering and assign it to a policy. Recursive protection - Subfolders","title":"Setup IDPA"},{"location":"Setup%20IDPA/#setup-idpa","text":"","title":"Setup IDPA"},{"location":"Setup%20IDPA/#version","text":"DP4400","title":"Version"},{"location":"Setup%20IDPA/#current-documentation","text":"Manual Setup Guide","title":"Current Documentation"},{"location":"Setup%20IDPA/#notes","text":"IDPA DP4400 model is a hyperconverged, 2U system that a user can install and configure onsite. The DP4400 includes a virtual edition of Avamar server (AVE) as the Backup Server node, a virtual edition of Data Domain system (DDVE) as the Protection Storage node, Cloud Disaster Recovery, IDPA System Manager as a centralized system management, an Appliance Configuration Manager(ACM) for simplified configuration and upgrades, Search, Reporting and Analytics, and a compute node that hosts the virtual components and the software.","title":"Notes"},{"location":"Setup%20IDPA/#components","text":"","title":"Components"},{"location":"Setup%20IDPA/#appliance-administration","text":"The ACM provides a web-based interface for configuring, monitoring, and upgrading the appliance.The ACM dashboard displays a summary of the configuration of the individual components. It also enables the administrators to monitor the appliance, modify configuration details such as expanding the Data Domain disk capacity, change the common password for the appliance, change LDAP settings, update customer information, and change the values in the General Settings panel. The ACM dashboard enables you to upgrade the system and its components. It also displays the health information of the Appliance Server and VMware components.","title":"Appliance administration"},{"location":"Setup%20IDPA/#backup-administration","text":"The IDPA uses Avamar Virtual Edition (AVE) servers fo-r the DP5xxx and DP4xxx models and a physical Avamar for DP8xxxx to perform backup operations, with the data being stored in a Data Domain system. Generally, when using the Avamar Administrator Management Console, all Avamar servers look and behave the same. The main differences among the Avamar server configurations are the number of nodes and disk drives that are reported in the Server Monitor console. You can also add an Avamar NDMP Accelerator (one NDMP Accelerator node is supported in DP4400 and DP5800) to enable backup and recovery of NAS systems. For more information about the configuration details, see Table 3. Configuration options for each model on page 9. The Avamar NDMP Accelerator uses the network data management protocol (NDMP) to enable backup and recovery of network attached storage (NAS) systems. The accelerator performs NDMP processing and then sends the data directly to the Data Domain Server (Data Domain Virtual Edition Storage).","title":"Backup administration"},{"location":"Setup%20IDPA/#instructions","text":"","title":"Instructions"},{"location":"Setup%20IDPA/#network-setup","text":"Set up 12 continuous IP addresses in DNS plus idrac. They must be in the same subnet. idrac can be separate TODO Register the 13 IP addresses in DNS with forward and reverse lookup entries for each address. Ensure that the router for the 13 IP addresses can be pinged. Cannot use _ in the hostname Addresses should cover the following:","title":"Network Setup"},{"location":"Setup%20IDPA/#my-configuration","text":"192.168.2.64 acm.lan 192.168.2.65 idpa-esxi.lan 192.168.2.66 idpa-ddve-backup-1.lan 192.168.2.67 idpa-ddve-backup-2.lan 192.168.2.68 idpa-ave-server.lan 192.168.2.69 idpa-proxy.lan 192.168.2.70 idpa-system-manager.lan 192.168.2.71 idpa-application-server.lan 192.168.2.72 datastore-server-host.lan 192.168.2.73 index-master-node.lan 192.168.2.74 cdra.lan","title":"My Configuration"},{"location":"Setup%20IDPA/#configuration","text":"On the switch: switch(config)# interface range ethernet 1/1/1 switch(config)# interface range ethernet 1/1/1-1/1/5 switch(conf-range-eth1/1/1-1/1/5)# switchport mode access switch(conf-range-eth1/1/1-1/1/5)# switchport access vlan 2 On a jump box set an IP of 192.168.100.98 Verify you can ping 192.168.100.100 Browse to: https://192.168.100.100:8543/ Log in with default creds: User root, password Idpa_1234 Follow the prompts and set up the networking. After you finish setting up the networking the UI will wait while the network settings are applied. Afterwards, it usually autoreconnects with updated IP information. Otherwise you can browse to `https:// :8543 Provide the information in the following screens: Proxy does image level backups. The system will backup the local VMs.","title":"Configuration"},{"location":"Setup%20IDPA/#adding-backups","text":"Do not use change block tracking with vCenter Static vs dynamic - dynamic is dependent on it being in a folder. If the VMs are in a sub folder and you select the folder itself, then when you import the folder, the folder will be purple. That means dynamic, if someone creates another VM it will automatically add it. You can do rule based filtering and assign it to a policy. Recursive protection - Subfolders","title":"Adding Backups"},{"location":"Site%20to%20Site%20VPN%20with%20PFSense%20and%20CentOS%208/","text":"Site to Site VPN with PFSense and CentOS 8 On PFSense Go to openvpn server creation Select UDP on IPv4 only with tun Use a Peer to Peer (Shared Key) For the shared key automatically generate it No other special settings required. After you create the server, save it, and then go back in and copy the shared key. Open port 1194 UDP on the firewall. On CentOS 8 Make sure everything is up to date. yum update -y && reboot . The reboot is important because if your kernel might update. If this happens you need to reboot to load the new kernel. Run yum install -y epel-release && yum update -y && yum install -y openvpn easy-rsa chrony && systemctl enable chronyd && chronyc makestep This is a long series of commands, but it installs openvpn and chrony. You need chrony to ensure your time is synched. WARNING : If the time is not synched between the server and your clients, the VPN will fail to connect! systemctl stop firewalld - otherwise you'll have to allow everything going to and from the networks on a case by case basis. Run sysctl -w net.ipv4.ip_forward=1 && echo 1 > /proc/sys/net/ipv4/ip_forward Use the following config file: dev ovpnc3 verb 6 dev-type tun dev-node /dev/tun3 writepid /var/run/openvpn_client3.pid user nobody group nobody script-security 3 keepalive 10 60 ping-timer-rem persist-tun persist-key proto udp4 cipher AES-128-CBC auth SHA256 local lport 0 management /etc/openvpn/client3.sock unix remote 1194 udp4 ifconfig route compress resolv-retry infinite secret /etc/openvpn/client/secret In my scenario the 192.168.2.0/24 was the remote site network and 192.168.1.1 was the local network.","title":"Site to Site VPN with PFSense and CentOS 8"},{"location":"Site%20to%20Site%20VPN%20with%20PFSense%20and%20CentOS%208/#site-to-site-vpn-with-pfsense-and-centos-8","text":"","title":"Site to Site VPN with PFSense and CentOS 8"},{"location":"Site%20to%20Site%20VPN%20with%20PFSense%20and%20CentOS%208/#on-pfsense","text":"Go to openvpn server creation Select UDP on IPv4 only with tun Use a Peer to Peer (Shared Key) For the shared key automatically generate it No other special settings required. After you create the server, save it, and then go back in and copy the shared key. Open port 1194 UDP on the firewall.","title":"On PFSense"},{"location":"Site%20to%20Site%20VPN%20with%20PFSense%20and%20CentOS%208/#on-centos-8","text":"Make sure everything is up to date. yum update -y && reboot . The reboot is important because if your kernel might update. If this happens you need to reboot to load the new kernel. Run yum install -y epel-release && yum update -y && yum install -y openvpn easy-rsa chrony && systemctl enable chronyd && chronyc makestep This is a long series of commands, but it installs openvpn and chrony. You need chrony to ensure your time is synched. WARNING : If the time is not synched between the server and your clients, the VPN will fail to connect! systemctl stop firewalld - otherwise you'll have to allow everything going to and from the networks on a case by case basis. Run sysctl -w net.ipv4.ip_forward=1 && echo 1 > /proc/sys/net/ipv4/ip_forward Use the following config file: dev ovpnc3 verb 6 dev-type tun","title":"On CentOS 8"},{"location":"Site%20to%20Site%20VPN%20with%20PFSense%20and%20CentOS%208/#dev-node-devtun3","text":"writepid /var/run/openvpn_client3.pid","title":"dev-node /dev/tun3"},{"location":"Site%20to%20Site%20VPN%20with%20PFSense%20and%20CentOS%208/#user-nobody","text":"","title":"user nobody"},{"location":"Site%20to%20Site%20VPN%20with%20PFSense%20and%20CentOS%208/#group-nobody","text":"script-security 3 keepalive 10 60 ping-timer-rem persist-tun persist-key proto udp4 cipher AES-128-CBC auth SHA256 local lport 0 management /etc/openvpn/client3.sock unix remote 1194 udp4 ifconfig route compress resolv-retry infinite secret /etc/openvpn/client/secret In my scenario the 192.168.2.0/24 was the remote site network and 192.168.1.1 was the local network.","title":"group nobody"},{"location":"Switch%20Directly%20to%20Client%20Test/","text":"Switch Directly to Client Test The purpose of this test is to see if a 10Gb/s switch with a 10Gb/s SFP will autonegotiate when plugged directly to a client. OS 10 Version OS10# show version Dell EMC Networking OS10 Enterprise Copyright (c) 1999-2019 by Dell Inc. All Rights Reserved. OS Version: 10.5.0.2 Build Version: 10.5.0.2.468 Build Time: 2019-10-19T00:29:00+0000 System Type: S4112F-ON Architecture: x86_64 Up Time: 00:03:39 Setup I first plugged a Dell Precision 7730 with Intel I219-LM network card into Ethernet 1/1/8. I later plugged in an ESXi instance with an Intel x710 to Ethernet 1/1/7. SFP Used Results I had to set the interface speed for it to pick up, but once the interface speed was set it worked. OS10# show ip interface brief Interface Name IP-Address OK Method Status Protocol ========================================================================================= Ethernet 1/1/1 unassigned NO unset up down Ethernet 1/1/2 unassigned NO unset up down Ethernet 1/1/3 unassigned NO unset up down Ethernet 1/1/4 unassigned NO unset up down Ethernet 1/1/5 unassigned NO unset up down Ethernet 1/1/6 unassigned NO unset up down Ethernet 1/1/7 unassigned NO unset up down Ethernet 1/1/8 unassigned NO unset up down Ethernet 1/1/9 unassigned NO unset up down Ethernet 1/1/10 unassigned NO unset up down Ethernet 1/1/11 unassigned NO unset up down Ethernet 1/1/12 unassigned NO unset up down Ethernet 1/1/13 unassigned NO unset up down Ethernet 1/1/14 unassigned NO unset up down Ethernet 1/1/15 unassigned NO unset up down Management 1/1/1 192.168.1.20/24 YES manual up up Vlan 1 unassigned NO unset up down Port-channel 1 unassigned NO unset up down OS10# configure terminal OS10(config)# interface ethernet 1/1/8 OS10(conf-if-eth1/1/8)# speed 10 1000 10000 OS10(conf-if-eth1/1/8)# speed 1000 OS10(conf-if-eth1/1/8)# show ip interface brief Interface Name IP-Address OK Method Status Protocol ========================================================================================= Ethernet 1/1/1 unassigned NO unset up down Ethernet 1/1/2 unassigned NO unset up down Ethernet 1/1/3 unassigned NO unset up down Ethernet 1/1/4 unassigned NO unset up down Ethernet 1/1/5 unassigned NO unset up down Ethernet 1/1/6 unassigned NO unset up down Ethernet 1/1/7 unassigned NO unset up down Ethernet 1/1/8 unassigned YES unset up up Ethernet 1/1/9 unassigned NO unset up down Ethernet 1/1/10 unassigned NO unset up down Ethernet 1/1/11 unassigned NO unset up down Ethernet 1/1/12 unassigned NO unset up down Ethernet 1/1/13 unassigned NO unset up down Ethernet 1/1/14 unassigned NO unset up down Ethernet 1/1/15 unassigned NO unset up down Management 1/1/1 192.168.1.20/24 YES manual up up Vlan 1 unassigned YES unset up up Port-channel 1 unassigned NO unset up down On the client side the network showed Unidentified both before and after the change in the speed configuration. After I brought the interface up I connect Ethernet 1/1/7 to the Intel x710 with ESXi hosting the card. I connected it to a RHEL 8 VM and confirmed ping worked between the 7730 and the ESXi instance. [root@rhel8 ~]# ping 10.0.0.1 PING 10.0.0.1 (10.0.0.1) 56(84) bytes of data. 64 bytes from 10.0.0.1: icmp_seq=1 ttl=128 time=0.953 ms 64 bytes from 10.0.0.1: icmp_seq=2 ttl=128 time=0.775 ms 64 bytes from 10.0.0.1: icmp_seq=3 ttl=128 time=0.571 ms 64 bytes from 10.0.0.1: icmp_seq=4 ttl=128 time=0.613 ms ^C --- 10.0.0.1 ping statistics --- 4 packets transmitted, 4 received, 0% packet loss, time 114ms rtt min/avg/max/mdev = 0.571/0.728/0.953/0.150 ms","title":"Switch Directly to Client Test"},{"location":"Switch%20Directly%20to%20Client%20Test/#switch-directly-to-client-test","text":"The purpose of this test is to see if a 10Gb/s switch with a 10Gb/s SFP will autonegotiate when plugged directly to a client.","title":"Switch Directly to Client Test"},{"location":"Switch%20Directly%20to%20Client%20Test/#os-10-version","text":"OS10# show version Dell EMC Networking OS10 Enterprise Copyright (c) 1999-2019 by Dell Inc. All Rights Reserved. OS Version: 10.5.0.2 Build Version: 10.5.0.2.468 Build Time: 2019-10-19T00:29:00+0000 System Type: S4112F-ON Architecture: x86_64 Up Time: 00:03:39","title":"OS 10 Version"},{"location":"Switch%20Directly%20to%20Client%20Test/#setup","text":"I first plugged a Dell Precision 7730 with Intel I219-LM network card into Ethernet 1/1/8. I later plugged in an ESXi instance with an Intel x710 to Ethernet 1/1/7.","title":"Setup"},{"location":"Switch%20Directly%20to%20Client%20Test/#sfp-used","text":"","title":"SFP Used"},{"location":"Switch%20Directly%20to%20Client%20Test/#results","text":"I had to set the interface speed for it to pick up, but once the interface speed was set it worked. OS10# show ip interface brief Interface Name IP-Address OK Method Status Protocol ========================================================================================= Ethernet 1/1/1 unassigned NO unset up down Ethernet 1/1/2 unassigned NO unset up down Ethernet 1/1/3 unassigned NO unset up down Ethernet 1/1/4 unassigned NO unset up down Ethernet 1/1/5 unassigned NO unset up down Ethernet 1/1/6 unassigned NO unset up down Ethernet 1/1/7 unassigned NO unset up down Ethernet 1/1/8 unassigned NO unset up down Ethernet 1/1/9 unassigned NO unset up down Ethernet 1/1/10 unassigned NO unset up down Ethernet 1/1/11 unassigned NO unset up down Ethernet 1/1/12 unassigned NO unset up down Ethernet 1/1/13 unassigned NO unset up down Ethernet 1/1/14 unassigned NO unset up down Ethernet 1/1/15 unassigned NO unset up down Management 1/1/1 192.168.1.20/24 YES manual up up Vlan 1 unassigned NO unset up down Port-channel 1 unassigned NO unset up down OS10# configure terminal OS10(config)# interface ethernet 1/1/8 OS10(conf-if-eth1/1/8)# speed 10 1000 10000 OS10(conf-if-eth1/1/8)# speed 1000 OS10(conf-if-eth1/1/8)# show ip interface brief Interface Name IP-Address OK Method Status Protocol ========================================================================================= Ethernet 1/1/1 unassigned NO unset up down Ethernet 1/1/2 unassigned NO unset up down Ethernet 1/1/3 unassigned NO unset up down Ethernet 1/1/4 unassigned NO unset up down Ethernet 1/1/5 unassigned NO unset up down Ethernet 1/1/6 unassigned NO unset up down Ethernet 1/1/7 unassigned NO unset up down Ethernet 1/1/8 unassigned YES unset up up Ethernet 1/1/9 unassigned NO unset up down Ethernet 1/1/10 unassigned NO unset up down Ethernet 1/1/11 unassigned NO unset up down Ethernet 1/1/12 unassigned NO unset up down Ethernet 1/1/13 unassigned NO unset up down Ethernet 1/1/14 unassigned NO unset up down Ethernet 1/1/15 unassigned NO unset up down Management 1/1/1 192.168.1.20/24 YES manual up up Vlan 1 unassigned YES unset up up Port-channel 1 unassigned NO unset up down On the client side the network showed Unidentified both before and after the change in the speed configuration. After I brought the interface up I connect Ethernet 1/1/7 to the Intel x710 with ESXi hosting the card. I connected it to a RHEL 8 VM and confirmed ping worked between the 7730 and the ESXi instance. [root@rhel8 ~]# ping 10.0.0.1 PING 10.0.0.1 (10.0.0.1) 56(84) bytes of data. 64 bytes from 10.0.0.1: icmp_seq=1 ttl=128 time=0.953 ms 64 bytes from 10.0.0.1: icmp_seq=2 ttl=128 time=0.775 ms 64 bytes from 10.0.0.1: icmp_seq=3 ttl=128 time=0.571 ms 64 bytes from 10.0.0.1: icmp_seq=4 ttl=128 time=0.613 ms ^C --- 10.0.0.1 ping statistics --- 4 packets transmitted, 4 received, 0% packet loss, time 114ms rtt min/avg/max/mdev = 0.571/0.728/0.953/0.150 ms","title":"Results"},{"location":"Testing%20Intel%20x520%20on%20RHEL%206/","text":"Testing Intel x520 on RHEL 6 RHEL Release [root@r440 ~]# cat /etc/*-release LSB_VERSION=base-4.0-amd64:base-4.0-noarch:core-4.0-amd64:core-4.0-noarch:graphics-4.0-amd64:graphics-4.0-noarch:printing-4.0-amd64:printing-4.0-noarch Red Hat Enterprise Linux Server release 6.10 (Santiago) Red Hat Enterprise Linux Server release 6.10 (Santiago) Server/Card Model # Server Dell R440 # Card Model [root@r440 ~]# lspci | grep Network 3b:00.0 Ethernet controller: Intel Corporation 82599ES 10-Gigabit SFI/SFP+ Network Connection (rev 01) 3b:00.1 Ethernet controller: Intel Corporation 82599ES 10-Gigabit SFI/SFP+ Network Connection (rev 01) Note: As shown in the product documentation is the name of the controller for the Intel x520. Server Inventory I have attached the server inventory so that it can be used for comparison. Note: Because I did not use an x520 that came with the box or flash it with idrac compatible drivers it does not appear under network devices, but it does appear under PCI devices as expected. SFPs Used Testing Basic Connectivity On initial install of RHEL 6.10 both the 1Gb/s ethernet SFP and the 10Gb/s fiber SFP were detected without issue and pulled DHCP addresses as expected: 4: p2p1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP qlen 1000 link/ether 90:e2:ba:8b:88:14 brd ff:ff:ff:ff:ff:ff inet 192.168.1.214/24 brd 192.168.1.255 scope global p2p1 inet6 2601:152:4100:212f:92e2:baff:fe8b:8814/64 scope global dynamic valid_lft 86393sec preferred_lft 14393sec inet6 fe80::92e2:baff:fe8b:8814/64 scope link valid_lft forever preferred_lft forever 5: p2p2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP qlen 1000 link/ether 90:e2:ba:8b:88:15 brd ff:ff:ff:ff:ff:ff inet 192.168.1.242/24 brd 192.168.1.255 scope global p2p2 inet6 2601:152:4100:212f:92e2:baff:fe8b:8815/64 scope global dynamic valid_lft 86393sec preferred_lft 14393sec inet6 fe80::92e2:baff:fe8b:8815/64 scope link valid_lft forever preferred_lft forever They both operated without modification at the correct speed: [root@r440 ~]# ethtool p2p1 | grep -i speed && ethtool p2p2 | grep -i speed Speed: 10000Mb/s Speed: 1000Mb/s My initial firmware version was: [root@r440 ~]# ethtool -i p2p1 driver: ixgbe version: 4.2.1-k firmware-version: 0x61c10001 bus-info: 0000:3b:00.0 supports-statistics: yes supports-test: yes supports-eeprom-access: yes supports-register-dump: yes supports-priv-flags: no Lights operated as expected: I went to Dell's support website and pulled the latest driver for the R440 / x520 which was released 30 November 2018. When I ran the script I was given additional version info: Update Package 18.08.200 (BLD_311) Copyright (c) 2003 Dell, Inc. All Rights Reserved. Release Title: Intel NIC Family Version 18.8.0 Firmware for I350, I354, X520, X540, and X550 adapters, 18.8.9, A00 Release Date: October 05, 2018 However this got me This Update Package is not compatible with your system configuration. . I didn't investigate why. I took the card out of another box and added it to this one so I wasn't overly surprised. I decided to pull directly from Intel. The latest driver I could find was 5.5.5 available here . Detailed Description Overview This is the most current release of the ixgbe driver for Linux*, which supports kernel versions 2.6.18 up through 4.20. It also has been tested on the following distributions: RHEL* 6.10 RHEL 7.6 SLES* 12SP4 SLES 15 Ubuntu* 18.04 Changes in this release: Added support for 4.20 kernel version Added support for SLES 12SP4 Added support for RHEL 7.6 I ran: make install rmmod ixgbe && insmod /root/Downloads/ixgbe-5.5.5/src/ixgbe.ko to load the driver. I did not see any problems with a speed drop: [root@r440 ~]# ethtool p2p1 | grep -i speed && ethtool p2p2 | grep -i speed Speed: 10000Mb/s Speed: 1000Mb/s Version confirmation: [root@r440 ~]# ethtool -i p2p1 driver: ixgbe version: 5.5.5 firmware-version: 0x61c10001, 255.65535.255 bus-info: 0000:3b:00.0 supports-statistics: yes supports-test: yes supports-eeprom-access: yes supports-register-dump: yes supports-priv-flags: yes I confirmed the lights continued to work as expected.","title":"Testing Intel x520 on RHEL 6"},{"location":"Testing%20Intel%20x520%20on%20RHEL%206/#testing-intel-x520-on-rhel-6","text":"","title":"Testing Intel x520 on RHEL 6"},{"location":"Testing%20Intel%20x520%20on%20RHEL%206/#rhel-release","text":"[root@r440 ~]# cat /etc/*-release LSB_VERSION=base-4.0-amd64:base-4.0-noarch:core-4.0-amd64:core-4.0-noarch:graphics-4.0-amd64:graphics-4.0-noarch:printing-4.0-amd64:printing-4.0-noarch Red Hat Enterprise Linux Server release 6.10 (Santiago) Red Hat Enterprise Linux Server release 6.10 (Santiago)","title":"RHEL Release"},{"location":"Testing%20Intel%20x520%20on%20RHEL%206/#servercard-model","text":"# Server Dell R440 # Card Model [root@r440 ~]# lspci | grep Network 3b:00.0 Ethernet controller: Intel Corporation 82599ES 10-Gigabit SFI/SFP+ Network Connection (rev 01) 3b:00.1 Ethernet controller: Intel Corporation 82599ES 10-Gigabit SFI/SFP+ Network Connection (rev 01) Note: As shown in the product documentation is the name of the controller for the Intel x520.","title":"Server/Card Model"},{"location":"Testing%20Intel%20x520%20on%20RHEL%206/#server-inventory","text":"I have attached the server inventory so that it can be used for comparison. Note: Because I did not use an x520 that came with the box or flash it with idrac compatible drivers it does not appear under network devices, but it does appear under PCI devices as expected.","title":"Server Inventory"},{"location":"Testing%20Intel%20x520%20on%20RHEL%206/#sfps-used","text":"","title":"SFPs Used"},{"location":"Testing%20Intel%20x520%20on%20RHEL%206/#testing","text":"","title":"Testing"},{"location":"Testing%20Intel%20x520%20on%20RHEL%206/#basic-connectivity","text":"On initial install of RHEL 6.10 both the 1Gb/s ethernet SFP and the 10Gb/s fiber SFP were detected without issue and pulled DHCP addresses as expected: 4: p2p1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP qlen 1000 link/ether 90:e2:ba:8b:88:14 brd ff:ff:ff:ff:ff:ff inet 192.168.1.214/24 brd 192.168.1.255 scope global p2p1 inet6 2601:152:4100:212f:92e2:baff:fe8b:8814/64 scope global dynamic valid_lft 86393sec preferred_lft 14393sec inet6 fe80::92e2:baff:fe8b:8814/64 scope link valid_lft forever preferred_lft forever 5: p2p2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP qlen 1000 link/ether 90:e2:ba:8b:88:15 brd ff:ff:ff:ff:ff:ff inet 192.168.1.242/24 brd 192.168.1.255 scope global p2p2 inet6 2601:152:4100:212f:92e2:baff:fe8b:8815/64 scope global dynamic valid_lft 86393sec preferred_lft 14393sec inet6 fe80::92e2:baff:fe8b:8815/64 scope link valid_lft forever preferred_lft forever They both operated without modification at the correct speed: [root@r440 ~]# ethtool p2p1 | grep -i speed && ethtool p2p2 | grep -i speed Speed: 10000Mb/s Speed: 1000Mb/s My initial firmware version was: [root@r440 ~]# ethtool -i p2p1 driver: ixgbe version: 4.2.1-k firmware-version: 0x61c10001 bus-info: 0000:3b:00.0 supports-statistics: yes supports-test: yes supports-eeprom-access: yes supports-register-dump: yes supports-priv-flags: no Lights operated as expected: I went to Dell's support website and pulled the latest driver for the R440 / x520 which was released 30 November 2018. When I ran the script I was given additional version info: Update Package 18.08.200 (BLD_311) Copyright (c) 2003 Dell, Inc. All Rights Reserved. Release Title: Intel NIC Family Version 18.8.0 Firmware for I350, I354, X520, X540, and X550 adapters, 18.8.9, A00 Release Date: October 05, 2018 However this got me This Update Package is not compatible with your system configuration. . I didn't investigate why. I took the card out of another box and added it to this one so I wasn't overly surprised. I decided to pull directly from Intel. The latest driver I could find was 5.5.5 available here . Detailed Description Overview This is the most current release of the ixgbe driver for Linux*, which supports kernel versions 2.6.18 up through 4.20. It also has been tested on the following distributions: RHEL* 6.10 RHEL 7.6 SLES* 12SP4 SLES 15 Ubuntu* 18.04 Changes in this release: Added support for 4.20 kernel version Added support for SLES 12SP4 Added support for RHEL 7.6 I ran: make install rmmod ixgbe && insmod /root/Downloads/ixgbe-5.5.5/src/ixgbe.ko to load the driver. I did not see any problems with a speed drop: [root@r440 ~]# ethtool p2p1 | grep -i speed && ethtool p2p2 | grep -i speed Speed: 10000Mb/s Speed: 1000Mb/s Version confirmation: [root@r440 ~]# ethtool -i p2p1 driver: ixgbe version: 5.5.5 firmware-version: 0x61c10001, 255.65535.255 bus-info: 0000:3b:00.0 supports-statistics: yes supports-test: yes supports-eeprom-access: yes supports-register-dump: yes supports-priv-flags: yes I confirmed the lights continued to work as expected.","title":"Basic Connectivity"},{"location":"Understanding%20Memory/","text":"Understanding Memory NOTE: Probably best to just read the Reddit post at the bottom. Understanding Memory What is a RAM channel? Analogy What is a DIMM (beyond the obvious)? What is a memory rank? What is DRAM? What is a chip select? Back to Memory Ranks Performance of multiple rank modules What is ECC Memory? What is Registered Memory? Why does the buffer allow for more total addressable memory? R-DIMMs LR-DIMMs What is a data line? Why does the data buffer matter? RAM and it's relation to CPU Speed More on Channels Best Memory for Different Circumstances What is a RAM channel? From RAM Channels Guide: The What, and The How To be clear, these memory channels are actual wires that exist on/in the motherboard. Though RAM kits may call their arrangements \"channels,\" the actual number of channels and the number of RAM sticks are independent of each other; any mention of channel count on a RAM kit\u2019s product/specification page is just an informal, technically-incorrect way of referring to how many sticks of RAM there are in the kit. In addition, the number of RAM slots on a motherboard is independent of the number of memory channels. A channel needs only one stick to be used, and any more than that doesn\u2019t necessarily stop things from working. In addition, CPUs also support a certain maximum amount of memory channels. You don\u2019t really need to worry about this, as every CPU will handle the amount of memory channels available on their supporting motherboards. There are only two notable exceptions: Intel\u2019s i5-7640X and i7-7740X, which are both LGA 2066 CPUs, and very odd purchases anyway. Analogy Imagine a manufacturer of products: Let\u2019s say this manufacturer (your CPU), with potentially many factories (cores) in need of materials, makes orders for materials from only one supplier (memory channel). Even if the supplier has a whole lot of materials (capacity / stored data), and may run multiple warehouses (RAM sticks) of their own, it has a limited capacity for making shipments, and so can\u2019t handle multiple shipments to the manufacturer at once. There may be multiple shipments ready to go, but they can\u2019t actually start shipping until the current shipment is done. A single-channel supplier warehouse attempting to serve a quad-factory manufacturer with one truck The problem is, this manufacturer can often use materials faster than their supplier can ship them, and the delay from waiting on the supplier\u2019s logistics system for consecutive orders can slow things down. Especially when this manufacturer\u2019s factories are being heavily loaded with orders of their own from vendors and customers (your other components) while relying on materials orders, the supplier can pose a problem. So, the manufacturer contracts with a second supplier in addition to the first. Now, the manufacturer does something efficient: They alternate orders between the two suppliers. This way, the manufacturer can have two simultaneous shipments coming their way, and they suddenly find that waiting on consecutive orders to be shipped is now significantly less of an issue, since their effective capacity for getting shipments has been doubled. This same idea can extend even further across more suppliers. Really, how much the number of suppliers the manufacturer uses actually matters all depends on: how quickly materials are being used, how many factories they have (since each might come in need of materials at any given moment), how busy the manufacturer or specific factories are, and how quickly the suppliers themselves can send shipments to the manufacturer. Most of the time, this isn\u2019t a big deal, but when things line up well or poorly, the number of suppliers (i.e. memory channels) can make a notable difference. What is a DIMM (beyond the obvious)? From What is LR-DIMM , LRDIMM Memory ? ( Load-Reduce DIMM) DIMM stands for Dual Inline Memory Module. It is the RAM memory we found in our desktop computer. It consists of a few black chips (IC) on a small PCB. It stores our file and data temporally when we turn on our computer. \"Dual Inline\" refers to pins on both side of the module. We generally call them \"gold fingers\". What is a memory rank? What is DRAM? From Dynamic random-access memory and MOSFET Dynamic random-access memory (DRAM) is a type of random access semiconductor memory that stores each bit of data in a memory cell consisting of a tiny capacitor and a transistor, both typically based on metal-oxide-semiconductor (MOS) technology. After this I Googled metal\u2013oxide\u2013semiconductor field-effect transistor and basically the gist of what I read is that it's just the type of transistor used to store the data. The way they fabricate them is by oxidizing silicon. Beyond that I hit the \"I believe button\". What is a chip select? From Chip select Chip select (CS) or slave select (SS) is the name of a control line in digital electronics used to select one (or a set) of integrated circuits (commonly called \"chips\") out of several connected to the same computer bus, usually utilizing the three-state logic. When an engineer needs to connect several devices to the same set of input wires (e.g., a computer bus), but retain the ability to send and receive data or commands to each device independently of the others on the bus, they can use a chip select. The chip select is a command pin on many integrated circuits which connects the I/O pins on the device to the internal circuitry of that device. Back to Memory Ranks From Memory rank A memory rank is a set of DRAM chips connected to the same chip select, which are therefore accessed simultaneously. In practice all DRAM chips share all of the other command and control signals, and only the chip select pins for each rank are separate (the data pins are shared across ranks). On a DDR, DDR2, or DDR3 memory module, each rank has a 64-bit-wide data bus (72 bits wide on DIMMs that support ECC). The number of physical DRAMs depends on their individual widths. For example, a rank of \u00d78 (8-bit wide) DRAMs would consist of eight physical chips (nine if ECC is supported), but a rank of \u00d74 (4-bit wide) DRAMs would consist of 16 physical chips (18, if ECC is supported). Multiple ranks can coexist on a single DIMM, and modern DIMMs can consist of one rank (single rank), two ranks (dual rank), four ranks (quad rank), or eight ranks (octal rank). Increasing the number of ranks per DIMM is mainly intended to increase the memory density per channel. Too many ranks in the channel can cause excessive loading and decrease the speed of the channel. Also some memory controllers have a maximum supported number of ranks. DRAM load on the command/address (CA) bus can be reduced by using registered memory. Performance of multiple rank modules From Memory rank There are several effects to consider regarding memory performance in multi-rank configurations: Multi-rank modules allow several open DRAM pages (row) in each rank (typically eight pages per rank). This increases the possibility of getting a hit on an already open row address. The performance gain that can be achieved is highly dependent on the application and the memory controller's ability to take advantage of open pages. Multi-rank modules have higher loading on the data bus (and on unbuffered DIMMs the CA bus as well). Therefore if more than dual rank DIMMS are connected in one channel, the speed might be reduced. Subject to some limitations, ranks can be accessed independently, although not simultaneously as the data lines are still shared between ranks on a channel. For example, the controller can send write data to one rank while it awaits read data previously selected from another rank. While the write data is consumed from the data bus, the other rank could perform read-related operations such as the activation of a row or internal transfer of the data to the output drivers. Once the CA bus is free from noise from the previous read, the DRAM can drive out the read data. Controlling interleaved accesses like so is done by the memory controller. There is a small performance reduction for multi-rank systems as they require some pipeline stalls between accessing different ranks. For two ranks on a single DIMM it might not even be required, but this parameter is often programmed independently of the rank location in the system (if on the same DIMM or different DIMMs). Nevertheless, this pipeline stall is negligible compared to the aforementioned effects. Grant Note : A pipeline still is when execution stops because some hazard has to be resolved. A hazard in computer architecture is when there would be a conflict due to concurrent execution for whatever reason. Basically you're waiting in limbo for something else to complete before it is safe for you to continue. What is ECC Memory? From Computer Memory Issues Standard memory, also called non-parity memory, uses 8 bits to store an 8-bit byte. ECC memory (Error Correcting Code memory), sometimes called parity memory, uses 9 bits to store an 8-bit byte. The extra bit provided by ECC memory is used to store error detection and correction information. A non-parity memory module can neither detect nor correct errors. An ECC memory module can detect all multi-bit errors, correct all single-bit errors, and correct some multi-bit errors. Memory errors are so rare that most desktop systems use non-parity memory, which is less expensive and faster than ECC memory. In fact, most desktop chipsets do not support ECC memory. If you install ECC memory in such a system, it may not recognize the memory at all, but more likely it will simply treat the ECC memory as non-parity memory, ignoring the extra bit. ECC memory is occasionally used in desktop systems, but is much more common in servers and other large, critical systems. Because ECC modules contain additional memory chips, in the ratio of 9:8, they typically cost 10% to 15% more than similar non-parity modules. Also, because the circuitry on ECC modules that calculates and stores ECC values imposes some overhead, ECC modules are marginally slower than similar non-parity modules. What is Registered Memory? From Computer Memory Issues Unbuffered memory modules allow the memory controller to interact directly with the memory chips on the module. Registered memory (also called buffered memory) modules place an additional layer of circuitry between the memory controller and the memory chips. Registered memory is necessary in some environments, because all memory controllers have limitations on how many devices (individual memory chips) they can control, which in turn limits the maximum capacity of the memory modules they can use. When a memory controller interacts with an unbuffered memory module, it controls every memory chip directly. When a memory controller interacts with a registered memory module, it works only with the buffer circuitry; the actual memory chips are invisible to it. The sole advantage of registered memory is that it permits a system to use higher-capacity memory modules. (The highest capacity modules at any given time are often available only in registered form.) The disadvantages of registered memory are that it is considerably more expensive than unbuffered memory and noticeably slower because of the additional overhead incurred from using a buffer. Why does the buffer allow for more total addressable memory? From Registered Memory Before this can make sense you have to read from Parasitic capacitance Parasitic capacitance, or stray capacitance is an unavoidable and usually unwanted capacitance that exists between the parts of an electronic component or circuit simply because of their proximity to each other. When two electrical conductors at different voltages are close together, the electric field between them causes electric charge to be stored on them; this effect is parasitic capacitance. All actual circuit elements such as inductors, diodes, and transistors have internal capacitance, which can cause their behavior to depart from that of 'ideal' circuit elements. Additionally, there is always non-zero capacitance between any two conductors; this can be significant at higher frequencies with closely spaced conductors, such as wires or printed circuit board traces. Parasitic capacitance is a significant problem in high frequency circuits and is often the factor limiting the operating frequency and bandwidth of electronic components and circuits. More on Parasitic capacitance Wait a minute! You just cannot keep adding memory into your computer without any penalty. At 1333Mhz (1.3GHz), noise gets involved. It generally is something called the signal integrity or signal reflection issue. At a point, the accumulated noise in the system would render the system not operable. At high frequency, there is also something called \"loading factor\". Each memory chip (IC) has input capacitance that tends to suppress the high frequency signal. Generally, each chip has about 3 to 5 pf of input capacitance. The more chips on the module, the more accumulated capacitance will weaken the signal to an in-operable state. R-DIMMs Registered (Buffered) DIMMs (R-DIMMs) insert a buffer between the command/address bus pins on the DIMM and the memory chips proper. A high-density DIMM might have 36 memory chips (assuming four ranks and ECC), each of which must receive the memory address, and their combined input capacitance limits the speed at which the memory bus can operate. By amplifying the signal on the DIMM, this allows more chips to be connected to the memory bus. The cost is one additional clock cycle of memory latency required for the address to traverse the additional buffer. Early registered RAM modules were physically incompatible with unregistered RAM modules, but SDRAM DIMMs are interchangeable, and some motherboards support both types. LR-DIMMs Load Reduced DIMMs (LR-DIMMs) modules are similar, but add a buffer to the data lines as well. As a result, LRDIMM memory provides large overall maximum memory capacities, while avoiding the performance and power consumption problems of FB-DIMM memory. What is a data line? From How Memory Works The processor specifies which memory cell it wants to use by giving a binary address. Some simple Boolean logic, address decoding, then converts the address into a row and column select and the memory still works and its efficient. Notice that at the moment we are looking at a single memory chip and this arrangement can only store a single bit. If you want to store a byte you need eight such chips, one for each bit, and eight data input and eight data output lines. The eight data lines are grouped together into a data input bus and a data output \"bus\" \u2013 a bus is just a group of wires. Early computers really did have separate buses for input and output but today\u2019s machines have a single unified data bus that can be used for either input or output. If you want more storage than a bank of eight chips can provide then you have to add another bank of eight chips and some additional address decoding logic to select the correct bank. The address lines that come from the processor are generally referred to as an address bus and now we have the fundamental architecture of a simple but modern computer. Why does the data buffer matter? From What is LR-DIMM, LRDIMM Memory? (Load-Reduce DIMM) and Basics of LRDIMM You just cannot keep adding memory into your computer without any penalty. At 1333Mhz (1.3GHz), noise gets involved. It generally is something called the signal integrity or signal reflection issue. At a point, the accumulated noise in the system would render the system not operable. Figure 1: LRDIMM conceptual drawing featuring one memory buffer on the front side of the memory module and multiple ranks of DRAM mounted on both front and back sides of the memory module. The memory buffer re-drives all of the data, command, address and clock signals from the host memory controller and provides them to the multiple ranks of DRAM. The memory buffer isolates the DRAM from the host, reducing the electrical load on all interfaces. Reducing the electrical loading in this manner allows a system to operate at a higher speed for a given memory capacity, or to support a higher memory capacity at a given speed. On existing RDIMM technology, the data bus connects directly to the multiple ranks of DRAM, increasing electrical load and limiting system speed as the desired server capacity increases. Figure 2 shows the system-level block diagram for RDIMM and LRDIMM. To maximize module and system-level memory capacity and overcome limitations on the number of chip selects per channel, the LRDIMM supports Rank Multiplication, where multiple physical DRAM ranks appear to the host controller as a single logical rank of a larger size. This is done by utilizing additional row address bits during the Activate command as sub-rank select bits. One of the primary advantages of the LRDIMM is the ability to dramatically increase total system memory capacity without sacrificing speed. By electrically isolating the DRAM from the data bus, additional ranks of DRAM can be added to each DIMM while maintaining signal integrity, and additional DIMMs can be installed on each system memory channel. LRDIMM capacities up to 32GB are possible today with 4Rx4 modules using 4 Gb, DDP (dual-die package) DRAM. Since each LRDIMM presents a single electrical load to the host, more DIMMs can be installed per channel as well. RAM and it's relation to CPU Speed From Understanding CPU Limitations with Memory There are more in the weeds details, but it really is as simple as the clock speed of the CPU's memory controller has to be as good as or better than the target RAM or the RAM will downclock to meet the CPU's speed. More on Channels See this Reddit thread Ever since SDRAM first hit the shelves, main memory inside PC's has been a 64 bit wide interface. That means that 64 bits are sent in each transaction. DDR just meant that two lots of 64 bit transactions are sent in one clock. Obviously you see that DDR would increase the available memory bandwidth (speed), because twice as much data is being sent per clock. Well there is another way to send more data per clock, that is to send more than 64 bits in one go. You could either, widen the memory bus, to say 128 bits (GPU's use 384/512bit interfaces, but it's more complicated than that in truth) or you could add another interface. So when we go from single 'channel' to dual channel, what we are doing is adding another 64 bit wide interface (and another 2 when going from dual to quad). Now, usually you have 4 slots on your mobo for RAM. So you either have 4 independent slots connected to their own memory channel, or you have 2 pairs of slots, each pair connected to it's own memory channel. Lets take one pair. This pair has 240 pins (in desktop DDR3) coming off the CPU, connecting to one slot, then the next. This memory bus has the signalling to talk to either slot, a read request is sent out and each chip on that bus determines if it is indeed the chip that contains the requested data (to put it simply) and once it has found it, sends the data back. It can take a little while to do all this so many read and write requests can end up queued and/or in the pipeline. You add your second channel and you interleave your data across it, so bit one is the first bit on channel 1, bit 2 is the first bit on channel 2, bit 3 the second on channel 1 etc (likely to be done in chunks larger than single bits though) In the exact same way that RAID can give you much faster hard disk access (but certain access patterns not so much) adding extra memory channels allows you to read extra data. Double your memory channels, double the data you can read per second. Of course in reality nothing ever scales perfectly but you get the idea. The flip side of having these extra memory channels is that you need to populate them all, or you're wasting them. So a quad channel system will need 4 memory modules, if it has any less than that it is only using a number of memory channels equal to the number of modules you're using. This was a pretty good explanation, until you got to the part about bit interleaving. That is definitely not how it works. I know you corrected yourself and said that it probably works at a chunkier granularity, but I only noticed that after I'd already written all of this, so sorry. I'm going to get a little technical, so ignore this unless you want lots of details. DRAM systems are logically organized into (going from largest to smallest sub-division): channels, ranks, banks, rows, and columns. A channel is the physical connection between the memory controller (which is on the CPU die these days), and the DIMMs (the boards with DRAM chips on them). It's the wires, the aforementioned 64-bit data interface, plus all the control signals. A channel may have one or more DIMMs on it (for DDR3 at least, but DDR4 will require only a single DIMM per channel), and when there is a memory request (be it read or write) all of the data for that request comes from a single channel. There is no bit-interleaving across channels. All of the CPUs you and I deal with on a daily basis use 64 byte cache lines, so DRAM is accessed at the granularity of 64 byte blocks. So when there is a DRAM read, all 64 bytes come from a single channel. Furthermore, all 64 bytes come from a single rank within that channel. A rank is a collection of DRAM chips on a DIMM that work together to service a DRAM transaction. In the desktop systems we interact with every day, a rank is typically comprised of 8 DRAM chips. A DIMM may have more than 1 rank on it (the DIMMs you are familiar with probably have 8 DRAM chips on each side, so 2 ranks). This means that when your CPU sends a DRAM read request along a channel, 8 DRAM chips on the DIMM work together to fulfill that request. Remember that the data bus width is 64 bits, so each of these 8 DRAM chips is only responsible for transmitting 8 bits each per transfer. They transmit at the exact same time, in lock step with each other, and so from the CPU's perspective it seems like a single 64-bit data bus. After one DRAM transaction is completed on that channel, the next transaction might come from a different rank on the same channel (perhaps the other set of 8 DRAM chips on the same DIMM, or a rank on another DIMM on the same channel), or it could be the same rank again. Here is the important part: separate channels can work 100% independently, because they have absolutely 0 resources they have to manage and share between them, but separate ranks on the same channel must serialize their actions, because they all share the same data and control buses, and they must take turns using these resources. Depending on the access pattern you expect to be most common, you can layout data across your DRAM channels/ranks/banks/rows in various ways. One high performance strategy is to layout consecutive cache lines across channels (so address A is in channel 1, and address A+64 is in channel 2). This lets you fetch A+64 while you're still working on A. There are other data layout strategies for other expected access patterns. What I'm saying is that it is common for cache lines to be striped (interleaved) across channels, but not individual bits. If anyone wants, I can go into more detail about how banks, rows, and columns work, but I think channels and ranks are all that is needed to answer the OP's question. One last thing, about GPU memory buses. In graphics card specs, they always say that the GPU has a \"384-bit bus\", but this is misleading. It does have 384 total bits of GDDR5 data bus, but it is not logically organized as a single bus (like the 8 DRAM chips of a rank work together to logically form a 64-bit bus in DDR3). Instead, a GPU with a \"384-bit bus\" will actually have 12 independent 32-bit GDDR5 data buses that all act independently from one another. Source: Ph.D. computer architecture student who's studied DRAM organization for the last 4 years. Best Memory for Different Circumstances https://www.microway.com/hpc-tech-tips/ddr4-rdimm-lrdimm-performance-comparison/","title":"Understanding Memory"},{"location":"Understanding%20Memory/#understanding-memory","text":"NOTE: Probably best to just read the Reddit post at the bottom. Understanding Memory What is a RAM channel? Analogy What is a DIMM (beyond the obvious)? What is a memory rank? What is DRAM? What is a chip select? Back to Memory Ranks Performance of multiple rank modules What is ECC Memory? What is Registered Memory? Why does the buffer allow for more total addressable memory? R-DIMMs LR-DIMMs What is a data line? Why does the data buffer matter? RAM and it's relation to CPU Speed More on Channels Best Memory for Different Circumstances","title":"Understanding Memory"},{"location":"Understanding%20Memory/#what-is-a-ram-channel","text":"From RAM Channels Guide: The What, and The How To be clear, these memory channels are actual wires that exist on/in the motherboard. Though RAM kits may call their arrangements \"channels,\" the actual number of channels and the number of RAM sticks are independent of each other; any mention of channel count on a RAM kit\u2019s product/specification page is just an informal, technically-incorrect way of referring to how many sticks of RAM there are in the kit. In addition, the number of RAM slots on a motherboard is independent of the number of memory channels. A channel needs only one stick to be used, and any more than that doesn\u2019t necessarily stop things from working. In addition, CPUs also support a certain maximum amount of memory channels. You don\u2019t really need to worry about this, as every CPU will handle the amount of memory channels available on their supporting motherboards. There are only two notable exceptions: Intel\u2019s i5-7640X and i7-7740X, which are both LGA 2066 CPUs, and very odd purchases anyway.","title":"What is a RAM channel?"},{"location":"Understanding%20Memory/#analogy","text":"Imagine a manufacturer of products: Let\u2019s say this manufacturer (your CPU), with potentially many factories (cores) in need of materials, makes orders for materials from only one supplier (memory channel). Even if the supplier has a whole lot of materials (capacity / stored data), and may run multiple warehouses (RAM sticks) of their own, it has a limited capacity for making shipments, and so can\u2019t handle multiple shipments to the manufacturer at once. There may be multiple shipments ready to go, but they can\u2019t actually start shipping until the current shipment is done. A single-channel supplier warehouse attempting to serve a quad-factory manufacturer with one truck The problem is, this manufacturer can often use materials faster than their supplier can ship them, and the delay from waiting on the supplier\u2019s logistics system for consecutive orders can slow things down. Especially when this manufacturer\u2019s factories are being heavily loaded with orders of their own from vendors and customers (your other components) while relying on materials orders, the supplier can pose a problem. So, the manufacturer contracts with a second supplier in addition to the first. Now, the manufacturer does something efficient: They alternate orders between the two suppliers. This way, the manufacturer can have two simultaneous shipments coming their way, and they suddenly find that waiting on consecutive orders to be shipped is now significantly less of an issue, since their effective capacity for getting shipments has been doubled. This same idea can extend even further across more suppliers. Really, how much the number of suppliers the manufacturer uses actually matters all depends on: how quickly materials are being used, how many factories they have (since each might come in need of materials at any given moment), how busy the manufacturer or specific factories are, and how quickly the suppliers themselves can send shipments to the manufacturer. Most of the time, this isn\u2019t a big deal, but when things line up well or poorly, the number of suppliers (i.e. memory channels) can make a notable difference.","title":"Analogy"},{"location":"Understanding%20Memory/#what-is-a-dimm-beyond-the-obvious","text":"From What is LR-DIMM , LRDIMM Memory ? ( Load-Reduce DIMM) DIMM stands for Dual Inline Memory Module. It is the RAM memory we found in our desktop computer. It consists of a few black chips (IC) on a small PCB. It stores our file and data temporally when we turn on our computer. \"Dual Inline\" refers to pins on both side of the module. We generally call them \"gold fingers\".","title":"What is a DIMM (beyond the obvious)?"},{"location":"Understanding%20Memory/#what-is-a-memory-rank","text":"","title":"What is a memory rank?"},{"location":"Understanding%20Memory/#what-is-dram","text":"From Dynamic random-access memory and MOSFET Dynamic random-access memory (DRAM) is a type of random access semiconductor memory that stores each bit of data in a memory cell consisting of a tiny capacitor and a transistor, both typically based on metal-oxide-semiconductor (MOS) technology. After this I Googled metal\u2013oxide\u2013semiconductor field-effect transistor and basically the gist of what I read is that it's just the type of transistor used to store the data. The way they fabricate them is by oxidizing silicon. Beyond that I hit the \"I believe button\".","title":"What is DRAM?"},{"location":"Understanding%20Memory/#what-is-a-chip-select","text":"From Chip select Chip select (CS) or slave select (SS) is the name of a control line in digital electronics used to select one (or a set) of integrated circuits (commonly called \"chips\") out of several connected to the same computer bus, usually utilizing the three-state logic. When an engineer needs to connect several devices to the same set of input wires (e.g., a computer bus), but retain the ability to send and receive data or commands to each device independently of the others on the bus, they can use a chip select. The chip select is a command pin on many integrated circuits which connects the I/O pins on the device to the internal circuitry of that device.","title":"What is a chip select?"},{"location":"Understanding%20Memory/#back-to-memory-ranks","text":"From Memory rank A memory rank is a set of DRAM chips connected to the same chip select, which are therefore accessed simultaneously. In practice all DRAM chips share all of the other command and control signals, and only the chip select pins for each rank are separate (the data pins are shared across ranks). On a DDR, DDR2, or DDR3 memory module, each rank has a 64-bit-wide data bus (72 bits wide on DIMMs that support ECC). The number of physical DRAMs depends on their individual widths. For example, a rank of \u00d78 (8-bit wide) DRAMs would consist of eight physical chips (nine if ECC is supported), but a rank of \u00d74 (4-bit wide) DRAMs would consist of 16 physical chips (18, if ECC is supported). Multiple ranks can coexist on a single DIMM, and modern DIMMs can consist of one rank (single rank), two ranks (dual rank), four ranks (quad rank), or eight ranks (octal rank). Increasing the number of ranks per DIMM is mainly intended to increase the memory density per channel. Too many ranks in the channel can cause excessive loading and decrease the speed of the channel. Also some memory controllers have a maximum supported number of ranks. DRAM load on the command/address (CA) bus can be reduced by using registered memory.","title":"Back to Memory Ranks"},{"location":"Understanding%20Memory/#performance-of-multiple-rank-modules","text":"From Memory rank There are several effects to consider regarding memory performance in multi-rank configurations: Multi-rank modules allow several open DRAM pages (row) in each rank (typically eight pages per rank). This increases the possibility of getting a hit on an already open row address. The performance gain that can be achieved is highly dependent on the application and the memory controller's ability to take advantage of open pages. Multi-rank modules have higher loading on the data bus (and on unbuffered DIMMs the CA bus as well). Therefore if more than dual rank DIMMS are connected in one channel, the speed might be reduced. Subject to some limitations, ranks can be accessed independently, although not simultaneously as the data lines are still shared between ranks on a channel. For example, the controller can send write data to one rank while it awaits read data previously selected from another rank. While the write data is consumed from the data bus, the other rank could perform read-related operations such as the activation of a row or internal transfer of the data to the output drivers. Once the CA bus is free from noise from the previous read, the DRAM can drive out the read data. Controlling interleaved accesses like so is done by the memory controller. There is a small performance reduction for multi-rank systems as they require some pipeline stalls between accessing different ranks. For two ranks on a single DIMM it might not even be required, but this parameter is often programmed independently of the rank location in the system (if on the same DIMM or different DIMMs). Nevertheless, this pipeline stall is negligible compared to the aforementioned effects. Grant Note : A pipeline still is when execution stops because some hazard has to be resolved. A hazard in computer architecture is when there would be a conflict due to concurrent execution for whatever reason. Basically you're waiting in limbo for something else to complete before it is safe for you to continue.","title":"Performance of multiple rank modules"},{"location":"Understanding%20Memory/#what-is-ecc-memory","text":"From Computer Memory Issues Standard memory, also called non-parity memory, uses 8 bits to store an 8-bit byte. ECC memory (Error Correcting Code memory), sometimes called parity memory, uses 9 bits to store an 8-bit byte. The extra bit provided by ECC memory is used to store error detection and correction information. A non-parity memory module can neither detect nor correct errors. An ECC memory module can detect all multi-bit errors, correct all single-bit errors, and correct some multi-bit errors. Memory errors are so rare that most desktop systems use non-parity memory, which is less expensive and faster than ECC memory. In fact, most desktop chipsets do not support ECC memory. If you install ECC memory in such a system, it may not recognize the memory at all, but more likely it will simply treat the ECC memory as non-parity memory, ignoring the extra bit. ECC memory is occasionally used in desktop systems, but is much more common in servers and other large, critical systems. Because ECC modules contain additional memory chips, in the ratio of 9:8, they typically cost 10% to 15% more than similar non-parity modules. Also, because the circuitry on ECC modules that calculates and stores ECC values imposes some overhead, ECC modules are marginally slower than similar non-parity modules.","title":"What is ECC Memory?"},{"location":"Understanding%20Memory/#what-is-registered-memory","text":"From Computer Memory Issues Unbuffered memory modules allow the memory controller to interact directly with the memory chips on the module. Registered memory (also called buffered memory) modules place an additional layer of circuitry between the memory controller and the memory chips. Registered memory is necessary in some environments, because all memory controllers have limitations on how many devices (individual memory chips) they can control, which in turn limits the maximum capacity of the memory modules they can use. When a memory controller interacts with an unbuffered memory module, it controls every memory chip directly. When a memory controller interacts with a registered memory module, it works only with the buffer circuitry; the actual memory chips are invisible to it. The sole advantage of registered memory is that it permits a system to use higher-capacity memory modules. (The highest capacity modules at any given time are often available only in registered form.) The disadvantages of registered memory are that it is considerably more expensive than unbuffered memory and noticeably slower because of the additional overhead incurred from using a buffer.","title":"What is Registered Memory?"},{"location":"Understanding%20Memory/#why-does-the-buffer-allow-for-more-total-addressable-memory","text":"From Registered Memory Before this can make sense you have to read from Parasitic capacitance Parasitic capacitance, or stray capacitance is an unavoidable and usually unwanted capacitance that exists between the parts of an electronic component or circuit simply because of their proximity to each other. When two electrical conductors at different voltages are close together, the electric field between them causes electric charge to be stored on them; this effect is parasitic capacitance. All actual circuit elements such as inductors, diodes, and transistors have internal capacitance, which can cause their behavior to depart from that of 'ideal' circuit elements. Additionally, there is always non-zero capacitance between any two conductors; this can be significant at higher frequencies with closely spaced conductors, such as wires or printed circuit board traces. Parasitic capacitance is a significant problem in high frequency circuits and is often the factor limiting the operating frequency and bandwidth of electronic components and circuits. More on Parasitic capacitance Wait a minute! You just cannot keep adding memory into your computer without any penalty. At 1333Mhz (1.3GHz), noise gets involved. It generally is something called the signal integrity or signal reflection issue. At a point, the accumulated noise in the system would render the system not operable. At high frequency, there is also something called \"loading factor\". Each memory chip (IC) has input capacitance that tends to suppress the high frequency signal. Generally, each chip has about 3 to 5 pf of input capacitance. The more chips on the module, the more accumulated capacitance will weaken the signal to an in-operable state.","title":"Why does the buffer allow for more total addressable memory?"},{"location":"Understanding%20Memory/#r-dimms","text":"Registered (Buffered) DIMMs (R-DIMMs) insert a buffer between the command/address bus pins on the DIMM and the memory chips proper. A high-density DIMM might have 36 memory chips (assuming four ranks and ECC), each of which must receive the memory address, and their combined input capacitance limits the speed at which the memory bus can operate. By amplifying the signal on the DIMM, this allows more chips to be connected to the memory bus. The cost is one additional clock cycle of memory latency required for the address to traverse the additional buffer. Early registered RAM modules were physically incompatible with unregistered RAM modules, but SDRAM DIMMs are interchangeable, and some motherboards support both types.","title":"R-DIMMs"},{"location":"Understanding%20Memory/#lr-dimms","text":"Load Reduced DIMMs (LR-DIMMs) modules are similar, but add a buffer to the data lines as well. As a result, LRDIMM memory provides large overall maximum memory capacities, while avoiding the performance and power consumption problems of FB-DIMM memory.","title":"LR-DIMMs"},{"location":"Understanding%20Memory/#what-is-a-data-line","text":"From How Memory Works The processor specifies which memory cell it wants to use by giving a binary address. Some simple Boolean logic, address decoding, then converts the address into a row and column select and the memory still works and its efficient. Notice that at the moment we are looking at a single memory chip and this arrangement can only store a single bit. If you want to store a byte you need eight such chips, one for each bit, and eight data input and eight data output lines. The eight data lines are grouped together into a data input bus and a data output \"bus\" \u2013 a bus is just a group of wires. Early computers really did have separate buses for input and output but today\u2019s machines have a single unified data bus that can be used for either input or output. If you want more storage than a bank of eight chips can provide then you have to add another bank of eight chips and some additional address decoding logic to select the correct bank. The address lines that come from the processor are generally referred to as an address bus and now we have the fundamental architecture of a simple but modern computer.","title":"What is a data line?"},{"location":"Understanding%20Memory/#why-does-the-data-buffer-matter","text":"From What is LR-DIMM, LRDIMM Memory? (Load-Reduce DIMM) and Basics of LRDIMM You just cannot keep adding memory into your computer without any penalty. At 1333Mhz (1.3GHz), noise gets involved. It generally is something called the signal integrity or signal reflection issue. At a point, the accumulated noise in the system would render the system not operable. Figure 1: LRDIMM conceptual drawing featuring one memory buffer on the front side of the memory module and multiple ranks of DRAM mounted on both front and back sides of the memory module. The memory buffer re-drives all of the data, command, address and clock signals from the host memory controller and provides them to the multiple ranks of DRAM. The memory buffer isolates the DRAM from the host, reducing the electrical load on all interfaces. Reducing the electrical loading in this manner allows a system to operate at a higher speed for a given memory capacity, or to support a higher memory capacity at a given speed. On existing RDIMM technology, the data bus connects directly to the multiple ranks of DRAM, increasing electrical load and limiting system speed as the desired server capacity increases. Figure 2 shows the system-level block diagram for RDIMM and LRDIMM. To maximize module and system-level memory capacity and overcome limitations on the number of chip selects per channel, the LRDIMM supports Rank Multiplication, where multiple physical DRAM ranks appear to the host controller as a single logical rank of a larger size. This is done by utilizing additional row address bits during the Activate command as sub-rank select bits. One of the primary advantages of the LRDIMM is the ability to dramatically increase total system memory capacity without sacrificing speed. By electrically isolating the DRAM from the data bus, additional ranks of DRAM can be added to each DIMM while maintaining signal integrity, and additional DIMMs can be installed on each system memory channel. LRDIMM capacities up to 32GB are possible today with 4Rx4 modules using 4 Gb, DDP (dual-die package) DRAM. Since each LRDIMM presents a single electrical load to the host, more DIMMs can be installed per channel as well.","title":"Why does the data buffer matter?"},{"location":"Understanding%20Memory/#ram-and-its-relation-to-cpu-speed","text":"From Understanding CPU Limitations with Memory There are more in the weeds details, but it really is as simple as the clock speed of the CPU's memory controller has to be as good as or better than the target RAM or the RAM will downclock to meet the CPU's speed.","title":"RAM and it's relation to CPU Speed"},{"location":"Understanding%20Memory/#more-on-channels","text":"See this Reddit thread Ever since SDRAM first hit the shelves, main memory inside PC's has been a 64 bit wide interface. That means that 64 bits are sent in each transaction. DDR just meant that two lots of 64 bit transactions are sent in one clock. Obviously you see that DDR would increase the available memory bandwidth (speed), because twice as much data is being sent per clock. Well there is another way to send more data per clock, that is to send more than 64 bits in one go. You could either, widen the memory bus, to say 128 bits (GPU's use 384/512bit interfaces, but it's more complicated than that in truth) or you could add another interface. So when we go from single 'channel' to dual channel, what we are doing is adding another 64 bit wide interface (and another 2 when going from dual to quad). Now, usually you have 4 slots on your mobo for RAM. So you either have 4 independent slots connected to their own memory channel, or you have 2 pairs of slots, each pair connected to it's own memory channel. Lets take one pair. This pair has 240 pins (in desktop DDR3) coming off the CPU, connecting to one slot, then the next. This memory bus has the signalling to talk to either slot, a read request is sent out and each chip on that bus determines if it is indeed the chip that contains the requested data (to put it simply) and once it has found it, sends the data back. It can take a little while to do all this so many read and write requests can end up queued and/or in the pipeline. You add your second channel and you interleave your data across it, so bit one is the first bit on channel 1, bit 2 is the first bit on channel 2, bit 3 the second on channel 1 etc (likely to be done in chunks larger than single bits though) In the exact same way that RAID can give you much faster hard disk access (but certain access patterns not so much) adding extra memory channels allows you to read extra data. Double your memory channels, double the data you can read per second. Of course in reality nothing ever scales perfectly but you get the idea. The flip side of having these extra memory channels is that you need to populate them all, or you're wasting them. So a quad channel system will need 4 memory modules, if it has any less than that it is only using a number of memory channels equal to the number of modules you're using. This was a pretty good explanation, until you got to the part about bit interleaving. That is definitely not how it works. I know you corrected yourself and said that it probably works at a chunkier granularity, but I only noticed that after I'd already written all of this, so sorry. I'm going to get a little technical, so ignore this unless you want lots of details. DRAM systems are logically organized into (going from largest to smallest sub-division): channels, ranks, banks, rows, and columns. A channel is the physical connection between the memory controller (which is on the CPU die these days), and the DIMMs (the boards with DRAM chips on them). It's the wires, the aforementioned 64-bit data interface, plus all the control signals. A channel may have one or more DIMMs on it (for DDR3 at least, but DDR4 will require only a single DIMM per channel), and when there is a memory request (be it read or write) all of the data for that request comes from a single channel. There is no bit-interleaving across channels. All of the CPUs you and I deal with on a daily basis use 64 byte cache lines, so DRAM is accessed at the granularity of 64 byte blocks. So when there is a DRAM read, all 64 bytes come from a single channel. Furthermore, all 64 bytes come from a single rank within that channel. A rank is a collection of DRAM chips on a DIMM that work together to service a DRAM transaction. In the desktop systems we interact with every day, a rank is typically comprised of 8 DRAM chips. A DIMM may have more than 1 rank on it (the DIMMs you are familiar with probably have 8 DRAM chips on each side, so 2 ranks). This means that when your CPU sends a DRAM read request along a channel, 8 DRAM chips on the DIMM work together to fulfill that request. Remember that the data bus width is 64 bits, so each of these 8 DRAM chips is only responsible for transmitting 8 bits each per transfer. They transmit at the exact same time, in lock step with each other, and so from the CPU's perspective it seems like a single 64-bit data bus. After one DRAM transaction is completed on that channel, the next transaction might come from a different rank on the same channel (perhaps the other set of 8 DRAM chips on the same DIMM, or a rank on another DIMM on the same channel), or it could be the same rank again. Here is the important part: separate channels can work 100% independently, because they have absolutely 0 resources they have to manage and share between them, but separate ranks on the same channel must serialize their actions, because they all share the same data and control buses, and they must take turns using these resources. Depending on the access pattern you expect to be most common, you can layout data across your DRAM channels/ranks/banks/rows in various ways. One high performance strategy is to layout consecutive cache lines across channels (so address A is in channel 1, and address A+64 is in channel 2). This lets you fetch A+64 while you're still working on A. There are other data layout strategies for other expected access patterns. What I'm saying is that it is common for cache lines to be striped (interleaved) across channels, but not individual bits. If anyone wants, I can go into more detail about how banks, rows, and columns work, but I think channels and ranks are all that is needed to answer the OP's question. One last thing, about GPU memory buses. In graphics card specs, they always say that the GPU has a \"384-bit bus\", but this is misleading. It does have 384 total bits of GDDR5 data bus, but it is not logically organized as a single bus (like the 8 DRAM chips of a rank work together to logically form a 64-bit bus in DDR3). Instead, a GPU with a \"384-bit bus\" will actually have 12 independent 32-bit GDDR5 data buses that all act independently from one another. Source: Ph.D. computer architecture student who's studied DRAM organization for the last 4 years.","title":"More on Channels"},{"location":"Understanding%20Memory/#best-memory-for-different-circumstances","text":"https://www.microway.com/hpc-tech-tips/ddr4-rdimm-lrdimm-performance-comparison/","title":"Best Memory for Different Circumstances"},{"location":"Use%20OS10%20as%20Aggregator/","text":"Use OS10 as Aggregator In this test case the goal is to create a simple load balancer using a reverse LAG port. The idea is to have one input port which is then mirrored to a logical LAG port and at the other end of the LAG port is a number of security sensors. Helpful Links ONIE Network Install Process Overview My Configuration OS 10 Version OS10(config)# do show version Dell EMC Networking OS10 Enterprise Copyright (c) 1999-2020 by Dell Inc. All Rights Reserved. OS Version: 10.5.1.0 Build Version: 10.5.1.0.124 Build Time: 2020-02-12T09:05:20+0000 System Type: S4112F-ON Architecture: x86_64 Up Time: 00:05:09 Configure Device for LAG Physical Configuration 2 10Gb/s fiber SFPs one each connected to two separate VMs. These will serve as our traffic generators. A third 10Gb/s SFP is plugged into a third VM and this will serve as our receiver. Configuration Configuration Test Scenario I played a PCAP made up entirely of HTTP from port 1 and PCAP made entirely of DNS from port 2 and then listened on the VM attached to port 3. Confirmed, all traffic showed up on port 3 as expected.","title":"Use OS10 as Aggregator"},{"location":"Use%20OS10%20as%20Aggregator/#use-os10-as-aggregator","text":"In this test case the goal is to create a simple load balancer using a reverse LAG port. The idea is to have one input port which is then mirrored to a logical LAG port and at the other end of the LAG port is a number of security sensors.","title":"Use OS10 as Aggregator"},{"location":"Use%20OS10%20as%20Aggregator/#helpful-links","text":"ONIE Network Install Process Overview","title":"Helpful Links"},{"location":"Use%20OS10%20as%20Aggregator/#my-configuration","text":"","title":"My Configuration"},{"location":"Use%20OS10%20as%20Aggregator/#os-10-version","text":"OS10(config)# do show version Dell EMC Networking OS10 Enterprise Copyright (c) 1999-2020 by Dell Inc. All Rights Reserved. OS Version: 10.5.1.0 Build Version: 10.5.1.0.124 Build Time: 2020-02-12T09:05:20+0000 System Type: S4112F-ON Architecture: x86_64 Up Time: 00:05:09","title":"OS 10 Version"},{"location":"Use%20OS10%20as%20Aggregator/#configure-device-for-lag","text":"","title":"Configure Device for LAG"},{"location":"Use%20OS10%20as%20Aggregator/#physical-configuration","text":"2 10Gb/s fiber SFPs one each connected to two separate VMs. These will serve as our traffic generators. A third 10Gb/s SFP is plugged into a third VM and this will serve as our receiver.","title":"Physical Configuration"},{"location":"Use%20OS10%20as%20Aggregator/#configuration","text":"Configuration","title":"Configuration"},{"location":"Use%20OS10%20as%20Aggregator/#test-scenario","text":"I played a PCAP made up entirely of HTTP from port 1 and PCAP made entirely of DNS from port 2 and then listened on the VM attached to port 3. Confirmed, all traffic showed up on port 3 as expected.","title":"Test Scenario"},{"location":"Using%20the%20iDRAC%20Service%20Module/","text":"Using the iDRAC Service Module Objective Establish direct, inter-chassis, communication between a host operating system and the iDRAC. My Setup Hardware Dell R840 Note : This will work with any Dell server with iDRAC support. Operating System NAME=\"Red Hat Enterprise Linux\" VERSION=\"8.4 (Ootpa)\" ID=\"rhel\" ID_LIKE=\"fedora\" VERSION_ID=\"8.4\" PLATFORM_ID=\"platform:el8\" PRETTY_NAME=\"Red Hat Enterprise Linux 8.4 (Ootpa)\" ANSI_COLOR=\"0;31\" CPE_NAME=\"cpe:/o:redhat:enterprise_linux:8.4:GA\" HOME_URL=\"https://www.redhat.com/\" DOCUMENTATION_URL=\"https://access.redhat.com/documentation/red_hat_enterprise_linux/8/\" BUG_REPORT_URL=\"https://bugzilla.redhat.com/\" REDHAT_BUGZILLA_PRODUCT=\"Red Hat Enterprise Linux 8\" REDHAT_BUGZILLA_PRODUCT_VERSION=8.4 REDHAT_SUPPORT_PRODUCT=\"Red Hat Enterprise Linux\" REDHAT_SUPPORT_PRODUCT_VERSION=\"8.4\" Red Hat Enterprise Linux release 8.4 (Ootpa) Red Hat Enterprise Linux release 8.4 (Ootpa) Installation Go to your server's support page. In my case this is for the R840 and select the appropriate operating system. Click \"Show All\" on the driver list to see all the different drivers Look for the entry which says \"iDRAC Service Module\" and download it. The Linux entry is here Move the file to your server, extract, and run setup.sh Establishing Direct Communication The way the iDRAC service module works is that it adds a driver which then adds a virtual NIC called iDRAC: [root@r8402 OM-SrvAdmin-Dell-Web-LX-10.1.0.0-4561.RHEL8.x86_64_A00]# ip a s 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2: eno145: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc mq state DOWN group default qlen 1000 link/ether e4:43:4b:9f:44:00 brd ff:ff:ff:ff:ff:ff 3: eno99: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP group default qlen 1000 link/ether e4:43:4b:9f:44:20 brd ff:ff:ff:ff:ff:ff inet 192.168.1.89/24 brd 192.168.1.255 scope global noprefixroute eno99 valid_lft forever preferred_lft forever inet6 fe80::e643:4bff:fe9f:4420/64 scope link noprefixroute valid_lft forever preferred_lft forever 4: eno146: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc mq state DOWN group default qlen 1000 link/ether e4:43:4b:9f:44:02 brd ff:ff:ff:ff:ff:ff 5: eno100: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc mq state DOWN group default qlen 1000 link/ether e4:43:4b:9f:44:21 brd ff:ff:ff:ff:ff:ff 6: idrac: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UNKNOWN group default qlen 1000 link/ether 70:b5:e8:e2:a0:d3 brd ff:ff:ff:ff:ff:ff inet 169.254.0.2/16 brd 169.254.255.255 scope global idrac valid_lft forever preferred_lft forever 7: virbr0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default qlen 1000 link/ether 52:54:00:b5:c5:1b brd ff:ff:ff:ff:ff:ff inet 192.168.122.1/24 brd 192.168.122.255 scope global virbr0 valid_lft forever preferred_lft forever 8: virbr0-nic: <BROADCAST,MULTICAST> mtu 1500 qdisc fq_codel master virbr0 state DOWN group default qlen 1000 link/ether 52:54:00:b5:c5:1b brd ff:ff:ff:ff:ff:ff The inter-chassis network uses the IPv4 link local subnet 169.254.0.0/16. In general, the server should be 169.254.0.2 and the iDRAC itself is 169.254.0.1. You can connect to 169.254.0.1 as you would any iDRAC address:","title":"Using the iDRAC Service Module"},{"location":"Using%20the%20iDRAC%20Service%20Module/#using-the-idrac-service-module","text":"","title":"Using the iDRAC Service Module"},{"location":"Using%20the%20iDRAC%20Service%20Module/#objective","text":"Establish direct, inter-chassis, communication between a host operating system and the iDRAC.","title":"Objective"},{"location":"Using%20the%20iDRAC%20Service%20Module/#my-setup","text":"","title":"My Setup"},{"location":"Using%20the%20iDRAC%20Service%20Module/#hardware","text":"Dell R840 Note : This will work with any Dell server with iDRAC support.","title":"Hardware"},{"location":"Using%20the%20iDRAC%20Service%20Module/#operating-system","text":"NAME=\"Red Hat Enterprise Linux\" VERSION=\"8.4 (Ootpa)\" ID=\"rhel\" ID_LIKE=\"fedora\" VERSION_ID=\"8.4\" PLATFORM_ID=\"platform:el8\" PRETTY_NAME=\"Red Hat Enterprise Linux 8.4 (Ootpa)\" ANSI_COLOR=\"0;31\" CPE_NAME=\"cpe:/o:redhat:enterprise_linux:8.4:GA\" HOME_URL=\"https://www.redhat.com/\" DOCUMENTATION_URL=\"https://access.redhat.com/documentation/red_hat_enterprise_linux/8/\" BUG_REPORT_URL=\"https://bugzilla.redhat.com/\" REDHAT_BUGZILLA_PRODUCT=\"Red Hat Enterprise Linux 8\" REDHAT_BUGZILLA_PRODUCT_VERSION=8.4 REDHAT_SUPPORT_PRODUCT=\"Red Hat Enterprise Linux\" REDHAT_SUPPORT_PRODUCT_VERSION=\"8.4\" Red Hat Enterprise Linux release 8.4 (Ootpa) Red Hat Enterprise Linux release 8.4 (Ootpa)","title":"Operating System"},{"location":"Using%20the%20iDRAC%20Service%20Module/#installation","text":"Go to your server's support page. In my case this is for the R840 and select the appropriate operating system. Click \"Show All\" on the driver list to see all the different drivers Look for the entry which says \"iDRAC Service Module\" and download it. The Linux entry is here Move the file to your server, extract, and run setup.sh","title":"Installation"},{"location":"Using%20the%20iDRAC%20Service%20Module/#establishing-direct-communication","text":"The way the iDRAC service module works is that it adds a driver which then adds a virtual NIC called iDRAC: [root@r8402 OM-SrvAdmin-Dell-Web-LX-10.1.0.0-4561.RHEL8.x86_64_A00]# ip a s 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2: eno145: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc mq state DOWN group default qlen 1000 link/ether e4:43:4b:9f:44:00 brd ff:ff:ff:ff:ff:ff 3: eno99: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP group default qlen 1000 link/ether e4:43:4b:9f:44:20 brd ff:ff:ff:ff:ff:ff inet 192.168.1.89/24 brd 192.168.1.255 scope global noprefixroute eno99 valid_lft forever preferred_lft forever inet6 fe80::e643:4bff:fe9f:4420/64 scope link noprefixroute valid_lft forever preferred_lft forever 4: eno146: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc mq state DOWN group default qlen 1000 link/ether e4:43:4b:9f:44:02 brd ff:ff:ff:ff:ff:ff 5: eno100: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc mq state DOWN group default qlen 1000 link/ether e4:43:4b:9f:44:21 brd ff:ff:ff:ff:ff:ff 6: idrac: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UNKNOWN group default qlen 1000 link/ether 70:b5:e8:e2:a0:d3 brd ff:ff:ff:ff:ff:ff inet 169.254.0.2/16 brd 169.254.255.255 scope global idrac valid_lft forever preferred_lft forever 7: virbr0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default qlen 1000 link/ether 52:54:00:b5:c5:1b brd ff:ff:ff:ff:ff:ff inet 192.168.122.1/24 brd 192.168.122.255 scope global virbr0 valid_lft forever preferred_lft forever 8: virbr0-nic: <BROADCAST,MULTICAST> mtu 1500 qdisc fq_codel master virbr0 state DOWN group default qlen 1000 link/ether 52:54:00:b5:c5:1b brd ff:ff:ff:ff:ff:ff The inter-chassis network uses the IPv4 link local subnet 169.254.0.0/16. In general, the server should be 169.254.0.2 and the iDRAC itself is 169.254.0.1. You can connect to 169.254.0.1 as you would any iDRAC address:","title":"Establishing Direct Communication"},{"location":"VEP%20Testing/","text":"VEP Testing Unscrew the tiny panel on the back. Underneath is microUSB. Plug in. Bring up device manager. Look at ports. After the automatic driver installation a port should appear there with notation Silicon Labs CP210x USB to UART Bridge (COMX) . This is what you want to connect to Open putty set to serial with speed 115200 and COM You will need to download ESXi from the Dell website using your serial number. This is very important as the ESXi installation has unique drivers injected into it for your convienience. You could load vanilla ESXi and load the drivers yourself. Burn ESXi to a flash drive using Rufus and plug it into the side of the VEP. Power cycle the VEP and hit delete when prompted to open the firmware screen. Set the flash drive as the highest priority for boot and then save and exit. Install ESXi normally via the serial console.","title":"VEP Testing"},{"location":"VEP%20Testing/#vep-testing","text":"Unscrew the tiny panel on the back. Underneath is microUSB. Plug in. Bring up device manager. Look at ports. After the automatic driver installation a port should appear there with notation Silicon Labs CP210x USB to UART Bridge (COMX) . This is what you want to connect to Open putty set to serial with speed 115200 and COM You will need to download ESXi from the Dell website using your serial number. This is very important as the ESXi installation has unique drivers injected into it for your convienience. You could load vanilla ESXi and load the drivers yourself. Burn ESXi to a flash drive using Rufus and plug it into the side of the VEP. Power cycle the VEP and hit delete when prompted to open the firmware screen. Set the flash drive as the highest priority for boot and then save and exit. Install ESXi normally via the serial console.","title":"VEP Testing"},{"location":"VMWare/Automate%20ESXi%20Installation/","text":"Automating ESXi Installation This Ansible playbook runs on CentOS. It will create a server which allows you to install ESXi automatically via PXE boot and Kickstart. For more information on ESXi's Kickstart capabilities see this link Prerequisites Install CentOS You can download and install CentOS here The installation services used by this program are very lightweight so you can install on just about anything. A 12GB hard drive, 2GB RAM, and a single processor are enough for what we are doing. Go ahead and install CentOS on whatever you like, the only requirement is that it must be reachable at layer 2 by the ESXi hosts you want to install. Install Ansible and git Before continuing you will need to install Ansible on your host by running yum install -y ansible git . Clone the Repo I typically use opt for optional programs. You may install wherever you like, but in this guide I will use opt. Clone the repo with: git clone https://github.com/grantcurell/esxi-autoinstall.git Move into the esxi-autoinstall directory with cd /opt/esxi-autoinstall Configure the Inventory File Ansible is controlled by what is called an inventory file. This inventory file contains all the configuration settings which will be used by Ansible. In our case, we will need to populate this with the settings we wish ESXi to have after installation. The inventory file is called inventory.yml You will need to fill in the following values: dns dhcp_start dhcp_end gateway netmask domain root_password server_ip iso_esxi_pth iso_esxi_checksum Finally you will also need to fill in the host information. These are the hosts on which you would like to install ESXi. Boot Drives Most of the values are self explanatory however, boot_drive and data_drives may be a bit confusing. These options allow you to tell Ansible where to install ESXi and on which disks to configure datastores. The official documentation on how ESXi names drives is here The disks are in the order in which ESXi detects them. To get the order you may have to manually install ESXi on a system. You can make an educated guess from the BIOS menu or if you have something like iDrac you can look at the order of the disks on the RAID controller (assuming you have one). Running the Code Once you are finished editing the inventory.yml file, cd to the root of the esxi-autoinstall directory and run make . This will run the makefile in the directory. You can see what it is doing by examining the file called Makefile Advanced You may want the installer to do some crazier things. The kickstart script used to configure the hosts can be modified to run arbitrary esxcli commands. You're on your own if you want to go this route, but you can find the kickstart file with the esxcli commands in roles/profiles/templates/ks.cfg.j2 .","title":"Automating ESXi Installation"},{"location":"VMWare/Automate%20ESXi%20Installation/#automating-esxi-installation","text":"This Ansible playbook runs on CentOS. It will create a server which allows you to install ESXi automatically via PXE boot and Kickstart. For more information on ESXi's Kickstart capabilities see this link","title":"Automating ESXi Installation"},{"location":"VMWare/Automate%20ESXi%20Installation/#prerequisites","text":"","title":"Prerequisites"},{"location":"VMWare/Automate%20ESXi%20Installation/#install-centos","text":"You can download and install CentOS here The installation services used by this program are very lightweight so you can install on just about anything. A 12GB hard drive, 2GB RAM, and a single processor are enough for what we are doing. Go ahead and install CentOS on whatever you like, the only requirement is that it must be reachable at layer 2 by the ESXi hosts you want to install.","title":"Install CentOS"},{"location":"VMWare/Automate%20ESXi%20Installation/#install-ansible-and-git","text":"Before continuing you will need to install Ansible on your host by running yum install -y ansible git .","title":"Install Ansible and git"},{"location":"VMWare/Automate%20ESXi%20Installation/#clone-the-repo","text":"I typically use opt for optional programs. You may install wherever you like, but in this guide I will use opt. Clone the repo with: git clone https://github.com/grantcurell/esxi-autoinstall.git Move into the esxi-autoinstall directory with cd /opt/esxi-autoinstall","title":"Clone the Repo"},{"location":"VMWare/Automate%20ESXi%20Installation/#configure-the-inventory-file","text":"Ansible is controlled by what is called an inventory file. This inventory file contains all the configuration settings which will be used by Ansible. In our case, we will need to populate this with the settings we wish ESXi to have after installation. The inventory file is called inventory.yml You will need to fill in the following values: dns dhcp_start dhcp_end gateway netmask domain root_password server_ip iso_esxi_pth iso_esxi_checksum Finally you will also need to fill in the host information. These are the hosts on which you would like to install ESXi.","title":"Configure the Inventory File"},{"location":"VMWare/Automate%20ESXi%20Installation/#boot-drives","text":"Most of the values are self explanatory however, boot_drive and data_drives may be a bit confusing. These options allow you to tell Ansible where to install ESXi and on which disks to configure datastores. The official documentation on how ESXi names drives is here The disks are in the order in which ESXi detects them. To get the order you may have to manually install ESXi on a system. You can make an educated guess from the BIOS menu or if you have something like iDrac you can look at the order of the disks on the RAID controller (assuming you have one).","title":"Boot Drives"},{"location":"VMWare/Automate%20ESXi%20Installation/#running-the-code","text":"Once you are finished editing the inventory.yml file, cd to the root of the esxi-autoinstall directory and run make . This will run the makefile in the directory. You can see what it is doing by examining the file called Makefile","title":"Running the Code"},{"location":"VMWare/Automate%20ESXi%20Installation/#advanced","text":"You may want the installer to do some crazier things. The kickstart script used to configure the hosts can be modified to run arbitrary esxcli commands. You're on your own if you want to go this route, but you can find the kickstart file with the esxcli commands in roles/profiles/templates/ks.cfg.j2 .","title":"Advanced"},{"location":"VMWare/Automate%20ESXi%20Installation/LICENSE/","text":"GNU GENERAL PUBLIC LICENSE Version 3, 29 June 2007 Copyright (C) 2007 Free Software Foundation, Inc. http://fsf.org/ Everyone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed. Preamble The GNU General Public License is a free, copyleft license for software and other kinds of works. The licenses for most software and other practical works are designed to take away your freedom to share and change the works. By contrast, the GNU General Public License is intended to guarantee your freedom to share and change all versions of a program--to make sure it remains free software for all its users. We, the Free Software Foundation, use the GNU General Public License for most of our software; it applies also to any other work released this way by its authors. You can apply it to your programs, too. When we speak of free software, we are referring to freedom, not price. Our General Public Licenses are designed to make sure that you have the freedom to distribute copies of free software (and charge for them if you wish), that you receive source code or can get it if you want it, that you can change the software or use pieces of it in new free programs, and that you know you can do these things. To protect your rights, we need to prevent others from denying you these rights or asking you to surrender the rights. Therefore, you have certain responsibilities if you distribute copies of the software, or if you modify it: responsibilities to respect the freedom of others. For example, if you distribute copies of such a program, whether gratis or for a fee, you must pass on to the recipients the same freedoms that you received. You must make sure that they, too, receive or can get the source code. And you must show them these terms so they know their rights. Developers that use the GNU GPL protect your rights with two steps: (1) assert copyright on the software, and (2) offer you this License giving you legal permission to copy, distribute and/or modify it. For the developers' and authors' protection, the GPL clearly explains that there is no warranty for this free software. For both users' and authors' sake, the GPL requires that modified versions be marked as changed, so that their problems will not be attributed erroneously to authors of previous versions. Some devices are designed to deny users access to install or run modified versions of the software inside them, although the manufacturer can do so. This is fundamentally incompatible with the aim of protecting users' freedom to change the software. The systematic pattern of such abuse occurs in the area of products for individuals to use, which is precisely where it is most unacceptable. Therefore, we have designed this version of the GPL to prohibit the practice for those products. If such problems arise substantially in other domains, we stand ready to extend this provision to those domains in future versions of the GPL, as needed to protect the freedom of users. Finally, every program is threatened constantly by software patents. States should not allow patents to restrict development and use of software on general-purpose computers, but in those that do, we wish to avoid the special danger that patents applied to a free program could make it effectively proprietary. To prevent this, the GPL assures that patents cannot be used to render the program non-free. The precise terms and conditions for copying, distribution and modification follow. TERMS AND CONDITIONS Definitions. \"This License\" refers to version 3 of the GNU General Public License. \"Copyright\" also means copyright-like laws that apply to other kinds of works, such as semiconductor masks. \"The Program\" refers to any copyrightable work licensed under this License. Each licensee is addressed as \"you\". \"Licensees\" and \"recipients\" may be individuals or organizations. To \"modify\" a work means to copy from or adapt all or part of the work in a fashion requiring copyright permission, other than the making of an exact copy. The resulting work is called a \"modified version\" of the earlier work or a work \"based on\" the earlier work. A \"covered work\" means either the unmodified Program or a work based on the Program. To \"propagate\" a work means to do anything with it that, without permission, would make you directly or secondarily liable for infringement under applicable copyright law, except executing it on a computer or modifying a private copy. Propagation includes copying, distribution (with or without modification), making available to the public, and in some countries other activities as well. To \"convey\" a work means any kind of propagation that enables other parties to make or receive copies. Mere interaction with a user through a computer network, with no transfer of a copy, is not conveying. An interactive user interface displays \"Appropriate Legal Notices\" to the extent that it includes a convenient and prominently visible feature that (1) displays an appropriate copyright notice, and (2) tells the user that there is no warranty for the work (except to the extent that warranties are provided), that licensees may convey the work under this License, and how to view a copy of this License. If the interface presents a list of user commands or options, such as a menu, a prominent item in the list meets this criterion. Source Code. The \"source code\" for a work means the preferred form of the work for making modifications to it. \"Object code\" means any non-source form of a work. A \"Standard Interface\" means an interface that either is an official standard defined by a recognized standards body, or, in the case of interfaces specified for a particular programming language, one that is widely used among developers working in that language. The \"System Libraries\" of an executable work include anything, other than the work as a whole, that (a) is included in the normal form of packaging a Major Component, but which is not part of that Major Component, and (b) serves only to enable use of the work with that Major Component, or to implement a Standard Interface for which an implementation is available to the public in source code form. A \"Major Component\", in this context, means a major essential component (kernel, window system, and so on) of the specific operating system (if any) on which the executable work runs, or a compiler used to produce the work, or an object code interpreter used to run it. The \"Corresponding Source\" for a work in object code form means all the source code needed to generate, install, and (for an executable work) run the object code and to modify the work, including scripts to control those activities. However, it does not include the work's System Libraries, or general-purpose tools or generally available free programs which are used unmodified in performing those activities but which are not part of the work. For example, Corresponding Source includes interface definition files associated with source files for the work, and the source code for shared libraries and dynamically linked subprograms that the work is specifically designed to require, such as by intimate data communication or control flow between those subprograms and other parts of the work. The Corresponding Source need not include anything that users can regenerate automatically from other parts of the Corresponding Source. The Corresponding Source for a work in source code form is that same work. Basic Permissions. All rights granted under this License are granted for the term of copyright on the Program, and are irrevocable provided the stated conditions are met. This License explicitly affirms your unlimited permission to run the unmodified Program. The output from running a covered work is covered by this License only if the output, given its content, constitutes a covered work. This License acknowledges your rights of fair use or other equivalent, as provided by copyright law. You may make, run and propagate covered works that you do not convey, without conditions so long as your license otherwise remains in force. You may convey covered works to others for the sole purpose of having them make modifications exclusively for you, or provide you with facilities for running those works, provided that you comply with the terms of this License in conveying all material for which you do not control copyright. Those thus making or running the covered works for you must do so exclusively on your behalf, under your direction and control, on terms that prohibit them from making any copies of your copyrighted material outside their relationship with you. Conveying under any other circumstances is permitted solely under the conditions stated below. Sublicensing is not allowed; section 10 makes it unnecessary. Protecting Users' Legal Rights From Anti-Circumvention Law. No covered work shall be deemed part of an effective technological measure under any applicable law fulfilling obligations under article 11 of the WIPO copyright treaty adopted on 20 December 1996, or similar laws prohibiting or restricting circumvention of such measures. When you convey a covered work, you waive any legal power to forbid circumvention of technological measures to the extent such circumvention is effected by exercising rights under this License with respect to the covered work, and you disclaim any intention to limit operation or modification of the work as a means of enforcing, against the work's users, your or third parties' legal rights to forbid circumvention of technological measures. Conveying Verbatim Copies. You may convey verbatim copies of the Program's source code as you receive it, in any medium, provided that you conspicuously and appropriately publish on each copy an appropriate copyright notice; keep intact all notices stating that this License and any non-permissive terms added in accord with section 7 apply to the code; keep intact all notices of the absence of any warranty; and give all recipients a copy of this License along with the Program. You may charge any price or no price for each copy that you convey, and you may offer support or warranty protection for a fee. Conveying Modified Source Versions. You may convey a work based on the Program, or the modifications to produce it from the Program, in the form of source code under the terms of section 4, provided that you also meet all of these conditions: a) The work must carry prominent notices stating that you modified it, and giving a relevant date. b) The work must carry prominent notices stating that it is released under this License and any conditions added under section 7. This requirement modifies the requirement in section 4 to \"keep intact all notices\". c) You must license the entire work, as a whole, under this License to anyone who comes into possession of a copy. This License will therefore apply, along with any applicable section 7 additional terms, to the whole of the work, and all its parts, regardless of how they are packaged. This License gives no permission to license the work in any other way, but it does not invalidate such permission if you have separately received it. d) If the work has interactive user interfaces, each must display Appropriate Legal Notices; however, if the Program has interactive interfaces that do not display Appropriate Legal Notices, your work need not make them do so. A compilation of a covered work with other separate and independent works, which are not by their nature extensions of the covered work, and which are not combined with it such as to form a larger program, in or on a volume of a storage or distribution medium, is called an \"aggregate\" if the compilation and its resulting copyright are not used to limit the access or legal rights of the compilation's users beyond what the individual works permit. Inclusion of a covered work in an aggregate does not cause this License to apply to the other parts of the aggregate. Conveying Non-Source Forms. You may convey a covered work in object code form under the terms of sections 4 and 5, provided that you also convey the machine-readable Corresponding Source under the terms of this License, in one of these ways: a) Convey the object code in, or embodied in, a physical product (including a physical distribution medium), accompanied by the Corresponding Source fixed on a durable physical medium customarily used for software interchange. b) Convey the object code in, or embodied in, a physical product (including a physical distribution medium), accompanied by a written offer, valid for at least three years and valid for as long as you offer spare parts or customer support for that product model, to give anyone who possesses the object code either (1) a copy of the Corresponding Source for all the software in the product that is covered by this License, on a durable physical medium customarily used for software interchange, for a price no more than your reasonable cost of physically performing this conveying of source, or (2) access to copy the Corresponding Source from a network server at no charge. c) Convey individual copies of the object code with a copy of the written offer to provide the Corresponding Source. This alternative is allowed only occasionally and noncommercially, and only if you received the object code with such an offer, in accord with subsection 6b. d) Convey the object code by offering access from a designated place (gratis or for a charge), and offer equivalent access to the Corresponding Source in the same way through the same place at no further charge. You need not require recipients to copy the Corresponding Source along with the object code. If the place to copy the object code is a network server, the Corresponding Source may be on a different server (operated by you or a third party) that supports equivalent copying facilities, provided you maintain clear directions next to the object code saying where to find the Corresponding Source. Regardless of what server hosts the Corresponding Source, you remain obligated to ensure that it is available for as long as needed to satisfy these requirements. e) Convey the object code using peer-to-peer transmission, provided you inform other peers where the object code and Corresponding Source of the work are being offered to the general public at no charge under subsection 6d. A separable portion of the object code, whose source code is excluded from the Corresponding Source as a System Library, need not be included in conveying the object code work. A \"User Product\" is either (1) a \"consumer product\", which means any tangible personal property which is normally used for personal, family, or household purposes, or (2) anything designed or sold for incorporation into a dwelling. In determining whether a product is a consumer product, doubtful cases shall be resolved in favor of coverage. For a particular product received by a particular user, \"normally used\" refers to a typical or common use of that class of product, regardless of the status of the particular user or of the way in which the particular user actually uses, or expects or is expected to use, the product. A product is a consumer product regardless of whether the product has substantial commercial, industrial or non-consumer uses, unless such uses represent the only significant mode of use of the product. \"Installation Information\" for a User Product means any methods, procedures, authorization keys, or other information required to install and execute modified versions of a covered work in that User Product from a modified version of its Corresponding Source. The information must suffice to ensure that the continued functioning of the modified object code is in no case prevented or interfered with solely because modification has been made. If you convey an object code work under this section in, or with, or specifically for use in, a User Product, and the conveying occurs as part of a transaction in which the right of possession and use of the User Product is transferred to the recipient in perpetuity or for a fixed term (regardless of how the transaction is characterized), the Corresponding Source conveyed under this section must be accompanied by the Installation Information. But this requirement does not apply if neither you nor any third party retains the ability to install modified object code on the User Product (for example, the work has been installed in ROM). The requirement to provide Installation Information does not include a requirement to continue to provide support service, warranty, or updates for a work that has been modified or installed by the recipient, or for the User Product in which it has been modified or installed. Access to a network may be denied when the modification itself materially and adversely affects the operation of the network or violates the rules and protocols for communication across the network. Corresponding Source conveyed, and Installation Information provided, in accord with this section must be in a format that is publicly documented (and with an implementation available to the public in source code form), and must require no special password or key for unpacking, reading or copying. Additional Terms. \"Additional permissions\" are terms that supplement the terms of this License by making exceptions from one or more of its conditions. Additional permissions that are applicable to the entire Program shall be treated as though they were included in this License, to the extent that they are valid under applicable law. If additional permissions apply only to part of the Program, that part may be used separately under those permissions, but the entire Program remains governed by this License without regard to the additional permissions. When you convey a copy of a covered work, you may at your option remove any additional permissions from that copy, or from any part of it. (Additional permissions may be written to require their own removal in certain cases when you modify the work.) You may place additional permissions on material, added by you to a covered work, for which you have or can give appropriate copyright permission. Notwithstanding any other provision of this License, for material you add to a covered work, you may (if authorized by the copyright holders of that material) supplement the terms of this License with terms: a) Disclaiming warranty or limiting liability differently from the terms of sections 15 and 16 of this License; or b) Requiring preservation of specified reasonable legal notices or author attributions in that material or in the Appropriate Legal Notices displayed by works containing it; or c) Prohibiting misrepresentation of the origin of that material, or requiring that modified versions of such material be marked in reasonable ways as different from the original version; or d) Limiting the use for publicity purposes of names of licensors or authors of the material; or e) Declining to grant rights under trademark law for use of some trade names, trademarks, or service marks; or f) Requiring indemnification of licensors and authors of that material by anyone who conveys the material (or modified versions of it) with contractual assumptions of liability to the recipient, for any liability that these contractual assumptions directly impose on those licensors and authors. All other non-permissive additional terms are considered \"further restrictions\" within the meaning of section 10. If the Program as you received it, or any part of it, contains a notice stating that it is governed by this License along with a term that is a further restriction, you may remove that term. If a license document contains a further restriction but permits relicensing or conveying under this License, you may add to a covered work material governed by the terms of that license document, provided that the further restriction does not survive such relicensing or conveying. If you add terms to a covered work in accord with this section, you must place, in the relevant source files, a statement of the additional terms that apply to those files, or a notice indicating where to find the applicable terms. Additional terms, permissive or non-permissive, may be stated in the form of a separately written license, or stated as exceptions; the above requirements apply either way. Termination. You may not propagate or modify a covered work except as expressly provided under this License. Any attempt otherwise to propagate or modify it is void, and will automatically terminate your rights under this License (including any patent licenses granted under the third paragraph of section 11). However, if you cease all violation of this License, then your license from a particular copyright holder is reinstated (a) provisionally, unless and until the copyright holder explicitly and finally terminates your license, and (b) permanently, if the copyright holder fails to notify you of the violation by some reasonable means prior to 60 days after the cessation. Moreover, your license from a particular copyright holder is reinstated permanently if the copyright holder notifies you of the violation by some reasonable means, this is the first time you have received notice of violation of this License (for any work) from that copyright holder, and you cure the violation prior to 30 days after your receipt of the notice. Termination of your rights under this section does not terminate the licenses of parties who have received copies or rights from you under this License. If your rights have been terminated and not permanently reinstated, you do not qualify to receive new licenses for the same material under section 10. Acceptance Not Required for Having Copies. You are not required to accept this License in order to receive or run a copy of the Program. Ancillary propagation of a covered work occurring solely as a consequence of using peer-to-peer transmission to receive a copy likewise does not require acceptance. However, nothing other than this License grants you permission to propagate or modify any covered work. These actions infringe copyright if you do not accept this License. Therefore, by modifying or propagating a covered work, you indicate your acceptance of this License to do so. Automatic Licensing of Downstream Recipients. Each time you convey a covered work, the recipient automatically receives a license from the original licensors, to run, modify and propagate that work, subject to this License. You are not responsible for enforcing compliance by third parties with this License. An \"entity transaction\" is a transaction transferring control of an organization, or substantially all assets of one, or subdividing an organization, or merging organizations. If propagation of a covered work results from an entity transaction, each party to that transaction who receives a copy of the work also receives whatever licenses to the work the party's predecessor in interest had or could give under the previous paragraph, plus a right to possession of the Corresponding Source of the work from the predecessor in interest, if the predecessor has it or can get it with reasonable efforts. You may not impose any further restrictions on the exercise of the rights granted or affirmed under this License. For example, you may not impose a license fee, royalty, or other charge for exercise of rights granted under this License, and you may not initiate litigation (including a cross-claim or counterclaim in a lawsuit) alleging that any patent claim is infringed by making, using, selling, offering for sale, or importing the Program or any portion of it. Patents. A \"contributor\" is a copyright holder who authorizes use under this License of the Program or a work on which the Program is based. The work thus licensed is called the contributor's \"contributor version\". A contributor's \"essential patent claims\" are all patent claims owned or controlled by the contributor, whether already acquired or hereafter acquired, that would be infringed by some manner, permitted by this License, of making, using, or selling its contributor version, but do not include claims that would be infringed only as a consequence of further modification of the contributor version. For purposes of this definition, \"control\" includes the right to grant patent sublicenses in a manner consistent with the requirements of this License. Each contributor grants you a non-exclusive, worldwide, royalty-free patent license under the contributor's essential patent claims, to make, use, sell, offer for sale, import and otherwise run, modify and propagate the contents of its contributor version. In the following three paragraphs, a \"patent license\" is any express agreement or commitment, however denominated, not to enforce a patent (such as an express permission to practice a patent or covenant not to sue for patent infringement). To \"grant\" such a patent license to a party means to make such an agreement or commitment not to enforce a patent against the party. If you convey a covered work, knowingly relying on a patent license, and the Corresponding Source of the work is not available for anyone to copy, free of charge and under the terms of this License, through a publicly available network server or other readily accessible means, then you must either (1) cause the Corresponding Source to be so available, or (2) arrange to deprive yourself of the benefit of the patent license for this particular work, or (3) arrange, in a manner consistent with the requirements of this License, to extend the patent license to downstream recipients. \"Knowingly relying\" means you have actual knowledge that, but for the patent license, your conveying the covered work in a country, or your recipient's use of the covered work in a country, would infringe one or more identifiable patents in that country that you have reason to believe are valid. If, pursuant to or in connection with a single transaction or arrangement, you convey, or propagate by procuring conveyance of, a covered work, and grant a patent license to some of the parties receiving the covered work authorizing them to use, propagate, modify or convey a specific copy of the covered work, then the patent license you grant is automatically extended to all recipients of the covered work and works based on it. A patent license is \"discriminatory\" if it does not include within the scope of its coverage, prohibits the exercise of, or is conditioned on the non-exercise of one or more of the rights that are specifically granted under this License. You may not convey a covered work if you are a party to an arrangement with a third party that is in the business of distributing software, under which you make payment to the third party based on the extent of your activity of conveying the work, and under which the third party grants, to any of the parties who would receive the covered work from you, a discriminatory patent license (a) in connection with copies of the covered work conveyed by you (or copies made from those copies), or (b) primarily for and in connection with specific products or compilations that contain the covered work, unless you entered into that arrangement, or that patent license was granted, prior to 28 March 2007. Nothing in this License shall be construed as excluding or limiting any implied license or other defenses to infringement that may otherwise be available to you under applicable patent law. No Surrender of Others' Freedom. If conditions are imposed on you (whether by court order, agreement or otherwise) that contradict the conditions of this License, they do not excuse you from the conditions of this License. If you cannot convey a covered work so as to satisfy simultaneously your obligations under this License and any other pertinent obligations, then as a consequence you may not convey it at all. For example, if you agree to terms that obligate you to collect a royalty for further conveying from those to whom you convey the Program, the only way you could satisfy both those terms and this License would be to refrain entirely from conveying the Program. Use with the GNU Affero General Public License. Notwithstanding any other provision of this License, you have permission to link or combine any covered work with a work licensed under version 3 of the GNU Affero General Public License into a single combined work, and to convey the resulting work. The terms of this License will continue to apply to the part which is the covered work, but the special requirements of the GNU Affero General Public License, section 13, concerning interaction through a network will apply to the combination as such. Revised Versions of this License. The Free Software Foundation may publish revised and/or new versions of the GNU General Public License from time to time. Such new versions will be similar in spirit to the present version, but may differ in detail to address new problems or concerns. Each version is given a distinguishing version number. If the Program specifies that a certain numbered version of the GNU General Public License \"or any later version\" applies to it, you have the option of following the terms and conditions either of that numbered version or of any later version published by the Free Software Foundation. If the Program does not specify a version number of the GNU General Public License, you may choose any version ever published by the Free Software Foundation. If the Program specifies that a proxy can decide which future versions of the GNU General Public License can be used, that proxy's public statement of acceptance of a version permanently authorizes you to choose that version for the Program. Later license versions may give you additional or different permissions. However, no additional obligations are imposed on any author or copyright holder as a result of your choosing to follow a later version. Disclaimer of Warranty. THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY APPLICABLE LAW. EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM \"AS IS\" WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM IS WITH YOU. SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF ALL NECESSARY SERVICING, REPAIR OR CORRECTION. Limitation of Liability. IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS), EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Interpretation of Sections 15 and 16. If the disclaimer of warranty and limitation of liability provided above cannot be given local legal effect according to their terms, reviewing courts shall apply local law that most closely approximates an absolute waiver of all civil liability in connection with the Program, unless a warranty or assumption of liability accompanies a copy of the Program in return for a fee. END OF TERMS AND CONDITIONS How to Apply These Terms to Your New Programs If you develop a new program, and you want it to be of the greatest possible use to the public, the best way to achieve this is to make it free software which everyone can redistribute and change under these terms. To do so, attach the following notices to the program. It is safest to attach them to the start of each source file to most effectively state the exclusion of warranty; and each file should have at least the \"copyright\" line and a pointer to where the full notice is found. <one line to give the program's name and a brief idea of what it does.> Copyright (C) <year> <name of author> This program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details. You should have received a copy of the GNU General Public License along with this program. If not, see <http://www.gnu.org/licenses/>. Also add information on how to contact you by electronic and paper mail. If the program does terminal interaction, make it output a short notice like this when it starts in an interactive mode: {project} Copyright (C) {year} {fullname} This program comes with ABSOLUTELY NO WARRANTY; for details type `show w'. This is free software, and you are welcome to redistribute it under certain conditions; type `show c' for details. The hypothetical commands show w' and show c' should show the appropriate parts of the General Public License. Of course, your program's commands might be different; for a GUI interface, you would use an \"about box\". You should also get your employer (if you work as a programmer) or school, if any, to sign a \"copyright disclaimer\" for the program, if necessary. For more information on this, and how to apply and follow the GNU GPL, see http://www.gnu.org/licenses/ . The GNU General Public License does not permit incorporating your program into proprietary programs. If your program is a subroutine library, you may consider it more useful to permit linking proprietary applications with the library. If this is what you want to do, use the GNU Lesser General Public License instead of this License. But first, please read http://www.gnu.org/philosophy/why-not-lgpl.html .","title":"LICENSE"},{"location":"VMWare/ESXi%20Architecture/","text":"VMWare Architecture Notes My Image Profile A copy of my image profile Example VIB Definition Example VIB definition What is the altbookbank Partition https://serverfault.com/questions/278895/how-is-the-altbootbank-partition-used-in-esxi VMWare Architecture Document https://www.vmware.com/content/dam/digitalmarketing/vmware/en/pdf/techpaper/ESXi_architecture.pdf What is dcism Appears to be the iDRAC service module ESXi Log File Locations https://docs.vmware.com/en/VMware-vSphere/7.0/com.vmware.vsphere.security.doc/GUID-832A2618-6B11-4A28-9672-93296DA931D0.html I suspect anything related to updates should be found in the hostd.log file on the ESXi server itself.","title":"VMWare Architecture Notes"},{"location":"VMWare/ESXi%20Architecture/#vmware-architecture-notes","text":"","title":"VMWare Architecture Notes"},{"location":"VMWare/ESXi%20Architecture/#my-image-profile","text":"A copy of my image profile","title":"My Image Profile"},{"location":"VMWare/ESXi%20Architecture/#example-vib-definition","text":"Example VIB definition","title":"Example VIB Definition"},{"location":"VMWare/ESXi%20Architecture/#what-is-the-altbookbank-partition","text":"https://serverfault.com/questions/278895/how-is-the-altbootbank-partition-used-in-esxi","title":"What is the altbookbank Partition"},{"location":"VMWare/ESXi%20Architecture/#vmware-architecture-document","text":"https://www.vmware.com/content/dam/digitalmarketing/vmware/en/pdf/techpaper/ESXi_architecture.pdf","title":"VMWare Architecture Document"},{"location":"VMWare/ESXi%20Architecture/#what-is-dcism","text":"Appears to be the iDRAC service module","title":"What is dcism"},{"location":"VMWare/ESXi%20Architecture/#esxi-log-file-locations","text":"https://docs.vmware.com/en/VMware-vSphere/7.0/com.vmware.vsphere.security.doc/GUID-832A2618-6B11-4A28-9672-93296DA931D0.html I suspect anything related to updates should be found in the hostd.log file on the ESXi server itself.","title":"ESXi Log File Locations"},{"location":"VMWare/Notes%20on%20VSAN/","text":"Notes on vSAN Notes on vSAN Disk Groups Deduplication and Replication What is a Replica (other source) Distributed Datastore Objects and Components RAID Architecture Required Networks Quarum Logic Fault Domains Sample Architecture Witness Alternate Explanation (includes stripes) Design Notes Networking Erasure Coding RAID 5 RAID 6 Internal Components vSAN Layers Objects and Components Another explanation vSAN Networking Roles vSAN RAID Tree Other Notes Minimum Drive Requirements Minimum Hosts for vSAN Minimum Hosts for Erasure Coding (RAID5/6) Maximum Size Deduplication and Compression Disk Groups Deduplication and Replication The scope of deduplication and compression exists only within each disk group. Hosken, Martin. VMware Software-Defined Storage (Kindle Locations 5278-5279). Wiley. Kindle Edition. Virtual SAN uses a distributed read cache mechanism whereby reads and writes are distributed to all hosts holding replicas. This way, if one host is busy, the other hosts holding replicas can still service I/ O requests. Hosken, Martin. VMware Software-Defined Storage (Kindle Locations 5317-5318). Wiley. Kindle Edition. Virtual SAN locates data in two or more locations across the distributed Virtual SAN datastore, in order to withstand host or disk failures. With data locality operating in this way, I/ O can come from any of the data replicas across the cluster, helping to mitigate potential host or disk bottlenecks and allowing Virtual SAN to run more efficiently, while still maintaining data availability and optimum performance. Hosken, Martin. VMware Software-Defined Storage (Kindle Locations 5324-5327). Wiley. Kindle Edition. The mechanism for destaging differs between the two Virtual SAN models, hybrid and all-flash; mechanical disks are typically good at handling sequential write workloads, so Virtual SAN uses this to make the process more efficient. In the hybrid model, an elevator algorithm runs independently on each disk group and decides, locally, whether to move any data to its capacity disks, and if so, when. This algorithm uses multiple criteria and batches together larger chunks of data that are physically proximal on a mechanical disk, and destages them together asynchronously. This mechanism writes to the disk sequentially for improved performance. However, the destaging mechanism is also conservative: it will not rush to move data if the space in the write buffer is not constringed. In addition, as data that is written tends to be overwritten quickly within a short period of time, this approach avoids writing the same blocks of data multiple times to the mechanical disks. Also note that the write buffers of the capacity layer disks are flushed onto the persistent storage devices before writes are discarded from the caching device. In the all-flash model, Virtual SAN uses 100 percent of the available capacity on the endurance flash device as a write buffer. In all-flash configurations, essentially the same mechanism is in place as that in the hybrid model. However, Virtual SAN does not take into account the proximal algorithm, making it a more efficient mechanism for destaging to capacity flash devices. Also, in the all-flash model, changes to the elevator algorithm allow the destaging of cold data from the write cache to the capacity tier, based on their data blocks' relative hotness or coldness. In addition, data blocks that are overwritten stay in the caching tier longer, which results in reducing the overall wear on the capacity tier flash devices, increasing their life expectancy. Hosken, Martin. VMware Software-Defined Storage (Kindle Locations 5339-5353). Wiley. Kindle Edition. What is a Replica (other source) Distributed Datastore Objects and Components RAID Architecture Required Networks Quarum Logic A Virtual SAN\u2013 enabled cluster uses a quorum-based system with witness components to ensure consistent operations across the distributed datastore. The quorum is the minimum number of votes that a distributed system must be able to obtain, in order to be allowed to perform an operation. In Virtual SAN, 50 percent of the votes that make up a virtual machine's storage object must be accessible at all times for that replica to be active. If less than 50 percent of the votes are accessible to the host, the object is not available and is marked as inaccessible in the Virtual SAN datastore. This can become a problem for Virtual SAN that can affect the availability of virtual machines, if after a host failure, the loss of quorum for a virtual machine object results in vSphere High Availability not being able to restart the virtual machine until the cluster quorum is restored. vSphere HA can guarantee that a virtual machine will restart only when it has a cluster quorum and can access the most recent copy of the virtual machine object. For instance, Figure 4.48 shows a three-node cluster that has a single virtual machine running on host 1, which has been assigned a storage policy with an FTT (Failures to Tolerate) of 1. If all three hosts fail in sequence (with host 3 the last host to fail), when host 1 and host 2 come back online, vSphere HA will be unable to restart the virtual machine because the last host that failed, host 3, retains the most recent copy of the virtual machine object components and is currently inaccessible. In this scenario, either all three hosts must recover at the same time or the two-host quorum must include host 3. If neither of these conditions is satisfied, vSphere High Availability will attempt to restart the virtual machine again when host 3 comes back online. Hosken, Martin. VMware Software-Defined Storage (p. 302). Wiley. Kindle Edition. Fault Domains Sample Architecture Witness Witnesses are zero-length components, containing just metadata. The purpose of the witness is to ensure that only one network partition can access an object at any one time. For instance, consider a failure scenario in which two vSphere hosts communicate over a Virtual SAN network. The VMDK object has been configured with a replica, so there is a copy of the VMDK on a second vSphere host. If the Virtual SAN network goes down, the two hosts are no longer able to communicate with one another, even though both hosts are still up and running and accessing other networks successfully. From which partition does the virtual machine access the data? Hosken, Martin. VMware Software-Defined Storage (p. 211). Wiley. Kindle Edition. The logic for the equation 2n+1 comes from the fact that you have to have replicas on two different hosts and then a third to act as a tie breaker with the witness. Alternate Explanation (includes stripes) Design Notes Networking Comparing the vSphere Standard and Distributed Virtual Switches When designing the virtual switch configuration, if not already dictated by other design factors, one required design decision may be whether to adopt the vSphere standard switch or to implement the vSphere Distributed Switch (VDS), with Virtual SAN deployment being supported on both options. The major benefit of the vSphere standard switch is simplicity of implementation. However, as the Virtual SAN environment grows, the design might benefit from several features offered only by the VDS, including Network I/ O Control (NIOC), Link Aggregation Control Protocol (LACP), and NetFlow. Another key consideration that factors into this design decision is whether VMware NSX is included in the overall environment architecture. One of the key benefits of using the vSphere Distributed Switch in a Virtual SAN environment is that NIOC can be used, which allows for the prioritization of bandwidth when there is network contention. For instance, replication and synchronization activities that Virtual SAN will impose on the network can cause contention. Depending on the number of virtual machines, their level of network activity, and Virtual SAN network utilization, 1 Gb/ s networks can easily be saturated and overwhelmed, particularly during rebuild and synchronization operations. Through the use of NIOC and QoS, vSphere Hosken, Martin. VMware Software-Defined Storage (p. 238). Wiley. Kindle Edition. Erasure Coding How it works: https://stonefly.com/blog/understanding-erasure-coding RAID 5 RAID 6 Internal Components vSAN Layers Objects and Components See https://masteringvmware.com/vsan-objects-and-components/ Component is an single file which you can say it as single VMDK. When you apply an storage policy to the Virtual Machine based on the policy components gets created and replicated. Let\u2019s take an Example where you have created a VM with RAID-1 (Mirroring). So now when you see at the VM placement you will see that different components gets created. Component has the maximum limit of 255GB. So that means if your VMDK is more then 255 GB in size then it will be striped and if the VMDK is less then 255GB in size then it will be single component. In vSAN 6.6 there is limit of maximum 9000 components per vSAN Host. vSAN Distributes the components across the hosts evenly for the availability and to maintain the balance. Another explanation This also describes stripe width vSAN Networking Roles CMMDS = Clustered metadata database and monitoring service Multicast addresses used are as follows: vSAN RAID Tree Other Notes Minimum Drive Requirements Each host contributing storage capacity to the vSAN cluster will require at least one flash device and one capacity device (magnetic disk or flash). Note that the capacity tier is either all-flash or all magnetic disk; they cannot be mixed in the same vSAN cluster. Hogan, Cormac; Epping, Duncan. VMware vSAN 6.7 U1 Deep Dive . Kindle Edition. Minimum Hosts for vSAN At a minimum, vSAN requires three hosts in your cluster to contribute storage (or two hosts if you decide to use a witness host, which is a common configuration for ROBO, this is discussed in chapter 8); other hosts in your cluster could leverage these storage resources without contributing storage resources to the cluster itself, although this is not common. Hogan, Cormac; Epping, Duncan. VMware vSAN 6.7 U1 Deep Dive . Kindle Edition. Minimum Hosts for Erasure Coding (RAID5/6) Maximum Size Today\u2019s boundary for vSAN in terms of both size and connectivity is a vSphere cluster. This means that vSAN supports single clusters/datastores of up to 64 hosts, but of course a single vCenter Server instance can manage many 64 host clusters. It is a common practice for most customers however to limit their clusters to around 20 hosts. This is for operational considerations like the time it takes to update a full cluster. Each host can run a supported maximum of 200 VMs, up to a total of 6,400 VMs within a 64-host vSAN cluster. Hogan, Cormac; Epping, Duncan. VMware vSAN 6.7 U1 Deep Dive . Kindle Edition. Deduplication and Compression When creating your vSAN cluster, if your vSAN cluster is an all-flash configuration, you have the option to enable \u201cdeduplication and compression.\u201d Deduplication and compression will play a big factor in available capacity for an all-flash configuration. Note that these data services are not available in a hybrid configuration. Hogan, Cormac; Epping, Duncan. VMware vSAN 6.7 U1 Deep Dive . Kindle Edition. But it is not just deduplication and compression that can provide space saving on the vSAN datastore. There is also the number of replica copies configured if the VM is using a RAID-1 policy. This is enabled through the policy-based management framework. Conversely, you may decide to use erasure coding polices such as RAID-5 and RAID-6 (but note that this space efficiency feature is only available on all-flash vSAN). These all determine how many VMs can be deployed on the datastore. Hogan, Cormac; Epping, Duncan. VMware vSAN 6.7 U1 Deep Dive . Kindle Edition.","title":"Notes on vSAN"},{"location":"VMWare/Notes%20on%20VSAN/#notes-on-vsan","text":"Notes on vSAN Disk Groups Deduplication and Replication What is a Replica (other source) Distributed Datastore Objects and Components RAID Architecture Required Networks Quarum Logic Fault Domains Sample Architecture Witness Alternate Explanation (includes stripes) Design Notes Networking Erasure Coding RAID 5 RAID 6 Internal Components vSAN Layers Objects and Components Another explanation vSAN Networking Roles vSAN RAID Tree Other Notes Minimum Drive Requirements Minimum Hosts for vSAN Minimum Hosts for Erasure Coding (RAID5/6) Maximum Size Deduplication and Compression","title":"Notes on vSAN"},{"location":"VMWare/Notes%20on%20VSAN/#disk-groups","text":"","title":"Disk Groups"},{"location":"VMWare/Notes%20on%20VSAN/#deduplication-and-replication","text":"The scope of deduplication and compression exists only within each disk group. Hosken, Martin. VMware Software-Defined Storage (Kindle Locations 5278-5279). Wiley. Kindle Edition. Virtual SAN uses a distributed read cache mechanism whereby reads and writes are distributed to all hosts holding replicas. This way, if one host is busy, the other hosts holding replicas can still service I/ O requests. Hosken, Martin. VMware Software-Defined Storage (Kindle Locations 5317-5318). Wiley. Kindle Edition. Virtual SAN locates data in two or more locations across the distributed Virtual SAN datastore, in order to withstand host or disk failures. With data locality operating in this way, I/ O can come from any of the data replicas across the cluster, helping to mitigate potential host or disk bottlenecks and allowing Virtual SAN to run more efficiently, while still maintaining data availability and optimum performance. Hosken, Martin. VMware Software-Defined Storage (Kindle Locations 5324-5327). Wiley. Kindle Edition. The mechanism for destaging differs between the two Virtual SAN models, hybrid and all-flash; mechanical disks are typically good at handling sequential write workloads, so Virtual SAN uses this to make the process more efficient. In the hybrid model, an elevator algorithm runs independently on each disk group and decides, locally, whether to move any data to its capacity disks, and if so, when. This algorithm uses multiple criteria and batches together larger chunks of data that are physically proximal on a mechanical disk, and destages them together asynchronously. This mechanism writes to the disk sequentially for improved performance. However, the destaging mechanism is also conservative: it will not rush to move data if the space in the write buffer is not constringed. In addition, as data that is written tends to be overwritten quickly within a short period of time, this approach avoids writing the same blocks of data multiple times to the mechanical disks. Also note that the write buffers of the capacity layer disks are flushed onto the persistent storage devices before writes are discarded from the caching device. In the all-flash model, Virtual SAN uses 100 percent of the available capacity on the endurance flash device as a write buffer. In all-flash configurations, essentially the same mechanism is in place as that in the hybrid model. However, Virtual SAN does not take into account the proximal algorithm, making it a more efficient mechanism for destaging to capacity flash devices. Also, in the all-flash model, changes to the elevator algorithm allow the destaging of cold data from the write cache to the capacity tier, based on their data blocks' relative hotness or coldness. In addition, data blocks that are overwritten stay in the caching tier longer, which results in reducing the overall wear on the capacity tier flash devices, increasing their life expectancy. Hosken, Martin. VMware Software-Defined Storage (Kindle Locations 5339-5353). Wiley. Kindle Edition.","title":"Deduplication and Replication"},{"location":"VMWare/Notes%20on%20VSAN/#what-is-a-replica-other-source","text":"","title":"What is a Replica (other source)"},{"location":"VMWare/Notes%20on%20VSAN/#distributed-datastore","text":"","title":"Distributed Datastore"},{"location":"VMWare/Notes%20on%20VSAN/#objects-and-components","text":"","title":"Objects and Components"},{"location":"VMWare/Notes%20on%20VSAN/#raid-architecture","text":"","title":"RAID Architecture"},{"location":"VMWare/Notes%20on%20VSAN/#required-networks","text":"","title":"Required Networks"},{"location":"VMWare/Notes%20on%20VSAN/#quarum-logic","text":"A Virtual SAN\u2013 enabled cluster uses a quorum-based system with witness components to ensure consistent operations across the distributed datastore. The quorum is the minimum number of votes that a distributed system must be able to obtain, in order to be allowed to perform an operation. In Virtual SAN, 50 percent of the votes that make up a virtual machine's storage object must be accessible at all times for that replica to be active. If less than 50 percent of the votes are accessible to the host, the object is not available and is marked as inaccessible in the Virtual SAN datastore. This can become a problem for Virtual SAN that can affect the availability of virtual machines, if after a host failure, the loss of quorum for a virtual machine object results in vSphere High Availability not being able to restart the virtual machine until the cluster quorum is restored. vSphere HA can guarantee that a virtual machine will restart only when it has a cluster quorum and can access the most recent copy of the virtual machine object. For instance, Figure 4.48 shows a three-node cluster that has a single virtual machine running on host 1, which has been assigned a storage policy with an FTT (Failures to Tolerate) of 1. If all three hosts fail in sequence (with host 3 the last host to fail), when host 1 and host 2 come back online, vSphere HA will be unable to restart the virtual machine because the last host that failed, host 3, retains the most recent copy of the virtual machine object components and is currently inaccessible. In this scenario, either all three hosts must recover at the same time or the two-host quorum must include host 3. If neither of these conditions is satisfied, vSphere High Availability will attempt to restart the virtual machine again when host 3 comes back online. Hosken, Martin. VMware Software-Defined Storage (p. 302). Wiley. Kindle Edition.","title":"Quarum Logic"},{"location":"VMWare/Notes%20on%20VSAN/#fault-domains","text":"","title":"Fault Domains"},{"location":"VMWare/Notes%20on%20VSAN/#sample-architecture","text":"","title":"Sample Architecture"},{"location":"VMWare/Notes%20on%20VSAN/#witness","text":"Witnesses are zero-length components, containing just metadata. The purpose of the witness is to ensure that only one network partition can access an object at any one time. For instance, consider a failure scenario in which two vSphere hosts communicate over a Virtual SAN network. The VMDK object has been configured with a replica, so there is a copy of the VMDK on a second vSphere host. If the Virtual SAN network goes down, the two hosts are no longer able to communicate with one another, even though both hosts are still up and running and accessing other networks successfully. From which partition does the virtual machine access the data? Hosken, Martin. VMware Software-Defined Storage (p. 211). Wiley. Kindle Edition. The logic for the equation 2n+1 comes from the fact that you have to have replicas on two different hosts and then a third to act as a tie breaker with the witness.","title":"Witness"},{"location":"VMWare/Notes%20on%20VSAN/#alternate-explanation-includes-stripes","text":"","title":"Alternate Explanation (includes stripes)"},{"location":"VMWare/Notes%20on%20VSAN/#design-notes","text":"","title":"Design Notes"},{"location":"VMWare/Notes%20on%20VSAN/#networking","text":"Comparing the vSphere Standard and Distributed Virtual Switches When designing the virtual switch configuration, if not already dictated by other design factors, one required design decision may be whether to adopt the vSphere standard switch or to implement the vSphere Distributed Switch (VDS), with Virtual SAN deployment being supported on both options. The major benefit of the vSphere standard switch is simplicity of implementation. However, as the Virtual SAN environment grows, the design might benefit from several features offered only by the VDS, including Network I/ O Control (NIOC), Link Aggregation Control Protocol (LACP), and NetFlow. Another key consideration that factors into this design decision is whether VMware NSX is included in the overall environment architecture. One of the key benefits of using the vSphere Distributed Switch in a Virtual SAN environment is that NIOC can be used, which allows for the prioritization of bandwidth when there is network contention. For instance, replication and synchronization activities that Virtual SAN will impose on the network can cause contention. Depending on the number of virtual machines, their level of network activity, and Virtual SAN network utilization, 1 Gb/ s networks can easily be saturated and overwhelmed, particularly during rebuild and synchronization operations. Through the use of NIOC and QoS, vSphere Hosken, Martin. VMware Software-Defined Storage (p. 238). Wiley. Kindle Edition.","title":"Networking"},{"location":"VMWare/Notes%20on%20VSAN/#erasure-coding","text":"How it works: https://stonefly.com/blog/understanding-erasure-coding","title":"Erasure Coding"},{"location":"VMWare/Notes%20on%20VSAN/#raid-5","text":"","title":"RAID 5"},{"location":"VMWare/Notes%20on%20VSAN/#raid-6","text":"","title":"RAID 6"},{"location":"VMWare/Notes%20on%20VSAN/#internal-components","text":"","title":"Internal Components"},{"location":"VMWare/Notes%20on%20VSAN/#vsan-layers","text":"","title":"vSAN Layers"},{"location":"VMWare/Notes%20on%20VSAN/#objects-and-components_1","text":"See https://masteringvmware.com/vsan-objects-and-components/ Component is an single file which you can say it as single VMDK. When you apply an storage policy to the Virtual Machine based on the policy components gets created and replicated. Let\u2019s take an Example where you have created a VM with RAID-1 (Mirroring). So now when you see at the VM placement you will see that different components gets created. Component has the maximum limit of 255GB. So that means if your VMDK is more then 255 GB in size then it will be striped and if the VMDK is less then 255GB in size then it will be single component. In vSAN 6.6 there is limit of maximum 9000 components per vSAN Host. vSAN Distributes the components across the hosts evenly for the availability and to maintain the balance.","title":"Objects and Components"},{"location":"VMWare/Notes%20on%20VSAN/#another-explanation","text":"This also describes stripe width","title":"Another explanation"},{"location":"VMWare/Notes%20on%20VSAN/#vsan-networking-roles","text":"CMMDS = Clustered metadata database and monitoring service Multicast addresses used are as follows:","title":"vSAN Networking Roles"},{"location":"VMWare/Notes%20on%20VSAN/#vsan-raid-tree","text":"","title":"vSAN RAID Tree"},{"location":"VMWare/Notes%20on%20VSAN/#other-notes","text":"","title":"Other Notes"},{"location":"VMWare/Notes%20on%20VSAN/#minimum-drive-requirements","text":"Each host contributing storage capacity to the vSAN cluster will require at least one flash device and one capacity device (magnetic disk or flash). Note that the capacity tier is either all-flash or all magnetic disk; they cannot be mixed in the same vSAN cluster. Hogan, Cormac; Epping, Duncan. VMware vSAN 6.7 U1 Deep Dive . Kindle Edition.","title":"Minimum Drive Requirements"},{"location":"VMWare/Notes%20on%20VSAN/#minimum-hosts-for-vsan","text":"At a minimum, vSAN requires three hosts in your cluster to contribute storage (or two hosts if you decide to use a witness host, which is a common configuration for ROBO, this is discussed in chapter 8); other hosts in your cluster could leverage these storage resources without contributing storage resources to the cluster itself, although this is not common. Hogan, Cormac; Epping, Duncan. VMware vSAN 6.7 U1 Deep Dive . Kindle Edition.","title":"Minimum Hosts for vSAN"},{"location":"VMWare/Notes%20on%20VSAN/#minimum-hosts-for-erasure-coding-raid56","text":"","title":"Minimum Hosts for Erasure Coding (RAID5/6)"},{"location":"VMWare/Notes%20on%20VSAN/#maximum-size","text":"Today\u2019s boundary for vSAN in terms of both size and connectivity is a vSphere cluster. This means that vSAN supports single clusters/datastores of up to 64 hosts, but of course a single vCenter Server instance can manage many 64 host clusters. It is a common practice for most customers however to limit their clusters to around 20 hosts. This is for operational considerations like the time it takes to update a full cluster. Each host can run a supported maximum of 200 VMs, up to a total of 6,400 VMs within a 64-host vSAN cluster. Hogan, Cormac; Epping, Duncan. VMware vSAN 6.7 U1 Deep Dive . Kindle Edition.","title":"Maximum Size"},{"location":"VMWare/Notes%20on%20VSAN/#deduplication-and-compression","text":"When creating your vSAN cluster, if your vSAN cluster is an all-flash configuration, you have the option to enable \u201cdeduplication and compression.\u201d Deduplication and compression will play a big factor in available capacity for an all-flash configuration. Note that these data services are not available in a hybrid configuration. Hogan, Cormac; Epping, Duncan. VMware vSAN 6.7 U1 Deep Dive . Kindle Edition. But it is not just deduplication and compression that can provide space saving on the vSAN datastore. There is also the number of replica copies configured if the VM is using a RAID-1 policy. This is enabled through the policy-based management framework. Conversely, you may decide to use erasure coding polices such as RAID-5 and RAID-6 (but note that this space efficiency feature is only available on all-flash vSAN). These all determine how many VMs can be deployed on the datastore. Hogan, Cormac; Epping, Duncan. VMware vSAN 6.7 U1 Deep Dive . Kindle Edition.","title":"Deduplication and Compression"},{"location":"VMWare/Setup%20VXRail/","text":"VxRail Setup VxRail Setup License Keys Networking What does MLD Querying Do and Why Can it Break VxRail Discovery RASR Process Deploy Witness After deploying the Witness: Set Up vCenter Set up the Manager for Discovery Install Advanced Troubleshooting Check for ESXi Logs Marvin Logs Digging Through VxRail's Guts Helpful Commands Check if VxRail is Running Get a List of VMs Check if VM is On Check Networking Set the VLAN for a Switch Get Interface IPs Get Interface List License Keys In case you have to upgrade or downgrade license keys see this KB article Networking I followed the VxRail Network Planning Guide to set up the network. WARNING : The discovery process for VxRail uses IPv6 multicast to discover itself. Dell switches come with MLD snooping globally enabled, but do not have the MLD querier enabled! This will cause the nodes to flap! Servers will discover themselves and then disappear. You must enable MLD snooping and the mld querier on the correct vlans with interface vlan # and then ipv6 mld snooping querier . What does MLD Querying Do and Why Can it Break VxRail Discovery By default, switches are not multicast aware so they will broadcast any multicast traffic on all ports assigned to a VLAN. For bandwidth optimization the switch will block any multicast messages to a segment it thinks does not have a device which wants those multicast messages. It discovers if there is an interested host by using MLD general queries or group specific queries. Enabling the querier will allow the switch to use these messages to discover interested hosts and subsequently ensure those hosts receive the appropriate IPv6 messages. RASR Process There is an IDSDM module on the box with the factor image on the box. To perform the RASR process you'll boot from that and it will copy over all necessary files to the internal BOSS drive. Just follow the prompts. WARNING RASRing a node blows away absolutely everything on all drives! DO NOT RUN ON A PRODUCTION CLUSTER Deploy Witness Note For the two node setup, you will need a third site with vCenter deployed in a cluster separate from VxRail. WARNING If someone tells you that 1 proc, 16GB R240 is sufficient for running the Witness and vCenter in a 2 node setup... that is strictly speaking true. Do not plan on putting anything else on that box. Download the witness appliance from the VMWare site . Make sure you pull the witness appliance for your setup! On VxRail 4.7 which is what I'm installing the correct version of Witness is 6.7. Follow the instructions here for setup. The only thing that could be confusing depending on what you've previously read is that the second NIC for the Witness goes on its own special VLAN separate from VSAN. This VLAN will be used only for Witness traffic. Point of Curiosity The Witness host looks exactly like an instance of ESXi because it is. It's only purpose is to pretend to be a third ESXi host and avoid split brain on vSAN. After deploying the Witness: Give vmk0 an IP on your management network and vmk1 an IP address on the vSAN network. You can use the esxcli network ip interface ipv4 set -i vmk0 --type static -I 192.168.5.130 -N 255.255.255.0 -g 192.168.5.1 command to IP an interface Go into vCenter and add the Witness Make sure vmk0 of the Witness has the same MAC address of vmk0 on the host OR set the security settings of the management portgroup you are using to promiscuous Set Up vCenter Make sure your DNS server can resolve all the VxRail ESXi IPs first before continuing. On vCenter double check that DNS and NTP are setup correctly. You can do this from the network menu by going to Home->Administration->System Configuration->Click on vCenter->Login. This should bring up the appliance management screen. Check Time and Networking. vCenter has to be able to resolve all ESXi names itself. This is not checked by VxRail's validator ! To double check name resolution, go to vCenter's console, hit alt+F1, use nslookup to check all the ESXi node names. Set up the Manager for Discovery David Ring's Install Notes are helpful David Ring's notes on how host discovery works are also helpful Loudmouth requires IPv6 multicast in order for VxRail Manager to perform a successful VxRail node discovery. IPv6 multicast is required only on the \u2018Private Management Network\u2019, this is an isolated management network solely for auto-discovery of the VxRail nodes during install or expansion. The default VLAN ID for the private network is 3939, which is configured on each VxRail node from the factory, this VLAN needs to be configured on the TOR switches and remains isolated on the TORs. If you wish to deviate from the default of VLAN 3939 then each node will need to be modified onsite otherwise node discovery will fail. The VxRail Manager comes defaulted to 192.168.10.200 however, it does not accept SSH connections Turn on the VxRail manager. vxrail-primary --setup --vxrail-address 192.168.2.100 --vxrail-netmask 255.255.255.0 --vxrail-gateway 192.168.2.1 . The IP you assign is not used for discovery. It is for reaching the VxRail appliance. Give it an IP on whatever network/vlan you plan on using for management. Details below in step 3. Make sure vmk0 of all ESXi hosts is on VLAN 3939 or whatever VLAN you're using for discovery. On each ESXi host make sure Private Management Network and Private VM Network are both on VLAN 3939. (Picture from David Ring's guide) The VxRail manager has two virtual NICs - eth0 and eth1. Eth1 is the NIC used for discovery. Make sure eth1 of the VxRail manager is on VLAN 3939. The first NIC (eth0) should be on whatever VLAN you are using for management. You will get to the VxRail manager webgui through eth0. At this point you should have full IPv6 connectivity between vmk0 on all ESXi instances and the VxRail appliance. You can test this with the following: Go to the ESXi console, press ALT+F1 and Pull the IPv6 address for vmk0 on ESXi with: esxcfg-vmknic -l | grep vmk0 On the ESXi host with the VxRail appliance, give another (not vmk0) VM kernel NIC you have access to an IP address with the esxcli network ip interface ipv4 set -i vmk0 --type static -I 192.168.5.130 -N 255.255.255.0 -g 192.168.5.1 command. For dhcp use the --type dhcp option. See Helpful Commands for how to list out the different portgroups and VMs Once you know all the IPv6 addresses for the various vmk0 nics, get on the VxRail appliance and use ping6 -l eth1 <ipv6 address> to test your IPv6 ping. This should work against all devices. If it doesn't you probably have a networking problem. You can also test the full server discovery process manually from the command line with: On the switch side you can use show lldp discovery to see what interfaces it has discovered and it will list the interfaces. On idrac it will show you what ports each nic is connected to under network devices. (My example is down - yours should not have a warning) For further troubleshooting check the Marvin log Assuming you've already checked DNS, the last thing you need to do is make sure NTP is up. I'm using chrony and checked like this on my NTP server: [root@services ~]# chronyc tracking Reference ID : D8EF230C (time4.google.com) Stratum : 2 Ref time (UTC) : Thu Dec 17 16:21:48 2020 System time : 0.000059723 seconds fast of NTP time Last offset : +0.000004093 seconds RMS offset : 0.000036436 seconds Frequency : 0.283 ppm fast Residual freq : -0.000 ppm Skew : 0.006 ppm Root delay : 0.023636952 seconds Root dispersion : 0.001011974 seconds Update interval : 1037.1 seconds Leap status : Normal Install Before running I strongly recommend you make sure NTP is up and running on any customer owned ESXi instances, customer vCenters, and the idracs. Mismatched time causes problems. All you have to do is browse to your VxRail Manager's address on follow the prompts. The only big gotcha I ran into is when it asks for vCenter Server Management Username what it wants is not administrator@vphere.whatever. It wants vCenter's root/password combo. Advanced Troubleshooting Check for ESXi Logs In my case I hit a failure where a node wouldn't add. A lot of times VxRail will have finished the ESXi deployment and you can actually go log into the ESXi instance in question. If you have a customer owned vCenter you can also go check its logs. A lot of the time failures during deployment can be found in one of those locations. Marvin Logs Another good place to look is to log into the VxRail appliance and check the Marvin logs at /var/log/vmware/marvin/tomcat Digging Through VxRail's Guts If you have problems during install you can pull VxRail's logs from the VxRail container by doing the below: Run docker ps -a and find the vxrail-system container. e8da567768e4 infra/openfaas/gateway:1.1.4 \"./gateway\" 2 days ago Up 2 days (healthy) 8080/tcp, 8082/tcp func_gateway.1.h5hmx70g8545d3apbwfxtfatr 2b71bfd81e90 infra/openfaas/queue-worker:1.1.4 \"./app\" 2 days ago Up 2 days (healthy) 8080/tcp func_queue-worker.1.897j8f0tbhas1989o0ygl5ez4 ba2f65a96664 infra/workflow_engine:1.1.6 \"./docker-entrypoint\u2026\" 2 days ago Up 2 days (healthy) 5000/tcp func_wfservice.1.wau7b1omhg3igjzzyvd865r6e d07929b37e2d infra/openfaas/gateway:1.1.4 \"./gateway\" 2 days ago Exited (1) 2 days ago func_gateway.1.stdin4eg1dysl3fajgpds5rwj e6f3ec734f8a infra/openfaas/queue-worker:1.1.4 \"./app\" 2 days ago Exited (2) 2 days ago func_queue-worker.1.uqy8d482650v4yquz98fnp9tq bc402e3b676c microservice/vxm-agent:1.1.6 \"/bin/bash entrypoin\u2026\" 2 days ago Up 2 days (healthy) 5000/tcp vxm-agent_vxm-agent_1 23e0ac7d3f92 microservice/nano-service:1.1.29 \"python -u index.py\" 2 days ago Up 2 days (healthy) 5000/tcp func_nano-service.1.dqo23zdkgmwbqf9ru6yn0t99r 54df5cb0af13 microservice/ms-day1-bringup:1.1.29 \"/bin/bash entrypoin\u2026\" 2 days ago Up 2 days (healthy) 5000/tcp func_ms-day1-bringup.1.xfe0xndtvsqbo17kapogyd7ke 87e26e7d27b2 infra/infra-lock-service:1.1.8 \"/bin/bash entrypoin\u2026\" 2 days ago Up 2 days (healthy) 5000/tcp func_lockservice.1.g2g7hg5viyph8r4ptcljeran9 4382600d0094 microservice/lockbox:1.1.11 \"/bin/bash entrypoin\u2026\" 2 days ago Up 2 days (healthy) 5000/tcp func_lockbox.1.q9hqbfuid54r5gelsazasksh2 f7964475a933 microservice/kgs-service:1.1.10 \"./boot.sh\" 2 days ago Up 2 days (healthy) 5000/tcp func_kgs-service.1.wpqm7hytpf5l9wo7ch3sc1wz7 75178ef078ea do-main/do-vxrail-system:1.1.35 \"/bin/bash entrypoin\u2026\" 2 days ago Up 2 days (healthy) 5000/tcp func_do-vxrail-system.1.e67dbhktpe88gw0wtmymmpthg 7abeddce2bb8 do-main/do-vm:1.1.35 \"/bin/bash entrypoin\u2026\" 2 days ago Up 2 days (healthy) 5000/tcp func_do-vm.1.g2fwikfzimyhv6di3zdalt1j3 3b557989c07e do-main/do-storage:1.1.35 \"/bin/bash entrypoin\u2026\" 2 days ago Up 2 days (healthy) 5000/tcp func_do-storage.1.w2nv1rfpidrv7g418xmknywso d0b15759e7f5 do-main/do-serviceability:1.1.35 \"/bin/bash entrypoin\u2026\" 2 days ago Up 2 days (healthy) func_do-serviceability.1.t5xkwt1eu2ypqvn0511i1gfy3 5e349b22f36a do-main/do-network:1.1.39 \"/bin/bash entrypoin\u2026\" 2 days ago Up 2 days (healthy) 5000/tcp func_do-network.1.vogoz7jtyrxf83h9jd0615r28 ae31c7cd94bd do-main/do-kgs:1.1.35 \"./boot.sh\" 2 days ago Up 2 days (healthy) 5000/tcp func_do-kgs.1.esudf6d39uy7e96dnvxoabaue 978447c9f6f8 do-main/do-host:1.1.39 \"/bin/bash entrypoin\u2026\" 2 days ago Up 2 days (healthy) 5000/tcp func_do-host.1.ravl83nt962k48up0v33qoviy 862926a58073 do-main/do-ecosystem:1.1.35 \"/bin/bash entrypoin\u2026\" 2 days ago Up 2 days (healthy) func_do-ecosystem.1.8k6cljhvo5p00632gt11f79uv 63dde78fc922 do-main/do-eservices:1.1.35 \"/bin/bash entrypoin\u2026\" 2 days ago Up 2 days (healthy) func_do-eservices.1.pnpnr1pofm0dvwv36trufl6g9 f1fb1d35457d do-main/do-cluster:1.1.35 \"/bin/bash entrypoin\u2026\" 2 days ago Up 2 days (healthy) 5000/tcp func_do-cluster.1.y4vs2diiyy5eau3ojuxdx12wx 2fc837e0726e infra/infra-config-service:1.1.9 \"/bin/bash entrypoin\u2026\" 2 days ago Up 2 days (healthy) 5000/tcp func_configservice.1.byjhptdpdxa3d04y7mr5lfp5z 685d833e0155 infra/openfaas/gateway:1.1.4 \"./gateway\" 2 days ago Exited (1) 2 days ago func_gateway.1.ztar1zozhfuh3skopwvlandwq 9af0891d5455 infra/openfaas/queue-worker:1.1.4 \"./app\" 2 days ago Exited (2) 2 days ago func_queue-worker.1.jmk9q8bkc6a2vlz3fg1e60y5f 0cb6d7948db9 infra/tracing:1.1.6 \"/go/bin/jaeger-all-\u2026\" 2 days ago Up 2 days (healthy) func_tracing.1.ymsjr3ba7ih2x21291xri4mz8 5e70fa63df88 infra/openfaas/faas-swarm:1.1.4 \"./faas-swarm\" 2 days ago Up 2 days (healthy) 8080/tcp func_faas-swarm.1.nfz0dm48hhx7vh9sh04mbagtz 620cce221e65 infra/nats-streaming:1.1.4 \"/nats-streaming-ser\u2026\" 2 days ago Up 2 days (healthy) 4222/tcp, 8222/tcp func_nats.1.l42vx07v08vhhtkv1lwxg1ipb dd16298b3e4c infra/nginx_gateway_vxrail:1.1.7 \"/bin/sh -c 'sudo /u\u2026\" 2 days ago Up 2 days (healthy) func_api-gateway.1.do9gzerxmmlti4rn8k3ab2gs4 4a8b6c976103 infra/gcr.io/etcd-development/etcd:1.1.4 \"/usr/local/bin/etcd\u2026\" 2 days ago Up 2 days (healthy) 2379-2380/tcp func_serviceregistry.1.u2gtw5wnsldulqgd1r47wkte5 acf803b61b04 infra/redis:1.1.4 \"docker-entrypoint.s\u2026\" 2 days ago Up 2 days (healthy) 6379/tcp func_cacheservice.1.subywzigvenobaskavjs6xyqc 3dac923f9bff infra/logging:1.1.4 \"tini -- /bin/entryp\u2026\" 2 days ago Up 2 days (healthy) 5140/tcp, 24224/tcp func_logging.1.kv7738umixulwryqpl38qvxvr bdd88fadc236 infra/prom/alertmanager:1.1.4 \"/bin/alertmanager -\u2026\" 2 days ago Up 2 days (healthy) 9093/tcp func_alertmanager.1.8uks430a7p10keeh8uqv625bv 7ec04254fee1 infra/prom/prometheus:1.1.4 \"/bin/prometheus --c\u2026\" 2 days ago Up 2 days (healthy) 9090/tcp func_prometheus.1.ti36b3wsder4w01v8nentdirx Find the log location with docker inspect --format='{{.LogPath}}' 75178ef078ea . They are in JSON format. For example, this is what my error looked like: This got me the reason for my failure: {\\\\\\\"error\\\\\\\": {\\\\\\\"result\\\\\\\": {\\\\\\\"error\\\\\\\": {\\\\\\\"code\\\\\\\": \\\\\\\"E3100_Cluster_22\\\\\\\", \\\\\\\"message\\\\\\\": \\\\\\\"failed to add host into cluster\\\\\\\", \\\\\\\"params\\\\\\\": [\\\\\\\"vxrail1-esxi.lan\\\\\\\"]}}}, \\\\\\\"id\\\\\\\": \\\\\\\"host_add_into_inventory\\\\\\\", \\\\\\\"internal_family\\\\\\\": \\\\\\\"node_one_bootstrap_with_cns_ready.primary_node_computer.host_add_into_inventory\\\\\\\", \\\\\\\"internal_id\\\\\\\": \\\\\\\"node_one_bootstrap_with_cns_ready.primary_node_computer.host_add_into_inventory_False____99ee8388_1147_4e_9d65c98cb2\\\\\\\", \", \"stream\": \"stderr\", \"time\": \"2020-12-18T13:24:42.028798775Z\" } {\"log\":\"\\\\\\\"params\\\\\\\": {\\\\\\\"cluster_name\\\\\\\": \\\\\\\"vxrail-cluster\\\\\\\", \\\\\\\"datacenter_name\\\\\\\": \\\\\\\"Datacenter\\\\\\\", \\\\\\\"host_conn_info\\\\\\\": {\\\\\\\"host\\\\\\\": \\\\\\\"192.168.2.31\\\\\\\", \\\\\\\"port\\\\\\\": 443, \\\\\\\"username\\\\\\\": \\\\\\\"root\\\\\\\"}, \\\\\\\"hostname\\\\\\\": \\\\\\\"vxrail1-esxi.lan\\\\\\\", \\\\\\\"ssl_thumbprint\\\\\\\": \\\\\\\"STUFF\\\\\\\", \\\\\\\"vc_conn_info\\\\\\\": {\\\\\\\"host\\\\\\\": \\\\\\\"vcenter.lan\\\\\\\", \\\\\\\"port\\\\\\\": 443, \\\\\\\"username\\\\\\\": \\\\\\\"administrator@vsphere.lan\\\\\\\"}, \\\\\\\"vm_folder_name\\\\\\\": \\\\\\\"VMware HCIA Folder vxrail-cluster\\\\\\\", \\\\\\\"vsan_name\\\\\\\": \\\\\\\"VxRail-Virtual-SAN-Datastore-22658cd0-5b17-428d-a23c-0d7e5a877bb7\\\\\\\"}, \\\\\\\"stage\\\\\\\": \\\\\\\"primary_node_computer\\\\\\\", \\\\\\\"startTime\\\\\\\": 1608297822107, \\\\\\\"status\\\\\\\": \\\\\\\"FAILED\\\\\\\"} Helpful Commands Check if VxRail is Running esxcli vm process list Get a List of VMs vim-cmd vmsvc/getallvms Check if VM is On vim-cmd vmsvc/power.getstate 1 Check Networking esxcli network vswitch standard portgroup list Set the VLAN for a Switch esxcli network vswitch standard portgroup set -p \"VM Network\" -v 120 Get Interface IPs esxcli network ip interface ipv4 get Get Interface List esxcli network ip interface list","title":"VxRail Setup"},{"location":"VMWare/Setup%20VXRail/#vxrail-setup","text":"VxRail Setup License Keys Networking What does MLD Querying Do and Why Can it Break VxRail Discovery RASR Process Deploy Witness After deploying the Witness: Set Up vCenter Set up the Manager for Discovery Install Advanced Troubleshooting Check for ESXi Logs Marvin Logs Digging Through VxRail's Guts Helpful Commands Check if VxRail is Running Get a List of VMs Check if VM is On Check Networking Set the VLAN for a Switch Get Interface IPs Get Interface List","title":"VxRail Setup"},{"location":"VMWare/Setup%20VXRail/#license-keys","text":"In case you have to upgrade or downgrade license keys see this KB article","title":"License Keys"},{"location":"VMWare/Setup%20VXRail/#networking","text":"I followed the VxRail Network Planning Guide to set up the network. WARNING : The discovery process for VxRail uses IPv6 multicast to discover itself. Dell switches come with MLD snooping globally enabled, but do not have the MLD querier enabled! This will cause the nodes to flap! Servers will discover themselves and then disappear. You must enable MLD snooping and the mld querier on the correct vlans with interface vlan # and then ipv6 mld snooping querier .","title":"Networking"},{"location":"VMWare/Setup%20VXRail/#what-does-mld-querying-do-and-why-can-it-break-vxrail-discovery","text":"By default, switches are not multicast aware so they will broadcast any multicast traffic on all ports assigned to a VLAN. For bandwidth optimization the switch will block any multicast messages to a segment it thinks does not have a device which wants those multicast messages. It discovers if there is an interested host by using MLD general queries or group specific queries. Enabling the querier will allow the switch to use these messages to discover interested hosts and subsequently ensure those hosts receive the appropriate IPv6 messages.","title":"What does MLD Querying Do and Why Can it Break VxRail Discovery"},{"location":"VMWare/Setup%20VXRail/#rasr-process","text":"There is an IDSDM module on the box with the factor image on the box. To perform the RASR process you'll boot from that and it will copy over all necessary files to the internal BOSS drive. Just follow the prompts. WARNING RASRing a node blows away absolutely everything on all drives! DO NOT RUN ON A PRODUCTION CLUSTER","title":"RASR Process"},{"location":"VMWare/Setup%20VXRail/#deploy-witness","text":"Note For the two node setup, you will need a third site with vCenter deployed in a cluster separate from VxRail. WARNING If someone tells you that 1 proc, 16GB R240 is sufficient for running the Witness and vCenter in a 2 node setup... that is strictly speaking true. Do not plan on putting anything else on that box. Download the witness appliance from the VMWare site . Make sure you pull the witness appliance for your setup! On VxRail 4.7 which is what I'm installing the correct version of Witness is 6.7. Follow the instructions here for setup. The only thing that could be confusing depending on what you've previously read is that the second NIC for the Witness goes on its own special VLAN separate from VSAN. This VLAN will be used only for Witness traffic. Point of Curiosity The Witness host looks exactly like an instance of ESXi because it is. It's only purpose is to pretend to be a third ESXi host and avoid split brain on vSAN.","title":"Deploy Witness"},{"location":"VMWare/Setup%20VXRail/#after-deploying-the-witness","text":"Give vmk0 an IP on your management network and vmk1 an IP address on the vSAN network. You can use the esxcli network ip interface ipv4 set -i vmk0 --type static -I 192.168.5.130 -N 255.255.255.0 -g 192.168.5.1 command to IP an interface Go into vCenter and add the Witness Make sure vmk0 of the Witness has the same MAC address of vmk0 on the host OR set the security settings of the management portgroup you are using to promiscuous","title":"After deploying the Witness:"},{"location":"VMWare/Setup%20VXRail/#set-up-vcenter","text":"Make sure your DNS server can resolve all the VxRail ESXi IPs first before continuing. On vCenter double check that DNS and NTP are setup correctly. You can do this from the network menu by going to Home->Administration->System Configuration->Click on vCenter->Login. This should bring up the appliance management screen. Check Time and Networking. vCenter has to be able to resolve all ESXi names itself. This is not checked by VxRail's validator ! To double check name resolution, go to vCenter's console, hit alt+F1, use nslookup to check all the ESXi node names.","title":"Set Up vCenter"},{"location":"VMWare/Setup%20VXRail/#set-up-the-manager-for-discovery","text":"David Ring's Install Notes are helpful David Ring's notes on how host discovery works are also helpful Loudmouth requires IPv6 multicast in order for VxRail Manager to perform a successful VxRail node discovery. IPv6 multicast is required only on the \u2018Private Management Network\u2019, this is an isolated management network solely for auto-discovery of the VxRail nodes during install or expansion. The default VLAN ID for the private network is 3939, which is configured on each VxRail node from the factory, this VLAN needs to be configured on the TOR switches and remains isolated on the TORs. If you wish to deviate from the default of VLAN 3939 then each node will need to be modified onsite otherwise node discovery will fail. The VxRail Manager comes defaulted to 192.168.10.200 however, it does not accept SSH connections Turn on the VxRail manager. vxrail-primary --setup --vxrail-address 192.168.2.100 --vxrail-netmask 255.255.255.0 --vxrail-gateway 192.168.2.1 . The IP you assign is not used for discovery. It is for reaching the VxRail appliance. Give it an IP on whatever network/vlan you plan on using for management. Details below in step 3. Make sure vmk0 of all ESXi hosts is on VLAN 3939 or whatever VLAN you're using for discovery. On each ESXi host make sure Private Management Network and Private VM Network are both on VLAN 3939. (Picture from David Ring's guide) The VxRail manager has two virtual NICs - eth0 and eth1. Eth1 is the NIC used for discovery. Make sure eth1 of the VxRail manager is on VLAN 3939. The first NIC (eth0) should be on whatever VLAN you are using for management. You will get to the VxRail manager webgui through eth0. At this point you should have full IPv6 connectivity between vmk0 on all ESXi instances and the VxRail appliance. You can test this with the following: Go to the ESXi console, press ALT+F1 and Pull the IPv6 address for vmk0 on ESXi with: esxcfg-vmknic -l | grep vmk0 On the ESXi host with the VxRail appliance, give another (not vmk0) VM kernel NIC you have access to an IP address with the esxcli network ip interface ipv4 set -i vmk0 --type static -I 192.168.5.130 -N 255.255.255.0 -g 192.168.5.1 command. For dhcp use the --type dhcp option. See Helpful Commands for how to list out the different portgroups and VMs Once you know all the IPv6 addresses for the various vmk0 nics, get on the VxRail appliance and use ping6 -l eth1 <ipv6 address> to test your IPv6 ping. This should work against all devices. If it doesn't you probably have a networking problem. You can also test the full server discovery process manually from the command line with: On the switch side you can use show lldp discovery to see what interfaces it has discovered and it will list the interfaces. On idrac it will show you what ports each nic is connected to under network devices. (My example is down - yours should not have a warning) For further troubleshooting check the Marvin log Assuming you've already checked DNS, the last thing you need to do is make sure NTP is up. I'm using chrony and checked like this on my NTP server: [root@services ~]# chronyc tracking Reference ID : D8EF230C (time4.google.com) Stratum : 2 Ref time (UTC) : Thu Dec 17 16:21:48 2020 System time : 0.000059723 seconds fast of NTP time Last offset : +0.000004093 seconds RMS offset : 0.000036436 seconds Frequency : 0.283 ppm fast Residual freq : -0.000 ppm Skew : 0.006 ppm Root delay : 0.023636952 seconds Root dispersion : 0.001011974 seconds Update interval : 1037.1 seconds Leap status : Normal","title":"Set up the Manager for Discovery"},{"location":"VMWare/Setup%20VXRail/#install","text":"Before running I strongly recommend you make sure NTP is up and running on any customer owned ESXi instances, customer vCenters, and the idracs. Mismatched time causes problems. All you have to do is browse to your VxRail Manager's address on follow the prompts. The only big gotcha I ran into is when it asks for vCenter Server Management Username what it wants is not administrator@vphere.whatever. It wants vCenter's root/password combo.","title":"Install"},{"location":"VMWare/Setup%20VXRail/#advanced-troubleshooting","text":"","title":"Advanced Troubleshooting"},{"location":"VMWare/Setup%20VXRail/#check-for-esxi-logs","text":"In my case I hit a failure where a node wouldn't add. A lot of times VxRail will have finished the ESXi deployment and you can actually go log into the ESXi instance in question. If you have a customer owned vCenter you can also go check its logs. A lot of the time failures during deployment can be found in one of those locations.","title":"Check for ESXi Logs"},{"location":"VMWare/Setup%20VXRail/#marvin-logs","text":"Another good place to look is to log into the VxRail appliance and check the Marvin logs at /var/log/vmware/marvin/tomcat","title":"Marvin Logs"},{"location":"VMWare/Setup%20VXRail/#digging-through-vxrails-guts","text":"If you have problems during install you can pull VxRail's logs from the VxRail container by doing the below: Run docker ps -a and find the vxrail-system container. e8da567768e4 infra/openfaas/gateway:1.1.4 \"./gateway\" 2 days ago Up 2 days (healthy) 8080/tcp, 8082/tcp func_gateway.1.h5hmx70g8545d3apbwfxtfatr 2b71bfd81e90 infra/openfaas/queue-worker:1.1.4 \"./app\" 2 days ago Up 2 days (healthy) 8080/tcp func_queue-worker.1.897j8f0tbhas1989o0ygl5ez4 ba2f65a96664 infra/workflow_engine:1.1.6 \"./docker-entrypoint\u2026\" 2 days ago Up 2 days (healthy) 5000/tcp func_wfservice.1.wau7b1omhg3igjzzyvd865r6e d07929b37e2d infra/openfaas/gateway:1.1.4 \"./gateway\" 2 days ago Exited (1) 2 days ago func_gateway.1.stdin4eg1dysl3fajgpds5rwj e6f3ec734f8a infra/openfaas/queue-worker:1.1.4 \"./app\" 2 days ago Exited (2) 2 days ago func_queue-worker.1.uqy8d482650v4yquz98fnp9tq bc402e3b676c microservice/vxm-agent:1.1.6 \"/bin/bash entrypoin\u2026\" 2 days ago Up 2 days (healthy) 5000/tcp vxm-agent_vxm-agent_1 23e0ac7d3f92 microservice/nano-service:1.1.29 \"python -u index.py\" 2 days ago Up 2 days (healthy) 5000/tcp func_nano-service.1.dqo23zdkgmwbqf9ru6yn0t99r 54df5cb0af13 microservice/ms-day1-bringup:1.1.29 \"/bin/bash entrypoin\u2026\" 2 days ago Up 2 days (healthy) 5000/tcp func_ms-day1-bringup.1.xfe0xndtvsqbo17kapogyd7ke 87e26e7d27b2 infra/infra-lock-service:1.1.8 \"/bin/bash entrypoin\u2026\" 2 days ago Up 2 days (healthy) 5000/tcp func_lockservice.1.g2g7hg5viyph8r4ptcljeran9 4382600d0094 microservice/lockbox:1.1.11 \"/bin/bash entrypoin\u2026\" 2 days ago Up 2 days (healthy) 5000/tcp func_lockbox.1.q9hqbfuid54r5gelsazasksh2 f7964475a933 microservice/kgs-service:1.1.10 \"./boot.sh\" 2 days ago Up 2 days (healthy) 5000/tcp func_kgs-service.1.wpqm7hytpf5l9wo7ch3sc1wz7 75178ef078ea do-main/do-vxrail-system:1.1.35 \"/bin/bash entrypoin\u2026\" 2 days ago Up 2 days (healthy) 5000/tcp func_do-vxrail-system.1.e67dbhktpe88gw0wtmymmpthg 7abeddce2bb8 do-main/do-vm:1.1.35 \"/bin/bash entrypoin\u2026\" 2 days ago Up 2 days (healthy) 5000/tcp func_do-vm.1.g2fwikfzimyhv6di3zdalt1j3 3b557989c07e do-main/do-storage:1.1.35 \"/bin/bash entrypoin\u2026\" 2 days ago Up 2 days (healthy) 5000/tcp func_do-storage.1.w2nv1rfpidrv7g418xmknywso d0b15759e7f5 do-main/do-serviceability:1.1.35 \"/bin/bash entrypoin\u2026\" 2 days ago Up 2 days (healthy) func_do-serviceability.1.t5xkwt1eu2ypqvn0511i1gfy3 5e349b22f36a do-main/do-network:1.1.39 \"/bin/bash entrypoin\u2026\" 2 days ago Up 2 days (healthy) 5000/tcp func_do-network.1.vogoz7jtyrxf83h9jd0615r28 ae31c7cd94bd do-main/do-kgs:1.1.35 \"./boot.sh\" 2 days ago Up 2 days (healthy) 5000/tcp func_do-kgs.1.esudf6d39uy7e96dnvxoabaue 978447c9f6f8 do-main/do-host:1.1.39 \"/bin/bash entrypoin\u2026\" 2 days ago Up 2 days (healthy) 5000/tcp func_do-host.1.ravl83nt962k48up0v33qoviy 862926a58073 do-main/do-ecosystem:1.1.35 \"/bin/bash entrypoin\u2026\" 2 days ago Up 2 days (healthy) func_do-ecosystem.1.8k6cljhvo5p00632gt11f79uv 63dde78fc922 do-main/do-eservices:1.1.35 \"/bin/bash entrypoin\u2026\" 2 days ago Up 2 days (healthy) func_do-eservices.1.pnpnr1pofm0dvwv36trufl6g9 f1fb1d35457d do-main/do-cluster:1.1.35 \"/bin/bash entrypoin\u2026\" 2 days ago Up 2 days (healthy) 5000/tcp func_do-cluster.1.y4vs2diiyy5eau3ojuxdx12wx 2fc837e0726e infra/infra-config-service:1.1.9 \"/bin/bash entrypoin\u2026\" 2 days ago Up 2 days (healthy) 5000/tcp func_configservice.1.byjhptdpdxa3d04y7mr5lfp5z 685d833e0155 infra/openfaas/gateway:1.1.4 \"./gateway\" 2 days ago Exited (1) 2 days ago func_gateway.1.ztar1zozhfuh3skopwvlandwq 9af0891d5455 infra/openfaas/queue-worker:1.1.4 \"./app\" 2 days ago Exited (2) 2 days ago func_queue-worker.1.jmk9q8bkc6a2vlz3fg1e60y5f 0cb6d7948db9 infra/tracing:1.1.6 \"/go/bin/jaeger-all-\u2026\" 2 days ago Up 2 days (healthy) func_tracing.1.ymsjr3ba7ih2x21291xri4mz8 5e70fa63df88 infra/openfaas/faas-swarm:1.1.4 \"./faas-swarm\" 2 days ago Up 2 days (healthy) 8080/tcp func_faas-swarm.1.nfz0dm48hhx7vh9sh04mbagtz 620cce221e65 infra/nats-streaming:1.1.4 \"/nats-streaming-ser\u2026\" 2 days ago Up 2 days (healthy) 4222/tcp, 8222/tcp func_nats.1.l42vx07v08vhhtkv1lwxg1ipb dd16298b3e4c infra/nginx_gateway_vxrail:1.1.7 \"/bin/sh -c 'sudo /u\u2026\" 2 days ago Up 2 days (healthy) func_api-gateway.1.do9gzerxmmlti4rn8k3ab2gs4 4a8b6c976103 infra/gcr.io/etcd-development/etcd:1.1.4 \"/usr/local/bin/etcd\u2026\" 2 days ago Up 2 days (healthy) 2379-2380/tcp func_serviceregistry.1.u2gtw5wnsldulqgd1r47wkte5 acf803b61b04 infra/redis:1.1.4 \"docker-entrypoint.s\u2026\" 2 days ago Up 2 days (healthy) 6379/tcp func_cacheservice.1.subywzigvenobaskavjs6xyqc 3dac923f9bff infra/logging:1.1.4 \"tini -- /bin/entryp\u2026\" 2 days ago Up 2 days (healthy) 5140/tcp, 24224/tcp func_logging.1.kv7738umixulwryqpl38qvxvr bdd88fadc236 infra/prom/alertmanager:1.1.4 \"/bin/alertmanager -\u2026\" 2 days ago Up 2 days (healthy) 9093/tcp func_alertmanager.1.8uks430a7p10keeh8uqv625bv 7ec04254fee1 infra/prom/prometheus:1.1.4 \"/bin/prometheus --c\u2026\" 2 days ago Up 2 days (healthy) 9090/tcp func_prometheus.1.ti36b3wsder4w01v8nentdirx Find the log location with docker inspect --format='{{.LogPath}}' 75178ef078ea . They are in JSON format. For example, this is what my error looked like: This got me the reason for my failure: {\\\\\\\"error\\\\\\\": {\\\\\\\"result\\\\\\\": {\\\\\\\"error\\\\\\\": {\\\\\\\"code\\\\\\\": \\\\\\\"E3100_Cluster_22\\\\\\\", \\\\\\\"message\\\\\\\": \\\\\\\"failed to add host into cluster\\\\\\\", \\\\\\\"params\\\\\\\": [\\\\\\\"vxrail1-esxi.lan\\\\\\\"]}}}, \\\\\\\"id\\\\\\\": \\\\\\\"host_add_into_inventory\\\\\\\", \\\\\\\"internal_family\\\\\\\": \\\\\\\"node_one_bootstrap_with_cns_ready.primary_node_computer.host_add_into_inventory\\\\\\\", \\\\\\\"internal_id\\\\\\\": \\\\\\\"node_one_bootstrap_with_cns_ready.primary_node_computer.host_add_into_inventory_False____99ee8388_1147_4e_9d65c98cb2\\\\\\\", \", \"stream\": \"stderr\", \"time\": \"2020-12-18T13:24:42.028798775Z\" } {\"log\":\"\\\\\\\"params\\\\\\\": {\\\\\\\"cluster_name\\\\\\\": \\\\\\\"vxrail-cluster\\\\\\\", \\\\\\\"datacenter_name\\\\\\\": \\\\\\\"Datacenter\\\\\\\", \\\\\\\"host_conn_info\\\\\\\": {\\\\\\\"host\\\\\\\": \\\\\\\"192.168.2.31\\\\\\\", \\\\\\\"port\\\\\\\": 443, \\\\\\\"username\\\\\\\": \\\\\\\"root\\\\\\\"}, \\\\\\\"hostname\\\\\\\": \\\\\\\"vxrail1-esxi.lan\\\\\\\", \\\\\\\"ssl_thumbprint\\\\\\\": \\\\\\\"STUFF\\\\\\\", \\\\\\\"vc_conn_info\\\\\\\": {\\\\\\\"host\\\\\\\": \\\\\\\"vcenter.lan\\\\\\\", \\\\\\\"port\\\\\\\": 443, \\\\\\\"username\\\\\\\": \\\\\\\"administrator@vsphere.lan\\\\\\\"}, \\\\\\\"vm_folder_name\\\\\\\": \\\\\\\"VMware HCIA Folder vxrail-cluster\\\\\\\", \\\\\\\"vsan_name\\\\\\\": \\\\\\\"VxRail-Virtual-SAN-Datastore-22658cd0-5b17-428d-a23c-0d7e5a877bb7\\\\\\\"}, \\\\\\\"stage\\\\\\\": \\\\\\\"primary_node_computer\\\\\\\", \\\\\\\"startTime\\\\\\\": 1608297822107, \\\\\\\"status\\\\\\\": \\\\\\\"FAILED\\\\\\\"}","title":"Digging Through VxRail's Guts"},{"location":"VMWare/Setup%20VXRail/#helpful-commands","text":"","title":"Helpful Commands"},{"location":"VMWare/Setup%20VXRail/#check-if-vxrail-is-running","text":"esxcli vm process list","title":"Check if VxRail is Running"},{"location":"VMWare/Setup%20VXRail/#get-a-list-of-vms","text":"vim-cmd vmsvc/getallvms","title":"Get a List of VMs"},{"location":"VMWare/Setup%20VXRail/#check-if-vm-is-on","text":"vim-cmd vmsvc/power.getstate 1","title":"Check if VM is On"},{"location":"VMWare/Setup%20VXRail/#check-networking","text":"esxcli network vswitch standard portgroup list","title":"Check Networking"},{"location":"VMWare/Setup%20VXRail/#set-the-vlan-for-a-switch","text":"esxcli network vswitch standard portgroup set -p \"VM Network\" -v 120","title":"Set the VLAN for a Switch"},{"location":"VMWare/Setup%20VXRail/#get-interface-ips","text":"esxcli network ip interface ipv4 get","title":"Get Interface IPs"},{"location":"VMWare/Setup%20VXRail/#get-interface-list","text":"esxcli network ip interface list","title":"Get Interface List"},{"location":"VMWare/Setup%20VXRail/vcenter_deploy/","text":"vCenter Deploy It looks like deploying vCenter is controlled by the file vc_deploy.py : vxrailmanager:~ # find / -iname vc_deploy.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vcdeploy/vc_deploy.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vcdeploy/vc_deploy.py Run docker inspect $(docker ps -qa) | jq -r 'map([.Name, .GraphDriver.Data.MergedDir]) | .[] | \"\\(.[0])\\t\\(.[1])\"' to figure out which container owns that overlay. Based on that it looks like there is a container called nano-service which owns that file: vxrailmanager:~ # docker inspect $(docker ps -qa) | jq -r 'map([.Name, .GraphDriver.Data.MergedDir]) | .[] | \"\\(.[0])\\t\\(.[1])\"' | grep 5c284f1529c19ad /func_nano-service.1.dqo23zdkgmwbqf9ru6yn0t99r /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged vxrailmanager:~ # docker inspect $(docker ps -qa) | jq -r 'map([.Name, .GraphDriver.Data.MergedDir]) | .[] | \"\\(.[0])\\t\\(.[1])\"' | grep 6e4546b5beb8a Drop to container command line: docker exec -it 23e0ac7d3f92 /bin/bash Browse around and take a look at vc_deploy.py : logger = logging.getLogger(__name__) ns = api.namespace('', description='Operations related to internal vc case') host_info_model = get_host_conn_info_model(api) storage_info_model = get_storage_info_model(api) sync_call_result = build_api_model_sync_call_result(api) vc_deploy_model = api.model('VCDeploy', { 'host_conn_info': fields.Nested(host_info_model), 'storage_info': fields.Nested(storage_info_model), 'cluster_type': fields.String(required=True, description='vxrail cluster type') }) VC_VM_NAME = 'VMware vCenter Server Appliance' do_vm = DoCaller(do_host='do-vm', do_prefix='do-vm') def perform(self, request_body): logger.info(\"start to perform VCDeploy, cluster type: {}\".format(self.cluster_type)) src_datastore = self._get_source_datastore_name() dst_datastore = self.storage_info.get('datastore') if self.cluster_type == ClusterType.COMPUTE.value else 'vsanDatastore' body = { 'host_conn_info': self.host_conn_info, 'src_datastore_name': src_datastore, 'dst_datastore_name': dst_datastore, 'vm_name': VC_VM_NAME } result = None try: result = do_vm.sync_call_do(do_vm.build_call_do_service_task_url(HOST_VM_OVA_DEPLOY_URL), body) except Exception as e: self.logger.info(e) self.raise_error('E3100_Cluster_24', params=(), message='Failed to deploy vCenter.') return result Doing a search on grep -rli 'external vc' / 2> /dev/null gets ... /var/lib/docker/overlay2/19f438b6cef2857e6067e49802d04d61de2afb7275bc6fb1323a41bac03fec1c/diff/home/app/worker/task/cluster.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/common/conf/config-messages.properties /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vcmgmtaccountpermissiongrant/vc_mgmt_account_permission_grant.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/externalvclicensevalidator/external_vc_license_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/externalvcaccesslivevalidator/external_vc_access_live_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/externalvccapacitylimitlivevalidator/external_vc_capacity_limit_live_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostaddtovds/host_add_to_vds.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hosttimesync/host_time_sync.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/timeconfigurationlivevalidator/time_configuration_live_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostunusedsystemvmsremove/host_unused_system_vms_remove.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/externalvcversioncompatibilitylivevalidator/external_vc_version_compatibility_live_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vxmtimesync/vxm_time_sync.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/timesyncconfigurationvalidator/time_sync_configuration_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vcsyslogconfig/vc_syslog_config.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/dnshostnameslivevalidator/dns_hostnames_live_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/cursoryvalidator/hostname_configuration_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/cursoryvalidator/tests/test_hostname_configuration_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/cursoryvalidator/tests/test_none_value_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/cursoryvalidator/tests/test_vcenter_configuration_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/cursoryvalidator/tests/test_dns_servers_configuration_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/cursoryvalidator/vcenter_configuration_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/cursoryvalidator/none_value_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/cursoryvalidator/dns_servers_configuration_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/cursoryvalidator/passwords_configuration_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/cursoryvalidator/netmask_configuration_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vcuserpermissionlivevalidator/vc_user_permission_live_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/createvmfolder/create_vm_folder.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/externalvcentitieslivevalidator/external_vc_entities_live_validator.py /var/lib/docker/overlay2/6df33189ca552a920f2d6c3259a034fac3ec47f8065634a6ba17ff7c74299377/merged/home/app/messages/messages_en_US.properties /var/lib/docker/overlay2/6df33189ca552a920f2d6c3259a034fac3ec47f8065634a6ba17ff7c74299377/merged/home/app/dayone/dayone_processor.py /var/lib/docker/overlay2/6df33189ca552a920f2d6c3259a034fac3ec47f8065634a6ba17ff7c74299377/merged/home/app/dayone/api.py /var/lib/docker/overlay2/1bc6389a4238217d15b1f5d931c6049e53891a92cef4255f6b402138e130d4d9/merged/home/app/lockbox_app/dao/postgres_dao.py /var/lib/docker/overlay2/1bc6389a4238217d15b1f5d931c6049e53891a92cef4255f6b402138e130d4d9/merged/home/app/lockbox_app/resources/verify_error_messages.json ... Remove the cursory checks: (base) grant@DESKTOP-2SV1E9O:~$ grep -v cursory test.txt /var/lib/docker/overlay2/19f438b6cef2857e6067e49802d04d61de2afb7275bc6fb1323a41bac03fec1c/diff/home/app/worker/task/cluster.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/common/conf/config-messages.properties /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vcmgmtaccountpermissiongrant/vc_mgmt_account_permission_grant.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/externalvclicensevalidator/external_vc_license_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/externalvcaccesslivevalidator/external_vc_access_live_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/externalvccapacitylimitlivevalidator/external_vc_capacity_limit_live_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostaddtovds/host_add_to_vds.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hosttimesync/host_time_sync.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/timeconfigurationlivevalidator/time_configuration_live_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostunusedsystemvmsremove/host_unused_system_vms_remove.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/externalvcversioncompatibilitylivevalidator/external_vc_version_compatibility_live_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vxmtimesync/vxm_time_sync.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/timesyncconfigurationvalidator/time_sync_configuration_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vcsyslogconfig/vc_syslog_config.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/dnshostnameslivevalidator/dns_hostnames_live_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vcuserpermissionlivevalidator/vc_user_permission_live_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/createvmfolder/create_vm_folder.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/externalvcentitieslivevalidator/external_vc_entities_live_validator.py /var/lib/docker/overlay2/6df33189ca552a920f2d6c3259a034fac3ec47f8065634a6ba17ff7c74299377/merged/home/app/messages/messages_en_US.properties /var/lib/docker/overlay2/6df33189ca552a920f2d6c3259a034fac3ec47f8065634a6ba17ff7c74299377/merged/home/app/dayone/dayone_processor.py /var/lib/docker/overlay2/6df33189ca552a920f2d6c3259a034fac3ec47f8065634a6ba17ff7c74299377/merged/home/app/dayone/api.py /var/lib/docker/overlay2/1bc6389a4238217d15b1f5d931c6049e53891a92cef4255f6b402138e130d4d9/merged/home/app/lockbox_app/dao/postgres_dao.py /var/lib/docker/overlay2/1bc6389a4238217d15b1f5d931c6049e53891a92cef4255f6b402138e130d4d9/merged/home/app/lockbox_app/resources/verify_error_messages.json Looking for the call build_call_do_service_task_url vxrailmanager:/var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vcdeploy # grep -rli 'build_call_do_service_task_url' /var/lib/docker/ 2> /dev/null /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/common/do.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/common/validation/ip_live_check_utils.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/common/network_prepare_aware_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vxmconfigurationdatapopulate/vxm_configuration_data_populate.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostaddintocluster/host_add_into_cluster.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/clusterconverttostretchedcluster/cluster_convert_to_stretched_cluster.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostpsipupdate/host_ps_ip_update.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/multidiskgroupconfigurationvalidator/multi_disk_group_configuration_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vcinitialboot/vc_initial_boot.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/clusterdasconfig/cluster_das_config.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostnetworkmanagementipchange/host_network_management_ip_change.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vcmgmtaccountpermissiongrant/vc_mgmt_account_permission_grant.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vxrailkgsbaselineload/vxm_kgs_baseline_load.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vcretreatmodeconfigure/vc_retreat_mode_configure.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/enableteaming/enableteaming.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/externalvcaccesslivevalidator/external_vc_access_live_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hosthostnamechange/host_hostname_change.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vxmupdatecertification/vxm_update_certification.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hoststorageleavevsan/host_storage_leave_vsan.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vxmconfigureslaacip/vxm_configure_slaacip.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vxmsystemservicesstop/vxm_system_services_stop.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/removedisconnecthostfromcluster/remove_disconnect_host_from_cluster.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostaddtovds/host_add_to_vds.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostassignactivepnictovds/host_assign_active_pnic_to_vds.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostassignvmktovds/host_assign_vmk_to_vds.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/clusterhighavailabilityenable/cluster_high_availability_enable.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/switchcreatenetworks/switch_create_networks.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vxmruntimepropertiesgenerate/vxm_runtime_properties_generate.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostvsanstorageenable/host_vsan_storage_enable.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hosttimesync/host_time_sync.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vcguestopsalias/vc_guest_ops_alias.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostwitnesstrafficconnectivity/host_witness_traffic_connectivity.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostvsandiskgroupscreate/host_vsan_diskgroups_create.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/createvds/createvds.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/switchconfiguplinks/switch_configure_uplinks.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/createportgroup/createportgroup.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vcpluginregister/vc_plugin_register.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vcdatacentercreate/vc_datacenter_create.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hoststoragevsandiskremove/host_storage_vsan_disk_remove.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostptagentipchange/host_ptagent_ip_change.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/customersuppliedvdsvlanlivevalidator/customer_supplied_vds_vlan_live_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostversioncompatibilitylivevalidator/host_version_compatibility_live_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/esxihostaccountvalidator/esxi_host_account_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostvsanstoragepolicyconfiguration/host_vsan_storage_policy_configuration.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostunusedsystemvmsremove/host_unused_system_vms_remove.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/switchconfigjumpbox/switch_configure_jumpbox.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vccertsgenerate/vc_certs_generate.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/clusterdrsenable/cluster_drs_enable.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vclicensing/vc_licensing.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostelectionservicedisable/host_election_service_disable.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/clusteraddexceptionaccounts/cluster_add_exception_accounts.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vcpasswordexpirationdisable/vc_password_expiration_disable.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vxmnetworkvnicremove/vxm_network_vnic_remove.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/externalvcversioncompatibilitylivevalidator/external_vc_version_compatibility_live_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/omeintegration/ome_integration.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostmarkpnicworkingmode/host_mark_pnic_working_mode.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/switchcreatedownlinks/switch_create_downlinks.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vxmevopassword/vxm_evo_password.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/movevxmvnictovds/vxm_network_vnic_add_to_vds.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostloudmouthrestart/host_loudmouth_restart.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hoststorageaddtovsan/host_storage_add_to_vsan.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostloggingsystemconfigure/host_logging_system_configure.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/clusteradvanceparamssetting/cluster_advance_params_setting.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/resetvspherealarms/reset_vsphere_alarms.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/networkcompatibilitylivevalidator/network_compatibility_live_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/switchauthentication/switch_authentication.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vxmtimesync/vxm_time_sync.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostsecurityservicedisable/host_security_service_disable.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/timesyncconfigurationvalidator/time_sync_configuration_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vxmstaticipchange/vxm_static_ipchange.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/createstoragepolicy/create_storage_policy.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vxmclustercleanup/vxm_cluster_clean_up.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/switchchangepassword/switch_change_password.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostplatformservicerestart/host_platform_service_restart.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostdnschange/host_dns_change.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vxmautostartenable/vxm_auto_start_enable.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vlanlivevalidator/vlan_live_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vxmserviceaccountvalidator/vxm_service_account_validator_task.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vxmhostnameset/vxm_hostname_set.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vcautostartenable/vc_auto_start_enable.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vcdeploy/vc_deploy.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostwitnessnetworking/host_witness_networking.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostmanagementaccountcreate/host_management_account_create.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/enablenioc/enablenioc.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vcsyslogconfig/vc_syslog_config.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vxmsystemservicesrestart/vxm_system_services_restart.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/checknodestatus/check_node_status.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vxmmovetostorage/vxm_move_to_storage.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostassignpnictovss/host_assign_pnic_to_vss.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vcmgmntaccountcreate/vc_mgmnt_account_create.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/clusterevcenable/cluster_evc_enable.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vxmsetdns/vxm_set_dns.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/cursoryvalidator/cursory_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostvssremove/host_vss_remove.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/clustercreate/clustercreate.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostaccountpasswordchange/host_account_password_change.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vxmrootaccountvalidator/vxm_root_account_validator_task.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostproxyservicedisable/host_proxy_service_disable.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/clustersystemvmstoragepolicyapply/cluster_system_vm_storage_policy_apply.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostassignstandbypnictovds/host_assign_standby_pnic_to_vds.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/createvmfolder/create_vm_folder.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/postdayoneconfiguration/post_dayone_configuration.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostnetworkmanagementvlanchange/host_network_management_vlan_change.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vxmupdateinternaldns/vxm_update_internal_dns.py /var/lib/docker/overlay2/6df33189ca552a920f2d6c3259a034fac3ec47f8065634a6ba17ff7c74299377/merged/home/app/dayone/do.py /var/lib/docker/overlay2/6df33189ca552a920f2d6c3259a034fac3ec47f8065634a6ba17ff7c74299377/merged/home/app/dayone/transfer_handler.py /var/lib/docker/overlay2/6df33189ca552a920f2d6c3259a034fac3ec47f8065634a6ba17ff7c74299377/merged/home/app/dayone/storage_discovery_callback.py /var/lib/docker/overlay2/6df33189ca552a920f2d6c3259a034fac3ec47f8065634a6ba17ff7c74299377/merged/home/app/vxm_ip_change_scheduler.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vxmconfigurationdatapopulate/vxm_configuration_data_populate.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hostaddintocluster/host_add_into_cluster.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/clusterconverttostretchedcluster/cluster_convert_to_stretched_cluster.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hostpsipupdate/host_ps_ip_update.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/multidiskgroupconfigurationvalidator/multi_disk_group_configuration_validator.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vcinitialboot/vc_initial_boot.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/clusterdasconfig/cluster_das_config.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hostnetworkmanagementipchange/host_network_management_ip_change.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vcmgmtaccountpermissiongrant/vc_mgmt_account_permission_grant.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vxrailkgsbaselineload/vxm_kgs_baseline_load.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vcretreatmodeconfigure/vc_retreat_mode_configure.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/enableteaming/enableteaming.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/externalvcaccesslivevalidator/external_vc_access_live_validator.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hosthostnamechange/host_hostname_change.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vxmupdatecertification/vxm_update_certification.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hoststorageleavevsan/host_storage_leave_vsan.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vxmconfigureslaacip/vxm_configure_slaacip.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vxmsystemservicesstop/vxm_system_services_stop.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/removedisconnecthostfromcluster/remove_disconnect_host_from_cluster.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hostaddtovds/host_add_to_vds.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hostassignactivepnictovds/host_assign_active_pnic_to_vds.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hostassignvmktovds/host_assign_vmk_to_vds.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/clusterhighavailabilityenable/cluster_high_availability_enable.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/switchcreatenetworks/switch_create_networks.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vxmruntimepropertiesgenerate/vxm_runtime_properties_generate.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hostvsanstorageenable/host_vsan_storage_enable.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hosttimesync/host_time_sync.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vcguestopsalias/vc_guest_ops_alias.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hostwitnesstrafficconnectivity/host_witness_traffic_connectivity.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hostvsandiskgroupscreate/host_vsan_diskgroups_create.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/createvds/createvds.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/switchconfiguplinks/switch_configure_uplinks.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/createportgroup/createportgroup.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vcpluginregister/vc_plugin_register.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vcdatacentercreate/vc_datacenter_create.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hoststoragevsandiskremove/host_storage_vsan_disk_remove.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hostptagentipchange/host_ptagent_ip_change.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/customersuppliedvdsvlanlivevalidator/customer_supplied_vds_vlan_live_validator.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hostversioncompatibilitylivevalidator/host_version_compatibility_live_validator.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/esxihostaccountvalidator/esxi_host_account_validator.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hostvsanstoragepolicyconfiguration/host_vsan_storage_policy_configuration.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hostunusedsystemvmsremove/host_unused_system_vms_remove.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/switchconfigjumpbox/switch_configure_jumpbox.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vccertsgenerate/vc_certs_generate.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/clusterdrsenable/cluster_drs_enable.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vclicensing/vc_licensing.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hostelectionservicedisable/host_election_service_disable.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/clusteraddexceptionaccounts/cluster_add_exception_accounts.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vcpasswordexpirationdisable/vc_password_expiration_disable.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vxmnetworkvnicremove/vxm_network_vnic_remove.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/externalvcversioncompatibilitylivevalidator/external_vc_version_compatibility_live_validator.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/omeintegration/ome_integration.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hostmarkpnicworkingmode/host_mark_pnic_working_mode.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/switchcreatedownlinks/switch_create_downlinks.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vxmevopassword/vxm_evo_password.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/movevxmvnictovds/vxm_network_vnic_add_to_vds.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hostloudmouthrestart/host_loudmouth_restart.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hoststorageaddtovsan/host_storage_add_to_vsan.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hostloggingsystemconfigure/host_logging_system_configure.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/clusteradvanceparamssetting/cluster_advance_params_setting.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/resetvspherealarms/reset_vsphere_alarms.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/networkcompatibilitylivevalidator/network_compatibility_live_validator.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/switchauthentication/switch_authentication.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vxmtimesync/vxm_time_sync.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hostsecurityservicedisable/host_security_service_disable.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/timesyncconfigurationvalidator/time_sync_configuration_validator.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vxmstaticipchange/vxm_static_ipchange.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/createstoragepolicy/create_storage_policy.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vxmclustercleanup/vxm_cluster_clean_up.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/common/do.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/common/validation/ip_live_check_utils.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/common/network_prepare_aware_validator.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/switchchangepassword/switch_change_password.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hostplatformservicerestart/host_platform_service_restart.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hostdnschange/host_dns_change.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vxmautostartenable/vxm_auto_start_enable.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vlanlivevalidator/vlan_live_validator.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vxmserviceaccountvalidator/vxm_service_account_validator_task.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vxmhostnameset/vxm_hostname_set.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vcautostartenable/vc_auto_start_enable.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vcdeploy/vc_deploy.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hostwitnessnetworking/host_witness_networking.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hostmanagementaccountcreate/host_management_account_create.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/enablenioc/enablenioc.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vcsyslogconfig/vc_syslog_config.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vxmsystemservicesrestart/vxm_system_services_restart.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/checknodestatus/check_node_status.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vxmmovetostorage/vxm_move_to_storage.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hostassignpnictovss/host_assign_pnic_to_vss.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vcmgmntaccountcreate/vc_mgmnt_account_create.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/clusterevcenable/cluster_evc_enable.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vxmsetdns/vxm_set_dns.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/cursoryvalidator/cursory_validator.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hostvssremove/host_vss_remove.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/clustercreate/clustercreate.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hostaccountpasswordchange/host_account_password_change.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vxmrootaccountvalidator/vxm_root_account_validator_task.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hostproxyservicedisable/host_proxy_service_disable.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/clustersystemvmstoragepolicyapply/cluster_system_vm_storage_policy_apply.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hostassignstandbypnictovds/host_assign_standby_pnic_to_vds.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/createvmfolder/create_vm_folder.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/postdayoneconfiguration/post_dayone_configuration.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hostnetworkmanagementvlanchange/host_network_management_vlan_change.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vxmupdateinternaldns/vxm_update_internal_dns.py /var/lib/docker/overlay2/0d8ac81dbb9015f2f3526cd468c2c4e7e433661e38008b04f70539deee19272a/diff/home/app/dayone/do.py /var/lib/docker/overlay2/0d8ac81dbb9015f2f3526cd468c2c4e7e433661e38008b04f70539deee19272a/diff/home/app/dayone/transfer_handler.py /var/lib/docker/overlay2/0d8ac81dbb9015f2f3526cd468c2c4e7e433661e38008b04f70539deee19272a/diff/home/app/dayone/storage_discovery_callback.py /var/lib/docker/overlay2/02a3e09ef808a5f704f821bbf2fcc89ece02f262e0a20b1c5a599fbf949a8c9b/diff/home/app/vxm_ip_change_scheduler.py Finding the definition: vxrailmanager:/var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vcdeploy # grep -rli 'def build_call_do_service_task_url' /var/lib/docker/ 2> /dev/null /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/common/do.py /var/lib/docker/overlay2/6df33189ca552a920f2d6c3259a034fac3ec47f8065634a6ba17ff7c74299377/merged/home/app/dayone/do.py /var/lib/docker/overlay2/6df33189ca552a920f2d6c3259a034fac3ec47f8065634a6ba17ff7c74299377/merged/home/app/vxm_ip_change_scheduler.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/common/do.py /var/lib/docker/overlay2/0d8ac81dbb9015f2f3526cd468c2c4e7e433661e38008b04f70539deee19272a/diff/home/app/dayone/do.py /var/lib/docker/overlay2/02a3e09ef808a5f704f821bbf2fcc89ece02f262e0a20b1c5a599fbf949a8c9b/diff/home/app/vxm_ip_change_scheduler.py There seems to be a file for licensing: Find embedded license: vxrailmanager:/var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vcdeploy # find / -iname license-vc-700-e1-marvin-c1-201909 \\/usr/lib/vmware-marvin/marvind/webapps/lcm/WEB-INF/classes/licensing/7.0/license-vc-700-e1-marvin-c1-201909 /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/common/conf/license-vc-700-e1-marvin-c1-201909 /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/common/conf/license-vc-700-e1-marvin-c1-201909 vxrailmanager:/var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vcdeploy # cat /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/common/conf/license-vc-700-e1-marvin-c1-201909 # VMware software license StartFields = \"Cpt, ProductID, LicenseVersion, LicenseType, LicenseEdition, Option, Epoch\" Cpt = \"COPYRIGHT (c) VMware, Inc.\" ProductID = \"VMware VirtualCenter Server\" LicenseVersion = \"7.0\" LicenseType = \"Site\" Epoch = \"2019-9-1\" LicenseEdition = \"vc.standard.instance.marvin\" Option = \"7\" Data = \"FileVersion=7.0.0.2;capacityType=server;enable=linkedvc,woe,xvp,vcha,backuprestore,appliancemigration;addon=vsa;disallowedHostEditions=esx.hypervisor.cpuPackage,esx.hypervisorEmbeddedOem.cpuPackage,esx.essentials.cpuPackage,esx.essentialsPlus.cpuPackage,esx.hypervisor.cpuPackageCoreLimited,esx.hypervisorEmbeddedOem.cpuPackageCoreLimited,esx.essentials.cpuPackageCoreLimited,esx.essentialsPlus.cpuPackageCoreLimited;desc=vCenter Server 7 Standard\" DataHash = \"60aebea4-31a0050c-c4e49494-27690fad-5a8a5258\" Hash = \"441d0833-d2f3bed9-0bea3fec-7d13664b-e222d862\"","title":"vCenter Deploy"},{"location":"VMWare/Setup%20VXRail/vcenter_deploy/#vcenter-deploy","text":"It looks like deploying vCenter is controlled by the file vc_deploy.py : vxrailmanager:~ # find / -iname vc_deploy.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vcdeploy/vc_deploy.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vcdeploy/vc_deploy.py Run docker inspect $(docker ps -qa) | jq -r 'map([.Name, .GraphDriver.Data.MergedDir]) | .[] | \"\\(.[0])\\t\\(.[1])\"' to figure out which container owns that overlay. Based on that it looks like there is a container called nano-service which owns that file: vxrailmanager:~ # docker inspect $(docker ps -qa) | jq -r 'map([.Name, .GraphDriver.Data.MergedDir]) | .[] | \"\\(.[0])\\t\\(.[1])\"' | grep 5c284f1529c19ad /func_nano-service.1.dqo23zdkgmwbqf9ru6yn0t99r /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged vxrailmanager:~ # docker inspect $(docker ps -qa) | jq -r 'map([.Name, .GraphDriver.Data.MergedDir]) | .[] | \"\\(.[0])\\t\\(.[1])\"' | grep 6e4546b5beb8a Drop to container command line: docker exec -it 23e0ac7d3f92 /bin/bash Browse around and take a look at vc_deploy.py : logger = logging.getLogger(__name__) ns = api.namespace('', description='Operations related to internal vc case') host_info_model = get_host_conn_info_model(api) storage_info_model = get_storage_info_model(api) sync_call_result = build_api_model_sync_call_result(api) vc_deploy_model = api.model('VCDeploy', { 'host_conn_info': fields.Nested(host_info_model), 'storage_info': fields.Nested(storage_info_model), 'cluster_type': fields.String(required=True, description='vxrail cluster type') }) VC_VM_NAME = 'VMware vCenter Server Appliance' do_vm = DoCaller(do_host='do-vm', do_prefix='do-vm') def perform(self, request_body): logger.info(\"start to perform VCDeploy, cluster type: {}\".format(self.cluster_type)) src_datastore = self._get_source_datastore_name() dst_datastore = self.storage_info.get('datastore') if self.cluster_type == ClusterType.COMPUTE.value else 'vsanDatastore' body = { 'host_conn_info': self.host_conn_info, 'src_datastore_name': src_datastore, 'dst_datastore_name': dst_datastore, 'vm_name': VC_VM_NAME } result = None try: result = do_vm.sync_call_do(do_vm.build_call_do_service_task_url(HOST_VM_OVA_DEPLOY_URL), body) except Exception as e: self.logger.info(e) self.raise_error('E3100_Cluster_24', params=(), message='Failed to deploy vCenter.') return result Doing a search on grep -rli 'external vc' / 2> /dev/null gets ... /var/lib/docker/overlay2/19f438b6cef2857e6067e49802d04d61de2afb7275bc6fb1323a41bac03fec1c/diff/home/app/worker/task/cluster.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/common/conf/config-messages.properties /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vcmgmtaccountpermissiongrant/vc_mgmt_account_permission_grant.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/externalvclicensevalidator/external_vc_license_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/externalvcaccesslivevalidator/external_vc_access_live_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/externalvccapacitylimitlivevalidator/external_vc_capacity_limit_live_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostaddtovds/host_add_to_vds.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hosttimesync/host_time_sync.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/timeconfigurationlivevalidator/time_configuration_live_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostunusedsystemvmsremove/host_unused_system_vms_remove.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/externalvcversioncompatibilitylivevalidator/external_vc_version_compatibility_live_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vxmtimesync/vxm_time_sync.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/timesyncconfigurationvalidator/time_sync_configuration_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vcsyslogconfig/vc_syslog_config.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/dnshostnameslivevalidator/dns_hostnames_live_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/cursoryvalidator/hostname_configuration_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/cursoryvalidator/tests/test_hostname_configuration_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/cursoryvalidator/tests/test_none_value_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/cursoryvalidator/tests/test_vcenter_configuration_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/cursoryvalidator/tests/test_dns_servers_configuration_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/cursoryvalidator/vcenter_configuration_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/cursoryvalidator/none_value_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/cursoryvalidator/dns_servers_configuration_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/cursoryvalidator/passwords_configuration_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/cursoryvalidator/netmask_configuration_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vcuserpermissionlivevalidator/vc_user_permission_live_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/createvmfolder/create_vm_folder.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/externalvcentitieslivevalidator/external_vc_entities_live_validator.py /var/lib/docker/overlay2/6df33189ca552a920f2d6c3259a034fac3ec47f8065634a6ba17ff7c74299377/merged/home/app/messages/messages_en_US.properties /var/lib/docker/overlay2/6df33189ca552a920f2d6c3259a034fac3ec47f8065634a6ba17ff7c74299377/merged/home/app/dayone/dayone_processor.py /var/lib/docker/overlay2/6df33189ca552a920f2d6c3259a034fac3ec47f8065634a6ba17ff7c74299377/merged/home/app/dayone/api.py /var/lib/docker/overlay2/1bc6389a4238217d15b1f5d931c6049e53891a92cef4255f6b402138e130d4d9/merged/home/app/lockbox_app/dao/postgres_dao.py /var/lib/docker/overlay2/1bc6389a4238217d15b1f5d931c6049e53891a92cef4255f6b402138e130d4d9/merged/home/app/lockbox_app/resources/verify_error_messages.json ... Remove the cursory checks: (base) grant@DESKTOP-2SV1E9O:~$ grep -v cursory test.txt /var/lib/docker/overlay2/19f438b6cef2857e6067e49802d04d61de2afb7275bc6fb1323a41bac03fec1c/diff/home/app/worker/task/cluster.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/common/conf/config-messages.properties /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vcmgmtaccountpermissiongrant/vc_mgmt_account_permission_grant.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/externalvclicensevalidator/external_vc_license_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/externalvcaccesslivevalidator/external_vc_access_live_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/externalvccapacitylimitlivevalidator/external_vc_capacity_limit_live_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostaddtovds/host_add_to_vds.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hosttimesync/host_time_sync.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/timeconfigurationlivevalidator/time_configuration_live_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostunusedsystemvmsremove/host_unused_system_vms_remove.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/externalvcversioncompatibilitylivevalidator/external_vc_version_compatibility_live_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vxmtimesync/vxm_time_sync.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/timesyncconfigurationvalidator/time_sync_configuration_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vcsyslogconfig/vc_syslog_config.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/dnshostnameslivevalidator/dns_hostnames_live_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vcuserpermissionlivevalidator/vc_user_permission_live_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/createvmfolder/create_vm_folder.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/externalvcentitieslivevalidator/external_vc_entities_live_validator.py /var/lib/docker/overlay2/6df33189ca552a920f2d6c3259a034fac3ec47f8065634a6ba17ff7c74299377/merged/home/app/messages/messages_en_US.properties /var/lib/docker/overlay2/6df33189ca552a920f2d6c3259a034fac3ec47f8065634a6ba17ff7c74299377/merged/home/app/dayone/dayone_processor.py /var/lib/docker/overlay2/6df33189ca552a920f2d6c3259a034fac3ec47f8065634a6ba17ff7c74299377/merged/home/app/dayone/api.py /var/lib/docker/overlay2/1bc6389a4238217d15b1f5d931c6049e53891a92cef4255f6b402138e130d4d9/merged/home/app/lockbox_app/dao/postgres_dao.py /var/lib/docker/overlay2/1bc6389a4238217d15b1f5d931c6049e53891a92cef4255f6b402138e130d4d9/merged/home/app/lockbox_app/resources/verify_error_messages.json Looking for the call build_call_do_service_task_url vxrailmanager:/var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vcdeploy # grep -rli 'build_call_do_service_task_url' /var/lib/docker/ 2> /dev/null /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/common/do.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/common/validation/ip_live_check_utils.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/common/network_prepare_aware_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vxmconfigurationdatapopulate/vxm_configuration_data_populate.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostaddintocluster/host_add_into_cluster.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/clusterconverttostretchedcluster/cluster_convert_to_stretched_cluster.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostpsipupdate/host_ps_ip_update.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/multidiskgroupconfigurationvalidator/multi_disk_group_configuration_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vcinitialboot/vc_initial_boot.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/clusterdasconfig/cluster_das_config.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostnetworkmanagementipchange/host_network_management_ip_change.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vcmgmtaccountpermissiongrant/vc_mgmt_account_permission_grant.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vxrailkgsbaselineload/vxm_kgs_baseline_load.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vcretreatmodeconfigure/vc_retreat_mode_configure.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/enableteaming/enableteaming.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/externalvcaccesslivevalidator/external_vc_access_live_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hosthostnamechange/host_hostname_change.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vxmupdatecertification/vxm_update_certification.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hoststorageleavevsan/host_storage_leave_vsan.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vxmconfigureslaacip/vxm_configure_slaacip.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vxmsystemservicesstop/vxm_system_services_stop.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/removedisconnecthostfromcluster/remove_disconnect_host_from_cluster.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostaddtovds/host_add_to_vds.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostassignactivepnictovds/host_assign_active_pnic_to_vds.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostassignvmktovds/host_assign_vmk_to_vds.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/clusterhighavailabilityenable/cluster_high_availability_enable.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/switchcreatenetworks/switch_create_networks.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vxmruntimepropertiesgenerate/vxm_runtime_properties_generate.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostvsanstorageenable/host_vsan_storage_enable.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hosttimesync/host_time_sync.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vcguestopsalias/vc_guest_ops_alias.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostwitnesstrafficconnectivity/host_witness_traffic_connectivity.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostvsandiskgroupscreate/host_vsan_diskgroups_create.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/createvds/createvds.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/switchconfiguplinks/switch_configure_uplinks.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/createportgroup/createportgroup.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vcpluginregister/vc_plugin_register.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vcdatacentercreate/vc_datacenter_create.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hoststoragevsandiskremove/host_storage_vsan_disk_remove.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostptagentipchange/host_ptagent_ip_change.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/customersuppliedvdsvlanlivevalidator/customer_supplied_vds_vlan_live_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostversioncompatibilitylivevalidator/host_version_compatibility_live_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/esxihostaccountvalidator/esxi_host_account_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostvsanstoragepolicyconfiguration/host_vsan_storage_policy_configuration.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostunusedsystemvmsremove/host_unused_system_vms_remove.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/switchconfigjumpbox/switch_configure_jumpbox.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vccertsgenerate/vc_certs_generate.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/clusterdrsenable/cluster_drs_enable.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vclicensing/vc_licensing.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostelectionservicedisable/host_election_service_disable.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/clusteraddexceptionaccounts/cluster_add_exception_accounts.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vcpasswordexpirationdisable/vc_password_expiration_disable.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vxmnetworkvnicremove/vxm_network_vnic_remove.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/externalvcversioncompatibilitylivevalidator/external_vc_version_compatibility_live_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/omeintegration/ome_integration.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostmarkpnicworkingmode/host_mark_pnic_working_mode.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/switchcreatedownlinks/switch_create_downlinks.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vxmevopassword/vxm_evo_password.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/movevxmvnictovds/vxm_network_vnic_add_to_vds.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostloudmouthrestart/host_loudmouth_restart.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hoststorageaddtovsan/host_storage_add_to_vsan.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostloggingsystemconfigure/host_logging_system_configure.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/clusteradvanceparamssetting/cluster_advance_params_setting.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/resetvspherealarms/reset_vsphere_alarms.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/networkcompatibilitylivevalidator/network_compatibility_live_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/switchauthentication/switch_authentication.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vxmtimesync/vxm_time_sync.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostsecurityservicedisable/host_security_service_disable.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/timesyncconfigurationvalidator/time_sync_configuration_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vxmstaticipchange/vxm_static_ipchange.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/createstoragepolicy/create_storage_policy.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vxmclustercleanup/vxm_cluster_clean_up.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/switchchangepassword/switch_change_password.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostplatformservicerestart/host_platform_service_restart.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostdnschange/host_dns_change.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vxmautostartenable/vxm_auto_start_enable.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vlanlivevalidator/vlan_live_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vxmserviceaccountvalidator/vxm_service_account_validator_task.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vxmhostnameset/vxm_hostname_set.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vcautostartenable/vc_auto_start_enable.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vcdeploy/vc_deploy.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostwitnessnetworking/host_witness_networking.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostmanagementaccountcreate/host_management_account_create.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/enablenioc/enablenioc.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vcsyslogconfig/vc_syslog_config.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vxmsystemservicesrestart/vxm_system_services_restart.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/checknodestatus/check_node_status.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vxmmovetostorage/vxm_move_to_storage.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostassignpnictovss/host_assign_pnic_to_vss.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vcmgmntaccountcreate/vc_mgmnt_account_create.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/clusterevcenable/cluster_evc_enable.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vxmsetdns/vxm_set_dns.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/cursoryvalidator/cursory_validator.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostvssremove/host_vss_remove.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/clustercreate/clustercreate.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostaccountpasswordchange/host_account_password_change.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vxmrootaccountvalidator/vxm_root_account_validator_task.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostproxyservicedisable/host_proxy_service_disable.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/clustersystemvmstoragepolicyapply/cluster_system_vm_storage_policy_apply.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostassignstandbypnictovds/host_assign_standby_pnic_to_vds.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/createvmfolder/create_vm_folder.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/postdayoneconfiguration/post_dayone_configuration.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/hostnetworkmanagementvlanchange/host_network_management_vlan_change.py /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vxmupdateinternaldns/vxm_update_internal_dns.py /var/lib/docker/overlay2/6df33189ca552a920f2d6c3259a034fac3ec47f8065634a6ba17ff7c74299377/merged/home/app/dayone/do.py /var/lib/docker/overlay2/6df33189ca552a920f2d6c3259a034fac3ec47f8065634a6ba17ff7c74299377/merged/home/app/dayone/transfer_handler.py /var/lib/docker/overlay2/6df33189ca552a920f2d6c3259a034fac3ec47f8065634a6ba17ff7c74299377/merged/home/app/dayone/storage_discovery_callback.py /var/lib/docker/overlay2/6df33189ca552a920f2d6c3259a034fac3ec47f8065634a6ba17ff7c74299377/merged/home/app/vxm_ip_change_scheduler.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vxmconfigurationdatapopulate/vxm_configuration_data_populate.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hostaddintocluster/host_add_into_cluster.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/clusterconverttostretchedcluster/cluster_convert_to_stretched_cluster.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hostpsipupdate/host_ps_ip_update.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/multidiskgroupconfigurationvalidator/multi_disk_group_configuration_validator.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vcinitialboot/vc_initial_boot.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/clusterdasconfig/cluster_das_config.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hostnetworkmanagementipchange/host_network_management_ip_change.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vcmgmtaccountpermissiongrant/vc_mgmt_account_permission_grant.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vxrailkgsbaselineload/vxm_kgs_baseline_load.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vcretreatmodeconfigure/vc_retreat_mode_configure.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/enableteaming/enableteaming.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/externalvcaccesslivevalidator/external_vc_access_live_validator.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hosthostnamechange/host_hostname_change.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vxmupdatecertification/vxm_update_certification.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hoststorageleavevsan/host_storage_leave_vsan.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vxmconfigureslaacip/vxm_configure_slaacip.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vxmsystemservicesstop/vxm_system_services_stop.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/removedisconnecthostfromcluster/remove_disconnect_host_from_cluster.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hostaddtovds/host_add_to_vds.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hostassignactivepnictovds/host_assign_active_pnic_to_vds.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hostassignvmktovds/host_assign_vmk_to_vds.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/clusterhighavailabilityenable/cluster_high_availability_enable.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/switchcreatenetworks/switch_create_networks.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vxmruntimepropertiesgenerate/vxm_runtime_properties_generate.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hostvsanstorageenable/host_vsan_storage_enable.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hosttimesync/host_time_sync.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vcguestopsalias/vc_guest_ops_alias.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hostwitnesstrafficconnectivity/host_witness_traffic_connectivity.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hostvsandiskgroupscreate/host_vsan_diskgroups_create.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/createvds/createvds.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/switchconfiguplinks/switch_configure_uplinks.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/createportgroup/createportgroup.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vcpluginregister/vc_plugin_register.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vcdatacentercreate/vc_datacenter_create.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hoststoragevsandiskremove/host_storage_vsan_disk_remove.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hostptagentipchange/host_ptagent_ip_change.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/customersuppliedvdsvlanlivevalidator/customer_supplied_vds_vlan_live_validator.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hostversioncompatibilitylivevalidator/host_version_compatibility_live_validator.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/esxihostaccountvalidator/esxi_host_account_validator.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hostvsanstoragepolicyconfiguration/host_vsan_storage_policy_configuration.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hostunusedsystemvmsremove/host_unused_system_vms_remove.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/switchconfigjumpbox/switch_configure_jumpbox.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vccertsgenerate/vc_certs_generate.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/clusterdrsenable/cluster_drs_enable.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vclicensing/vc_licensing.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hostelectionservicedisable/host_election_service_disable.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/clusteraddexceptionaccounts/cluster_add_exception_accounts.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vcpasswordexpirationdisable/vc_password_expiration_disable.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vxmnetworkvnicremove/vxm_network_vnic_remove.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/externalvcversioncompatibilitylivevalidator/external_vc_version_compatibility_live_validator.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/omeintegration/ome_integration.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hostmarkpnicworkingmode/host_mark_pnic_working_mode.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/switchcreatedownlinks/switch_create_downlinks.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vxmevopassword/vxm_evo_password.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/movevxmvnictovds/vxm_network_vnic_add_to_vds.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hostloudmouthrestart/host_loudmouth_restart.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hoststorageaddtovsan/host_storage_add_to_vsan.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hostloggingsystemconfigure/host_logging_system_configure.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/clusteradvanceparamssetting/cluster_advance_params_setting.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/resetvspherealarms/reset_vsphere_alarms.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/networkcompatibilitylivevalidator/network_compatibility_live_validator.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/switchauthentication/switch_authentication.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vxmtimesync/vxm_time_sync.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hostsecurityservicedisable/host_security_service_disable.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/timesyncconfigurationvalidator/time_sync_configuration_validator.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vxmstaticipchange/vxm_static_ipchange.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/createstoragepolicy/create_storage_policy.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vxmclustercleanup/vxm_cluster_clean_up.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/common/do.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/common/validation/ip_live_check_utils.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/common/network_prepare_aware_validator.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/switchchangepassword/switch_change_password.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hostplatformservicerestart/host_platform_service_restart.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hostdnschange/host_dns_change.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vxmautostartenable/vxm_auto_start_enable.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vlanlivevalidator/vlan_live_validator.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vxmserviceaccountvalidator/vxm_service_account_validator_task.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vxmhostnameset/vxm_hostname_set.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vcautostartenable/vc_auto_start_enable.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vcdeploy/vc_deploy.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hostwitnessnetworking/host_witness_networking.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hostmanagementaccountcreate/host_management_account_create.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/enablenioc/enablenioc.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vcsyslogconfig/vc_syslog_config.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vxmsystemservicesrestart/vxm_system_services_restart.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/checknodestatus/check_node_status.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vxmmovetostorage/vxm_move_to_storage.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hostassignpnictovss/host_assign_pnic_to_vss.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vcmgmntaccountcreate/vc_mgmnt_account_create.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/clusterevcenable/cluster_evc_enable.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vxmsetdns/vxm_set_dns.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/cursoryvalidator/cursory_validator.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hostvssremove/host_vss_remove.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/clustercreate/clustercreate.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hostaccountpasswordchange/host_account_password_change.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vxmrootaccountvalidator/vxm_root_account_validator_task.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hostproxyservicedisable/host_proxy_service_disable.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/clustersystemvmstoragepolicyapply/cluster_system_vm_storage_policy_apply.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hostassignstandbypnictovds/host_assign_standby_pnic_to_vds.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/createvmfolder/create_vm_folder.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/postdayoneconfiguration/post_dayone_configuration.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/hostnetworkmanagementvlanchange/host_network_management_vlan_change.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/vxmupdateinternaldns/vxm_update_internal_dns.py /var/lib/docker/overlay2/0d8ac81dbb9015f2f3526cd468c2c4e7e433661e38008b04f70539deee19272a/diff/home/app/dayone/do.py /var/lib/docker/overlay2/0d8ac81dbb9015f2f3526cd468c2c4e7e433661e38008b04f70539deee19272a/diff/home/app/dayone/transfer_handler.py /var/lib/docker/overlay2/0d8ac81dbb9015f2f3526cd468c2c4e7e433661e38008b04f70539deee19272a/diff/home/app/dayone/storage_discovery_callback.py /var/lib/docker/overlay2/02a3e09ef808a5f704f821bbf2fcc89ece02f262e0a20b1c5a599fbf949a8c9b/diff/home/app/vxm_ip_change_scheduler.py Finding the definition: vxrailmanager:/var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vcdeploy # grep -rli 'def build_call_do_service_task_url' /var/lib/docker/ 2> /dev/null /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/common/do.py /var/lib/docker/overlay2/6df33189ca552a920f2d6c3259a034fac3ec47f8065634a6ba17ff7c74299377/merged/home/app/dayone/do.py /var/lib/docker/overlay2/6df33189ca552a920f2d6c3259a034fac3ec47f8065634a6ba17ff7c74299377/merged/home/app/vxm_ip_change_scheduler.py /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/common/do.py /var/lib/docker/overlay2/0d8ac81dbb9015f2f3526cd468c2c4e7e433661e38008b04f70539deee19272a/diff/home/app/dayone/do.py /var/lib/docker/overlay2/02a3e09ef808a5f704f821bbf2fcc89ece02f262e0a20b1c5a599fbf949a8c9b/diff/home/app/vxm_ip_change_scheduler.py There seems to be a file for licensing: Find embedded license: vxrailmanager:/var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vcdeploy # find / -iname license-vc-700-e1-marvin-c1-201909 \\/usr/lib/vmware-marvin/marvind/webapps/lcm/WEB-INF/classes/licensing/7.0/license-vc-700-e1-marvin-c1-201909 /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/common/conf/license-vc-700-e1-marvin-c1-201909 /var/lib/docker/overlay2/eabf8f9430b1c90892414b04cd2934b09bbbadf357b3e97a6e4546b5beb8a192/diff/home/app/common/conf/license-vc-700-e1-marvin-c1-201909 vxrailmanager:/var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/vcdeploy # cat /var/lib/docker/overlay2/de1ff478dd295cc17b5a7f2da36c356a893a18f10ed215c284f1529c19adf5da/merged/home/app/common/conf/license-vc-700-e1-marvin-c1-201909 # VMware software license StartFields = \"Cpt, ProductID, LicenseVersion, LicenseType, LicenseEdition, Option, Epoch\" Cpt = \"COPYRIGHT (c) VMware, Inc.\" ProductID = \"VMware VirtualCenter Server\" LicenseVersion = \"7.0\" LicenseType = \"Site\" Epoch = \"2019-9-1\" LicenseEdition = \"vc.standard.instance.marvin\" Option = \"7\" Data = \"FileVersion=7.0.0.2;capacityType=server;enable=linkedvc,woe,xvp,vcha,backuprestore,appliancemigration;addon=vsa;disallowedHostEditions=esx.hypervisor.cpuPackage,esx.hypervisorEmbeddedOem.cpuPackage,esx.essentials.cpuPackage,esx.essentialsPlus.cpuPackage,esx.hypervisor.cpuPackageCoreLimited,esx.hypervisorEmbeddedOem.cpuPackageCoreLimited,esx.essentials.cpuPackageCoreLimited,esx.essentialsPlus.cpuPackageCoreLimited;desc=vCenter Server 7 Standard\" DataHash = \"60aebea4-31a0050c-c4e49494-27690fad-5a8a5258\" Hash = \"441d0833-d2f3bed9-0bea3fec-7d13664b-e222d862\"","title":"vCenter Deploy"},{"location":"VMWare/Troubleshooting%20vSAN/","text":"Troubleshooting vSAN Troubleshooting vSAN Helpful Resources vSAN Support Insight Overall Approach Technical Approach Identify Using vSAN Observer Running from a remote machine Running vSAN Observer without Internet Access vSAN Observer Case Studies Anatomy of a Write/Read Operation in Hybrid Config Helpful Commands Networking Determine Disk Group Assignmnts Check Failed Disks Determine what drives are attached to which storage adapters Get Detailed Version Simple List Interpreting the Output List SCSI devices and their sizes Get a list of all PCI devices (allows you to check for rebranding) Determine Adapter Driver and Version Determine Driver Parameters Get a list of VIBs Check failures to tolerate and stripe Check what the fault domains are Inspect a Specific Object (VM) in vSAN Understanding Drive Types VCG Check Summary Absent vs Degraded Failure Object Compliance Status: Compliant vs Non Compliant Object Operational State: Healthy vs Unhealthy VM Accessibility: Inaccessible vs Orphaned TODO Helpful Resources Virtual SAN Diagnostics and Troubleshooting Reference Manual - Outside of what I have recorded here, chapter 7 - Understanding expected failure behavior may be useful. It covers what you'll see in the logs, UI, etc when different types of failure occur. It also describes things like what happens when you plug in a new disk in both the capacity and cache tiers. If you don't know how to handle a problem this is the place to look. I have a lot of the highlights below but there are a lot of scenarios they cover that I don't. VMware Ruby vSphere Console Command Reference for Virtual SAN Troubleshooting vSAN Performance Monitoring with vSAN Observer - Covers interpreting and understanding all the functionality of vSAN observer. My Notes on vSAN - I documented how a number of vSAN features function and the variosu highlights. vSAN Support Insight Overall Approach vSAN observer vSAN health test Time/resource permitting vSAN HCI benchmark I'll also do a general network survey to make sure I understand the physical and logical layout paying particular attention to the various VLANs https://core.vmware.com/resource/troubleshooting-vsan-performance Identify and quantify. This step helps to clearly define the issue. Clarifying questions can help properly qualify the problem statement, which will allow for a more targeted approach to addressing. This process helps sort out real versus perceived issues, and focuses on the end result and supporting symptoms, without implying the cause of the issue. Discovery/Review - Environment. This step takes a review of the current configuration. This will help eliminate previously unnoticed basic configuration or topology issues that might be plaguing the environment. Discovery/Review - Workload. This step will help the reader review the applications and workflows. This will help a virtualization administrator better understand what the application is attempting to perform, and why. Performance Metrics - Insight. This step will review some of the key performance metrics to view, and how to interpret some of the findings the reader may see when observing their workloads. It clarifies what the performance metrics means, and how they relate to each other.. Mitigation - Options in potential software and hardware changes. This step will help the reader step through the potential actions for mitigation. Technical Approach Potential Bottlenecks: https://core.vmware.com/blog/understanding-performance-bottlenecks Identify Check VxRail version Perform a network survey and understand what switches and devices are in between each of the hosts Verify everything is licensed Hosts that are participating in the vSAN cluster but not providing storage still require a license Check that the flash cache to capacity ratio is 1:10 Check cluster health with esxcli vsan health cluster list Cross reference in RVC with vsan.cluster_info <target> . This is good for getting a big picture view of what is going on. Check the overall state of the cluster in RVC with vsan.check_state <target> . This will check for inaccessible vSAN objects, invalid/inaccessible VMs, VMs for which VC/hostd/vmx are out of sync. Inaccessible vSAN Objects : Inaccessible Virtual SAN objects are an indication that there is probably a failure somewhere in the cluster, but that Virtual SAN is still able to track the virtual machine. An invalid or inaccessible object is when the VM has objects that have lost the majority of its components or votes, again due to hardware failures. Note that for a VM\u2019s object to be accessible, it must have a full, intact mirror and greater than 50% of its components/votes available. Invalid/inaccessible VMs : The next check is for invalid or inaccessible VMs. These are VMs that, most likely due to the fact that the failure(s) that have occurred in the cluster, have been impacted so much that it is no longer accessible by the vCenter server or the ESXi hosts. This is likely be due to the fact that the VM Home Namespace, where the .vmx file resides, is no longer online. Common causes are clusters that have had multiple failures, but the virtual machines have been configured to tolerate only one failure, or network outages. Check that vCenter and hosts are in sync : The final check just makes sure vCenter Server (VC) and hosts agree on the state of the cluster. Use RVC to check that the cluster is operating within limits with vsan.check_limits From a network perspective the most important things to check are associations (Assocs) and the the socket count. Associations track peer-to-peer network state within the vSAN and you should not run out (max 45k). Sockts are used for various things. The max is 10,000. Check that the cluster has enough resources to tolerate a failure with RVC command vsan.whatif_host_failures RC reservations means Read Cache reservations and is only used in hybrid configurations. You can dedicate a certain amount of read cache to a specific VM. HDD capacity only refers to the capacity tier. Check host connectivity Note it may be helpful if you need to identify where a host is plugged in to use LLDP with the RVC command vsan.lldpnetmap localhost/datacenter/computers/vSAN\\ Cluster/ Check cluster ping with vmkping -I <vmk_interface> <target_ip> Check to make sure all hosts are on the same network segment with esxcli network ip neighbor list Check round trip time with esxcli network diag ping Check to make sure multicast is working and a host is receiving heartbeats with tcpdump-uw \u2013i <your_vmk> udp port 23451 \u2013v -s0 Each ESXi node in the vSAN cluster will send out IGMP membership reports (aka joins) every 90-300 seconds. Check for receipt with tcpdump-uw -i <yourvmk> igmp Make sure the following ports are accessible through any firewalls: Use iperf to get a baseline idea of the network bandwidth available. This will cause degredation of performance! Most useful for initial setup. See this KB article You can also use RVC's vsan.vm_perf_stats command to get a feel for the performance. Ex vsan.vm_perf_stats ~/vms/W2k12-SQL2k12 --interval 10 --show-objects . IOPS = (MBps Throughput / KB per IO) * 1024. MBps = (IOPS * KB per IO) / 1024 NOTE : Virtual SAN Diagnostics and Troubleshooting Reference Manual covers a lot of other scenarios I didn't here. Consider Kick off vSAN Observer. See Using vSAN Observer . Details on using vSAN observer available in this PDF . It is also covered starting on page 197 in Virtual SAN Diagnostics and Troubleshooting Reference Manual Check the latency as seen by a guest VM running the application of interest (check on the VMs tab) VM Home - This is where the VM's configuration file, log files, and other VM related small files are stored. The RAID tree subsection shows the different component owners for VM Home. Virtual Disk - This shows the different virtual disks attached to the VM. Each disk displas stats specific to that disk. You can drill down to individual virtual disks. The VSCSI layer shows the aggregate latency, IOPS and throughput numbers for a particular virtual disk of a VM. NOTE : On the various full graphs you may see RecovWrite - these are the number of writes that are being used for component rebuilds. Note that this metric will be 0 for vSAN clients because they do not have visibility into rebuild I/O - DOM does that. You will also see Write LE - this refers to write log entry. For every write there are actually two I/Os: the actual data and a log entry. Check for high outstanding IOPs (vSAN client tab and vSAN disks). On the vSAN disks tab make sure that outstanding IO is well balanced across the hosts. A high outstanding IO may be something like 200. On the vSAN disks tab look for a high number of evictions. These occur when vSAN has to evict data blocks from the write buffer to make room for new data blocks. In an optimized system, the working data for an application should mostly reside in write cache. We should not see too many evictions. Maybe this workload is not suitable? Check for high latencies (time it takes to complete on I/O operation from application viewpoint). (vSAN client tab and vSAN disk tab). Consier that if the latency in the vSAN client tab is much higher than the disk tab than it is more likely the network is the problem. Make sure that what we see on the vSAN client tab correspondings to what is on the vSAN disk tabs Common causes of high latency: Large average I/O sizes, which leads to increased latencies Large number of writes Large number of I/Os Slow SSD that isn't keeping up Too many random reads causing cache misses in the SSD. Latency formula: outstanding IOs / drive max write or I/O = x ms If we kick off a lot of read ops on something we generally expect there to a spike in latency followed by a drop as things are cached (assuming the same thing is being read) The standard deviation graph is telling you the frequency you are outside a single standard deviation Check bandwidth utilization Lots of small I/Os could cause you to hit I/O ceiling before bandwidth Large I/Os may exhaust bandwidth Check buffer utilization. You can see this on the client and disk tabs. On the deep dive tab you can check RC hit rate for the various hosts. If we are seeing a lot of misse on the read cache this may indicate the cache isn't large enough and we expect to see a spike in hits against our capacity drives. Check PCPU utilization. It isn't uncommon to see 30-50% utilization when under I/O load. Sustained high CPU utilization could indicate a problem. If you have problems but don't expect them, make sure that if the server has a power management setting that it isn't set to a lower performance setting Check memory consumption paying specific attention to congestion. The vmkernel log will commonly display this error in the event of a memory shortage: Or this error when trying to add a disk group: An error will also be displayed in the GUI: Check distribution of components . The line should be uniform indicating roughly equal distribution of components Check that the storage controller is supported and running normaly with esxcli storage core device list . Compare to VCG. You can cross reference this with esxcli core storage adapter list and esxcfg-scsidevs -a You may see the word degraded - this occurs when there is only a single path to teh device. If there are multiple paths this will not show. This is not an issue for local disk configurations. If you see Logical Volume in the model field it implies there is a RAID volume configuration on the disk - probably RAID0. Certain storage controllers can pass disk devices directly to the ESXi host; this is called pass-through mode. You may also see it called JBOD or HBA mode. Other storage controllers require each disk device be configured as a RAID0 volume before ESXi can recognize it. You can check the VCG to determine whether something supports pass through or not: Double check the VCG! If a controller is supported in RAID0 mode only, it may still work in pass-through mode but the disks will frequently error. Make sure the storage adapter is supported and the driver is up to date. See Get a list of all PCI devices Review the VCG check summary and make sure everything is good to go If you are having general performace issues with vSAN you can go to a host seeing the issues and check the controller queue depth with esxtop . You can use the D option to see the queue stats. Hit enter to return to the general view and then you can check AQLEN. ESXi requires a depth greater than 256. You can also check it with the command esxcfg-info \u2013s | grep \u201c==+SCSI Interface\u201d \u2013A 1 Disable controller caching. This is the VMWare recommended setting. If you can't disable it try setting the cache to 100% read. Make sure all disks that are meant to be claimed by vSAN are with the command esxcli vsan storage list . Make sure in the output that CMMDS is set to true. This implies that the cluster membership and directory services know about the disk in question and that the capacity of the disk is contributing to the capactiy of the vSAN datastore. Using vSAN Observer SSH to vCenter and then run rvc administrator@vsphere.lan@localhost vsan.observer /localhost/datacenter/computers/vSAN\\ Cluster/ --run-webserver --force Go to https://vcenter.lan:8010/ (it must be https) You can also write the output to a JSON file. If you only have the JSON output you can produce static HTML with the vsan.observer_process_statsfile <stats_file> <output_folder> Running from a remote machine Install Ruby: https://rubyinstaller.org/downloads/ Install Ruby vSphere Console: https://rubydoc.info/gems/rvc/1.6.0 gem install rvc Running vSAN Observer without Internet Access See page 198 of Virtual SAN Diagnostics and Troubleshooting Reference Manual vSAN Observer Case Studies Starts on page 262 of Virtual SAN Diagnostics and Troubleshooting Reference Manual Anatomy of a Write/Read Operation in Hybrid Config For more details see page 245 of Virtual SAN Diagnostics and Troubleshooting Reference Manual Helpful Commands Networking esxcli network ip interface list esxcli network ip interface ipv4 get esxcli network ip neighbor list # view ARP table esxcli vsan network list # Determine which vmk interface is used for vSAN esxcli network nic list # List the physical interfaces # This can be run at the cluster level and also gives vmk interface for vSAN vsan.cluster_info <vSAN cluster> # Use LLDP to obtain upstream switch info vsan.lldpnetmap localhost/datacenter/computers/vSAN\\ Cluster/ Determine Disk Group Assignmnts Check Failed Disks esxcli storage core device stats get Determine what drives are attached to which storage adapters Get Detailed Version esxcli storage core path list Simple List esxcfg-scsidevs \u2013A Interpreting the Output NAA stands for Network Addressing Authority identifier. EUI stands for Extended Unique Identifier. The number is guaranteed to be unique to that LUN. The NAA or EUI identifier is the preferred method of identifying LUNs and the storage device generates the number. Since the NAA or EUI is unique to the LUN, if the LUN is presented the same way across all ESXi hosts, the NAA or EUI identifier remains the same. Some devices do not provide the NAA number described above. In these circumstances, an MPX Identifier is generated by ESXi to represent the LUN or disk. The identifier takes the form similar to that of the canonical name of previous versions of ESXi with the \u201cmpx.\u201d prefix. This identifier can be used in the exact same way as the NAA Identifier described above.\u201d Refer to VMware KB article 1014953 for further information. List SCSI devices and their sizes esxcfg-scsidevs \u2013c Get a list of all PCI devices (allows you to check for rebranding) You an use a combination of the vendor IDs, device IDs, sub-vendor IDs, and sub-device IDs, to verify an adapter is on the VCG. Some vendors rebrand devices to reflect their own range of products however, the vendor and device ids remain the same. It is not uncommon to find that one vendor's controller is not listed under the vendor name in the VCG but is actually listed under another vendor. esxcli hardware pci list Determine Adapter Driver and Version Module Name indicates the driver that is currently in use. You can use the vmkload_mod -s <driver name> or esxcli system module get -m command to determine the driver version. Check that against the VCG; as bugs arise the supported drivers can change. You can also check the adapters with vsan.disks_info localhost/datacenter/computers/vSAN\\ Cluster/hosts/vsan1.lan --show-adapters in RVC. Determine Driver Parameters vmkload_mod -s <driver name> Get a list of VIBs esxcli software vib list If someone built using a custom ISO, it is common that they may have upgraded the driver to too high a version and that this version is unsupported. Check failures to tolerate and stripe You can see this on a per object level in vsan.object_info Check what the fault domains are vsan.cluster_info computers/vSAN\\ Cluster/ Inspect a Specific Object (VM) in vSAN vsan.vm_object_info <target> Understanding Drive Types Flash devices are placed in classes. Performance classes are measured in writes per second (not sure if this is up to date): Endurance is measured in Terabytes Written. Drives are qualified by the number of terabytes they are capable of writing per some time period per the vendor. VCG Check Summary Absent vs Degraded Failure Object Compliance Status: Compliant vs Non Compliant Object Operational State: Healthy vs Unhealthy VM Accessibility: Inaccessible vs Orphaned TODO Review P141 - Verifying Virtual SAN storage operation - RVC in Virtual SAN Diagnostics and Troubleshooting Reference Manual Review P152 - Testing vSAN functionality - deploying VMs in Virtual SAN Diagnostics and Troubleshooting Reference Manual P156 of Virtual SAN Diagnostics and Troubleshooting Reference Manual","title":"Troubleshooting vSAN"},{"location":"VMWare/Troubleshooting%20vSAN/#troubleshooting-vsan","text":"Troubleshooting vSAN Helpful Resources vSAN Support Insight Overall Approach Technical Approach Identify Using vSAN Observer Running from a remote machine Running vSAN Observer without Internet Access vSAN Observer Case Studies Anatomy of a Write/Read Operation in Hybrid Config Helpful Commands Networking Determine Disk Group Assignmnts Check Failed Disks Determine what drives are attached to which storage adapters Get Detailed Version Simple List Interpreting the Output List SCSI devices and their sizes Get a list of all PCI devices (allows you to check for rebranding) Determine Adapter Driver and Version Determine Driver Parameters Get a list of VIBs Check failures to tolerate and stripe Check what the fault domains are Inspect a Specific Object (VM) in vSAN Understanding Drive Types VCG Check Summary Absent vs Degraded Failure Object Compliance Status: Compliant vs Non Compliant Object Operational State: Healthy vs Unhealthy VM Accessibility: Inaccessible vs Orphaned TODO","title":"Troubleshooting vSAN"},{"location":"VMWare/Troubleshooting%20vSAN/#helpful-resources","text":"Virtual SAN Diagnostics and Troubleshooting Reference Manual - Outside of what I have recorded here, chapter 7 - Understanding expected failure behavior may be useful. It covers what you'll see in the logs, UI, etc when different types of failure occur. It also describes things like what happens when you plug in a new disk in both the capacity and cache tiers. If you don't know how to handle a problem this is the place to look. I have a lot of the highlights below but there are a lot of scenarios they cover that I don't. VMware Ruby vSphere Console Command Reference for Virtual SAN Troubleshooting vSAN Performance Monitoring with vSAN Observer - Covers interpreting and understanding all the functionality of vSAN observer. My Notes on vSAN - I documented how a number of vSAN features function and the variosu highlights.","title":"Helpful Resources"},{"location":"VMWare/Troubleshooting%20vSAN/#vsan-support-insight","text":"","title":"vSAN Support Insight"},{"location":"VMWare/Troubleshooting%20vSAN/#overall-approach","text":"vSAN observer vSAN health test Time/resource permitting vSAN HCI benchmark I'll also do a general network survey to make sure I understand the physical and logical layout paying particular attention to the various VLANs https://core.vmware.com/resource/troubleshooting-vsan-performance Identify and quantify. This step helps to clearly define the issue. Clarifying questions can help properly qualify the problem statement, which will allow for a more targeted approach to addressing. This process helps sort out real versus perceived issues, and focuses on the end result and supporting symptoms, without implying the cause of the issue. Discovery/Review - Environment. This step takes a review of the current configuration. This will help eliminate previously unnoticed basic configuration or topology issues that might be plaguing the environment. Discovery/Review - Workload. This step will help the reader review the applications and workflows. This will help a virtualization administrator better understand what the application is attempting to perform, and why. Performance Metrics - Insight. This step will review some of the key performance metrics to view, and how to interpret some of the findings the reader may see when observing their workloads. It clarifies what the performance metrics means, and how they relate to each other.. Mitigation - Options in potential software and hardware changes. This step will help the reader step through the potential actions for mitigation.","title":"Overall Approach"},{"location":"VMWare/Troubleshooting%20vSAN/#technical-approach","text":"Potential Bottlenecks: https://core.vmware.com/blog/understanding-performance-bottlenecks","title":"Technical Approach"},{"location":"VMWare/Troubleshooting%20vSAN/#identify","text":"Check VxRail version Perform a network survey and understand what switches and devices are in between each of the hosts Verify everything is licensed Hosts that are participating in the vSAN cluster but not providing storage still require a license Check that the flash cache to capacity ratio is 1:10 Check cluster health with esxcli vsan health cluster list Cross reference in RVC with vsan.cluster_info <target> . This is good for getting a big picture view of what is going on. Check the overall state of the cluster in RVC with vsan.check_state <target> . This will check for inaccessible vSAN objects, invalid/inaccessible VMs, VMs for which VC/hostd/vmx are out of sync. Inaccessible vSAN Objects : Inaccessible Virtual SAN objects are an indication that there is probably a failure somewhere in the cluster, but that Virtual SAN is still able to track the virtual machine. An invalid or inaccessible object is when the VM has objects that have lost the majority of its components or votes, again due to hardware failures. Note that for a VM\u2019s object to be accessible, it must have a full, intact mirror and greater than 50% of its components/votes available. Invalid/inaccessible VMs : The next check is for invalid or inaccessible VMs. These are VMs that, most likely due to the fact that the failure(s) that have occurred in the cluster, have been impacted so much that it is no longer accessible by the vCenter server or the ESXi hosts. This is likely be due to the fact that the VM Home Namespace, where the .vmx file resides, is no longer online. Common causes are clusters that have had multiple failures, but the virtual machines have been configured to tolerate only one failure, or network outages. Check that vCenter and hosts are in sync : The final check just makes sure vCenter Server (VC) and hosts agree on the state of the cluster. Use RVC to check that the cluster is operating within limits with vsan.check_limits From a network perspective the most important things to check are associations (Assocs) and the the socket count. Associations track peer-to-peer network state within the vSAN and you should not run out (max 45k). Sockts are used for various things. The max is 10,000. Check that the cluster has enough resources to tolerate a failure with RVC command vsan.whatif_host_failures RC reservations means Read Cache reservations and is only used in hybrid configurations. You can dedicate a certain amount of read cache to a specific VM. HDD capacity only refers to the capacity tier. Check host connectivity Note it may be helpful if you need to identify where a host is plugged in to use LLDP with the RVC command vsan.lldpnetmap localhost/datacenter/computers/vSAN\\ Cluster/ Check cluster ping with vmkping -I <vmk_interface> <target_ip> Check to make sure all hosts are on the same network segment with esxcli network ip neighbor list Check round trip time with esxcli network diag ping Check to make sure multicast is working and a host is receiving heartbeats with tcpdump-uw \u2013i <your_vmk> udp port 23451 \u2013v -s0 Each ESXi node in the vSAN cluster will send out IGMP membership reports (aka joins) every 90-300 seconds. Check for receipt with tcpdump-uw -i <yourvmk> igmp Make sure the following ports are accessible through any firewalls: Use iperf to get a baseline idea of the network bandwidth available. This will cause degredation of performance! Most useful for initial setup. See this KB article You can also use RVC's vsan.vm_perf_stats command to get a feel for the performance. Ex vsan.vm_perf_stats ~/vms/W2k12-SQL2k12 --interval 10 --show-objects . IOPS = (MBps Throughput / KB per IO) * 1024. MBps = (IOPS * KB per IO) / 1024 NOTE : Virtual SAN Diagnostics and Troubleshooting Reference Manual covers a lot of other scenarios I didn't here. Consider Kick off vSAN Observer. See Using vSAN Observer . Details on using vSAN observer available in this PDF . It is also covered starting on page 197 in Virtual SAN Diagnostics and Troubleshooting Reference Manual Check the latency as seen by a guest VM running the application of interest (check on the VMs tab) VM Home - This is where the VM's configuration file, log files, and other VM related small files are stored. The RAID tree subsection shows the different component owners for VM Home. Virtual Disk - This shows the different virtual disks attached to the VM. Each disk displas stats specific to that disk. You can drill down to individual virtual disks. The VSCSI layer shows the aggregate latency, IOPS and throughput numbers for a particular virtual disk of a VM. NOTE : On the various full graphs you may see RecovWrite - these are the number of writes that are being used for component rebuilds. Note that this metric will be 0 for vSAN clients because they do not have visibility into rebuild I/O - DOM does that. You will also see Write LE - this refers to write log entry. For every write there are actually two I/Os: the actual data and a log entry. Check for high outstanding IOPs (vSAN client tab and vSAN disks). On the vSAN disks tab make sure that outstanding IO is well balanced across the hosts. A high outstanding IO may be something like 200. On the vSAN disks tab look for a high number of evictions. These occur when vSAN has to evict data blocks from the write buffer to make room for new data blocks. In an optimized system, the working data for an application should mostly reside in write cache. We should not see too many evictions. Maybe this workload is not suitable? Check for high latencies (time it takes to complete on I/O operation from application viewpoint). (vSAN client tab and vSAN disk tab). Consier that if the latency in the vSAN client tab is much higher than the disk tab than it is more likely the network is the problem. Make sure that what we see on the vSAN client tab correspondings to what is on the vSAN disk tabs Common causes of high latency: Large average I/O sizes, which leads to increased latencies Large number of writes Large number of I/Os Slow SSD that isn't keeping up Too many random reads causing cache misses in the SSD. Latency formula: outstanding IOs / drive max write or I/O = x ms If we kick off a lot of read ops on something we generally expect there to a spike in latency followed by a drop as things are cached (assuming the same thing is being read) The standard deviation graph is telling you the frequency you are outside a single standard deviation Check bandwidth utilization Lots of small I/Os could cause you to hit I/O ceiling before bandwidth Large I/Os may exhaust bandwidth Check buffer utilization. You can see this on the client and disk tabs. On the deep dive tab you can check RC hit rate for the various hosts. If we are seeing a lot of misse on the read cache this may indicate the cache isn't large enough and we expect to see a spike in hits against our capacity drives. Check PCPU utilization. It isn't uncommon to see 30-50% utilization when under I/O load. Sustained high CPU utilization could indicate a problem. If you have problems but don't expect them, make sure that if the server has a power management setting that it isn't set to a lower performance setting Check memory consumption paying specific attention to congestion. The vmkernel log will commonly display this error in the event of a memory shortage: Or this error when trying to add a disk group: An error will also be displayed in the GUI: Check distribution of components . The line should be uniform indicating roughly equal distribution of components Check that the storage controller is supported and running normaly with esxcli storage core device list . Compare to VCG. You can cross reference this with esxcli core storage adapter list and esxcfg-scsidevs -a You may see the word degraded - this occurs when there is only a single path to teh device. If there are multiple paths this will not show. This is not an issue for local disk configurations. If you see Logical Volume in the model field it implies there is a RAID volume configuration on the disk - probably RAID0. Certain storage controllers can pass disk devices directly to the ESXi host; this is called pass-through mode. You may also see it called JBOD or HBA mode. Other storage controllers require each disk device be configured as a RAID0 volume before ESXi can recognize it. You can check the VCG to determine whether something supports pass through or not: Double check the VCG! If a controller is supported in RAID0 mode only, it may still work in pass-through mode but the disks will frequently error. Make sure the storage adapter is supported and the driver is up to date. See Get a list of all PCI devices Review the VCG check summary and make sure everything is good to go If you are having general performace issues with vSAN you can go to a host seeing the issues and check the controller queue depth with esxtop . You can use the D option to see the queue stats. Hit enter to return to the general view and then you can check AQLEN. ESXi requires a depth greater than 256. You can also check it with the command esxcfg-info \u2013s | grep \u201c==+SCSI Interface\u201d \u2013A 1 Disable controller caching. This is the VMWare recommended setting. If you can't disable it try setting the cache to 100% read. Make sure all disks that are meant to be claimed by vSAN are with the command esxcli vsan storage list . Make sure in the output that CMMDS is set to true. This implies that the cluster membership and directory services know about the disk in question and that the capacity of the disk is contributing to the capactiy of the vSAN datastore.","title":"Identify"},{"location":"VMWare/Troubleshooting%20vSAN/#using-vsan-observer","text":"SSH to vCenter and then run rvc administrator@vsphere.lan@localhost vsan.observer /localhost/datacenter/computers/vSAN\\ Cluster/ --run-webserver --force Go to https://vcenter.lan:8010/ (it must be https) You can also write the output to a JSON file. If you only have the JSON output you can produce static HTML with the vsan.observer_process_statsfile <stats_file> <output_folder>","title":"Using vSAN Observer"},{"location":"VMWare/Troubleshooting%20vSAN/#running-from-a-remote-machine","text":"Install Ruby: https://rubyinstaller.org/downloads/ Install Ruby vSphere Console: https://rubydoc.info/gems/rvc/1.6.0 gem install rvc","title":"Running from a remote machine"},{"location":"VMWare/Troubleshooting%20vSAN/#running-vsan-observer-without-internet-access","text":"See page 198 of Virtual SAN Diagnostics and Troubleshooting Reference Manual","title":"Running vSAN Observer without Internet Access"},{"location":"VMWare/Troubleshooting%20vSAN/#vsan-observer-case-studies","text":"Starts on page 262 of Virtual SAN Diagnostics and Troubleshooting Reference Manual","title":"vSAN Observer Case Studies"},{"location":"VMWare/Troubleshooting%20vSAN/#anatomy-of-a-writeread-operation-in-hybrid-config","text":"For more details see page 245 of Virtual SAN Diagnostics and Troubleshooting Reference Manual","title":"Anatomy of a Write/Read Operation in Hybrid Config"},{"location":"VMWare/Troubleshooting%20vSAN/#helpful-commands","text":"","title":"Helpful Commands"},{"location":"VMWare/Troubleshooting%20vSAN/#networking","text":"esxcli network ip interface list esxcli network ip interface ipv4 get esxcli network ip neighbor list # view ARP table esxcli vsan network list # Determine which vmk interface is used for vSAN esxcli network nic list # List the physical interfaces # This can be run at the cluster level and also gives vmk interface for vSAN vsan.cluster_info <vSAN cluster> # Use LLDP to obtain upstream switch info vsan.lldpnetmap localhost/datacenter/computers/vSAN\\ Cluster/","title":"Networking"},{"location":"VMWare/Troubleshooting%20vSAN/#determine-disk-group-assignmnts","text":"","title":"Determine Disk Group Assignmnts"},{"location":"VMWare/Troubleshooting%20vSAN/#check-failed-disks","text":"esxcli storage core device stats get","title":"Check Failed Disks"},{"location":"VMWare/Troubleshooting%20vSAN/#determine-what-drives-are-attached-to-which-storage-adapters","text":"","title":"Determine what drives are attached to which storage adapters"},{"location":"VMWare/Troubleshooting%20vSAN/#get-detailed-version","text":"esxcli storage core path list","title":"Get Detailed Version"},{"location":"VMWare/Troubleshooting%20vSAN/#simple-list","text":"esxcfg-scsidevs \u2013A","title":"Simple List"},{"location":"VMWare/Troubleshooting%20vSAN/#interpreting-the-output","text":"NAA stands for Network Addressing Authority identifier. EUI stands for Extended Unique Identifier. The number is guaranteed to be unique to that LUN. The NAA or EUI identifier is the preferred method of identifying LUNs and the storage device generates the number. Since the NAA or EUI is unique to the LUN, if the LUN is presented the same way across all ESXi hosts, the NAA or EUI identifier remains the same. Some devices do not provide the NAA number described above. In these circumstances, an MPX Identifier is generated by ESXi to represent the LUN or disk. The identifier takes the form similar to that of the canonical name of previous versions of ESXi with the \u201cmpx.\u201d prefix. This identifier can be used in the exact same way as the NAA Identifier described above.\u201d Refer to VMware KB article 1014953 for further information.","title":"Interpreting the Output"},{"location":"VMWare/Troubleshooting%20vSAN/#list-scsi-devices-and-their-sizes","text":"esxcfg-scsidevs \u2013c","title":"List SCSI devices and their sizes"},{"location":"VMWare/Troubleshooting%20vSAN/#get-a-list-of-all-pci-devices-allows-you-to-check-for-rebranding","text":"You an use a combination of the vendor IDs, device IDs, sub-vendor IDs, and sub-device IDs, to verify an adapter is on the VCG. Some vendors rebrand devices to reflect their own range of products however, the vendor and device ids remain the same. It is not uncommon to find that one vendor's controller is not listed under the vendor name in the VCG but is actually listed under another vendor. esxcli hardware pci list","title":"Get a list of all PCI devices (allows you to check for rebranding)"},{"location":"VMWare/Troubleshooting%20vSAN/#determine-adapter-driver-and-version","text":"Module Name indicates the driver that is currently in use. You can use the vmkload_mod -s <driver name> or esxcli system module get -m command to determine the driver version. Check that against the VCG; as bugs arise the supported drivers can change. You can also check the adapters with vsan.disks_info localhost/datacenter/computers/vSAN\\ Cluster/hosts/vsan1.lan --show-adapters in RVC.","title":"Determine Adapter Driver and Version"},{"location":"VMWare/Troubleshooting%20vSAN/#determine-driver-parameters","text":"vmkload_mod -s <driver name>","title":"Determine Driver Parameters"},{"location":"VMWare/Troubleshooting%20vSAN/#get-a-list-of-vibs","text":"esxcli software vib list If someone built using a custom ISO, it is common that they may have upgraded the driver to too high a version and that this version is unsupported.","title":"Get a list of VIBs"},{"location":"VMWare/Troubleshooting%20vSAN/#check-failures-to-tolerate-and-stripe","text":"You can see this on a per object level in vsan.object_info","title":"Check failures to tolerate and stripe"},{"location":"VMWare/Troubleshooting%20vSAN/#check-what-the-fault-domains-are","text":"vsan.cluster_info computers/vSAN\\ Cluster/","title":"Check what the fault domains are"},{"location":"VMWare/Troubleshooting%20vSAN/#inspect-a-specific-object-vm-in-vsan","text":"vsan.vm_object_info <target>","title":"Inspect a Specific Object (VM) in vSAN"},{"location":"VMWare/Troubleshooting%20vSAN/#understanding-drive-types","text":"Flash devices are placed in classes. Performance classes are measured in writes per second (not sure if this is up to date): Endurance is measured in Terabytes Written. Drives are qualified by the number of terabytes they are capable of writing per some time period per the vendor.","title":"Understanding Drive Types"},{"location":"VMWare/Troubleshooting%20vSAN/#vcg-check-summary","text":"","title":"VCG Check Summary"},{"location":"VMWare/Troubleshooting%20vSAN/#absent-vs-degraded-failure","text":"","title":"Absent vs Degraded Failure"},{"location":"VMWare/Troubleshooting%20vSAN/#object-compliance-status-compliant-vs-non-compliant","text":"","title":"Object Compliance Status: Compliant vs Non Compliant"},{"location":"VMWare/Troubleshooting%20vSAN/#object-operational-state-healthy-vs-unhealthy","text":"","title":"Object Operational State: Healthy vs Unhealthy"},{"location":"VMWare/Troubleshooting%20vSAN/#vm-accessibility-inaccessible-vs-orphaned","text":"","title":"VM Accessibility: Inaccessible vs Orphaned"},{"location":"VMWare/Troubleshooting%20vSAN/#todo","text":"Review P141 - Verifying Virtual SAN storage operation - RVC in Virtual SAN Diagnostics and Troubleshooting Reference Manual Review P152 - Testing vSAN functionality - deploying VMs in Virtual SAN Diagnostics and Troubleshooting Reference Manual P156 of Virtual SAN Diagnostics and Troubleshooting Reference Manual","title":"TODO"},{"location":"VMWare/Troubleshooting%20vSAN/setting_up_vsan_lab/","text":"Testing vSAN See notes on vSAN for how vSAN works. VM Setup I created three VMs with the following config: - I had to use the VMXNET 3 adapter to get ESXi to recognize the network card. - I used all NVMe drives - For the networks you must use a portgroup with the security settings disabled (because under the hood ESXi is effectively \"spoofing\" MAC addresses - or so the security settings will think): I then setup DNS entries for vsan1.lan, vsan2.lan, and vsan3.lan Configure NTP on vCenter (time must be synched before build) https://kb.vmware.com/s/article/57146 If your root password is expired: https://buildvirtual.net/vcenter-exception-in-invoking-authentication-handler-user-password-expired/ Make sure time on ESXi is working https://docs.vmware.com/en/VMware-vSphere/7.0/com.vmware.vsphere.vcenterhost.doc/GUID-8756D419-A878-4AE0-9183-C6D5A91A8FB1.html Configure vSAN https://docs.vmware.com/en/VMware-vSphere/7.0/com.vmware.vsphere.vsan-planning.doc/GUID-CF9767B6-B3F5-4787-9AF3-D661987AE525.html - I had to update my lab's HCL: https://kb.vmware.com/s/article/2145116 (the button didn't work for me)","title":"Testing vSAN"},{"location":"VMWare/Troubleshooting%20vSAN/setting_up_vsan_lab/#testing-vsan","text":"See notes on vSAN for how vSAN works.","title":"Testing vSAN"},{"location":"VMWare/Troubleshooting%20vSAN/setting_up_vsan_lab/#vm-setup","text":"I created three VMs with the following config: - I had to use the VMXNET 3 adapter to get ESXi to recognize the network card. - I used all NVMe drives - For the networks you must use a portgroup with the security settings disabled (because under the hood ESXi is effectively \"spoofing\" MAC addresses - or so the security settings will think): I then setup DNS entries for vsan1.lan, vsan2.lan, and vsan3.lan Configure NTP on vCenter (time must be synched before build) https://kb.vmware.com/s/article/57146 If your root password is expired: https://buildvirtual.net/vcenter-exception-in-invoking-authentication-handler-user-password-expired/ Make sure time on ESXi is working https://docs.vmware.com/en/VMware-vSphere/7.0/com.vmware.vsphere.vcenterhost.doc/GUID-8756D419-A878-4AE0-9183-C6D5A91A8FB1.html","title":"VM Setup"},{"location":"VMWare/Troubleshooting%20vSAN/setting_up_vsan_lab/#configure-vsan","text":"https://docs.vmware.com/en/VMware-vSphere/7.0/com.vmware.vsphere.vsan-planning.doc/GUID-CF9767B6-B3F5-4787-9AF3-D661987AE525.html - I had to update my lab's HCL: https://kb.vmware.com/s/article/2145116 (the button didn't work for me)","title":"Configure vSAN"},{"location":"VMWare/VMWare%20APIs/","text":"VMWare APIs VMWare APIs VxRail Upgrading VxRail with Windows Curl Example Code for PowerShell 5 Version Used for Testing Log Locations VxRail API Cookbook Upgrading VxRail with Windows Curl curl -k --user \"administrator@vsphere.local:<PASSWORD>\" --request POST \"https://<VXRAIL_IP_ADDRESS>/rest/vxm/v1/lcm/upgrade\" --header \"Content-Type: application/json\" --data \"{\\\"bundle_file_locator\\\": \\\"/tmp/VXRAIL_COMPOSITE-4.7.for_4.x.x.zip \\\",\\\"vxrail\\\": {\\\"vxm_root_user\\\": {\\\"username\\\": \\\"root\\\",\\\"password\\\": \\\"<PASSWORD>\\\"}},\\\"vcenter\\\":{\\\"vc_admin_user\\\": {\\\"username\\\": \\\"administrator@vsphere.local\\\",\\\"password\\\": \\\"<PASSWORD>\\\"},\\\"vcsa_root_user\\\": {\\\"username\\\": \\\"root\\\",\\\"password\\\": \\\"<PASSWORD>\\\"},\\\"psc_root_user\\\": {\\\"username\\\": \\\"root\\\",\\\"password\\\": \\\"<PASSWORD>\\\"}}}\" Note: psc_root_user should be the same as vCenter root user Example Code for PowerShell 5 $RESTAPIServer = \"YOUR_VCENTER\" $RESTAPIUser = \"administrator@vsphere.local\" // Or whatever user you want $RESTAPIPassword = \"PASSWORD\" add-type @\" using System.Net; using System.Security.Cryptography.X509Certificates; public class TrustAllCertsPolicy : ICertificatePolicy { public bool CheckValidationResult( ServicePoint srvPoint, X509Certificate certificate, WebRequest request, int certificateProblem) { return true; } } \"@ [System.Net.ServicePointManager]::CertificatePolicy = New-Object TrustAllCertsPolicy [System.Net.ServicePointManager]::SecurityProtocol = [System.Net.SecurityProtocolType]'Ssl3,Tls,Tls11,Tls12' $BaseAuthURL = \"https://\" + $RESTAPIServer + \"/rest/com/vmware/cis/\" $BaseURL = \"https://\" + $RESTAPIServer + \"/rest/vcenter/\" $vCenterSessionURL = $BaseAuthURL + \"session\" $Header = @{\"Authorization\" = \"Basic \"+[System.Convert]::ToBase64String([System.Text.Encoding]::UTF8.GetBytes($RESTAPIUser+\":\"+$RESTAPIPassword))} $Type = \"application/json\" Try { $vCenterSessionResponse = Invoke-RestMethod -Uri $vCenterSessionURL -Headers $Header -Method POST -ContentType $Type } Catch { $_.Exception.ToString() $error[0] | Format-List -Force } # Extracting the session ID from the response $vCenterSessionHeader = @{'vmware-api-session-id' = $vCenterSessionResponse.value} $VMListURL = $BaseURL+\"datacenter\" Write-Host \"Sending request to URL: ${VMListURL}\" Try { $VMListJSON = Invoke-RestMethod -Method Get -Uri $VMListURL -TimeoutSec 100 -Headers $vCenterSessionHeader -ContentType $Type $VMList = $VMListJSON.value } Catch { $_.Exception.ToString() $error[0] | Format-List -Force } $VMList | Format-Table -AutoSize Version Used for Testing PS C:\\Users\\grant\\Downloads> (Get-Host).Version Major Minor Build Revision ----- ----- ----- -------- 5 1 19041 1023 Log Locations Relevant logs are in /var/log/vmware/vapi/endpoint The vcentershim.log will tell you about errors occurring (like a 500 internal server error) The endpoint.log will tell you about things like 404 errors endpoint-access.log will show you who is accessing the server.","title":"VMWare APIs"},{"location":"VMWare/VMWare%20APIs/#vmware-apis","text":"VMWare APIs VxRail Upgrading VxRail with Windows Curl Example Code for PowerShell 5 Version Used for Testing Log Locations","title":"VMWare APIs"},{"location":"VMWare/VMWare%20APIs/#vxrail","text":"API Cookbook","title":"VxRail"},{"location":"VMWare/VMWare%20APIs/#upgrading-vxrail-with-windows-curl","text":"curl -k --user \"administrator@vsphere.local:<PASSWORD>\" --request POST \"https://<VXRAIL_IP_ADDRESS>/rest/vxm/v1/lcm/upgrade\" --header \"Content-Type: application/json\" --data \"{\\\"bundle_file_locator\\\": \\\"/tmp/VXRAIL_COMPOSITE-4.7.for_4.x.x.zip \\\",\\\"vxrail\\\": {\\\"vxm_root_user\\\": {\\\"username\\\": \\\"root\\\",\\\"password\\\": \\\"<PASSWORD>\\\"}},\\\"vcenter\\\":{\\\"vc_admin_user\\\": {\\\"username\\\": \\\"administrator@vsphere.local\\\",\\\"password\\\": \\\"<PASSWORD>\\\"},\\\"vcsa_root_user\\\": {\\\"username\\\": \\\"root\\\",\\\"password\\\": \\\"<PASSWORD>\\\"},\\\"psc_root_user\\\": {\\\"username\\\": \\\"root\\\",\\\"password\\\": \\\"<PASSWORD>\\\"}}}\" Note: psc_root_user should be the same as vCenter root user","title":"Upgrading VxRail with Windows Curl"},{"location":"VMWare/VMWare%20APIs/#example-code-for-powershell-5","text":"$RESTAPIServer = \"YOUR_VCENTER\" $RESTAPIUser = \"administrator@vsphere.local\" // Or whatever user you want $RESTAPIPassword = \"PASSWORD\" add-type @\" using System.Net; using System.Security.Cryptography.X509Certificates; public class TrustAllCertsPolicy : ICertificatePolicy { public bool CheckValidationResult( ServicePoint srvPoint, X509Certificate certificate, WebRequest request, int certificateProblem) { return true; } } \"@ [System.Net.ServicePointManager]::CertificatePolicy = New-Object TrustAllCertsPolicy [System.Net.ServicePointManager]::SecurityProtocol = [System.Net.SecurityProtocolType]'Ssl3,Tls,Tls11,Tls12' $BaseAuthURL = \"https://\" + $RESTAPIServer + \"/rest/com/vmware/cis/\" $BaseURL = \"https://\" + $RESTAPIServer + \"/rest/vcenter/\" $vCenterSessionURL = $BaseAuthURL + \"session\" $Header = @{\"Authorization\" = \"Basic \"+[System.Convert]::ToBase64String([System.Text.Encoding]::UTF8.GetBytes($RESTAPIUser+\":\"+$RESTAPIPassword))} $Type = \"application/json\" Try { $vCenterSessionResponse = Invoke-RestMethod -Uri $vCenterSessionURL -Headers $Header -Method POST -ContentType $Type } Catch { $_.Exception.ToString() $error[0] | Format-List -Force } # Extracting the session ID from the response $vCenterSessionHeader = @{'vmware-api-session-id' = $vCenterSessionResponse.value} $VMListURL = $BaseURL+\"datacenter\" Write-Host \"Sending request to URL: ${VMListURL}\" Try { $VMListJSON = Invoke-RestMethod -Method Get -Uri $VMListURL -TimeoutSec 100 -Headers $vCenterSessionHeader -ContentType $Type $VMList = $VMListJSON.value } Catch { $_.Exception.ToString() $error[0] | Format-List -Force } $VMList | Format-Table -AutoSize","title":"Example Code for PowerShell 5"},{"location":"VMWare/VMWare%20APIs/#version-used-for-testing","text":"PS C:\\Users\\grant\\Downloads> (Get-Host).Version Major Minor Build Revision ----- ----- ----- -------- 5 1 19041 1023","title":"Version Used for Testing"},{"location":"VMWare/VMWare%20APIs/#log-locations","text":"Relevant logs are in /var/log/vmware/vapi/endpoint The vcentershim.log will tell you about errors occurring (like a 500 internal server error) The endpoint.log will tell you about things like 404 errors endpoint-access.log will show you who is accessing the server.","title":"Log Locations"},{"location":"VMWare/VxRail%20Architecture%20and%20Troubleshooting/","text":"VxRail Architecture and Troubleshooting VxRail Architecture and Troubleshooting VxRail Architecture vxrail-mystic-lcm-7.0.202.rpm vxrail-system.zip VXRAIL_7.0.200-17911444.zip Inspecting VIB files Inspecting Java Class Files Upgrades How Upgrades Work Failed Upgrade Observed Errors Fix Action Understanding Upgrades in ESXi Understanding the VxRail Update Bundle Manifest File and Update Order Adding Nodes Troubleshooting How Adding Nodes Works During Cluster Construction Disappearing Browse Button Log Info Fixing Half Upgrade vCenter Logs Altbootbank Add nodes NIC config page Add Node Error Error Node General Error Redeploying VxRail Manager Logs Generating Logs VxRail Microservice (container) Logs RASR Process Tomcat Logs Upgrade Logs API Logs Helpful Commands Helpful KB Articles VxRail API Info VxRail Architecture Most meaningful information on VxRail can be gleaned by reverse engineering the update package. There are three files in the update package of particular interest: bundles/vxrail-mystic-lcm-7.0.202.rpm vxrail-system.zip VXRAIL_7.0.200-17911444.zip vxrail-mystic-lcm-7.0.202.rpm The mystic LCM process governs all upgrades. Any code related to the upgrade seems to be part of this process. RPMs are just archives and you can unarchive this file with 7-zip and take a look at its contents for any debugging desired. vxrail-system.zip Defines all the baselines available in JSON format. VXRAIL_7.0.200-17911444.zip This file contains all the files for VxRail that aren't associated with the upgrade subsystem. Inspecting VIB files There's a good chance you may want to be able to look inside VIB files. VIB files use a proprietary compression technology. You can get the type by opening the file in a hex editor and inspecting the headers: On any ESXi host you can use the vmtar program to open these with the command: vmtar -x <INPUT_VMTAR_FILE> -o <OUTPUT>.tar . The output will be a regular tar file which you can then open with tar xvf <OUTPUT>.tar Inspecting Java Class Files Most of the Java I saw was contained in the vmware-marvin file or in the LCM Marvin RPM. I was wrestling with the error: web.log: ERROR [tomcat-http--48] com.emc.mystic.manager.commons.emc.webutil.LocaleUtil LocaleUtil.getBundleMessage:198 - No en_US resource bundle for ExpansionValidation with key <SERIAL_NUMBER> I was able to find the offending file in question at VXRAIL_COMPOSITE-7.0.202-27047874_for_4.7.x\\bundles\\vxrail-mystic-lcm-7.0.202\\vxrail-mystic-lcm-7.0.202-27047874.noarch\\usr\\lib\\vmware-marvin\\marvind\\webapps\\lcm\\WEB-INF\\lib\\lcm-module-7.0.202\\com\\vce\\lcm\\emc (that's post decompression of the vmware-marvin file inside the VIB). I find that compressing the three core files for VxRail and searching them with grep for keywords from the error messages was pretty effective for locating what I wanted. In my case the following got me what I wanted for the above error: (base) grant@DESKTOP-2SV1E9O:/mnt/c/Users/grant/Downloads/VXRAIL_COMPOSITE-7.0.202-27047874_for_4.7.x/bundles/vxrail-mystic-lcm-7.0.202/vxrail-mystic-lcm-7.0.202-27047874.noarch$ grep -inr 'resource bundle' ./* Binary file ./usr/lib/vmware-marvin/marvind/webapps/lcm/WEB-INF/lib/lcm-module-7.0.202/com/vce/lcm/emc/LocaleUtil.class matches I used the program JD Project to decompile the Java bytecode in question and it worked quite well. I was able to search the decompiled bytecode normally and find the offending function: which then allowed me to determine that this error message was actually erronious and just a product of VxRail trying and failing to localize an error message related to disk validation. Upgrades How Upgrades Work During the upgrade process the old VxRail manager will have its IP and hostname removed and be shutdown while the new one assumes the previous manager's hostname and IP. Upgrades are performed with the service account you configured when you built VxRail - not root . Failed Upgrade If an upgrade fails midflight this typically leaves the cluster in a state where there are two VxRail managers. In my team's experience trying to resume the upgrade using the new VxRail manager leads to a host of issues. Observed Errors Specifically, we encountered the following errors: VxRail Update ran into a problem... Please refer to KB488889 article. Failed to upload bundle: VXRAIL_COMPOSITE-SLIM-7.0.132-26894200_for_4.7x.zip: Failure occured in executing the checker: Pre-checking the version compatibility among components. General unexpected error occurs during getting the information of firmwares.. Please refer to the KB517433 article. Error extracting upgrade bundle 7.0.132-26894200. Failed to upload bundle. Please refer to log for details. This error is covered by Dell EMC VxRail: LCM failed at Error extracting update bundle 7.0.132 We also saw: Failure occurred while running an upgrade for bundle VXRAIL_COMPOSITE-SLIM-7.0.132-26894200_for_4.7.x.zip. The error message [DependencyError] VIB DEL_bootbank_dcism_3.6.0.2249-DEL.700.0.0.15843807 requires esx-version >= 7.0.0 but the requirement cannot be satisfied within the ImageProfile. Please refer to the log file for more details. Fix Action The most reliable way to fix a stuck upgrade is to revert to the old VxRail Manager and then run the upgrade again. You can do this by shutting down the new VxRail Manager, deleting it, and then logging into the previous VxRail Manager on the console, re-IPing it, give it it's previous hostname, and then restarting the upgrade. Understanding Upgrades in ESXi See What is the altbookbank Partition Understanding the VxRail Update Bundle At the top level the upgrade is governed and controlled by the file node-upgrade.py in the root of the upgrade bundle: The bundles directory has all the possible files that could be installed with the update. The exact files which are installed vary by system. More on that in the section Manifest File and Update Order . Manifest File and Update Order The manifest file defines which VIBs will be installed based on system parameters and in what order they will be installed in. For example: ESXi: <Package InstallOrder=\"100\"> <ComponentType>ESXi</ComponentType> <DisplayName>ESXi</DisplayName> <Version>7.0.2</Version> <Build>17867351</Build> <File>bundles/ESXi-7.0.2-ep1_17867351-34777dce-standard.zip</File> <Size>393239990</Size> <HighestFormatVersionSupported>11</HighestFormatVersionSupported> <UpgradeTime>5</UpgradeTime> <RebootFlag>True</RebootFlag> <Package InstallOrder=\"101\"> <ComponentType>ESXi_No_Tools</ComponentType> <DisplayName>VMware ESXi No Tools</DisplayName> <Version>7.0.2</Version> <Build>17867351</Build> <File>bundles/ESXi-7.0.2-ep1_17867351-34777dce-upgrade-no-tools.zip</File> <Size>207115430</Size> <HighestFormatVersionSupported>11</HighestFormatVersionSupported> <UpgradeTime>5</UpgradeTime> <RebootFlag>True</RebootFlag> </Package> <Package InstallOrder=\"1001\"> <ComponentType>ESXi_VIB</ComponentType> <DisplayName>Dell iSM for vSphere 7</DisplayName> <Version>3.6.0.2249</Version> <Build>DEL.700.0.0.15843807</Build> <SystemName>dcism</SystemName> This is parsed by the function _parse_esxi_patch, _parse_install_vib, and _parse_update_firmware in node-upgrade.py on line 1381, 1454, and 1492: def _parse_esxi_patch(self, element): task = { 'type': \"esxi_patch\", 'name': \"Install ESXi VMware patch\", 'async': False, 'visible': True, } task['install_order'] = int(element.get('InstallOrder')) task['args'] = self._extract_args(element, [ 'ComponentType', 'DisplayName', 'Version', 'Build', 'File', 'Size', 'HighestFormatVersionSupported', ]) return task ---SNIP--- def _parse_install_vib(self, element): task = { 'type': \"install_vib\", 'async': False, 'visible': True, 'package_type': 'vib', } task['name'] = \"Install %s\" % element.find('DisplayName').text task['install_order'] = int(element.get('InstallOrder')) task['args'] = self._extract_args(element, [ 'ComponentType', 'DisplayName', 'Version', 'Build', 'SystemName', 'File', 'Size', 'ReplaceTargetInfo/ReplaceTarget/SystemName', ]) component_type = task['args'].get('ComponentType', '') display_name = task['args'].get('DisplayName', '') if equals_ignore_case(display_name, 'VxRail VIB') or \\ equals_ignore_case(component_type, 'VXRAIL_'): task['args']['SystemName'] = \"vmware-marvin\" pkg_file = task['args'].get('File', '') file_path = os.path.join(self._bundle_dir, pkg_file) vlcm_bundle_info = self._vlcm_bundle_info(file_path) if vlcm_bundle_info: task['package_type'] = 'component' task['component_name'] = vlcm_bundle_info[0] task['component_version'] = vlcm_bundle_info[1] return task def _parse_update_firmware(self, element): task = { 'type': \"update_firmware\", 'async': True, 'visible': True, 'runtime_check': False, } task['name'] = \"Update %s\" % element.find('DisplayName').text task['install_order'] = int(element.get('InstallOrder')) task['args'] = self._extract_args(element, [ 'ComponentType', 'DisplayName', 'Version', 'Build', 'File', 'Size', ]) nic_models = self._extract_target_models(element, 'TargetNicModelInfo') component_models = self._extract_target_models( element, 'TargetComponentModelInfo') fw_models = nic_models + component_models if fw_models: task['args']['FirmwareModels'] = fw_models else: task['args']['FirmwareModels'] = None return task These functions are used to create tasks which are stored in the required_tasks variable: After all tasks are added to required tasks they are sorted with a lambda function on line 1898: required_tasks.sort(key=lambda t: t['install_order']) Subsequently it is safe to assume a linear sort on the integer value. We can use this to diagnose any problems we encounter with install order. Adding Nodes Version Compatibility Matrix: Link Make sure before you try to do anything with adding nodes that you verify the versions are mutually compatible. Troubleshooting We bumped into several errors while attempting to add nodes and they varied in type. One of the errors we encountered was when adding the node the interface for the VxRail plugin failed entirely. We were able to fix it with the following process: Take a snapshot of VxM psql -U postgres vxrail -c \"DELETE FROM system.operation_status WHERE state='IN_PROGRESS';\" enable advanced mode by changing it in /var/lib/vmware-marvin/lcm_advanced_mode.properties Change the contents of /var/lib/vmware-mariv to {\"state\":\"NONE\",\"vc_plugin_updated\":false,\"deployed_for_public_api\":false} Restart Marvin with system restart vmware-marvin In one instance we were able to clear all problems by restarting all nodes in the cluster, the VxRail manager, and vCenter. After a full reboot of everything, a series of general errors we had been receiving during validation cleared. How Adding Nodes Works During Cluster Construction All nodes after being RASR have a fully built vSAN disk group. During the initial cluster construction VxRail will select a primary node and then add it to the cluster. On every subsequent node it will delete that node's vSAN disk group and then add it to the cluster's existing disk group. Disappearing Browse Button Take a snapshot of VxM psql -U postgres vxrail -c \"DELETE FROM system.operation_status WHERE state='IN_PROGRESS';\" enable advanced mode by change it in /var/lib/vmware-marvin/lcm_advanced_mode.properties Change the contents of /var/lib/vmware-mariv to {\"state\":\"NONE\",\"vc_plugin_updated\":false,\"deployed_for_public_api\":false} Result system restart vmware-marvin Log Info lcm-web.log - Shows upgrade related info web.log - combined web output Fixing Half Upgrade Roll back to previous VxRail manager, then use it to complete the upgrade. Uses ESXi management account for updates Uses PSC log vCenter Logs Altbootbank You can load the old version with shift+r Add nodes NIC config page When you hit https:// /ui/vxrail/rest/vxm/private/system/cluster-hosts?$$objectID=urn:vmomi:ClusterComputeResource:domain &lang=en-us you should get back the existing hosts in the cluster with their hostname, model, serial_number, and vmnics https:// /ui/vxrail/rest/vxm/private/system/available-hosts?$filter=serial_number%20in%20(SERIAL,SERIAL)&$$objectId=urn:vmomi:ClusterComputeResource:domain- &lang=en-us should get you the two hosts you are going to add Add Node Error \"Could not find NSX-T network information\" Http failure response for https:// /ui/vxrail/rest/vxm/private/cluster/network/nsxt???objectId=urn:vmomi:ClusterComputeResource:domain-ID=en-US: 404 OK Error Node Validation Errors Disk Grouping Error (JKSLH63) Error occurs when validating disk group on host JKSLH63 web.log: ERROR [tomcat-http--48] com.emc.mystic.manager.commons.emc.webutil.LocaleUtil LocaleUtil.getBundleMessage:198 - No en_US resource bundle for ExpansionValidation with key JKSLH63 General Error web.log: [WARN] com.vce.commons.config.ConfigServiceImpl$NotFoundHandler ConfigServiceImpl$NotFoundHandler.handleNotFound:114 - provided key is not present: [404, {\"message\":\"404 Not Found: bandwidth_throttling_level does not exist\"}] Redeploying VxRail Manager If VxRail Manager was previously deployed but something happened to it you can rebuild it. If you have the old VxRail manager, but the cluster has changed on ESXi, you will have to do: Update /var/lib/vmware-marvin/runtime.properties with the new cluster UUID Connect to the postgresql database on the manager, go to the settings table and update it with the new settings. Logs Generating Logs If you want to get a full dump of all the logs associated with VxRail you can generate a dump by going to the VxRail plugin on vCenter, navigating to the support tab, and then clicking the generate log button. VxRail Microservice (container) Logs /var/log/microservice/vxrail-manager RASR Process The totality of the RASR process' output is stored in the images folder on the local datastore in fist.log. Tomcat Logs /var/log/marvin/tomcat/logs /var/log/mystic/web.log Upgrade Logs The LCM process handles upgrades. It's logs are all in /var/log/mystic/lcm*. You can also try checking the PSC log (haven't personally done this) API Logs These are on vCenter. See VMWARE API Logs Helpful Commands See Helpful Commands Helpful KB Articles https://www.dell.com/support/kbdoc/en-uk/000021742/dell-emc-vxrail-troubleshooting-guide-for-vxrail-micro-services-infrastructure https://www.dell.com/support/kbdoc/en-uk/000157667/dell-emc-vxrail-how-to-query-vxrail-cluster-configuration-data-in-7-0-010-or-higher https://www.dell.com/support/kbdoc/en-us/000157715/dell-emc-vxrail-troubleshooting-guide-for-upgrading-vxrail-manager-to-7-0-010-or-higher https://www.dell.com/support/kbdoc/en-us/000157662/dell-emc-vxrail-how-to-get-or-update-management-account-in-vxrail-7-0-010-or-higher https://www.dell.com/support/kbdoc/en-us/000181759/dell-emc-vxrail-upgrading-vxrail-manager-to-7-0-x-release-failed-vcenter-plugin-no-loading https://www.dell.com/support/kbdoc/en-us/000181712/dell-emc-vxrail-how-to-update-vxrail-manager-without-lcm VxRail API Info See VMWare APIs","title":"VxRail Architecture and Troubleshooting"},{"location":"VMWare/VxRail%20Architecture%20and%20Troubleshooting/#vxrail-architecture-and-troubleshooting","text":"VxRail Architecture and Troubleshooting VxRail Architecture vxrail-mystic-lcm-7.0.202.rpm vxrail-system.zip VXRAIL_7.0.200-17911444.zip Inspecting VIB files Inspecting Java Class Files Upgrades How Upgrades Work Failed Upgrade Observed Errors Fix Action Understanding Upgrades in ESXi Understanding the VxRail Update Bundle Manifest File and Update Order Adding Nodes Troubleshooting How Adding Nodes Works During Cluster Construction Disappearing Browse Button Log Info Fixing Half Upgrade vCenter Logs Altbootbank Add nodes NIC config page Add Node Error Error Node General Error Redeploying VxRail Manager Logs Generating Logs VxRail Microservice (container) Logs RASR Process Tomcat Logs Upgrade Logs API Logs Helpful Commands Helpful KB Articles VxRail API Info","title":"VxRail Architecture and Troubleshooting"},{"location":"VMWare/VxRail%20Architecture%20and%20Troubleshooting/#vxrail-architecture","text":"Most meaningful information on VxRail can be gleaned by reverse engineering the update package. There are three files in the update package of particular interest: bundles/vxrail-mystic-lcm-7.0.202.rpm vxrail-system.zip VXRAIL_7.0.200-17911444.zip","title":"VxRail Architecture"},{"location":"VMWare/VxRail%20Architecture%20and%20Troubleshooting/#vxrail-mystic-lcm-70202rpm","text":"The mystic LCM process governs all upgrades. Any code related to the upgrade seems to be part of this process. RPMs are just archives and you can unarchive this file with 7-zip and take a look at its contents for any debugging desired.","title":"vxrail-mystic-lcm-7.0.202.rpm"},{"location":"VMWare/VxRail%20Architecture%20and%20Troubleshooting/#vxrail-systemzip","text":"Defines all the baselines available in JSON format.","title":"vxrail-system.zip"},{"location":"VMWare/VxRail%20Architecture%20and%20Troubleshooting/#vxrail_70200-17911444zip","text":"This file contains all the files for VxRail that aren't associated with the upgrade subsystem.","title":"VXRAIL_7.0.200-17911444.zip"},{"location":"VMWare/VxRail%20Architecture%20and%20Troubleshooting/#inspecting-vib-files","text":"There's a good chance you may want to be able to look inside VIB files. VIB files use a proprietary compression technology. You can get the type by opening the file in a hex editor and inspecting the headers: On any ESXi host you can use the vmtar program to open these with the command: vmtar -x <INPUT_VMTAR_FILE> -o <OUTPUT>.tar . The output will be a regular tar file which you can then open with tar xvf <OUTPUT>.tar","title":"Inspecting VIB files"},{"location":"VMWare/VxRail%20Architecture%20and%20Troubleshooting/#inspecting-java-class-files","text":"Most of the Java I saw was contained in the vmware-marvin file or in the LCM Marvin RPM. I was wrestling with the error: web.log: ERROR [tomcat-http--48] com.emc.mystic.manager.commons.emc.webutil.LocaleUtil LocaleUtil.getBundleMessage:198 - No en_US resource bundle for ExpansionValidation with key <SERIAL_NUMBER> I was able to find the offending file in question at VXRAIL_COMPOSITE-7.0.202-27047874_for_4.7.x\\bundles\\vxrail-mystic-lcm-7.0.202\\vxrail-mystic-lcm-7.0.202-27047874.noarch\\usr\\lib\\vmware-marvin\\marvind\\webapps\\lcm\\WEB-INF\\lib\\lcm-module-7.0.202\\com\\vce\\lcm\\emc (that's post decompression of the vmware-marvin file inside the VIB). I find that compressing the three core files for VxRail and searching them with grep for keywords from the error messages was pretty effective for locating what I wanted. In my case the following got me what I wanted for the above error: (base) grant@DESKTOP-2SV1E9O:/mnt/c/Users/grant/Downloads/VXRAIL_COMPOSITE-7.0.202-27047874_for_4.7.x/bundles/vxrail-mystic-lcm-7.0.202/vxrail-mystic-lcm-7.0.202-27047874.noarch$ grep -inr 'resource bundle' ./* Binary file ./usr/lib/vmware-marvin/marvind/webapps/lcm/WEB-INF/lib/lcm-module-7.0.202/com/vce/lcm/emc/LocaleUtil.class matches I used the program JD Project to decompile the Java bytecode in question and it worked quite well. I was able to search the decompiled bytecode normally and find the offending function: which then allowed me to determine that this error message was actually erronious and just a product of VxRail trying and failing to localize an error message related to disk validation.","title":"Inspecting Java Class Files"},{"location":"VMWare/VxRail%20Architecture%20and%20Troubleshooting/#upgrades","text":"","title":"Upgrades"},{"location":"VMWare/VxRail%20Architecture%20and%20Troubleshooting/#how-upgrades-work","text":"During the upgrade process the old VxRail manager will have its IP and hostname removed and be shutdown while the new one assumes the previous manager's hostname and IP. Upgrades are performed with the service account you configured when you built VxRail - not root .","title":"How Upgrades Work"},{"location":"VMWare/VxRail%20Architecture%20and%20Troubleshooting/#failed-upgrade","text":"If an upgrade fails midflight this typically leaves the cluster in a state where there are two VxRail managers. In my team's experience trying to resume the upgrade using the new VxRail manager leads to a host of issues.","title":"Failed Upgrade"},{"location":"VMWare/VxRail%20Architecture%20and%20Troubleshooting/#observed-errors","text":"Specifically, we encountered the following errors: VxRail Update ran into a problem... Please refer to KB488889 article. Failed to upload bundle: VXRAIL_COMPOSITE-SLIM-7.0.132-26894200_for_4.7x.zip: Failure occured in executing the checker: Pre-checking the version compatibility among components. General unexpected error occurs during getting the information of firmwares.. Please refer to the KB517433 article. Error extracting upgrade bundle 7.0.132-26894200. Failed to upload bundle. Please refer to log for details. This error is covered by Dell EMC VxRail: LCM failed at Error extracting update bundle 7.0.132 We also saw: Failure occurred while running an upgrade for bundle VXRAIL_COMPOSITE-SLIM-7.0.132-26894200_for_4.7.x.zip. The error message [DependencyError] VIB DEL_bootbank_dcism_3.6.0.2249-DEL.700.0.0.15843807 requires esx-version >= 7.0.0 but the requirement cannot be satisfied within the ImageProfile. Please refer to the log file for more details.","title":"Observed Errors"},{"location":"VMWare/VxRail%20Architecture%20and%20Troubleshooting/#fix-action","text":"The most reliable way to fix a stuck upgrade is to revert to the old VxRail Manager and then run the upgrade again. You can do this by shutting down the new VxRail Manager, deleting it, and then logging into the previous VxRail Manager on the console, re-IPing it, give it it's previous hostname, and then restarting the upgrade.","title":"Fix Action"},{"location":"VMWare/VxRail%20Architecture%20and%20Troubleshooting/#understanding-upgrades-in-esxi","text":"See What is the altbookbank Partition","title":"Understanding Upgrades in ESXi"},{"location":"VMWare/VxRail%20Architecture%20and%20Troubleshooting/#understanding-the-vxrail-update-bundle","text":"At the top level the upgrade is governed and controlled by the file node-upgrade.py in the root of the upgrade bundle: The bundles directory has all the possible files that could be installed with the update. The exact files which are installed vary by system. More on that in the section Manifest File and Update Order .","title":"Understanding the VxRail Update Bundle"},{"location":"VMWare/VxRail%20Architecture%20and%20Troubleshooting/#manifest-file-and-update-order","text":"The manifest file defines which VIBs will be installed based on system parameters and in what order they will be installed in. For example: ESXi: <Package InstallOrder=\"100\"> <ComponentType>ESXi</ComponentType> <DisplayName>ESXi</DisplayName> <Version>7.0.2</Version> <Build>17867351</Build> <File>bundles/ESXi-7.0.2-ep1_17867351-34777dce-standard.zip</File> <Size>393239990</Size> <HighestFormatVersionSupported>11</HighestFormatVersionSupported> <UpgradeTime>5</UpgradeTime> <RebootFlag>True</RebootFlag> <Package InstallOrder=\"101\"> <ComponentType>ESXi_No_Tools</ComponentType> <DisplayName>VMware ESXi No Tools</DisplayName> <Version>7.0.2</Version> <Build>17867351</Build> <File>bundles/ESXi-7.0.2-ep1_17867351-34777dce-upgrade-no-tools.zip</File> <Size>207115430</Size> <HighestFormatVersionSupported>11</HighestFormatVersionSupported> <UpgradeTime>5</UpgradeTime> <RebootFlag>True</RebootFlag> </Package> <Package InstallOrder=\"1001\"> <ComponentType>ESXi_VIB</ComponentType> <DisplayName>Dell iSM for vSphere 7</DisplayName> <Version>3.6.0.2249</Version> <Build>DEL.700.0.0.15843807</Build> <SystemName>dcism</SystemName> This is parsed by the function _parse_esxi_patch, _parse_install_vib, and _parse_update_firmware in node-upgrade.py on line 1381, 1454, and 1492: def _parse_esxi_patch(self, element): task = { 'type': \"esxi_patch\", 'name': \"Install ESXi VMware patch\", 'async': False, 'visible': True, } task['install_order'] = int(element.get('InstallOrder')) task['args'] = self._extract_args(element, [ 'ComponentType', 'DisplayName', 'Version', 'Build', 'File', 'Size', 'HighestFormatVersionSupported', ]) return task ---SNIP--- def _parse_install_vib(self, element): task = { 'type': \"install_vib\", 'async': False, 'visible': True, 'package_type': 'vib', } task['name'] = \"Install %s\" % element.find('DisplayName').text task['install_order'] = int(element.get('InstallOrder')) task['args'] = self._extract_args(element, [ 'ComponentType', 'DisplayName', 'Version', 'Build', 'SystemName', 'File', 'Size', 'ReplaceTargetInfo/ReplaceTarget/SystemName', ]) component_type = task['args'].get('ComponentType', '') display_name = task['args'].get('DisplayName', '') if equals_ignore_case(display_name, 'VxRail VIB') or \\ equals_ignore_case(component_type, 'VXRAIL_'): task['args']['SystemName'] = \"vmware-marvin\" pkg_file = task['args'].get('File', '') file_path = os.path.join(self._bundle_dir, pkg_file) vlcm_bundle_info = self._vlcm_bundle_info(file_path) if vlcm_bundle_info: task['package_type'] = 'component' task['component_name'] = vlcm_bundle_info[0] task['component_version'] = vlcm_bundle_info[1] return task def _parse_update_firmware(self, element): task = { 'type': \"update_firmware\", 'async': True, 'visible': True, 'runtime_check': False, } task['name'] = \"Update %s\" % element.find('DisplayName').text task['install_order'] = int(element.get('InstallOrder')) task['args'] = self._extract_args(element, [ 'ComponentType', 'DisplayName', 'Version', 'Build', 'File', 'Size', ]) nic_models = self._extract_target_models(element, 'TargetNicModelInfo') component_models = self._extract_target_models( element, 'TargetComponentModelInfo') fw_models = nic_models + component_models if fw_models: task['args']['FirmwareModels'] = fw_models else: task['args']['FirmwareModels'] = None return task These functions are used to create tasks which are stored in the required_tasks variable: After all tasks are added to required tasks they are sorted with a lambda function on line 1898: required_tasks.sort(key=lambda t: t['install_order']) Subsequently it is safe to assume a linear sort on the integer value. We can use this to diagnose any problems we encounter with install order.","title":"Manifest File and Update Order"},{"location":"VMWare/VxRail%20Architecture%20and%20Troubleshooting/#adding-nodes","text":"Version Compatibility Matrix: Link Make sure before you try to do anything with adding nodes that you verify the versions are mutually compatible.","title":"Adding Nodes"},{"location":"VMWare/VxRail%20Architecture%20and%20Troubleshooting/#troubleshooting","text":"We bumped into several errors while attempting to add nodes and they varied in type. One of the errors we encountered was when adding the node the interface for the VxRail plugin failed entirely. We were able to fix it with the following process: Take a snapshot of VxM psql -U postgres vxrail -c \"DELETE FROM system.operation_status WHERE state='IN_PROGRESS';\" enable advanced mode by changing it in /var/lib/vmware-marvin/lcm_advanced_mode.properties Change the contents of /var/lib/vmware-mariv to {\"state\":\"NONE\",\"vc_plugin_updated\":false,\"deployed_for_public_api\":false} Restart Marvin with system restart vmware-marvin In one instance we were able to clear all problems by restarting all nodes in the cluster, the VxRail manager, and vCenter. After a full reboot of everything, a series of general errors we had been receiving during validation cleared.","title":"Troubleshooting"},{"location":"VMWare/VxRail%20Architecture%20and%20Troubleshooting/#how-adding-nodes-works-during-cluster-construction","text":"All nodes after being RASR have a fully built vSAN disk group. During the initial cluster construction VxRail will select a primary node and then add it to the cluster. On every subsequent node it will delete that node's vSAN disk group and then add it to the cluster's existing disk group.","title":"How Adding Nodes Works During Cluster Construction"},{"location":"VMWare/VxRail%20Architecture%20and%20Troubleshooting/#disappearing-browse-button","text":"Take a snapshot of VxM psql -U postgres vxrail -c \"DELETE FROM system.operation_status WHERE state='IN_PROGRESS';\" enable advanced mode by change it in /var/lib/vmware-marvin/lcm_advanced_mode.properties Change the contents of /var/lib/vmware-mariv to {\"state\":\"NONE\",\"vc_plugin_updated\":false,\"deployed_for_public_api\":false} Result system restart vmware-marvin","title":"Disappearing Browse Button"},{"location":"VMWare/VxRail%20Architecture%20and%20Troubleshooting/#log-info","text":"lcm-web.log - Shows upgrade related info web.log - combined web output","title":"Log Info"},{"location":"VMWare/VxRail%20Architecture%20and%20Troubleshooting/#fixing-half-upgrade","text":"Roll back to previous VxRail manager, then use it to complete the upgrade. Uses ESXi management account for updates Uses PSC log","title":"Fixing Half Upgrade"},{"location":"VMWare/VxRail%20Architecture%20and%20Troubleshooting/#vcenter-logs","text":"","title":"vCenter Logs"},{"location":"VMWare/VxRail%20Architecture%20and%20Troubleshooting/#altbootbank","text":"You can load the old version with shift+r","title":"Altbootbank"},{"location":"VMWare/VxRail%20Architecture%20and%20Troubleshooting/#add-nodes-nic-config-page","text":"When you hit https:// /ui/vxrail/rest/vxm/private/system/cluster-hosts?$$objectID=urn:vmomi:ClusterComputeResource:domain &lang=en-us you should get back the existing hosts in the cluster with their hostname, model, serial_number, and vmnics https:// /ui/vxrail/rest/vxm/private/system/available-hosts?$filter=serial_number%20in%20(SERIAL,SERIAL)&$$objectId=urn:vmomi:ClusterComputeResource:domain- &lang=en-us should get you the two hosts you are going to add","title":"Add nodes NIC config page"},{"location":"VMWare/VxRail%20Architecture%20and%20Troubleshooting/#add-node-error","text":"\"Could not find NSX-T network information\" Http failure response for https:// /ui/vxrail/rest/vxm/private/cluster/network/nsxt???objectId=urn:vmomi:ClusterComputeResource:domain-ID=en-US: 404 OK","title":"Add Node Error"},{"location":"VMWare/VxRail%20Architecture%20and%20Troubleshooting/#error-node","text":"Validation Errors Disk Grouping Error (JKSLH63) Error occurs when validating disk group on host JKSLH63 web.log: ERROR [tomcat-http--48] com.emc.mystic.manager.commons.emc.webutil.LocaleUtil LocaleUtil.getBundleMessage:198 - No en_US resource bundle for ExpansionValidation with key JKSLH63","title":"Error Node"},{"location":"VMWare/VxRail%20Architecture%20and%20Troubleshooting/#general-error","text":"web.log: [WARN] com.vce.commons.config.ConfigServiceImpl$NotFoundHandler ConfigServiceImpl$NotFoundHandler.handleNotFound:114 - provided key is not present: [404, {\"message\":\"404 Not Found: bandwidth_throttling_level does not exist\"}]","title":"General Error"},{"location":"VMWare/VxRail%20Architecture%20and%20Troubleshooting/#redeploying-vxrail-manager","text":"If VxRail Manager was previously deployed but something happened to it you can rebuild it. If you have the old VxRail manager, but the cluster has changed on ESXi, you will have to do: Update /var/lib/vmware-marvin/runtime.properties with the new cluster UUID Connect to the postgresql database on the manager, go to the settings table and update it with the new settings.","title":"Redeploying VxRail Manager"},{"location":"VMWare/VxRail%20Architecture%20and%20Troubleshooting/#logs","text":"","title":"Logs"},{"location":"VMWare/VxRail%20Architecture%20and%20Troubleshooting/#generating-logs","text":"If you want to get a full dump of all the logs associated with VxRail you can generate a dump by going to the VxRail plugin on vCenter, navigating to the support tab, and then clicking the generate log button.","title":"Generating Logs"},{"location":"VMWare/VxRail%20Architecture%20and%20Troubleshooting/#vxrail-microservice-container-logs","text":"/var/log/microservice/vxrail-manager","title":"VxRail Microservice (container) Logs"},{"location":"VMWare/VxRail%20Architecture%20and%20Troubleshooting/#rasr-process","text":"The totality of the RASR process' output is stored in the images folder on the local datastore in fist.log.","title":"RASR Process"},{"location":"VMWare/VxRail%20Architecture%20and%20Troubleshooting/#tomcat-logs","text":"/var/log/marvin/tomcat/logs /var/log/mystic/web.log","title":"Tomcat Logs"},{"location":"VMWare/VxRail%20Architecture%20and%20Troubleshooting/#upgrade-logs","text":"The LCM process handles upgrades. It's logs are all in /var/log/mystic/lcm*. You can also try checking the PSC log (haven't personally done this)","title":"Upgrade Logs"},{"location":"VMWare/VxRail%20Architecture%20and%20Troubleshooting/#api-logs","text":"These are on vCenter. See VMWARE API Logs","title":"API Logs"},{"location":"VMWare/VxRail%20Architecture%20and%20Troubleshooting/#helpful-commands","text":"See Helpful Commands","title":"Helpful Commands"},{"location":"VMWare/VxRail%20Architecture%20and%20Troubleshooting/#helpful-kb-articles","text":"https://www.dell.com/support/kbdoc/en-uk/000021742/dell-emc-vxrail-troubleshooting-guide-for-vxrail-micro-services-infrastructure https://www.dell.com/support/kbdoc/en-uk/000157667/dell-emc-vxrail-how-to-query-vxrail-cluster-configuration-data-in-7-0-010-or-higher https://www.dell.com/support/kbdoc/en-us/000157715/dell-emc-vxrail-troubleshooting-guide-for-upgrading-vxrail-manager-to-7-0-010-or-higher https://www.dell.com/support/kbdoc/en-us/000157662/dell-emc-vxrail-how-to-get-or-update-management-account-in-vxrail-7-0-010-or-higher https://www.dell.com/support/kbdoc/en-us/000181759/dell-emc-vxrail-upgrading-vxrail-manager-to-7-0-x-release-failed-vcenter-plugin-no-loading https://www.dell.com/support/kbdoc/en-us/000181712/dell-emc-vxrail-how-to-update-vxrail-manager-without-lcm","title":"Helpful KB Articles"},{"location":"VMWare/VxRail%20Architecture%20and%20Troubleshooting/#vxrail-api-info","text":"See VMWare APIs","title":"VxRail API Info"},{"location":"VMWare/vRealize%20Setup%20%28Incomplete%29/","text":"Setting Up vRealize Setting Credentials I had to get on the Linux command line to set the credentials. I did the following: When you hit the grub menu, add init=/bin/bash to drop to a command line. Once you get to the command line run sudo passwd root and set that password. Run sudo passwd admin to reset the local admin account. Note: This is not the admin account you need to get to the web frontend. Run $VMWARE_PYTHON_BIN $VCOPS_BASE/../vmware-vcopssuite/utilities/sliceConfiguration/bin/vcopsSetAdminPassword.py --reset to reset the admin account for the web GUI. Note: Despite what the documentation says, the username is not admin@local, it is admin. There are also password complexity requirements.","title":"Setting Up vRealize"},{"location":"VMWare/vRealize%20Setup%20%28Incomplete%29/#setting-up-vrealize","text":"","title":"Setting Up vRealize"},{"location":"VMWare/vRealize%20Setup%20%28Incomplete%29/#setting-credentials","text":"I had to get on the Linux command line to set the credentials. I did the following: When you hit the grub menu, add init=/bin/bash to drop to a command line. Once you get to the command line run sudo passwd root and set that password. Run sudo passwd admin to reset the local admin account. Note: This is not the admin account you need to get to the web frontend. Run $VMWARE_PYTHON_BIN $VCOPS_BASE/../vmware-vcopssuite/utilities/sliceConfiguration/bin/vcopsSetAdminPassword.py --reset to reset the admin account for the web GUI. Note: Despite what the documentation says, the username is not admin@local, it is admin. There are also password complexity requirements.","title":"Setting Credentials"},{"location":"Web%20Traffic%20Generator/","text":"Web Traffic Generator See: https://github.com/grantcurell/generatewebtraffic","title":"Web Traffic Generator"},{"location":"Web%20Traffic%20Generator/#web-traffic-generator","text":"See: https://github.com/grantcurell/generatewebtraffic","title":"Web Traffic Generator"},{"location":"esrally%20%28INCOMPLETE%29/","text":"esrally (INCOMPLETE) My Environment CentOS Version Info CentOS Linux release 7.7.1908 (Core) NAME=\"CentOS Linux\" VERSION=\"7 (Core)\" ID=\"centos\" ID_LIKE=\"rhel fedora\" VERSION_ID=\"7\" PRETTY_NAME=\"CentOS Linux 7 (Core)\" ANSI_COLOR=\"0;31\" CPE_NAME=\"cpe:/o:centos:centos:7\" HOME_URL=\"https://www.centos.org/\" BUG_REPORT_URL=\"https://bugs.centos.org/\" CENTOS_MANTISBT_PROJECT=\"CentOS-7\" CENTOS_MANTISBT_PROJECT_VERSION=\"7\" REDHAT_SUPPORT_PRODUCT=\"centos\" REDHAT_SUPPORT_PRODUCT_VERSION=\"7\" CentOS Linux release 7.7.1908 (Core) CentOS Linux release 7.7.1908 (Core) Kernel Version Info [root@elk ~]# uname -a Linux hostname 3.10.0-1062.9.1.el7.x86_64 #1 SMP Fri Dec 6 15:49:49 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux Elasticsearch/Kibana Version Info I used 6.8.5 to set this up. I only used a single node for this setup. Installation Run install yum install -y python3 Download Java 12 from the Java archive . You will unfortunately have to make an account. Add the wandisco repo to install a current version of git. vim /etc/yum.repos.d/wandisco-git.repo . Add the below to the file. [wandisco-git] name=Wandisco GIT Repository baseurl=http://opensource.wandisco.com/centos/7/git/$basearch/ enabled=1 gpgcheck=1 gpgkey=http://opensource.wandisco.com/RPM-GPG-KEY-WANdisco Add their GPG keys with rpm --import http://opensource.wandisco.com/RPM-GPG-KEY-WANdisco Run yum install -y git gcc python3-devel Run pip3 install esrally to install Rally.","title":"esrally (INCOMPLETE)"},{"location":"esrally%20%28INCOMPLETE%29/#esrally-incomplete","text":"","title":"esrally (INCOMPLETE)"},{"location":"esrally%20%28INCOMPLETE%29/#my-environment","text":"","title":"My Environment"},{"location":"esrally%20%28INCOMPLETE%29/#centos-version-info","text":"CentOS Linux release 7.7.1908 (Core) NAME=\"CentOS Linux\" VERSION=\"7 (Core)\" ID=\"centos\" ID_LIKE=\"rhel fedora\" VERSION_ID=\"7\" PRETTY_NAME=\"CentOS Linux 7 (Core)\" ANSI_COLOR=\"0;31\" CPE_NAME=\"cpe:/o:centos:centos:7\" HOME_URL=\"https://www.centos.org/\" BUG_REPORT_URL=\"https://bugs.centos.org/\" CENTOS_MANTISBT_PROJECT=\"CentOS-7\" CENTOS_MANTISBT_PROJECT_VERSION=\"7\" REDHAT_SUPPORT_PRODUCT=\"centos\" REDHAT_SUPPORT_PRODUCT_VERSION=\"7\" CentOS Linux release 7.7.1908 (Core) CentOS Linux release 7.7.1908 (Core)","title":"CentOS Version Info"},{"location":"esrally%20%28INCOMPLETE%29/#kernel-version-info","text":"[root@elk ~]# uname -a Linux hostname 3.10.0-1062.9.1.el7.x86_64 #1 SMP Fri Dec 6 15:49:49 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux","title":"Kernel Version Info"},{"location":"esrally%20%28INCOMPLETE%29/#elasticsearchkibana-version-info","text":"I used 6.8.5 to set this up. I only used a single node for this setup.","title":"Elasticsearch/Kibana Version Info"},{"location":"esrally%20%28INCOMPLETE%29/#installation","text":"Run install yum install -y python3 Download Java 12 from the Java archive . You will unfortunately have to make an account. Add the wandisco repo to install a current version of git. vim /etc/yum.repos.d/wandisco-git.repo . Add the below to the file. [wandisco-git] name=Wandisco GIT Repository baseurl=http://opensource.wandisco.com/centos/7/git/$basearch/ enabled=1 gpgcheck=1 gpgkey=http://opensource.wandisco.com/RPM-GPG-KEY-WANdisco Add their GPG keys with rpm --import http://opensource.wandisco.com/RPM-GPG-KEY-WANdisco Run yum install -y git gcc python3-devel Run pip3 install esrally to install Rally.","title":"Installation"},{"location":"idrac%20with%20LDAP/","text":"idrac with LDAP Setting Up idrac with FreeIPA Version Info Setup FreeIPA Helpful Commands Setup idrac YouTube Video of Login Version Info Fedora release 33 (Thirty Three) NAME=Fedora VERSION=\"33 (Workstation Edition)\" ID=fedora VERSION_ID=33 VERSION_CODENAME=\"\" PLATFORM_ID=\"platform:f33\" PRETTY_NAME=\"Fedora 33 (Workstation Edition)\" ANSI_COLOR=\"0;38;2;60;110;180\" LOGO=fedora-logo-icon CPE_NAME=\"cpe:/o:fedoraproject:fedora:33\" HOME_URL=\"https://fedoraproject.org/\" DOCUMENTATION_URL=\"https://docs.fedoraproject.org/en-US/fedora/f33/system-administrators-guide/\" SUPPORT_URL=\"https://fedoraproject.org/wiki/Communicating_and_getting_help\" BUG_REPORT_URL=\"https://bugzilla.redhat.com/\" REDHAT_BUGZILLA_PRODUCT=\"Fedora\" REDHAT_BUGZILLA_PRODUCT_VERSION=33 REDHAT_SUPPORT_PRODUCT=\"Fedora\" REDHAT_SUPPORT_PRODUCT_VERSION=33 PRIVACY_POLICY_URL=\"https://fedoraproject.org/wiki/Legal:PrivacyPolicy\" VARIANT=\"Workstation Edition\" VARIANT_ID=workstation Fedora release 33 (Thirty Three) Fedora release 33 (Thirty Three) Setup FreeIPA Install Fedora Change hostname hostname freeipa.grant.lan && hostnamectl set-hostname freeipa.grant.lan Change in /etc/hostname Configure DNS to return for this hostname. Double check with dig +short freeipa.grant.lan A && dig +short -x 192.168.1.95 Follow RHEL's instructions I used Chapter 5 for primary installation Make sure you add the requested DNS entries at the end Run firewall-cmd --permanent --add-port={80/tcp,443/tcp,389/tcp,636/tcp,88/tcp,464/tcp,88/udp,464/udp,123/udp} && firewall-cmd --reload to allow the appropriate ports Run kinit admin - this allows you to use the command line tools otherwise they'll complain about kerberos. Log into FreeIPA server at https://<your_hostname> . In my case, Windows popped up a username and password prompt. That prompt didn't work - I had to exit it and then log into the webGUI. Add a user other than administrator. Create a new user and new group in the UI and assign the new user to the new group. WARNING I had to actually add the user to a new group. The group could not be admins or it wouldn't work. When I dumped ldapsearch -x -H ldap://localhost -b \"cn=admins,cn=groups,cn=accounts,dc=grant,dc=lan\" -D \"uid=grant,cn=users,cn=accounts,dc=grant,dc=lan\" -w PASSWORD | less my user actually didn't show in the admins group. When I created my own group and checked it worked fine. Helpful Commands To start the IPA service use ipactl start|stop|restart . You can check the status with ipactl status . Test LDAP credentials: ldapwhoami -vvv -h 192.168.1.95 -p 389 -D 'uid=grant,cn=users,cn=accounts,dc=grant,dc=lan' -x -w <PASSWORD> Dump the structure of FreeIPA: ldapsearch -x -H ldap://localhost -b \"cn=accounts,dc=grant,dc=lan\" -D \"uid=grant,cn=users,cn=accounts,dc=grant,dc=lan\" -w <YOUR_USERS_PASSWORD> | less You can also use this to check to see if a user is a member of a group by changing the base path to search (-b). Change it to what you think the FQDN of the group is. The user in question should show up. Setup idrac Go into the idrac and click the directories prompt. I ran without certs I used the following values: Generic LDAP: Enabled Use Distinguished Name to Search Group Membership: Enabled LDAP Server Address: freeipa.grant.lan LDAP Server Port: 636 Bind DN: uid=grant,cn=users,cn=accounts,dc=grant,dc=lan Bind Password: Your password Base DN to Search: cn=accounts,dc=grant,dc=lan Attribute of User Login: uid Attribute of Group Membership: member Click next and for Group DN I used cn=grantgroup,cn=groups,cn=accounts,dc=grant,dc=lan where grantgroup was the name of the user group I added I ran the test with the following results: # Test Results Ping Directory Server Not Run Directory Server DNS Name Passed LDAP connection to the Directory Server Passed User DN existence Passed Certificate Validation Disabled User Authentication Passed User Authorization Passed # Test Log 16:49:14 Initiating Directory Services Settings Diagnostics: 16:49:14 trying LDAP server freeipa.grant.lan:636 16:49:14 Server Address freeipa.grant.lan resolved to 192.168.1.95 16:49:14 connect to 192.168.1.95:636 passed 16:49:14 Connecting to ldaps://[freeipa.grant.lan]:636... 16:49:14 Test user authenticated user=uid=grant,cn=users,cn=accounts,dc=grant,dc=lan host=freeipa.grant.lan 16:49:14 Search command: Bind DN: uid=grant,cn=users,cn=accounts,dc=grant,dc=lan Scope: subtree Base DN: cn=accounts,dc=grant,dc=lan Search filter: (uid=grant) Attribute list: objectClass memberOf dn uid objectCategory defaultNamingContext namingContexts ldapServiceName supportedControl supportedExtension 16:49:14 Connecting to ldaps://[freeipa.grant.lan]:636... 16:49:14 Test user authenticated user=uid=grant,cn=users,cn=accounts,dc=grant,dc=lan host=freeipa.grant.lan 16:49:15 Connecting to ldaps://[freeipa.grant.lan]:636... 16:49:15 Test user authenticated user=uid=grant,cn=users,cn=accounts,dc=grant,dc=lan host=freeipa.grant.lan 16:49:15 Search command: Bind DN: uid=grant,cn=users,cn=accounts,dc=grant,dc=lan Scope: base Base DN: cn=grantgroup,cn=groups,cn=accounts,dc=grant,dc=lan Search filter: (member=uid=grant,cn=users,cn=accounts,dc=grant,dc=lan) Attribute list: objectClass memberOf dn uid objectCategory defaultNamingContext namingContexts ldapServiceName supportedControl supportedExtension 16:49:15 Privileges gained from role group 'cn=grantgroup,cn=groups,cn=accounts,dc=grant,dc=lan': Login Config iDRAC Config User Clear Logs Server Control Virtual Console Virtual Media Test Alerts Diagnostic Command 16:49:15 Test user grant authorized 16:49:15 Cumulative privileges gained: Login Config iDRAC Config User Clear Logs Server Control Virtual Console Virtual Media Test Alerts Diagnostic Command YouTube Video of Login See: https://youtu.be/-fHlHhF9vH4","title":"idrac with LDAP"},{"location":"idrac%20with%20LDAP/#idrac-with-ldap","text":"Setting Up idrac with FreeIPA Version Info Setup FreeIPA Helpful Commands Setup idrac YouTube Video of Login","title":"idrac with LDAP"},{"location":"idrac%20with%20LDAP/#version-info","text":"Fedora release 33 (Thirty Three) NAME=Fedora VERSION=\"33 (Workstation Edition)\" ID=fedora VERSION_ID=33 VERSION_CODENAME=\"\" PLATFORM_ID=\"platform:f33\" PRETTY_NAME=\"Fedora 33 (Workstation Edition)\" ANSI_COLOR=\"0;38;2;60;110;180\" LOGO=fedora-logo-icon CPE_NAME=\"cpe:/o:fedoraproject:fedora:33\" HOME_URL=\"https://fedoraproject.org/\" DOCUMENTATION_URL=\"https://docs.fedoraproject.org/en-US/fedora/f33/system-administrators-guide/\" SUPPORT_URL=\"https://fedoraproject.org/wiki/Communicating_and_getting_help\" BUG_REPORT_URL=\"https://bugzilla.redhat.com/\" REDHAT_BUGZILLA_PRODUCT=\"Fedora\" REDHAT_BUGZILLA_PRODUCT_VERSION=33 REDHAT_SUPPORT_PRODUCT=\"Fedora\" REDHAT_SUPPORT_PRODUCT_VERSION=33 PRIVACY_POLICY_URL=\"https://fedoraproject.org/wiki/Legal:PrivacyPolicy\" VARIANT=\"Workstation Edition\" VARIANT_ID=workstation Fedora release 33 (Thirty Three) Fedora release 33 (Thirty Three)","title":"Version Info"},{"location":"idrac%20with%20LDAP/#setup-freeipa","text":"Install Fedora Change hostname hostname freeipa.grant.lan && hostnamectl set-hostname freeipa.grant.lan Change in /etc/hostname Configure DNS to return for this hostname. Double check with dig +short freeipa.grant.lan A && dig +short -x 192.168.1.95 Follow RHEL's instructions I used Chapter 5 for primary installation Make sure you add the requested DNS entries at the end Run firewall-cmd --permanent --add-port={80/tcp,443/tcp,389/tcp,636/tcp,88/tcp,464/tcp,88/udp,464/udp,123/udp} && firewall-cmd --reload to allow the appropriate ports Run kinit admin - this allows you to use the command line tools otherwise they'll complain about kerberos. Log into FreeIPA server at https://<your_hostname> . In my case, Windows popped up a username and password prompt. That prompt didn't work - I had to exit it and then log into the webGUI. Add a user other than administrator. Create a new user and new group in the UI and assign the new user to the new group. WARNING I had to actually add the user to a new group. The group could not be admins or it wouldn't work. When I dumped ldapsearch -x -H ldap://localhost -b \"cn=admins,cn=groups,cn=accounts,dc=grant,dc=lan\" -D \"uid=grant,cn=users,cn=accounts,dc=grant,dc=lan\" -w PASSWORD | less my user actually didn't show in the admins group. When I created my own group and checked it worked fine.","title":"Setup FreeIPA"},{"location":"idrac%20with%20LDAP/#helpful-commands","text":"To start the IPA service use ipactl start|stop|restart . You can check the status with ipactl status . Test LDAP credentials: ldapwhoami -vvv -h 192.168.1.95 -p 389 -D 'uid=grant,cn=users,cn=accounts,dc=grant,dc=lan' -x -w <PASSWORD> Dump the structure of FreeIPA: ldapsearch -x -H ldap://localhost -b \"cn=accounts,dc=grant,dc=lan\" -D \"uid=grant,cn=users,cn=accounts,dc=grant,dc=lan\" -w <YOUR_USERS_PASSWORD> | less You can also use this to check to see if a user is a member of a group by changing the base path to search (-b). Change it to what you think the FQDN of the group is. The user in question should show up.","title":"Helpful Commands"},{"location":"idrac%20with%20LDAP/#setup-idrac","text":"Go into the idrac and click the directories prompt. I ran without certs I used the following values: Generic LDAP: Enabled Use Distinguished Name to Search Group Membership: Enabled LDAP Server Address: freeipa.grant.lan LDAP Server Port: 636 Bind DN: uid=grant,cn=users,cn=accounts,dc=grant,dc=lan Bind Password: Your password Base DN to Search: cn=accounts,dc=grant,dc=lan Attribute of User Login: uid Attribute of Group Membership: member Click next and for Group DN I used cn=grantgroup,cn=groups,cn=accounts,dc=grant,dc=lan where grantgroup was the name of the user group I added I ran the test with the following results: # Test Results Ping Directory Server Not Run Directory Server DNS Name Passed LDAP connection to the Directory Server Passed User DN existence Passed Certificate Validation Disabled User Authentication Passed User Authorization Passed # Test Log 16:49:14 Initiating Directory Services Settings Diagnostics: 16:49:14 trying LDAP server freeipa.grant.lan:636 16:49:14 Server Address freeipa.grant.lan resolved to 192.168.1.95 16:49:14 connect to 192.168.1.95:636 passed 16:49:14 Connecting to ldaps://[freeipa.grant.lan]:636... 16:49:14 Test user authenticated user=uid=grant,cn=users,cn=accounts,dc=grant,dc=lan host=freeipa.grant.lan 16:49:14 Search command: Bind DN: uid=grant,cn=users,cn=accounts,dc=grant,dc=lan Scope: subtree Base DN: cn=accounts,dc=grant,dc=lan Search filter: (uid=grant) Attribute list: objectClass memberOf dn uid objectCategory defaultNamingContext namingContexts ldapServiceName supportedControl supportedExtension 16:49:14 Connecting to ldaps://[freeipa.grant.lan]:636... 16:49:14 Test user authenticated user=uid=grant,cn=users,cn=accounts,dc=grant,dc=lan host=freeipa.grant.lan 16:49:15 Connecting to ldaps://[freeipa.grant.lan]:636... 16:49:15 Test user authenticated user=uid=grant,cn=users,cn=accounts,dc=grant,dc=lan host=freeipa.grant.lan 16:49:15 Search command: Bind DN: uid=grant,cn=users,cn=accounts,dc=grant,dc=lan Scope: base Base DN: cn=grantgroup,cn=groups,cn=accounts,dc=grant,dc=lan Search filter: (member=uid=grant,cn=users,cn=accounts,dc=grant,dc=lan) Attribute list: objectClass memberOf dn uid objectCategory defaultNamingContext namingContexts ldapServiceName supportedControl supportedExtension 16:49:15 Privileges gained from role group 'cn=grantgroup,cn=groups,cn=accounts,dc=grant,dc=lan': Login Config iDRAC Config User Clear Logs Server Control Virtual Console Virtual Media Test Alerts Diagnostic Command 16:49:15 Test user grant authorized 16:49:15 Cumulative privileges gained: Login Config iDRAC Config User Clear Logs Server Control Virtual Console Virtual Media Test Alerts Diagnostic Command","title":"Setup idrac"},{"location":"idrac%20with%20LDAP/#youtube-video-of-login","text":"See: https://youtu.be/-fHlHhF9vH4","title":"YouTube Video of Login"},{"location":"idrac%20with%20LDAP/open-LDAP-failed/","text":"Configuring idrac with OpenLDAP Configuring idrac with OpenLDAP Setup OpenLDAP Debugging Configure idrac Stopped Setup OpenLDAP I used osixia's openldap container for testing. I used osixia's phpLDAPadmin container for administration. Add an entry to your DNS server for ldap.granttest.lan podman run -p 389:389 -p 636:636 --name my-openldap-container --env LDAP_TLS=false --env LDAP_LOG_LEVEL=8 --env LDAP_ORGANISATION=\"Grant Test\" --env LDAP_DOMAIN=\"granttest.lan\" --env LDAP_ADMIN_PASSWORD=\"admin\" --detach osixia/openldap:1.5.0 --loglevel debug && podman run -p 6443:443 --env PHPLDAPADMIN_LDAP_HOSTS=ldap.granttest.lan --detach osixia/phpldapadmin:0.9.0 Test the container: podman exec my-openldap-container ldapsearch -x -H ldap://localhost -b dc=granttest,dc=lan -D \"cn=admin,dc=granttest,dc=lan\" -w admin . That should output: # extended LDIF # # LDAPv3 # base <dc=example,dc=org> with scope subtree # filter: (objectclass=*) # requesting: ALL # [...] # numResponses: 3 # numEntries: 2 Configure firewall: firewall-cmd --add-port=389/tcp --permanent && firewall-cmd --add-port=636/tcp --permanent && firewall-cmd --add-port=6443/tcp --permanent && firewall-cmd --reload Make sure you can log into https://<YOUR_IP_ADDRESS>:6443 with username cn=admin,dc=granttest,dc=lan and password admin Debugging You can use podman inspect <container_name> and then search for Log to find the location of the logs. Configure idrac I disabled certificates (yes I was lazy) Stopped For whatever reason the container networking never quite cooperated. Even though DNS entries were present for the external IP when either OME or idrac would try to hit it they would both say the LDAP server was unavailable. phpldapadmin worked without issue. I know it's a probably with the networking but decided it wasn't worth diving into so I abandonded this approach and went with a baremetal FreeIPA instance.","title":"Configuring idrac with OpenLDAP"},{"location":"idrac%20with%20LDAP/open-LDAP-failed/#configuring-idrac-with-openldap","text":"Configuring idrac with OpenLDAP Setup OpenLDAP Debugging Configure idrac Stopped","title":"Configuring idrac with OpenLDAP"},{"location":"idrac%20with%20LDAP/open-LDAP-failed/#setup-openldap","text":"I used osixia's openldap container for testing. I used osixia's phpLDAPadmin container for administration. Add an entry to your DNS server for ldap.granttest.lan podman run -p 389:389 -p 636:636 --name my-openldap-container --env LDAP_TLS=false --env LDAP_LOG_LEVEL=8 --env LDAP_ORGANISATION=\"Grant Test\" --env LDAP_DOMAIN=\"granttest.lan\" --env LDAP_ADMIN_PASSWORD=\"admin\" --detach osixia/openldap:1.5.0 --loglevel debug && podman run -p 6443:443 --env PHPLDAPADMIN_LDAP_HOSTS=ldap.granttest.lan --detach osixia/phpldapadmin:0.9.0 Test the container: podman exec my-openldap-container ldapsearch -x -H ldap://localhost -b dc=granttest,dc=lan -D \"cn=admin,dc=granttest,dc=lan\" -w admin . That should output: # extended LDIF # # LDAPv3 # base <dc=example,dc=org> with scope subtree # filter: (objectclass=*) # requesting: ALL # [...] # numResponses: 3 # numEntries: 2 Configure firewall: firewall-cmd --add-port=389/tcp --permanent && firewall-cmd --add-port=636/tcp --permanent && firewall-cmd --add-port=6443/tcp --permanent && firewall-cmd --reload Make sure you can log into https://<YOUR_IP_ADDRESS>:6443 with username cn=admin,dc=granttest,dc=lan and password admin","title":"Setup OpenLDAP"},{"location":"idrac%20with%20LDAP/open-LDAP-failed/#debugging","text":"You can use podman inspect <container_name> and then search for Log to find the location of the logs.","title":"Debugging"},{"location":"idrac%20with%20LDAP/open-LDAP-failed/#configure-idrac","text":"I disabled certificates (yes I was lazy)","title":"Configure idrac"},{"location":"idrac%20with%20LDAP/open-LDAP-failed/#stopped","text":"For whatever reason the container networking never quite cooperated. Even though DNS entries were present for the external IP when either OME or idrac would try to hit it they would both say the LDAP server was unavailable. phpldapadmin worked without issue. I know it's a probably with the networking but decided it wasn't worth diving into so I abandonded this approach and went with a baremetal FreeIPA instance.","title":"Stopped"}]}